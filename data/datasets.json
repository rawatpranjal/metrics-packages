[
  {
    "name": "JD.com 2020 (MSOM-20)",
    "description": "2.5M customers (457k purchasers) and 31,868 SKUs from JD.com",
    "category": "E-Commerce",
    "url": "https://huggingface.co/datasets/a6687543/MSOM_Data_Driven_Challenge_2020",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "customers",
      "SKUs",
      "INFORMS",
      "operations"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The JD.com 2020 dataset contains information on 2.5 million customers, including 457,000 purchasers, and 31,868 SKUs from JD.com. This dataset can be utilized to analyze consumer purchasing behavior, SKU performance, and operational efficiencies within the e-commerce sector.",
    "use_cases": [
      "Analyzing customer purchasing patterns",
      "Evaluating SKU performance",
      "Studying operational efficiencies in e-commerce",
      "Conducting market segmentation analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the JD.com 2020 dataset?",
      "How many customers are included in the JD.com dataset?",
      "What types of SKUs are available in the JD.com dataset?",
      "What insights can be derived from the JD.com customer data?",
      "How can I analyze purchasing behavior using the JD.com dataset?",
      "What are the key variables in the JD.com 2020 dataset?",
      "How does JD.com customer data support e-commerce research?",
      "What are the limitations of the JD.com 2020 dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2020",
    "size_category": "medium",
    "model_score": 0.0305,
    "image_url": "/images/datasets/jdcom-2020-msom-20.png",
    "embedding_text": "The JD.com 2020 dataset is a comprehensive collection of data that encompasses 2.5 million customers, including 457,000 purchasers, and features 31,868 stock-keeping units (SKUs) from the prominent e-commerce platform JD.com. This dataset is structured in a tabular format, with rows representing individual customer transactions and columns detailing various attributes such as customer demographics, purchase history, SKU characteristics, and transaction details. The data is instrumental for researchers and analysts interested in exploring consumer behavior, operational efficiencies, and market dynamics within the e-commerce landscape.\n\nThe collection methodology for the JD.com dataset involves aggregating transactional data directly from the JD.com platform, ensuring that the information is both relevant and up-to-date. The dataset captures a wide range of variables, including customer IDs, SKU IDs, purchase dates, quantities, prices, and other relevant metrics that provide insights into consumer purchasing patterns. Key variables such as customer demographics (age, gender, location) and SKU attributes (category, price, availability) allow for a nuanced analysis of how different factors influence purchasing decisions.\n\nWhile the dataset is rich in information, it is essential to acknowledge potential limitations regarding data quality. Issues such as missing values, outliers, or inconsistencies in SKU categorization may arise, necessitating common preprocessing steps such as data cleaning, normalization, and transformation before analysis. Researchers often employ various analytical techniques, including regression analysis, machine learning models, and descriptive statistics, to extract meaningful insights from the data.\n\nThe JD.com dataset supports a myriad of research questions, such as identifying trends in consumer purchasing behavior, evaluating the effectiveness of marketing strategies, and understanding the dynamics of SKU performance over time. Analysts can leverage this dataset to conduct market segmentation studies, assess customer loyalty, and explore the impact of pricing strategies on sales performance. Overall, the JD.com 2020 dataset serves as a valuable resource for both academic and industry researchers aiming to deepen their understanding of the e-commerce sector and consumer behavior."
  },
  {
    "name": "Retail Rocket",
    "description": "2.76M events (views, carts, purchases) from 1.4M visitors",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "user events",
      "conversions",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Retail Rocket dataset consists of 2.76 million events, including views, carts, and purchases, recorded from 1.4 million unique visitors. This dataset allows researchers and analysts to explore user behavior in e-commerce settings, analyze conversion rates, and develop insights into consumer purchasing patterns.",
    "use_cases": [
      "Analyzing user conversion rates from views to purchases",
      "Exploring patterns in shopping cart abandonment",
      "Investigating the impact of user events on sales",
      "Segmenting visitors based on their interaction with the platform"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Retail Rocket dataset?",
      "How can I analyze user events in e-commerce?",
      "What insights can be gained from 2.76M retail events?",
      "What are the conversion rates in the Retail Rocket dataset?",
      "How to use the Retail Rocket dataset for consumer behavior analysis?",
      "What types of analyses can be performed on e-commerce data?",
      "Where can I find datasets related to user events and conversions?",
      "What are the common tags associated with the Retail Rocket dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0299,
    "image_url": "/images/datasets/retail-rocket.png",
    "embedding_text": "The Retail Rocket dataset is a comprehensive collection of user interaction events within an e-commerce environment, encompassing a total of 2.76 million events generated by 1.4 million unique visitors. This dataset is structured in a tabular format, where each row represents an event, and the columns include variables such as event type (view, cart, purchase), user ID, timestamp, and potentially other relevant metrics that capture user engagement. The dataset is particularly valuable for researchers and data scientists interested in understanding consumer behavior, as it provides a rich source of information on how users navigate through an online retail platform, what products they are interested in, and how these interests translate into actual purchases.\n\nThe data collection methodology likely involves tracking user interactions on the retail website through event logging systems that capture each action taken by the user. This could include page views, items added to shopping carts, and completed purchases. The data may be sourced from web analytics tools or custom tracking solutions implemented by the e-commerce platform. Given the scale of the dataset, it is suitable for various types of analyses, including regression analysis, machine learning models, and descriptive statistics, allowing researchers to derive insights into user behavior and optimize marketing strategies.\n\nKey variables within the dataset measure critical aspects of user engagement, such as the frequency of visits, the number of items added to carts, and the conversion rates from views to purchases. These variables enable analysts to identify trends and patterns in consumer behavior, assess the effectiveness of marketing campaigns, and inform product placement strategies. However, researchers should be aware of potential data quality issues, such as missing values or inconsistencies in event logging, which may require preprocessing steps like data cleaning and normalization before analysis.\n\nCommon preprocessing steps might include filtering out irrelevant events, handling missing data, and aggregating events over specific time frames to facilitate analysis. Researchers often use this dataset to address various research questions, such as understanding the factors that influence purchase decisions, identifying segments of high-value customers, or evaluating the impact of promotional activities on sales performance. The insights gained from such analyses can help e-commerce businesses enhance user experience, improve conversion rates, and ultimately drive revenue growth. Overall, the Retail Rocket dataset serves as a powerful tool for anyone looking to delve into the dynamics of online retail and consumer behavior."
  },
  {
    "name": "BestBuy",
    "description": "Mobile website clicks (~42k) for Xbox games from BestBuy",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/competitions/acm-sf-chapter-hackathon-big",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "clicks",
      "mobile",
      "gaming",
      "hackathon"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "gaming"
    ],
    "summary": "The BestBuy dataset contains approximately 42,000 mobile website clicks specifically for Xbox games. This dataset can be utilized to analyze consumer behavior in the gaming sector, particularly how mobile interactions influence purchasing decisions.",
    "use_cases": [
      "Analyzing consumer engagement with Xbox games on mobile platforms",
      "Investigating the impact of mobile clicks on sales conversions",
      "Studying user behavior patterns in the gaming e-commerce sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the mobile website click patterns for Xbox games at BestBuy?",
      "How do mobile clicks correlate with sales for Xbox games?",
      "What demographic factors influence mobile clicks on gaming products?",
      "What time of day do users most frequently click on Xbox games?",
      "How does the click-through rate for Xbox games compare to other gaming products?",
      "What trends can be identified in mobile clicks for Xbox games over time?",
      "How do promotional events affect mobile clicks for Xbox games?",
      "What are the most popular Xbox games based on mobile clicks?"
    ],
    "domain_tags": [
      "retail",
      "gaming"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0232,
    "image_url": "/images/datasets/bestbuy.png",
    "embedding_text": "The BestBuy dataset is a rich collection of mobile website clicks, specifically focusing on Xbox games, with a total of approximately 42,000 recorded interactions. This dataset is structured in a tabular format, where each row represents an individual click event, and the columns capture various attributes related to the click, such as timestamp, game title, user device type, and potentially other engagement metrics. The primary variables of interest include the number of clicks per game, the time of day the clicks occurred, and the device used for the interaction. These variables are crucial for understanding user engagement and behavior patterns in the mobile gaming market.\n\nThe data collection methodology likely involves tracking user interactions on the BestBuy mobile website through web analytics tools that log click events. This process typically captures user clicks in real-time, allowing for a comprehensive view of consumer behavior as they navigate through the mobile site. However, it is important to note that the dataset may have limitations regarding data quality, such as potential duplicates, missing values, or biases introduced by the tracking methodology. Researchers should be aware of these factors when conducting analyses.\n\nIn terms of coverage, the dataset does not explicitly mention temporal or geographic dimensions, but it provides a snapshot of user interactions over a specific period. The absence of demographic data means that researchers may need to combine this dataset with other sources to gain insights into the characteristics of the user base engaging with Xbox games on mobile platforms.\n\nCommon preprocessing steps for this dataset may include cleaning the data to remove duplicates, handling missing values, and possibly aggregating clicks by game or time period to facilitate analysis. Researchers can use this dataset to address various research questions, such as identifying trends in mobile clicks for Xbox games, understanding the impact of promotional campaigns on user engagement, and exploring the relationship between mobile clicks and sales conversions.\n\nThe types of analyses supported by this dataset range from descriptive statistics to more advanced techniques such as regression analysis and machine learning models. For instance, researchers might employ regression analysis to determine the factors that significantly influence the number of clicks per game or use machine learning algorithms to predict future click patterns based on historical data. Overall, the BestBuy dataset serves as a valuable resource for researchers and analysts interested in the intersection of e-commerce and gaming, providing insights into consumer behavior and engagement in the mobile domain."
  },
  {
    "name": "LMSYS-Chat-1M",
    "description": "1M real-world conversations with 25 state-of-the-art LLMs spanning 154 languages",
    "category": "AI & LLM",
    "url": "https://huggingface.co/datasets/lmsys/lmsys-chat-1m",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "LLM",
      "conversations",
      "multilingual",
      "chatbot"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The LMSYS-Chat-1M dataset comprises 1 million real-world conversations generated by 25 state-of-the-art large language models (LLMs) across 154 languages. This dataset is ideal for researchers and developers interested in analyzing conversational AI, multilingual interactions, and chatbot performance.",
    "use_cases": [
      "Analyzing the performance of different LLMs in conversational settings.",
      "Studying multilingual chatbot interactions and their effectiveness.",
      "Exploring user engagement and response patterns in AI-driven conversations."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LMSYS-Chat-1M dataset?",
      "How can I access the LMSYS-Chat-1M dataset?",
      "What types of conversations are included in the LMSYS-Chat-1M dataset?",
      "What languages are represented in the LMSYS-Chat-1M dataset?",
      "How many conversations are in the LMSYS-Chat-1M dataset?",
      "What are the applications of the LMSYS-Chat-1M dataset?",
      "What state-of-the-art LLMs are included in the LMSYS-Chat-1M dataset?",
      "What research can be conducted using the LMSYS-Chat-1M dataset?"
    ],
    "domain_tags": [
      "AI",
      "chatbot",
      "multilingual"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.021,
    "image_url": "/images/datasets/lmsys-chat-1m.png",
    "embedding_text": "The LMSYS-Chat-1M dataset is a comprehensive collection of 1 million real-world conversations generated by 25 advanced large language models (LLMs), covering an impressive range of 154 languages. This dataset is structured primarily as text, with each entry representing a unique conversation between users and chatbots, allowing for extensive analysis of conversational dynamics in various linguistic contexts. The conversations are designed to reflect real-world interactions, providing a rich resource for researchers and developers in the field of artificial intelligence and natural language processing. The collection methodology involves generating conversations through state-of-the-art LLMs, which are trained on diverse datasets to ensure a broad understanding of language and context. While the specific data sources for the training of these LLMs are not disclosed, the dataset aims to encapsulate a wide array of conversational scenarios, making it a valuable tool for examining the capabilities and limitations of AI in understanding and generating human-like dialogue. The key variables in the dataset include the conversation ID, the model used, the language of the conversation, and the text of the dialogue itself. These variables allow researchers to measure various aspects of conversational AI, such as response accuracy, engagement levels, and language proficiency. However, as with any dataset, there are limitations to consider. The quality of the conversations may vary based on the underlying models and their training data, which can introduce biases or inaccuracies in certain contexts. Common preprocessing steps for utilizing this dataset may include text normalization, tokenization, and filtering out any irrelevant or low-quality conversations. Researchers can leverage the LMSYS-Chat-1M dataset to address a variety of research questions, such as evaluating the effectiveness of different LLMs in generating coherent and contextually appropriate responses, analyzing user engagement metrics across languages, and exploring the nuances of multilingual interactions in conversational settings. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and industry research. Ultimately, the LMSYS-Chat-1M dataset serves as a foundational tool for advancing the understanding of conversational AI and its applications in real-world scenarios."
  },
  {
    "name": "Open E-Commerce 1.0 (MIT)",
    "description": "1.8M Amazon purchases with demographics (age, gender, location). Real household e-commerce behavior at scale",
    "category": "E-Commerce",
    "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YGAVK9",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Amazon",
      "demographics",
      "purchases",
      "households",
      "MIT"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Open E-Commerce 1.0 dataset contains 1.8 million Amazon purchases along with demographic information such as age, gender, and location. This dataset provides insights into real household e-commerce behavior at scale, allowing researchers to analyze consumer purchasing patterns and demographic influences on buying decisions.",
    "use_cases": [
      "Analyzing the impact of demographics on purchasing decisions",
      "Identifying trends in e-commerce behavior over time",
      "Examining the relationship between consumer demographics and product categories",
      "Evaluating marketing strategies based on consumer behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the demographics of Amazon purchases?",
      "How does age influence e-commerce behavior?",
      "What purchasing trends can be identified from Amazon data?",
      "What is the impact of gender on online shopping habits?",
      "How can demographic data enhance e-commerce strategies?",
      "What insights can be drawn from household purchasing behavior?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0188,
    "image_url": "/images/logos/harvard.png",
    "embedding_text": "The Open E-Commerce 1.0 dataset is a comprehensive collection of 1.8 million Amazon purchases, enriched with demographic data including age, gender, and location. This dataset is structured in a tabular format, where each row represents an individual purchase, and the columns include variables such as purchase ID, product category, purchase amount, demographic attributes, and timestamps of transactions. The data collection methodology involves aggregating purchase records from Amazon, ensuring a diverse representation of consumer behavior across various demographics. However, while the dataset provides a wealth of information, researchers should be aware of potential limitations regarding data quality, such as missing values or biases in demographic representation. Common preprocessing steps may include data cleaning to handle missing values, normalization of demographic variables, and encoding categorical data for analysis. Researchers can leverage this dataset to address various research questions, such as how demographic factors influence purchasing decisions, the identification of trends in consumer behavior over time, and the evaluation of marketing strategies based on consumer insights. The dataset supports a range of analyses, including regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize purchasing patterns. Overall, the Open E-Commerce 1.0 dataset serves as a valuable resource for researchers and practitioners interested in understanding the dynamics of e-commerce and consumer behavior."
  },
  {
    "name": "DiQAD",
    "description": "100K real-world user dialogues with comprehensive 6-dimension quality assessment",
    "category": "AI & LLM",
    "url": "https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation",
    "docs_url": null,
    "github_url": "https://github.com/yukunZhao/Dataset_Dialogue_quality_evaluation",
    "tags": [
      "dialogue",
      "quality assessment",
      "NLP"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "NLP",
      "dialogue systems",
      "quality assessment"
    ],
    "summary": "The DiQAD dataset consists of 100,000 real-world user dialogues that have been assessed across six dimensions of quality. This dataset can be utilized for training and evaluating natural language processing models, particularly in dialogue systems, to enhance the quality of interactions and responses.",
    "use_cases": [
      "Evaluating dialogue systems for quality improvement",
      "Training NLP models for better user interaction",
      "Conducting research on user dialogue patterns",
      "Analyzing the effectiveness of conversational agents"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the DiQAD dataset?",
      "How can I use the DiQAD dataset for NLP?",
      "What are the quality dimensions in the DiQAD dataset?",
      "Where can I find user dialogue datasets?",
      "What are the applications of dialogue quality assessment?",
      "How large is the DiQAD dataset?",
      "What kind of analyses can be performed with the DiQAD dataset?",
      "What is the significance of quality assessment in dialogue systems?"
    ],
    "domain_tags": [
      "AI",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0168,
    "image_url": "/images/datasets/diqad.png",
    "embedding_text": "The DiQAD dataset is a comprehensive collection of 100,000 real-world user dialogues, meticulously curated to provide insights into the quality of conversational interactions. Each dialogue in the dataset is assessed across six distinct dimensions of quality, which allows researchers and developers to evaluate and enhance the performance of natural language processing (NLP) models, particularly those focused on dialogue systems. The dataset is structured in a tabular format, where each row represents a unique dialogue instance, and the columns correspond to various attributes such as user inputs, system responses, and quality assessment scores across the six dimensions. These dimensions may include aspects such as coherence, relevance, informativeness, and user satisfaction, among others. The collection methodology for the DiQAD dataset involves gathering dialogues from diverse sources, ensuring a wide range of conversational contexts and user interactions. This diversity is crucial for training models that can generalize well across different scenarios and user behaviors. However, it is important to note that while the dataset is extensive, researchers should be aware of potential limitations in data quality, such as biases that may arise from the sources of dialogues or the subjective nature of quality assessments. Common preprocessing steps for utilizing the DiQAD dataset may include text normalization, tokenization, and the removal of noise or irrelevant information to prepare the dialogues for analysis. Researchers can leverage this dataset to address various research questions related to dialogue quality, such as identifying factors that contribute to effective user interactions or evaluating the performance of different dialogue management strategies. The dataset supports a range of analytical approaches, including regression analysis, machine learning modeling, and descriptive statistics, making it a versatile resource for both academic and industry applications. By utilizing the DiQAD dataset, researchers can gain valuable insights into user dialogue patterns, improve the design of conversational agents, and ultimately enhance the overall quality of human-computer interactions."
  },
  {
    "name": "Instacart",
    "description": "3.4M orders, 206k+ users, 49k+ products with reorder behavior",
    "category": "Food & Delivery",
    "url": "https://www.kaggle.com/datasets/psparks/instacart-market-basket-analysis",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "orders",
      "reorder prediction"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Instacart dataset consists of 3.4 million orders placed by over 206,000 users, featuring more than 49,000 products. This dataset is ideal for analyzing reorder behavior and can be utilized for predictive modeling in grocery shopping patterns.",
    "use_cases": [
      "Analyzing reorder patterns among users",
      "Predicting future orders based on past behavior",
      "Identifying popular products for targeted marketing",
      "Studying consumer behavior in online grocery shopping"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the reorder behavior of Instacart users?",
      "How many orders are placed by users in the Instacart dataset?",
      "What products are most commonly reordered on Instacart?",
      "What patterns can be identified in grocery shopping behavior?",
      "How does user behavior change over time in grocery orders?",
      "What insights can be derived from the Instacart dataset for grocery delivery services?",
      "How can machine learning be applied to predict reorder behavior?",
      "What are the demographics of users in the Instacart dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0154,
    "image_url": "/images/datasets/instacart.png",
    "embedding_text": "The Instacart dataset is a comprehensive collection of grocery order data that includes 3.4 million orders from over 206,000 users, encompassing more than 49,000 products. This dataset is structured in a tabular format, with rows representing individual orders and columns capturing various attributes such as user ID, product ID, order timestamp, and reorder behavior. The data is particularly valuable for researchers and data scientists interested in understanding consumer behavior in the online grocery sector. The collection methodology likely involves aggregating transaction data from the Instacart platform, which provides insights into how users interact with the service, including their purchasing habits and preferences. The dataset can be used to address a variety of research questions, such as identifying which products are frequently reordered, analyzing user demographics, and predicting future purchasing behavior based on historical data. Key variables in the dataset include user IDs, product IDs, order dates, and reorder flags, which measure the frequency of product reorders. However, potential limitations may include biases in user behavior, as the dataset only represents users of the Instacart platform and may not capture broader grocery shopping trends. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing product IDs, and encoding categorical variables for analysis. Researchers typically employ this dataset for various types of analyses, including regression modeling, machine learning applications, and descriptive statistics to uncover trends and patterns in grocery shopping behavior. Overall, the Instacart dataset serves as a rich resource for exploring the dynamics of online grocery shopping and can inform strategies for improving customer engagement and optimizing inventory management."
  },
  {
    "name": "Athey's Course Datasets",
    "description": "Datasets related to causal inference and experimental design from Susan Athey",
    "category": "Education",
    "url": "https://github.com/itamarcaspi/experimentdatar",
    "docs_url": null,
    "github_url": "https://github.com/itamarcaspi/experimentdatar",
    "tags": [
      "causal inference",
      "experiments",
      "research"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "causal inference",
      "experimental design",
      "research"
    ],
    "summary": "Athey's Course Datasets provide valuable resources for understanding causal inference and experimental design. These datasets are particularly useful for researchers and students looking to explore the intricacies of experimental methodologies and their applications in various fields.",
    "use_cases": [
      "Analyzing the impact of interventions in experimental design",
      "Conducting causal inference studies using provided datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are Athey's Course Datasets?",
      "How can I use Athey's datasets for causal inference?",
      "What types of experiments are included in Athey's datasets?",
      "Where can I find datasets related to experimental design?",
      "What research questions can be addressed using Athey's datasets?",
      "How do I analyze Athey's Course Datasets?",
      "What is the structure of Athey's datasets?",
      "What are the prerequisites for using Athey's Course Datasets?"
    ],
    "domain_tags": [
      "education",
      "research"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0154,
    "image_url": "/images/datasets/atheys-course-datasets.png",
    "embedding_text": "Athey's Course Datasets are a collection of datasets specifically designed to facilitate the understanding of causal inference and experimental design, primarily sourced from the work of Susan Athey, a prominent figure in the field of economics and data science. These datasets are structured in a tabular format, featuring rows that represent individual observations and columns that capture various variables pertinent to the experiments conducted. The key variables typically include treatment assignments, outcome measures, and covariates that allow for the analysis of causal relationships. The datasets are collected through rigorous experimental methodologies, ensuring that they reflect the complexities of real-world scenarios while adhering to the principles of randomized controlled trials. Researchers and students can leverage these datasets to explore a wide range of research questions, such as the effectiveness of different interventions, the impact of policy changes, and the dynamics of consumer behavior in response to various stimuli. Common preprocessing steps may include data cleaning, normalization, and the handling of missing values, which are essential for ensuring the integrity of the analyses performed. The datasets support various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making them versatile tools for both academic and practical applications. However, users should be aware of potential limitations, such as the specific contexts in which the data was collected, which may affect the generalizability of findings. Overall, Athey's Course Datasets serve as a foundational resource for those looking to deepen their understanding of causal inference and experimental design, providing a rich basis for empirical research and analysis."
  },
  {
    "name": "Amazon Sessions (KDD Cup 23)",
    "description": "Sessions from 6 locales with 40k-500k products per locale",
    "category": "E-Commerce",
    "url": "https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Amazon",
      "sessions",
      "multilingual",
      "KDD"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Amazon Sessions dataset from KDD Cup 23 contains session data from six different locales, encompassing a range of products between 40,000 to 500,000 per locale. This dataset can be utilized for analyzing consumer behavior, product interactions, and multilingual e-commerce dynamics.",
    "use_cases": [
      "Analyzing consumer behavior across different locales",
      "Studying product interaction patterns",
      "Exploring multilingual e-commerce dynamics",
      "Identifying trends in session data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the session patterns in Amazon across different locales?",
      "How do product interactions vary in Amazon sessions?",
      "What insights can be drawn from multilingual Amazon session data?",
      "How can consumer behavior be analyzed using Amazon session data?",
      "What is the product diversity in Amazon sessions?",
      "How do session lengths differ across locales?",
      "What trends can be identified in Amazon sessions over time?",
      "How can regression analysis be applied to Amazon session data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.011,
    "image_url": "/images/datasets/amazon-sessions-kdd-cup-23.jpg",
    "embedding_text": "The Amazon Sessions dataset, part of the KDD Cup 23, is a comprehensive collection of session data derived from Amazon's e-commerce platform, featuring a diverse array of products across six distinct locales. Each locale presents a unique set of products, ranging from 40,000 to 500,000, allowing for a rich exploration of consumer interactions and behaviors in a multilingual context. The dataset is structured in a tabular format, consisting of rows representing individual sessions and columns capturing various attributes such as session ID, product ID, locale, timestamp, and user interactions. This structure facilitates straightforward analysis and manipulation using data analysis tools like pandas. The collection methodology for this dataset likely involved tracking user sessions on the Amazon platform, capturing interactions as users navigate through product listings, add items to their carts, and make purchases. While specific details on data collection are not provided, it can be inferred that the dataset encompasses a wide range of user demographics, reflecting the global reach of Amazon. Key variables within the dataset include session ID, which uniquely identifies each session; product ID, linking to specific products; and various interaction metrics that measure user engagement, such as time spent on each product page and the sequence of interactions. These variables are crucial for understanding consumer behavior and product performance. However, researchers should be aware of potential limitations in data quality, such as incomplete session data or biases in user interactions that may not represent the entire population. Common preprocessing steps may include cleaning the data to handle missing values, normalizing interaction metrics, and encoding categorical variables for analysis. The dataset supports a variety of analytical approaches, including regression analysis to identify factors influencing purchase decisions, machine learning techniques for predictive modeling, and descriptive statistics to summarize user behavior patterns. Researchers typically utilize this dataset to address questions related to consumer behavior, product popularity, and the effectiveness of marketing strategies across different locales. By leveraging the insights gained from this dataset, businesses can enhance their e-commerce strategies, optimize product offerings, and improve user experience on their platforms."
  },
  {
    "name": "BLP US Car Data",
    "description": "Classic dataset (1971-1990) for demand model estimation",
    "category": "Automotive",
    "url": "https://pyblp.readthedocs.io/en/stable/_notebooks/tutorial/blp.html",
    "docs_url": "https://pyblp.readthedocs.io/en/stable/_notebooks/tutorial/blp.html",
    "github_url": null,
    "tags": [
      "demand estimation",
      "BLP",
      "research",
      "classic"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "automotive",
      "demand estimation",
      "econometrics"
    ],
    "summary": "The BLP US Car Data is a classic dataset spanning from 1971 to 1990, specifically designed for demand model estimation in the automotive sector. Researchers can utilize this dataset to analyze consumer preferences and pricing strategies within the car market during this period.",
    "use_cases": [
      "Estimating demand for different car models based on consumer preferences.",
      "Analyzing the impact of pricing strategies on car sales.",
      "Evaluating the effects of demographic changes on automotive demand.",
      "Conducting regression analysis to understand market trends."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the BLP US Car Data?",
      "How can I use the BLP US Car Data for demand estimation?",
      "What variables are included in the BLP US Car Data?",
      "Where can I find the BLP US Car Data for research?",
      "What time period does the BLP US Car Data cover?",
      "What are common analyses performed using the BLP US Car Data?",
      "How does the BLP US Car Data help in understanding consumer behavior?",
      "What are the limitations of the BLP US Car Data?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1971-1990",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/readthedocs.png",
    "embedding_text": "The BLP US Car Data is a well-known dataset that spans the years 1971 to 1990, primarily utilized for demand model estimation in the automotive industry. This dataset is structured in a tabular format, comprising various rows and columns that represent different car models and their attributes, such as price, engine size, fuel efficiency, and other relevant characteristics. The data collection methodology involved gathering information from multiple sources, including market reports, consumer surveys, and sales data, ensuring a comprehensive representation of the automotive market during this time frame. The dataset is particularly valuable for researchers interested in understanding consumer behavior and preferences in the context of car purchases. Key variables within the dataset measure aspects such as demand elasticity, price sensitivity, and consumer demographics, which are crucial for conducting robust econometric analyses. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in consumer reporting, which may affect the results of their analyses. Common preprocessing steps include cleaning the data to handle missing values, normalizing variables for comparative analysis, and transforming categorical variables into numerical formats suitable for regression models. The BLP US Car Data supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing researchers to explore a wide range of research questions. Typical research questions addressed using this dataset include the impact of pricing on demand, the influence of demographic factors on consumer preferences, and the overall trends in the automotive market during the late 20th century. Researchers often leverage this dataset to build econometric models that can predict future demand based on historical trends, making it a crucial resource for both academic and industry studies in the automotive sector.",
    "geographic_scope": "US",
    "benchmark_usage": [
      "Demand estimation in automotive research"
    ]
  },
  {
    "name": "Uber Movement",
    "description": "Zone-to-zone travel times and street speeds for 50+ cities worldwide. Congestion patterns from actual Uber rides",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/ishandutta/uber-travel-movement-data-2-billion-trips",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Uber",
      "travel times",
      "congestion",
      "cities",
      "transportation"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban planning",
      "data analysis"
    ],
    "summary": "The Uber Movement dataset provides valuable insights into zone-to-zone travel times and street speeds across more than 50 cities worldwide. By analyzing congestion patterns derived from actual Uber rides, researchers and urban planners can better understand traffic dynamics and improve transportation systems.",
    "use_cases": [
      "Analyzing peak traffic times in urban areas",
      "Comparing travel times across different cities",
      "Studying the impact of road construction on travel speeds",
      "Evaluating the effectiveness of public transportation alternatives"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the travel times between different zones in major cities?",
      "How does Uber data reflect congestion patterns in urban areas?",
      "What insights can be gained from analyzing street speeds in cities?",
      "How can Uber Movement data be used to improve urban transportation planning?",
      "What are the average travel times for Uber rides in various cities?",
      "How does congestion vary across different times of the day in urban areas?",
      "What factors influence travel times in cities based on Uber data?",
      "How can this dataset help in understanding urban mobility trends?"
    ],
    "domain_tags": [
      "transportation",
      "urban planning",
      "mobility"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/datasets/uber-movement.jpg",
    "embedding_text": "The Uber Movement dataset is a comprehensive collection of zone-to-zone travel times and street speeds for over 50 cities worldwide, derived from real-time data collected from Uber rides. This dataset is structured in a tabular format, with rows representing individual trips and columns including key variables such as origin and destination zones, travel times, street speeds, and timestamps. The data is particularly valuable for urban planners, researchers, and data scientists interested in understanding traffic patterns and congestion dynamics in metropolitan areas. The collection methodology involves aggregating data from actual Uber rides, ensuring that the dataset reflects real-world conditions and behaviors. This makes it a reliable source for analyzing urban mobility trends. However, users should be aware of potential limitations, such as data quality issues arising from incomplete trip records or variations in ride-sharing behaviors across different cities. Common preprocessing steps may include cleaning the data to remove outliers, normalizing travel times, and segmenting the data by time of day or specific geographic areas. Researchers can leverage this dataset to address various research questions, such as identifying peak congestion times, comparing travel efficiency across different urban areas, and evaluating the impact of infrastructure changes on travel patterns. The dataset supports a range of analyses, including regression analysis to model travel time predictors, machine learning techniques for pattern recognition, and descriptive statistics to summarize travel behaviors. Overall, the Uber Movement dataset serves as a vital resource for those looking to enhance urban transportation systems and improve mobility outcomes."
  },
  {
    "name": "IEEE-CIS Fraud Detection",
    "description": "590K card-not-present transactions with 393 features from Vesta Corp. Real messy fraud data (3.5% fraud rate)",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/competitions/ieee-fraud-detection",
    "docs_url": null,
    "github_url": "https://github.com/amazon-science/fraud-dataset-benchmark",
    "tags": [
      "fraud detection",
      "transactions",
      "large-scale",
      "messy data",
      "fintech",
      "competition"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "fraud detection",
      "financial analysis",
      "data cleaning"
    ],
    "summary": "The IEEE-CIS Fraud Detection dataset comprises 590,000 card-not-present transactions, featuring 393 variables that capture various aspects of each transaction. This dataset is particularly valuable for developing and testing fraud detection algorithms, given its real-world context and the inherent challenges posed by messy data.",
    "use_cases": [
      "Building predictive models to identify fraudulent transactions.",
      "Analyzing transaction patterns to understand fraud behavior.",
      "Testing the effectiveness of various fraud detection algorithms.",
      "Conducting exploratory data analysis to uncover insights from messy data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the IEEE-CIS Fraud Detection dataset?",
      "How can I use the IEEE-CIS Fraud Detection dataset for machine learning?",
      "What features are included in the IEEE-CIS Fraud Detection dataset?",
      "What is the fraud rate in the IEEE-CIS Fraud Detection dataset?",
      "Where can I find the IEEE-CIS Fraud Detection dataset?",
      "What types of analyses can be performed on the IEEE-CIS Fraud Detection dataset?",
      "What preprocessing steps are needed for the IEEE-CIS Fraud Detection dataset?",
      "What are the challenges of using the IEEE-CIS Fraud Detection dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "model_score": 0.0077,
    "image_url": "/images/datasets/ieee-cis-fraud-detection.jpg",
    "embedding_text": "The IEEE-CIS Fraud Detection dataset is a comprehensive collection of 590,000 card-not-present transactions, meticulously curated to aid in the development and evaluation of fraud detection methodologies. With 393 features, this dataset encompasses a wide array of variables that characterize each transaction, including transaction amounts, user behavior, and other relevant metrics. The data is sourced from Vesta Corp, a company specializing in fraud detection, which adds credibility and real-world relevance to the dataset. Given the nature of the data, it is expected to be messy, presenting unique challenges for data scientists and researchers. The dataset has a fraud rate of 3.5%, making it particularly useful for training models to identify fraudulent activities in financial transactions. Researchers can leverage this dataset to explore various research questions, such as what factors contribute to the likelihood of fraud, how transaction patterns differ between legitimate and fraudulent transactions, and the effectiveness of different machine learning algorithms in detecting fraud. The dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, allowing for a multifaceted exploration of the data. However, users should be aware of potential data quality issues, such as missing values or outliers, which may require common preprocessing steps like data cleaning, normalization, and feature engineering. Overall, the IEEE-CIS Fraud Detection dataset serves as a valuable resource for those looking to delve into the complexities of fraud detection in the financial sector, providing a robust foundation for both academic research and practical applications."
  },
  {
    "name": "Chicago TNC Trips",
    "description": "100M+ rideshare trips with fares (unlike NYC which lacks fare data). Trip-level pricing for Uber/Lyft economic analysis",
    "category": "Transportation & Mobility",
    "url": "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "rideshare",
      "fares",
      "Uber",
      "Lyft",
      "Chicago",
      "pricing"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "transportation",
      "mobility",
      "pricing"
    ],
    "summary": "The Chicago TNC Trips dataset comprises over 100 million rideshare trips, providing detailed trip-level pricing data for Uber and Lyft services in Chicago. This dataset enables economic analysis of rideshare pricing and usage patterns, allowing researchers to explore fare structures and consumer behavior in the rideshare market.",
    "use_cases": [
      "Analyzing fare trends over time",
      "Comparing pricing strategies between Uber and Lyft",
      "Studying the impact of external factors on rideshare usage",
      "Evaluating consumer behavior in rideshare markets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Chicago rideshare trip data",
      "Uber Lyft fare analysis Chicago",
      "rideshare pricing dataset",
      "TNC trip data for economic analysis",
      "Chicago transportation data",
      "rideshare trip statistics",
      "Uber Lyft trip-level data"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Chicago",
    "size_category": "massive",
    "model_score": 0.0077,
    "image_url": "/images/logos/cityofchicago.png",
    "embedding_text": "The Chicago TNC Trips dataset is an extensive collection of over 100 million rideshare trips, specifically focusing on Uber and Lyft services within the city of Chicago. This dataset is structured in a tabular format, consisting of rows representing individual trips and columns detailing various attributes such as trip start and end times, distances traveled, fare amounts, and other relevant metrics. The dataset's schema is designed to facilitate in-depth analysis of rideshare pricing and usage patterns, making it a valuable resource for researchers and analysts interested in the transportation and mobility sectors. The collection methodology for this dataset involves aggregating trip data directly from rideshare companies, ensuring that the information is both comprehensive and accurate. This direct sourcing allows for a rich dataset that reflects real-world usage of rideshare services in an urban environment. Coverage is specifically focused on the geographic area of Chicago, providing insights into the local transportation landscape. While the dataset does not explicitly mention temporal coverage, it is assumed to include a wide range of trips over an extended period, capturing variations in rideshare usage across different times of day, week, and potentially season. Key variables within the dataset include trip duration, fare amounts, pickup and drop-off locations, and passenger counts, each of which plays a critical role in understanding the dynamics of rideshare pricing and consumer behavior. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in fare reporting, which may necessitate common preprocessing steps like data cleaning and normalization before analysis. The dataset supports a variety of analytical approaches, including regression analysis to identify factors influencing fare prices, machine learning techniques for predictive modeling, and descriptive statistics to summarize rideshare usage trends. Researchers typically utilize this dataset to explore research questions related to the economic implications of rideshare pricing, the impact of regulatory changes on service usage, and the overall effectiveness of rideshare services in meeting urban transportation needs. By leveraging the rich data provided in the Chicago TNC Trips dataset, analysts can gain valuable insights into the evolving landscape of urban mobility and the role of rideshare services in contemporary transportation systems.",
    "benchmark_usage": [
      "Economic analysis of rideshare pricing",
      "Consumer behavior studies in transportation"
    ]
  },
  {
    "name": "OPTN Organ Transplant",
    "description": "Complete US organ donation records since 1987. Waiting lists, donor-recipient matches, outcomes. Market design and matching research",
    "category": "Healthcare",
    "url": "https://optn.transplant.hrsa.gov/data/",
    "docs_url": "https://optn.transplant.hrsa.gov/data/about-data/",
    "github_url": null,
    "tags": [
      "organ transplant",
      "matching",
      "market design",
      "healthcare",
      "waitlists"
    ],
    "best_for": "Understanding healthcare analytics, patient outcomes, and clinical predictions",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OPTN Organ Transplant dataset contains comprehensive records of organ donations in the United States since 1987, including details about waiting lists, donor-recipient matches, and outcomes. Researchers can utilize this dataset for market design studies and matching research to improve organ allocation and transplantation strategies.",
    "use_cases": [
      "Analyzing trends in organ donation and transplantation outcomes over time.",
      "Investigating the effectiveness of different matching algorithms for donor-recipient pairs.",
      "Studying the impact of demographic factors on waiting list times and transplant success.",
      "Evaluating the effects of policy changes on organ donation rates."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the trends in organ donation rates since 1987?",
      "How do donor-recipient matches affect transplant outcomes?",
      "What factors influence waiting list durations for organ transplants?",
      "How has market design evolved in organ transplantation?",
      "What are the demographics of organ donors and recipients?",
      "How do different states compare in organ donation rates?",
      "What is the impact of policy changes on organ transplant outcomes?",
      "How can machine learning improve matching algorithms for organ transplants?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1987-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/hrsa.png",
    "embedding_text": "The OPTN Organ Transplant dataset is a rich and comprehensive collection of organ donation records from the United States, spanning from 1987 to the present. This dataset is structured in a tabular format, containing rows that represent individual organ donations and transplantation events, while columns include various attributes such as donor and recipient demographics, organ types, waiting list statuses, and transplant outcomes. The data is meticulously collected by the Organ Procurement and Transplantation Network (OPTN), which is responsible for managing the national transplant system in the U.S. The collection methodology involves gathering information from hospitals, transplant centers, and organ procurement organizations, ensuring a high level of data integrity and accuracy. The dataset covers a wide temporal range, providing insights into trends and changes in organ donation practices over several decades. Geographically, it encompasses all states within the United States, allowing for comparative analyses across different regions. Key variables in the dataset include donor age, recipient age, organ type, waiting list duration, and transplant success rates, which measure critical aspects of the organ transplantation process. However, researchers should be aware of potential limitations, such as incomplete records or variations in reporting practices among different states and institutions. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing demographic variables, and encoding categorical data for analysis. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a valuable resource for researchers exploring questions related to organ donation and transplantation. Researchers typically use this dataset to address critical research questions, such as the factors influencing waiting times for organ transplants, the effectiveness of matching algorithms, and the impact of demographic variables on transplant outcomes. By leveraging this dataset, researchers can contribute to the ongoing discourse on improving organ allocation strategies and enhancing the overall efficiency of the transplant system."
  },
  {
    "name": "CMS Hospital Price Transparency",
    "description": "Hospital pricing data mandated since 2021. Negotiated rates, chargemaster prices across 6,000+ hospitals. Healthcare pricing research",
    "category": "Healthcare",
    "url": "https://www.cms.gov/hospital-price-transparency",
    "docs_url": "https://www.cms.gov/hospital-price-transparency/resources",
    "github_url": null,
    "tags": [
      "healthcare",
      "hospital pricing",
      "transparency",
      "CMS",
      "insurance"
    ],
    "best_for": "Understanding healthcare analytics, patient outcomes, and clinical predictions",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "hospital pricing",
      "insurance"
    ],
    "summary": "The CMS Hospital Price Transparency dataset provides detailed hospital pricing information mandated since 2021, including negotiated rates and chargemaster prices from over 6,000 hospitals. Researchers can utilize this dataset to analyze healthcare pricing trends, compare hospital costs, and assess the impact of transparency on consumer behavior.",
    "use_cases": [
      "Analyzing the impact of hospital pricing transparency on consumer choice.",
      "Comparing hospital prices across different regions and types of services.",
      "Investigating the relationship between negotiated rates and chargemaster prices.",
      "Evaluating the effectiveness of pricing transparency regulations."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the CMS Hospital Price Transparency dataset?",
      "How can I access hospital pricing data from CMS?",
      "What are the negotiated rates for hospitals in the CMS dataset?",
      "What information does the chargemaster price data include?",
      "How has hospital pricing changed since the 2021 mandate?",
      "What hospitals are included in the CMS Hospital Price Transparency dataset?",
      "How can I analyze healthcare pricing using CMS data?",
      "What insights can be gained from hospital pricing transparency data?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2021",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/cms.png",
    "embedding_text": "The CMS Hospital Price Transparency dataset is a comprehensive collection of hospital pricing information that has been mandated since 2021. This dataset includes negotiated rates and chargemaster prices from over 6,000 hospitals across the United States, providing a valuable resource for healthcare pricing research. The data is structured in a tabular format, with rows representing individual hospitals and columns detailing various pricing metrics, including service descriptions, negotiated rates, and chargemaster prices. Each entry in the dataset is designed to offer insights into the pricing strategies employed by hospitals, allowing researchers to explore trends and disparities in healthcare costs. The collection methodology for this dataset is rooted in regulatory requirements set forth by the Centers for Medicare & Medicaid Services (CMS), which mandates hospitals to disclose their pricing information. As such, the data is sourced directly from hospital chargemasters and negotiated contracts, ensuring a level of accuracy and reliability. However, researchers should be aware of potential limitations, such as variations in how hospitals report their prices and the absence of certain services in some datasets. The temporal coverage of this dataset is explicitly noted as starting from 2021, aligning with the implementation of the transparency mandate. While the dataset does not specify geographic scope, it encompasses hospitals across the United States, reflecting a diverse range of healthcare facilities. Key variables within the dataset include hospital names, service descriptions, negotiated rates, and chargemaster prices, each measuring different aspects of hospital pricing. Researchers can leverage this dataset to address a variety of research questions, such as examining the impact of pricing transparency on patient choices, analyzing price variations across different hospitals, and investigating the correlation between negotiated rates and overall healthcare costs. Common preprocessing steps may include data cleaning to handle inconsistencies, normalization of price data for comparative analysis, and aggregation of services for broader insights. The dataset supports various types of analyses, including regression analysis to identify factors influencing pricing, machine learning models to predict costs, and descriptive statistics to summarize pricing trends. Researchers typically utilize this dataset in studies focused on healthcare economics, policy evaluation, and consumer behavior, making it a crucial tool for understanding the dynamics of hospital pricing in a transparent environment."
  },
  {
    "name": "Medicare Provider Utilization",
    "description": "All Medicare providers with service utilization and payment data. CMS public use files for healthcare analytics",
    "category": "Healthcare",
    "url": "https://data.cms.gov/provider-summary-by-type-of-service",
    "docs_url": "https://data.cms.gov/",
    "github_url": null,
    "tags": [
      "Medicare",
      "healthcare",
      "providers",
      "payments",
      "CMS"
    ],
    "best_for": "Understanding healthcare analytics, patient outcomes, and clinical predictions",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "data-analysis",
      "policy-research"
    ],
    "summary": "The Medicare Provider Utilization dataset contains comprehensive information on all Medicare providers, including their service utilization and payment data. This dataset can be utilized for healthcare analytics, policy evaluation, and understanding provider behavior in the Medicare system.",
    "use_cases": [
      "Analyzing trends in Medicare provider payments over time.",
      "Evaluating the impact of policy changes on healthcare provider utilization.",
      "Comparing service utilization rates among different types of Medicare providers.",
      "Identifying outlier providers based on payment and service utilization metrics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Medicare Provider Utilization dataset?",
      "How can I access Medicare provider payment data?",
      "What types of analyses can be performed with Medicare service utilization data?",
      "Where can I find CMS public use files for healthcare analytics?",
      "What are the key variables in the Medicare Provider Utilization dataset?",
      "How do Medicare providers' payment data vary across different regions?",
      "What insights can be gained from analyzing Medicare provider utilization?",
      "What methodologies are used to collect Medicare provider data?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0077,
    "image_url": "/images/logos/cms.png",
    "embedding_text": "The Medicare Provider Utilization dataset is a vital resource for researchers and analysts interested in the dynamics of healthcare provision under the Medicare program. This dataset comprises a structured collection of data points that include various attributes related to Medicare providers, such as their identification numbers, types of services rendered, utilization rates, and associated payment amounts. Each row in the dataset typically represents a unique Medicare provider, while the columns contain variables that detail the provider's characteristics, service types, and financial metrics. The data is collected through the Centers for Medicare & Medicaid Services (CMS), which compiles public use files that are essential for healthcare analytics and research. The methodology for data collection involves aggregating information from multiple sources, including claims data, provider enrollment records, and service utilization reports, ensuring a comprehensive view of provider activities within the Medicare system. While the dataset provides a wealth of information, researchers should be aware of potential limitations, such as data completeness and accuracy, which can vary based on reporting practices and the nature of the services provided. Common preprocessing steps may include data cleaning to handle missing values, normalization of payment amounts, and categorization of service types for analysis. Researchers can leverage this dataset to address a range of research questions, such as examining the correlation between provider payment rates and service utilization, assessing the impact of demographic factors on healthcare access, and evaluating the effectiveness of Medicare policies on provider behavior. The dataset supports various types of analyses, including regression analysis, machine learning models, and descriptive statistics, enabling users to derive meaningful insights from the data. Overall, the Medicare Provider Utilization dataset serves as a foundational tool for understanding the complexities of healthcare provision and payment systems, facilitating informed decision-making and policy development in the healthcare sector."
  },
  {
    "name": "Brazilian eCommerce",
    "description": "100,000 orders (2016-2018) structured in 9 relational tables from Olist",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "orders",
      "Brazil",
      "relational data",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Brazilian eCommerce dataset consists of 100,000 orders from Olist, structured in 9 relational tables, covering the years 2016 to 2018. This dataset can be utilized for various analyses related to consumer behavior, order trends, and pricing strategies in the Brazilian e-commerce market.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating pricing strategies",
      "Studying order fulfillment processes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Brazilian eCommerce dataset",
      "Olist orders data",
      "eCommerce data analysis Brazil",
      "relational tables eCommerce dataset",
      "Kaggle Brazilian orders dataset",
      "2016-2018 eCommerce orders Brazil"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2016-2018",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0052,
    "image_url": "/images/datasets/brazilian-ecommerce.png",
    "embedding_text": "The Brazilian eCommerce dataset is a rich resource for analyzing consumer behavior and order trends in the Brazilian online retail market. It comprises 100,000 orders collected from Olist, structured across 9 relational tables. Each table contains various attributes that provide insights into the orders, such as customer details, product information, payment methods, and shipping logistics. The data spans from 2016 to 2018, offering a temporal perspective on how e-commerce has evolved in Brazil during this period. The dataset is particularly valuable for researchers and data scientists interested in the dynamics of online shopping, as it allows for the exploration of key variables such as order value, customer demographics, and product categories. The collection methodology involves aggregating data from Olist's platform, which connects small and medium-sized businesses to consumers, thus representing a diverse range of products and services. The key variables include order ID, customer ID, product ID, payment type, and shipping details, which collectively measure aspects of the purchasing process and customer interactions. However, users should be aware of potential limitations in data quality, such as missing values or inconsistencies in customer information, which may necessitate preprocessing steps like data cleaning and normalization. Common preprocessing tasks might include handling missing data, transforming categorical variables into numerical formats, and aggregating data for analysis. This dataset supports various types of analyses, including regression analysis to understand factors influencing order value, machine learning models for predicting customer behavior, and descriptive statistics to summarize order trends. Researchers typically use this dataset to address questions related to consumer preferences, the impact of pricing on sales, and the effectiveness of different payment methods. Overall, the Brazilian eCommerce dataset is an essential tool for anyone looking to gain insights into the rapidly growing e-commerce sector in Brazil, providing a solid foundation for both academic research and practical applications in data science."
  },
  {
    "name": "Open CDP",
    "description": "Omnichannel interaction tracking with AI-driven identity resolution",
    "category": "E-Commerce",
    "url": "https://rees46.com/en/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "omnichannel",
      "customer data",
      "identity resolution"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Open CDP dataset provides comprehensive tracking of omnichannel interactions, utilizing AI-driven identity resolution to enhance customer insights. Researchers and analysts can leverage this data to understand consumer behavior across various platforms and improve marketing strategies.",
    "use_cases": [
      "Analyzing customer behavior across different channels",
      "Improving marketing strategies through data-driven insights"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Open CDP dataset?",
      "How does omnichannel interaction tracking work?",
      "What are the benefits of AI-driven identity resolution?",
      "How can I analyze customer data from the Open CDP?",
      "What insights can be gained from omnichannel customer interactions?",
      "What are the applications of the Open CDP dataset in e-commerce?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0052,
    "image_url": "/images/datasets/open-cdp.jpg",
    "embedding_text": "The Open CDP dataset is designed to facilitate the tracking of omnichannel interactions, providing a rich source of data for understanding consumer behavior in the e-commerce sector. This dataset typically consists of rows representing individual customer interactions across various platforms, with columns capturing key variables such as customer ID, interaction type, timestamp, and channel used. The data structure allows for a comprehensive view of how customers engage with brands across multiple touchpoints, enabling researchers to analyze patterns and trends in consumer behavior. The collection methodology for this dataset likely involves aggregating data from various sources, including web analytics, CRM systems, and social media platforms, ensuring a holistic view of customer interactions. However, it is important to note that the quality of the data may vary based on the sources used, and there may be limitations related to data completeness or accuracy. Common preprocessing steps may include data cleaning to handle missing values, normalization of interaction types, and transformation of timestamps into a consistent format. Researchers can use the Open CDP dataset to address a variety of research questions, such as identifying which channels are most effective for customer engagement or understanding the impact of different marketing strategies on consumer behavior. The dataset supports various types of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize customer interactions. Overall, the Open CDP dataset serves as a valuable resource for researchers and analysts looking to gain insights into omnichannel customer behavior and improve their marketing efforts."
  },
  {
    "name": "Alibaba Ads (IJCAI-18)",
    "description": "6 billion display ad/click logs over 8 days from 100M users",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/147588",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "clicks",
      "large-scale",
      "Alibaba"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "advertising",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Ads dataset consists of 6 billion display ad and click logs collected over 8 days from 100 million users. This dataset can be utilized to analyze user behavior in response to advertisements, assess the effectiveness of ad campaigns, and develop predictive models for click-through rates.",
    "use_cases": [
      "Analyzing user engagement with advertisements",
      "Predicting click-through rates using machine learning",
      "Evaluating the effectiveness of advertising strategies",
      "Understanding consumer behavior in response to ads"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the click patterns in Alibaba Ads data?",
      "How can we analyze user engagement with display ads?",
      "What factors influence click rates in e-commerce advertising?",
      "How does user behavior vary across different demographics in Alibaba Ads?",
      "What insights can be drawn from 6 billion ad logs?",
      "How can machine learning be applied to predict ad clicks?",
      "What is the scale of user interaction with Alibaba's advertising platform?",
      "How can regression analysis be used to understand ad performance?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0052,
    "embedding_text": "The Alibaba Ads dataset is a comprehensive collection of display ad and click logs, amounting to 6 billion records gathered over a span of 8 days from a substantial user base of 100 million individuals. This dataset is structured in a tabular format, where each row represents a unique ad impression or click event, and the columns include various attributes such as user identifiers, ad identifiers, timestamps, and click status. The data collection methodology involves tracking user interactions with display ads on the Alibaba platform, capturing detailed logs that reflect user engagement and behavior in real-time. The dataset's temporal coverage is limited to the 8-day period during which the data was collected, providing a snapshot of user interactions within that timeframe. However, it does not specify any geographic scope or demographic details about the users, which may limit certain types of analyses that require such information. Key variables in the dataset include user IDs, ad IDs, timestamps of interactions, and whether a click occurred, which are essential for measuring user engagement and ad performance. Researchers can leverage this dataset to address a variety of research questions, such as identifying patterns in user behavior, understanding the factors that influence click rates, and evaluating the effectiveness of different advertising strategies. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing timestamps, and encoding categorical variables for analysis. The dataset supports various types of analyses, including regression analysis, machine learning modeling, and descriptive statistics, making it a versatile resource for researchers interested in the intersection of advertising and consumer behavior. Typically, researchers utilize this dataset to develop predictive models for click-through rates, analyze the impact of advertising on consumer decision-making, and derive insights that can inform marketing strategies in the e-commerce sector."
  },
  {
    "name": "Coveo Shopping (SIGIR-21)",
    "description": "30M+ browsing events with query and image vectors for e-commerce search",
    "category": "E-Commerce",
    "url": "https://github.com/coveooss/SIGIR-ecom-data-challenge",
    "docs_url": null,
    "github_url": "https://github.com/coveooss/SIGIR-ecom-data-challenge",
    "tags": [
      "browsing",
      "search",
      "embeddings",
      "SIGIR"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Coveo Shopping dataset consists of over 30 million browsing events, providing valuable insights into e-commerce search behaviors. Researchers can utilize this dataset to analyze consumer interactions with search queries and image embeddings, enabling them to improve search algorithms and understand user preferences.",
    "use_cases": [
      "Analyzing consumer search behavior to optimize e-commerce platforms",
      "Improving search algorithms using query and image vector data",
      "Studying the impact of different search strategies on consumer engagement",
      "Exploring trends in browsing events to inform marketing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the browsing events in the Coveo Shopping dataset?",
      "How can I analyze e-commerce search behaviors using the Coveo dataset?",
      "What insights can be gained from the Coveo Shopping dataset's query and image vectors?",
      "How does the Coveo dataset support research in consumer behavior?",
      "What are the key variables in the Coveo Shopping dataset?",
      "How can I access the Coveo Shopping dataset for e-commerce analysis?",
      "What types of analyses can be performed on the Coveo Shopping dataset?",
      "What are the limitations of the Coveo Shopping dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0051,
    "image_url": "/images/datasets/coveo-shopping-sigir-21.png",
    "embedding_text": "The Coveo Shopping dataset is a comprehensive collection of over 30 million browsing events specifically designed for e-commerce search analysis. This dataset includes a variety of data structures and schemas, featuring rows that represent individual browsing events and columns that capture key variables such as search queries, user interactions, and image vectors. The dataset is structured to facilitate the analysis of consumer behavior in response to search functionalities, providing a rich resource for researchers and practitioners in the field of e-commerce.\n\nCollection methodology for the Coveo Shopping dataset involves the aggregation of browsing events from various e-commerce platforms, ensuring a diverse representation of user interactions. The data sources are primarily derived from real-time user engagement on these platforms, capturing the nuances of search behaviors as users navigate through product offerings. This methodology ensures that the dataset reflects authentic consumer interactions, making it a valuable asset for understanding e-commerce dynamics.\n\nWhile the dataset does not explicitly mention temporal or geographic coverage, it encompasses a wide range of browsing events that can be analyzed to uncover trends and patterns over time. The demographic coverage is also broad, as it includes data from varied consumer segments engaging with e-commerce platforms. Key variables in the dataset measure aspects such as the frequency of searches, the types of products browsed, and the effectiveness of search queries in leading to purchases.\n\nData quality is a critical aspect of the Coveo Shopping dataset. Researchers should be aware of potential limitations, such as biases in user behavior, incomplete data entries, or variations in search algorithms across different platforms. Common preprocessing steps may include cleaning the data to remove duplicates, normalizing search queries, and encoding categorical variables for analysis. These steps are essential to ensure the integrity and usability of the dataset for analytical purposes.\n\nThe Coveo Shopping dataset supports a variety of research questions, including inquiries into how search queries influence consumer behavior, the effectiveness of image embeddings in enhancing search results, and the overall impact of search functionalities on e-commerce performance. Types of analyses that can be performed using this dataset range from regression analyses to machine learning applications and descriptive statistics, allowing researchers to derive actionable insights from the data.\n\nResearchers typically use the Coveo Shopping dataset in studies aimed at improving e-commerce search algorithms, understanding consumer preferences, and optimizing user experiences on e-commerce platforms. By leveraging the rich information contained within this dataset, analysts can contribute to the development of more effective search strategies that cater to the evolving needs of online shoppers."
  },
  {
    "name": "Google Merchandise",
    "description": "3 months obfuscated GA4 e-commerce data (Nov 2020-Jan 2021)",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/bigquery/google-analytics-sample",
    "docs_url": "https://support.google.com/analytics/answer/7586738",
    "github_url": null,
    "tags": [
      "Google Analytics",
      "GA4",
      "web analytics"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "web analytics"
    ],
    "summary": "The Google Merchandise dataset contains three months of obfuscated Google Analytics 4 (GA4) e-commerce data, specifically from November 2020 to January 2021. This dataset can be utilized to analyze consumer behavior, sales trends, and website performance in an e-commerce context.",
    "use_cases": [
      "Analyzing sales trends over the three-month period.",
      "Examining consumer behavior patterns based on website interactions.",
      "Evaluating the effectiveness of marketing campaigns.",
      "Identifying key performance indicators for e-commerce success."
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Google Merchandise dataset?",
      "How can I access Google Analytics 4 e-commerce data?",
      "What insights can be derived from GA4 data?",
      "What are the key metrics in e-commerce analytics?",
      "How does consumer behavior change over time in e-commerce?",
      "What trends can be identified in the Google Merchandise dataset?",
      "How to analyze web analytics data for e-commerce?",
      "What tools can be used to visualize GA4 e-commerce data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2020-11 to 2021-01",
    "size_category": "medium",
    "model_score": 0.0051,
    "image_url": "/images/datasets/google-merchandise.jpeg",
    "embedding_text": "The Google Merchandise dataset is a rich source of e-commerce data derived from Google Analytics 4 (GA4), encompassing three months of obfuscated data from November 2020 to January 2021. This dataset is structured in a tabular format, containing rows that represent individual user sessions and transactions, while columns include various metrics such as session duration, page views, conversion rates, and revenue generated. Key variables in this dataset measure essential aspects of e-commerce performance, including user engagement, sales volume, and customer acquisition metrics. The collection methodology involves the use of GA4 tracking codes embedded within the Google Merchandise Store's website, capturing user interactions and transactions in a privacy-conscious manner through obfuscation. Researchers and analysts can utilize this dataset to address a variety of research questions, such as identifying trends in consumer purchasing behavior, evaluating the impact of promotional campaigns, and understanding the relationship between website traffic and sales performance. Common preprocessing steps may include data cleaning to handle missing values, normalization of metrics for comparative analysis, and transformation of categorical variables for analytical modeling. The dataset supports various types of analyses, including regression analysis to predict sales based on user engagement metrics, machine learning techniques for customer segmentation, and descriptive statistics to summarize overall performance. Researchers typically leverage this dataset in studies focused on e-commerce optimization, marketing effectiveness, and user experience enhancement, making it a valuable resource for both academic and industry-related inquiries."
  },
  {
    "name": "Shopee",
    "description": "Dataset from Shopee's 2020 Code League competition",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/c/shopee-code-league-2021",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Shopee",
      "competition",
      "Southeast Asia"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Shopee dataset from the 2020 Code League competition provides insights into consumer behavior and pricing strategies within the e-commerce sector. Researchers and data scientists can utilize this dataset to analyze trends, develop predictive models, and enhance understanding of market dynamics in Southeast Asia.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Developing pricing strategies based on competition",
      "Predicting sales trends in e-commerce",
      "Evaluating the effectiveness of marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Shopee dataset from the 2020 Code League competition?",
      "How can I analyze consumer behavior using the Shopee dataset?",
      "What insights can be derived from the Shopee e-commerce data?",
      "What are the key features of the Shopee dataset?",
      "How is the Shopee dataset structured?",
      "What types of analyses can be performed with the Shopee dataset?",
      "What are the limitations of the Shopee dataset?",
      "In what ways can the Shopee dataset inform pricing strategies?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2020",
    "geographic_scope": "Southeast Asia",
    "size_category": "medium",
    "model_score": 0.0051,
    "image_url": "/images/datasets/shopee.jpg",
    "embedding_text": "The Shopee dataset, derived from the 2020 Code League competition, serves as a valuable resource for those interested in the e-commerce landscape, particularly within Southeast Asia. This dataset is structured in a tabular format, consisting of multiple rows and columns that encapsulate various variables relevant to consumer behavior and pricing strategies. The data schema typically includes fields such as product IDs, prices, sales volume, user ratings, and timestamps, among others. Each variable is designed to measure specific aspects of the e-commerce experience, allowing for a comprehensive analysis of market trends. The collection methodology for this dataset is rooted in Shopee's operational data during the competition, which provides a rich context for understanding consumer interactions and preferences. However, researchers should be aware of potential limitations in data quality, such as missing values or biases inherent in user-generated content. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. The Shopee dataset can address a variety of research questions, including those focused on identifying key drivers of consumer purchasing decisions, evaluating the impact of pricing strategies on sales performance, and exploring the relationships between user ratings and product sales. Analysts can employ various techniques, including regression analysis, machine learning models, and descriptive statistics, to extract meaningful insights from the data. Researchers typically utilize this dataset to inform studies on consumer behavior, optimize pricing strategies, and enhance marketing efforts, making it a crucial asset for anyone looking to delve into the intricacies of e-commerce in the Southeast Asian market."
  },
  {
    "name": "Flipkart",
    "description": "Sales dataset from Indian e-commerce platform Flipkart",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/iyumrahul/flipkartsalesdataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "India",
      "sales",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "sales-analysis"
    ],
    "summary": "The Flipkart dataset contains sales data from the Indian e-commerce platform Flipkart, providing insights into consumer purchasing behavior and sales trends. Researchers and analysts can use this dataset to perform various analyses, such as sales forecasting, market basket analysis, and consumer segmentation.",
    "use_cases": [
      "Sales forecasting",
      "Market basket analysis",
      "Consumer segmentation"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Flipkart sales dataset",
      "E-commerce sales data India",
      "Kaggle Flipkart dataset",
      "Indian e-commerce sales analysis",
      "Consumer behavior in Flipkart",
      "Sales trends in Indian e-commerce"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0037,
    "image_url": "/images/datasets/flipkart.png",
    "embedding_text": "The Flipkart dataset is a comprehensive collection of sales data from the Indian e-commerce platform Flipkart, one of the largest online retailers in India. This dataset is structured in a tabular format, comprising multiple rows and columns that capture various aspects of sales transactions. Each row represents a unique sales transaction, while the columns include key variables such as product ID, category, price, quantity sold, and timestamps of the transactions. The dataset is particularly valuable for researchers and data scientists interested in understanding consumer behavior, sales patterns, and market dynamics within the Indian e-commerce sector. The data collection methodology likely involves direct extraction from Flipkart's transactional databases, ensuring that the dataset reflects real-world sales activities. However, it is essential to consider potential limitations in data quality, such as missing values or inconsistencies that may arise from the data extraction process. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers can leverage this dataset to address various research questions, such as identifying trends in consumer purchasing behavior, analyzing the impact of promotional campaigns on sales, and exploring the relationship between product pricing and sales volume. The dataset supports a range of analytical techniques, including regression analysis, machine learning algorithms, and descriptive statistics, making it a versatile resource for both academic and industry research. Typically, analysts use this dataset to derive actionable insights that can inform business strategies, optimize inventory management, and enhance customer targeting efforts. Overall, the Flipkart sales dataset serves as a rich resource for exploring the intricacies of e-commerce in India, providing a foundation for data-driven decision-making in the retail sector."
  },
  {
    "name": "Pakistan e-commerce",
    "description": "500k+ transactions (Mar 2016 - Aug 2018) from Pakistan's largest e-commerce",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/zusmani/pakistans-largest-ecommerce-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Pakistan",
      "transactions",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Pakistan e-commerce dataset contains over 500,000 transactions from the largest e-commerce platform in Pakistan, covering the period from March 2016 to August 2018. This dataset can be utilized for analyzing consumer purchasing behavior, transaction trends, and market dynamics in the e-commerce sector.",
    "use_cases": [
      "Analyzing seasonal trends in e-commerce transactions.",
      "Studying the impact of promotional campaigns on sales.",
      "Investigating consumer preferences and purchasing patterns.",
      "Evaluating the effectiveness of pricing strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the transaction trends in Pakistan's e-commerce from 2016 to 2018?",
      "How can consumer behavior be analyzed using e-commerce transaction data?",
      "What insights can be drawn from the largest e-commerce dataset in Pakistan?",
      "How does the volume of transactions vary over the specified period?",
      "What are the common products purchased in Pakistan's e-commerce market?",
      "How can this dataset be used to study pricing strategies in e-commerce?",
      "What demographic factors can influence online shopping behavior in Pakistan?",
      "What are the key variables in the Pakistan e-commerce transaction dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2016-03 to 2018-08",
    "geographic_scope": "Pakistan",
    "size_category": "massive",
    "model_score": 0.0033,
    "image_url": "/images/datasets/pakistan-e-commerce.jpg",
    "embedding_text": "The Pakistan e-commerce dataset is a comprehensive collection of over 500,000 transaction records from the largest e-commerce platform in Pakistan, spanning from March 2016 to August 2018. This dataset is structured in a tabular format, with rows representing individual transactions and columns capturing various attributes such as transaction ID, product details, customer demographics, timestamps, and payment methods. The data collection methodology involved aggregating transaction records from the e-commerce platform's backend systems, ensuring a rich source of information for analysis. The dataset provides temporal coverage from early 2016 to mid-2018, allowing researchers to explore trends and changes in consumer behavior over time. Geographically, the dataset is focused on Pakistan, making it particularly valuable for studies related to the South Asian market. Key variables within the dataset include transaction amounts, product categories, customer IDs, and timestamps, which can be used to measure purchasing behavior, frequency of transactions, and product popularity. Researchers should be aware of potential data quality issues, such as missing values or inconsistencies in product categorization, which may require preprocessing steps like data cleaning and normalization. Common preprocessing steps may include handling missing data, encoding categorical variables, and aggregating transactions by time periods for trend analysis. The dataset supports a variety of analyses, including regression analysis to predict sales trends, machine learning models for customer segmentation, and descriptive statistics to summarize purchasing patterns. Researchers typically use this dataset to address questions related to consumer behavior, market dynamics, and the effectiveness of marketing strategies in the e-commerce sector. Overall, the Pakistan e-commerce dataset serves as a valuable resource for understanding the intricacies of online shopping in a rapidly growing market."
  },
  {
    "name": "Art Auction (Artists for Lahaina)",
    "description": "Artists for Lahaina benefit art auction data (2023)",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/flkuhm/art-price-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "art",
      "charity",
      "auctions"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "charity",
      "art"
    ],
    "summary": "The 'Art Auction (Artists for Lahaina)' dataset contains data from a benefit art auction held in 2023, aimed at supporting the community of Lahaina. This dataset can be utilized to analyze auction dynamics, pricing strategies, and the impact of charitable events on art sales.",
    "use_cases": [
      "Analyzing pricing trends in charity art auctions",
      "Evaluating the impact of auction events on community support",
      "Studying buyer behavior in art auctions",
      "Comparing auction results across different charity events"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Art Auction (Artists for Lahaina) dataset?",
      "How can I analyze auction data for charity events?",
      "What insights can be gained from art auction pricing?",
      "What are the trends in art auctions for charity?",
      "How does the Artists for Lahaina auction data compare to other art auctions?",
      "What variables are included in the Art Auction dataset?",
      "How can I use this dataset for market analysis?",
      "What are the benefits of analyzing charity auction data?"
    ],
    "domain_tags": [
      "art",
      "charity",
      "auctions"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2023",
    "size_category": "medium",
    "model_score": 0.0026,
    "image_url": "/images/datasets/art-auction-artists-for-lahaina.png",
    "embedding_text": "The 'Art Auction (Artists for Lahaina)' dataset is a structured collection of data derived from a benefit art auction that took place in 2023, specifically aimed at supporting the Lahaina community. This dataset is organized in a tabular format, consisting of rows and columns that encapsulate various attributes related to the auction. Key variables may include auction item details, bid amounts, bidder information, and timestamps of bids, among others. The collection methodology likely involved gathering data during the auction event, possibly through a combination of manual entry and automated systems that track bids in real-time. This dataset serves as a valuable resource for researchers and analysts interested in the intersection of art, charity, and market dynamics. It can help address research questions related to pricing strategies, buyer behavior, and the overall effectiveness of charity auctions in generating funds for community support. The dataset's temporal coverage is specifically noted as 2023, and its geographic scope is limited to Lahaina, providing a focused context for analysis. While the dataset offers rich insights, it may also have limitations in terms of data quality, such as potential inaccuracies in bid entries or incomplete bidder information. Common preprocessing steps might include cleaning the data to handle any missing values, normalizing bid amounts for comparative analysis, and categorizing auction items for more straightforward evaluation. Analysts can employ various types of analyses, including regression analysis to understand pricing trends, machine learning techniques to predict auction outcomes, and descriptive statistics to summarize bidder behavior. Researchers typically utilize this dataset to explore the dynamics of art auctions, assess the impact of charitable events, and derive insights that can inform future fundraising strategies. Overall, the 'Art Auction (Artists for Lahaina)' dataset is a rich source of information that can facilitate a deeper understanding of the art auction market, particularly in the context of charitable initiatives.",
    "geographic_scope": "Lahaina"
  },
  {
    "name": "Arena Human Preference (55K)",
    "description": "55K+ real-world conversations with human preference labels from Chatbot Arena",
    "category": "AI & LLM",
    "url": "https://huggingface.co/datasets/lmarena-ai/arena-human-preference-55k",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "human preference",
      "LLM evaluation",
      "chatbot arena"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Arena Human Preference dataset consists of over 55,000 real-world conversations annotated with human preference labels, sourced from the Chatbot Arena. This dataset can be utilized for evaluating and improving chatbot performance, as well as understanding user preferences in conversational AI.",
    "use_cases": [
      "Evaluating chatbot performance",
      "Improving user experience in conversational agents",
      "Analyzing human preferences in dialogue systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Arena Human Preference dataset?",
      "How can I access the 55K conversations from Chatbot Arena?",
      "What are the human preference labels in the Arena dataset?",
      "How is the Arena dataset used for LLM evaluation?",
      "What insights can be gained from analyzing chatbot conversations?",
      "Where can I find datasets for chatbot training?",
      "What are the applications of human preference data in AI?",
      "How does the Arena dataset contribute to AI research?"
    ],
    "domain_tags": [
      "AI",
      "chatbots"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0025,
    "image_url": "/images/datasets/arena-human-preference-55k.png",
    "embedding_text": "The Arena Human Preference dataset is a rich resource comprising over 55,000 real-world conversations that have been meticulously annotated with human preference labels. This dataset is derived from the Chatbot Arena, a platform designed to evaluate and compare the performance of various conversational agents. The primary structure of the dataset includes rows representing individual conversations, with columns capturing key variables such as the conversation ID, user inputs, chatbot responses, and the corresponding human preference labels. Each conversation is a unique interaction that reflects user preferences in response to different chatbot behaviors, making it a valuable asset for researchers and developers in the field of artificial intelligence and natural language processing. The collection methodology involves gathering dialogues from actual user interactions within the Chatbot Arena, ensuring that the data reflects authentic conversational dynamics. This approach enhances the dataset's relevance and applicability in real-world scenarios, as it captures the nuances of human communication and preference. While the dataset does not explicitly mention temporal or geographic coverage, it is assumed to encompass a diverse range of conversational contexts, given the nature of the Chatbot Arena. Key variables in the dataset include the user inputs, which represent the queries or statements made by users, and the chatbot responses, which are the outputs generated by the AI. The human preference labels serve as a critical measure of user satisfaction and preference, indicating which responses were deemed more favorable by users. Researchers utilizing this dataset should be aware of potential limitations, such as biases in user responses or variations in chatbot performance across different contexts. Common preprocessing steps may include cleaning the text data, normalizing user inputs, and encoding preference labels for analysis. The dataset supports various types of analyses, including regression analysis to identify factors influencing user preferences, machine learning models to predict user satisfaction, and descriptive statistics to summarize conversation characteristics. Researchers typically leverage this dataset to address questions related to user engagement, the effectiveness of conversational strategies, and the overall performance of chatbots in meeting user needs. By analyzing the Arena Human Preference dataset, researchers can gain valuable insights into the intricacies of human-chatbot interactions, ultimately contributing to the advancement of more effective and user-friendly conversational agents."
  },
  {
    "name": "Yahoo A1 Search Advertising Dataset",
    "description": "Search advertising competition dataset with sponsored search auction features and click outcomes",
    "category": "Advertising",
    "url": "https://webscope.sandbox.yahoo.com/catalog.php?datatype=a",
    "docs_url": "https://webscope.sandbox.yahoo.com/",
    "github_url": null,
    "tags": [
      "search advertising",
      "sponsored search",
      "Yahoo",
      "auctions"
    ],
    "best_for": "Sponsored search click prediction and auction dynamics research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "search advertising",
      "auctions"
    ],
    "summary": "The Yahoo A1 Search Advertising Dataset is a comprehensive resource for analyzing search advertising competition, featuring various sponsored search auction characteristics and click outcomes. Researchers can utilize this dataset to explore the dynamics of online advertising, optimize bidding strategies, and understand consumer behavior in response to sponsored search results.",
    "use_cases": [
      "Analyzing the effectiveness of different bidding strategies in search advertising.",
      "Investigating the impact of auction features on click outcomes.",
      "Exploring consumer behavior patterns in response to sponsored search ads.",
      "Optimizing ad placements based on historical auction data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Yahoo A1 Search Advertising Dataset?",
      "How can I access the Yahoo A1 Search Advertising Dataset?",
      "What features are included in the Yahoo A1 Search Advertising Dataset?",
      "What research can be conducted using the Yahoo A1 Search Advertising Dataset?",
      "What are the click outcomes in the Yahoo A1 Search Advertising Dataset?",
      "How does the Yahoo A1 Search Advertising Dataset relate to sponsored search auctions?",
      "What insights can be gained from analyzing the Yahoo A1 Search Advertising Dataset?",
      "What are the applications of the Yahoo A1 Search Advertising Dataset in advertising research?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "varies",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0024,
    "embedding_text": "The Yahoo A1 Search Advertising Dataset is a structured collection of data that captures various aspects of search advertising, particularly focusing on sponsored search auctions and their outcomes. This dataset typically consists of rows representing individual auction instances and columns detailing features such as bid amounts, click-through rates, and other relevant metrics. Researchers and data scientists can leverage this dataset to gain insights into the competitive landscape of online advertising, analyze consumer interactions with ads, and develop strategies for optimizing ad performance. The collection methodology for this dataset likely involves aggregating data from real-time auction events, providing a rich source of information for analysis. Key variables within the dataset may include auction IDs, advertiser IDs, bid amounts, impressions, clicks, and conversion rates, each measuring distinct aspects of advertising performance. However, like any dataset, it may have limitations regarding data quality, such as missing values or inconsistencies arising from the dynamic nature of online auctions. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. Researchers can address various research questions using this dataset, such as the effectiveness of different bidding strategies, the influence of auction characteristics on click outcomes, and the overall impact of sponsored search on consumer behavior. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and industry research. Typically, researchers utilize this dataset to conduct studies that aim to enhance understanding of the mechanisms driving online advertising success and to develop data-driven strategies that can improve advertising outcomes in competitive environments.",
    "image_url": "/images/logos/yahoo.png"
  },
  {
    "name": "Used Car Auction (PakWheels)",
    "description": "Listings from PakWheels Pakistani automobile marketplace",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/asimzahid/pakistans-largest-pakwheels-automobiles-listings",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cars",
      "Pakistan",
      "listings"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Used Car Auction dataset from PakWheels contains listings from a prominent Pakistani automobile marketplace. This dataset allows users to analyze car prices, market trends, and consumer preferences in the used car sector in Pakistan.",
    "use_cases": [
      "Analyzing price trends of used cars in the Pakistani market.",
      "Investigating consumer preferences for different car brands and models.",
      "Evaluating the impact of car age and condition on auction prices."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the current used car listings on PakWheels?",
      "How do prices of used cars vary across different regions in Pakistan?",
      "What are the most popular car brands listed on PakWheels?",
      "How does the age of a car affect its auction price?",
      "What trends can be observed in used car prices over time?",
      "What features are most commonly associated with higher auction prices for used cars?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Pakistan",
    "size_category": "medium",
    "model_score": 0.0022,
    "image_url": "/images/datasets/used-car-auction-pakwheels.jpg",
    "embedding_text": "The Used Car Auction dataset from PakWheels is a valuable resource for researchers and analysts interested in the dynamics of the used car market in Pakistan. This dataset is structured in a tabular format, comprising rows that represent individual car listings and columns that detail various attributes of each listing. Key variables typically include car make, model, year of manufacture, mileage, auction price, and location, among others. These variables provide insights into the characteristics that influence car pricing and consumer choices in the marketplace. The data is collected from the PakWheels platform, which is a well-known online marketplace for buying and selling cars in Pakistan. The collection methodology involves scraping or aggregating data from user-generated listings, ensuring a comprehensive overview of the current market offerings. However, users should be aware of potential limitations in data quality, such as inconsistencies in user-reported information, variations in listing formats, and the possibility of outdated listings. Common preprocessing steps may include cleaning the data to handle missing values, standardizing formats for categorical variables, and normalizing numerical values to facilitate analysis. Researchers can leverage this dataset to address a variety of research questions, such as understanding how different factors affect used car prices, identifying trends in consumer behavior, and evaluating the effectiveness of pricing strategies in the used car market. The dataset supports various types of analyses, including regression analysis to model price determinants, machine learning techniques for predictive modeling, and descriptive statistics to summarize the data. Overall, the Used Car Auction dataset serves as an essential tool for anyone looking to explore the intricacies of the used car market in Pakistan, providing a foundation for informed decision-making and strategic planning in the automotive sector."
  },
  {
    "name": "Crypto Art (SuperRare)",
    "description": "Bids and transactions from SuperRare NFT platform",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/franceschet/superrare",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "NFT",
      "crypto",
      "art",
      "auctions"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Crypto Art dataset from SuperRare contains detailed information on bids and transactions occurring on the SuperRare NFT platform. This dataset allows researchers and analysts to explore the dynamics of the NFT art market, including pricing trends, buyer behavior, and auction outcomes.",
    "use_cases": [
      "Analyzing pricing trends in the NFT art market",
      "Examining buyer behavior and preferences on SuperRare",
      "Studying the impact of auction strategies on final sale prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest bids on SuperRare?",
      "How do transaction volumes vary over time on SuperRare?",
      "What is the average price of NFTs sold on SuperRare?",
      "How many unique artists are represented in SuperRare transactions?",
      "What trends can be observed in NFT auctions on SuperRare?",
      "How do bids fluctuate during auction periods on SuperRare?"
    ],
    "domain_tags": [
      "retail",
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0021,
    "image_url": "/images/datasets/crypto-art-superrare.png",
    "embedding_text": "The Crypto Art dataset from SuperRare provides a comprehensive view of the bids and transactions that take place on one of the leading NFT platforms. This dataset is structured in a tabular format, with rows representing individual transactions and columns capturing key variables such as bid amounts, timestamps, artist identifiers, and transaction outcomes. The dataset is collected directly from the SuperRare platform, ensuring that the information is current and relevant to ongoing trends in the NFT market. While the dataset does not specify temporal or geographic coverage, it encompasses a wide range of transactions that reflect the evolving landscape of digital art and collectibles. Key variables in the dataset include bid amounts, which measure the financial engagement of buyers, and timestamps that provide insight into the timing of bids relative to auction events. Researchers utilizing this dataset may encounter limitations related to data quality, such as potential inconsistencies in bid reporting or variations in how transactions are recorded. Common preprocessing steps may involve cleaning the data to handle missing values or outliers, as well as normalizing bid amounts for comparative analysis. The dataset supports various types of analyses, including regression analysis to understand factors influencing bid amounts, machine learning approaches to predict auction outcomes, and descriptive statistics to summarize transaction trends. Researchers typically leverage this dataset to address research questions related to market dynamics, buyer behavior, and the overall performance of the NFT art market, making it a valuable resource for those studying the intersection of technology, economics, and digital art."
  },
  {
    "name": "Online Auctions Collection",
    "description": "Collection of datasets from eBay and experimental auctions",
    "category": "Auctions & Marketplaces",
    "url": "https://www.modelingonlineauctions.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "auctions",
      "eBay",
      "bidding"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Online Auctions Collection is a comprehensive dataset that includes various data points from eBay and experimental auctions. Researchers can utilize this dataset to analyze bidding behaviors, auction dynamics, and consumer preferences in online marketplaces.",
    "use_cases": [
      "Analyzing bidding strategies in online auctions",
      "Studying consumer behavior in e-commerce",
      "Evaluating the impact of auction design on bidding outcomes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from eBay auctions?",
      "How can I analyze bidding patterns in online auctions?",
      "What insights can be gained from experimental auction data?",
      "Where can I find datasets related to auctions and marketplaces?",
      "What variables are included in the Online Auctions Collection?",
      "How do consumer behaviors vary in online auctions?",
      "What are the common practices in analyzing auction data?",
      "What types of analyses can be performed on eBay auction data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0019,
    "embedding_text": "The Online Auctions Collection is a rich dataset that encompasses a variety of data points collected from eBay and experimental auctions, providing a valuable resource for researchers interested in the dynamics of online bidding and consumer behavior. The dataset is structured in a tabular format, with rows representing individual auction instances and columns capturing key variables such as item descriptions, starting bids, final sale prices, bidder identities, and timestamps. This structure allows for detailed analysis of auction outcomes and bidder interactions. The collection methodology involves aggregating data from eBay's public listings as well as controlled experimental auctions designed to test specific hypotheses about bidding behavior. The data sources include both real-world auction data from eBay and synthetic data generated from experimental setups, ensuring a diverse range of scenarios for analysis. While the dataset does not specify temporal or geographic coverage, it is assumed to reflect a broad spectrum of auction types and consumer demographics typical of online marketplaces. Key variables in the dataset measure aspects such as bid increments, auction duration, and bidder engagement, which can provide insights into how different factors influence auction outcomes. However, researchers should be aware of potential limitations in data quality, including missing values or inconsistencies in bidder identities, which may arise from the nature of online transactions. Common preprocessing steps may include cleaning the data to handle missing entries, normalizing bid amounts, and categorizing items based on their characteristics. The dataset supports various types of analyses, including regression modeling to predict auction outcomes, machine learning techniques to classify bidding strategies, and descriptive statistics to summarize bidding behaviors. Researchers typically use this dataset to address questions related to auction efficiency, the impact of auction design on bidding behavior, and the factors influencing consumer decisions in online marketplaces. Overall, the Online Auctions Collection serves as a foundational resource for understanding the complexities of online auctions and the economic behaviors of participants in these digital environments."
  },
  {
    "name": "WRDS (Wharton Research Data Services)",
    "description": "350+ terabytes from CRSP, Compustat, TAQ - de facto standard for academic finance",
    "category": "Dataset Aggregators",
    "url": "https://wrds-www.wharton.upenn.edu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "finance",
      "CRSP",
      "Compustat",
      "academic",
      "premium"
    ],
    "best_for": "Publication in top finance journals - requires institutional subscription",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "finance",
      "academic"
    ],
    "summary": "The WRDS (Wharton Research Data Services) dataset provides access to over 350 terabytes of financial data from sources such as CRSP, Compustat, and TAQ. Researchers and analysts can utilize this comprehensive dataset to conduct various financial analyses, including stock performance evaluation, corporate financial health assessment, and market trend analysis.",
    "use_cases": [
      "Analyzing stock market trends using CRSP data",
      "Evaluating corporate financial performance with Compustat",
      "Conducting high-frequency trading analysis with TAQ data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the WRDS dataset?",
      "How can I access CRSP data through WRDS?",
      "What types of analyses can be performed with Compustat data?",
      "What financial metrics are available in the WRDS dataset?",
      "How does WRDS support academic finance research?",
      "What are the key features of the TAQ dataset within WRDS?",
      "What is the size of the WRDS dataset?",
      "What are the main data sources for WRDS?"
    ],
    "domain_tags": [
      "finance"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0019,
    "image_url": "/images/logos/upenn.png",
    "embedding_text": "The WRDS (Wharton Research Data Services) dataset is a comprehensive repository of financial data, encompassing over 350 terabytes sourced from leading financial databases such as CRSP (Center for Research in Security Prices), Compustat, and TAQ (Trade and Quote). This dataset serves as a de facto standard for academic finance research, providing a rich foundation for various analyses in the field. The data is structured in a tabular format, consisting of numerous rows and columns that represent different financial instruments, time periods, and associated metrics. Each dataset within WRDS contains key variables that measure essential financial indicators, such as stock prices, trading volumes, corporate earnings, and other relevant financial metrics. Researchers can leverage this data to address a wide range of research questions, including the evaluation of stock market performance, the assessment of corporate financial health, and the exploration of market trends over time. The collection methodology for WRDS involves aggregating data from reputable financial sources, ensuring a high level of data quality and reliability. However, users should be aware of potential limitations, such as data gaps or discrepancies that may arise from the nature of financial reporting. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. WRDS supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, enabling researchers to derive insights from the data effectively. Researchers typically utilize WRDS in their studies to conduct empirical research, validate financial theories, and inform investment strategies, making it an invaluable resource for both academic and professional finance practitioners.",
    "benchmark_usage": [
      "Common uses include stock performance evaluation, corporate financial analysis, and market trend analysis."
    ]
  },
  {
    "name": "LaDe (Cainiao)",
    "description": "10.6M+ packages with 619K trajectories and GPS data from Alibaba logistics",
    "category": "Logistics & Supply Chain",
    "url": "https://huggingface.co/datasets/Cainiao-AI/LaDe",
    "docs_url": null,
    "github_url": "https://huggingface.co/datasets/Cainiao-AI/LaDe",
    "tags": [
      "packages",
      "trajectories",
      "Alibaba",
      "delivery"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "logistics",
      "supply-chain",
      "data-analysis"
    ],
    "summary": "The LaDe (Cainiao) dataset contains over 10.6 million packages with 619,000 trajectories and GPS data sourced from Alibaba logistics. This dataset can be utilized for analyzing delivery patterns, optimizing logistics operations, and understanding consumer behavior in the e-commerce sector.",
    "use_cases": [
      "Analyzing delivery efficiency in logistics",
      "Optimizing route planning for package delivery",
      "Studying consumer behavior based on delivery patterns",
      "Evaluating the impact of logistics on e-commerce performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LaDe (Cainiao) dataset?",
      "How can I analyze Alibaba logistics data?",
      "What insights can be gained from package trajectories?",
      "What are the key variables in the LaDe dataset?",
      "How many packages are included in the LaDe dataset?",
      "What type of data does the LaDe dataset provide?",
      "How can GPS data improve logistics analysis?",
      "What are common use cases for the LaDe dataset?"
    ],
    "domain_tags": [
      "logistics",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0017,
    "image_url": "/images/datasets/lade-cainiao.png",
    "embedding_text": "The LaDe (Cainiao) dataset is a comprehensive collection of logistics data that encompasses over 10.6 million packages, featuring 619,000 distinct trajectories and associated GPS data. This dataset is primarily structured in a tabular format, where each row represents a unique package delivery instance, and columns include variables such as package ID, trajectory coordinates, timestamps, and delivery status. The data is collected through Alibaba's logistics network, which allows for real-time tracking and monitoring of package movements across various geographical locations. While the dataset does not specify temporal or geographic coverage, it is understood that the data spans a significant period and covers a wide range of delivery locations due to Alibaba's extensive operational reach. Key variables in the dataset measure aspects such as delivery times, route efficiency, and GPS coordinates, which can be pivotal in analyzing logistics performance. Researchers may encounter data quality issues such as missing values or inconsistencies in GPS data, necessitating common preprocessing steps like data cleaning, normalization, and handling of outliers. The LaDe dataset supports a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for researchers interested in logistics optimization, consumer behavior analysis, and operational efficiency in the e-commerce sector. Typical research questions that can be addressed using this dataset include inquiries into the factors affecting delivery times, the impact of route optimization on logistics costs, and the correlation between delivery efficiency and customer satisfaction. Overall, the LaDe (Cainiao) dataset serves as a valuable asset for data scientists and researchers aiming to derive insights from large-scale logistics data, facilitating advancements in the understanding of supply chain dynamics and enhancing the operational capabilities of logistics providers."
  },
  {
    "name": "Apple App Store Dataset",
    "description": "7,200 iOS apps with pricing, ratings, genres, in-app purchases. Apple app marketplace analysis",
    "category": "App Stores",
    "url": "https://www.kaggle.com/datasets/ramamet4/app-store-apple-data-set-10k-apps",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Apple",
      "iOS",
      "apps",
      "pricing",
      "App Store"
    ],
    "best_for": "Learning app stores analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Apple App Store Dataset consists of 7,200 iOS applications, providing insights into app pricing, user ratings, genres, and in-app purchases. This dataset can be utilized for analyzing trends in the Apple app marketplace, understanding consumer behavior, and evaluating pricing strategies.",
    "use_cases": [
      "Analyzing pricing strategies of iOS apps",
      "Evaluating the impact of in-app purchases on user ratings",
      "Studying consumer behavior in app selection",
      "Identifying trends in app genres over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the pricing trends in the Apple App Store?",
      "How do user ratings correlate with app genres?",
      "What percentage of iOS apps offer in-app purchases?",
      "Which genres of apps are most popular among users?",
      "How does app pricing vary across different categories?",
      "What is the average rating of apps with in-app purchases?",
      "How many apps are available in the Apple App Store?",
      "What factors influence app ratings in the Apple App Store?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0016,
    "image_url": "/images/datasets/apple-app-store-dataset.jpg",
    "embedding_text": "The Apple App Store Dataset is a comprehensive collection of data regarding 7,200 iOS applications, meticulously designed to facilitate a myriad of analyses pertinent to the app marketplace. This dataset encompasses various dimensions of app characteristics, including pricing, user ratings, genres, and the availability of in-app purchases. The data is structured in a tabular format, featuring rows that represent individual apps and columns that capture key attributes such as app name, price, rating, genre, and in-app purchase options. Each variable within the dataset serves a specific purpose; for instance, the pricing variable allows for the examination of economic trends within the app market, while the rating variable provides insights into user satisfaction and app quality. The collection methodology for this dataset likely involved scraping data from the Apple App Store, ensuring that the information is current and reflective of the marketplace at the time of data collection. However, it is important to note that the dataset may have limitations regarding data quality, such as potential inaccuracies in user ratings or missing values for certain apps. Researchers utilizing this dataset can expect to engage in various preprocessing steps, including data cleaning to handle missing or inconsistent entries, normalization of pricing data, and categorization of apps into genres for more nuanced analyses. The dataset supports a range of research questions, such as exploring the relationship between app pricing and user ratings, identifying which app genres are most successful in terms of user engagement, and analyzing the prevalence of in-app purchases across different categories. Furthermore, the dataset is conducive to various types of analyses, including regression analysis to predict app success based on pricing and ratings, machine learning models to classify apps into genres, and descriptive statistics to summarize key trends within the dataset. Researchers typically leverage this dataset to gain insights into consumer behavior, evaluate market dynamics, and inform strategic decisions regarding app development and marketing. By analyzing the Apple App Store Dataset, stakeholders can better understand the competitive landscape of mobile applications and make data-driven decisions that enhance user engagement and profitability."
  },
  {
    "name": "Nasdaq Data Link",
    "description": "250+ datasets from 400+ publishers with API access - formerly Quandl",
    "category": "Dataset Aggregators",
    "url": "https://data.nasdaq.com",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "finance",
      "Quandl",
      "alternative data",
      "API"
    ],
    "best_for": "Financial econometrics and alternative data research with Python/R/Excel",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "finance",
      "alternative data",
      "API"
    ],
    "summary": "The Nasdaq Data Link provides access to over 250 datasets from more than 400 publishers, enabling users to retrieve financial data through an API. This platform, previously known as Quandl, allows users to perform various analyses and gain insights from a wide range of financial information.",
    "use_cases": [
      "Financial analysis using historical data",
      "Market trend analysis",
      "Comparative analysis of financial metrics",
      "Research on alternative data sources"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Nasdaq Data Link?",
      "How to access datasets from Nasdaq Data Link?",
      "What types of financial datasets are available on Nasdaq Data Link?",
      "How can I use the Nasdaq Data Link API?",
      "What are the benefits of using Nasdaq Data Link?",
      "Where can I find alternative financial data?",
      "What publishers contribute to Nasdaq Data Link?",
      "How does Nasdaq Data Link compare to other data aggregators?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0015,
    "image_url": "/images/datasets/nasdaq-data-link.jpg",
    "embedding_text": "The Nasdaq Data Link is a comprehensive platform that aggregates over 250 datasets from more than 400 publishers, offering a rich resource for financial data analysis. This dataset is structured in a tabular format, where each dataset typically consists of rows representing individual data points and columns that define various attributes or variables associated with those data points. Common variables may include stock prices, trading volumes, and other financial metrics, which are essential for conducting in-depth financial analyses. The collection methodology involves sourcing data from reputable financial institutions and publishers, ensuring a high level of data quality. However, users should be aware of potential limitations such as data gaps or inconsistencies that may arise from the diverse origins of the datasets. Researchers and analysts often preprocess the data to clean and format it appropriately, which may include handling missing values, normalizing data, or transforming variables for specific analyses. The Nasdaq Data Link supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for financial research. Typical research questions that can be addressed using this dataset include inquiries into market trends, the impact of economic indicators on stock performance, and the evaluation of alternative data sources in financial decision-making. Overall, the Nasdaq Data Link serves as a valuable asset for anyone looking to explore financial data and derive meaningful insights from it."
  },
  {
    "name": "Twitch Gamers Social Network",
    "description": "168K nodes with mutual follower relationships. 6 ML tasks including churn, affiliate status, view count prediction",
    "category": "Entertainment & Media",
    "url": "https://snap.stanford.edu/data/twitch-social-networks.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Twitch",
      "social network",
      "gaming",
      "followers",
      "SNAP"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "social networks",
      "gaming"
    ],
    "summary": "The Twitch Gamers Social Network dataset consists of 168,000 nodes representing mutual follower relationships among Twitch users. It supports various machine learning tasks such as predicting user churn, affiliate status, and view counts, making it a valuable resource for analyzing social dynamics in gaming communities.",
    "use_cases": [
      "Predicting user churn based on follower interactions",
      "Analyzing the impact of affiliate status on view counts",
      "Exploring social dynamics within the Twitch gaming community",
      "Identifying influential users based on mutual follower relationships"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Twitch Gamers Social Network dataset?",
      "How can I analyze mutual follower relationships on Twitch?",
      "What machine learning tasks can be performed with Twitch gaming data?",
      "Where can I find datasets related to Twitch and social networks?",
      "What are the key variables in the Twitch Gamers Social Network dataset?",
      "How does follower interaction influence view counts on Twitch?",
      "What insights can be gained from analyzing Twitch gamers' social networks?",
      "What is the size of the Twitch Gamers Social Network dataset?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "graph",
    "size_category": "medium",
    "model_score": 0.0015,
    "image_url": "/images/logos/stanford.png",
    "embedding_text": "The Twitch Gamers Social Network dataset is a comprehensive collection of 168,000 nodes that represent mutual follower relationships among users on the popular streaming platform, Twitch. This dataset is particularly valuable for researchers and data scientists interested in understanding the social dynamics within gaming communities and the interactions that occur among users. The data structure consists of nodes that correspond to individual Twitch users, with edges representing follower relationships, thereby forming a graph that encapsulates the social network of Twitch gamers. The dataset includes various key variables such as user IDs, follower counts, and engagement metrics, which can be utilized to measure the influence and reach of individual users within the network. Researchers can employ this dataset to address several research questions, including how follower interactions impact user engagement and view counts, as well as the factors that contribute to user churn. The dataset supports a range of analyses, including regression, machine learning, and descriptive statistics, allowing for a multifaceted exploration of the data. Common preprocessing steps may include cleaning the data to remove inactive users, normalizing follower counts, and transforming the data into a suitable format for analysis. However, it is important to note that the dataset may have limitations regarding data quality, such as the presence of bots or inactive accounts, which could skew results. Overall, the Twitch Gamers Social Network dataset serves as a rich resource for exploring the intricate relationships among Twitch users and offers insights into the broader implications of social networking in the gaming industry."
  },
  {
    "name": "ASOS Experiments",
    "description": "99 real e-commerce experiments with daily checkpoints from ASOS",
    "category": "Fashion & Apparel",
    "url": "https://www.kaggle.com/datasets/marinazmieva/asos-digital-experiments-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "A/B testing",
      "e-commerce",
      "fashion",
      "Kaggle"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The ASOS Experiments dataset contains 99 real e-commerce experiments conducted by ASOS, featuring daily checkpoints that provide insights into various aspects of online retail. Researchers can analyze the data to understand consumer behavior, test pricing strategies, and evaluate the effectiveness of A/B testing in the fashion industry.",
    "use_cases": [
      "Analyzing the impact of A/B testing on sales",
      "Evaluating consumer behavior changes over time",
      "Testing different pricing strategies",
      "Understanding the effectiveness of marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "ASOS e-commerce experiments dataset",
      "real e-commerce A/B testing data",
      "fashion retail experiments dataset",
      "daily checkpoints ASOS experiments",
      "Kaggle ASOS dataset",
      "consumer behavior in fashion e-commerce",
      "A/B testing in online retail",
      "e-commerce experiments analysis"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0014,
    "image_url": "/images/datasets/asos-experiments.png",
    "embedding_text": "The ASOS Experiments dataset is a rich resource for researchers and practitioners interested in the intersection of fashion and e-commerce. It comprises 99 real-world experiments conducted by ASOS, a leading online fashion retailer, with daily checkpoints that track various metrics over time. The dataset is structured in a tabular format, containing rows for each experiment and columns that detail key variables such as experiment ID, date of the experiment, metrics measured (e.g., conversion rates, average order value), and any specific changes implemented during the experiment (e.g., changes in website layout, pricing adjustments). This structure allows for straightforward analysis and manipulation using common data analysis tools. The collection methodology for this dataset is rooted in ASOS's commitment to data-driven decision-making, leveraging real-time data from their e-commerce platform to conduct experiments that test hypotheses about consumer behavior and marketing effectiveness. While the dataset does not specify temporal or geographic coverage, it is implied that the data reflects the dynamics of the online fashion retail environment during the period of experimentation. Key variables in the dataset measure various aspects of consumer interaction with the ASOS platform, including engagement metrics and sales performance, which are crucial for understanding the effectiveness of different marketing strategies. However, researchers should be aware of potential limitations in data quality, such as variations in external factors that could influence consumer behavior during the experiments, which may not be fully captured in the dataset. Common preprocessing steps might include cleaning the data for missing values, normalizing metrics for comparative analysis, and segmenting experiments based on specific criteria (e.g., type of change implemented). The dataset supports a variety of analyses, including regression analysis to identify relationships between changes made and consumer responses, machine learning techniques for predictive modeling, and descriptive statistics to summarize the findings of each experiment. Researchers typically use this dataset to address questions related to the effectiveness of A/B testing methodologies, the impact of pricing strategies on sales, and broader inquiries into consumer behavior in the fashion e-commerce space. Overall, the ASOS Experiments dataset serves as a valuable tool for those looking to explore the nuances of online retail and the factors that drive consumer engagement and sales performance."
  },
  {
    "name": "Real-Time Advertisers Auction",
    "description": "Real-time advertiser auction dataset for RTB research",
    "category": "Advertising",
    "url": "https://www.kaggle.com/datasets/saurav9786/real-time-advertisers-auction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "RTB",
      "auctions",
      "programmatic"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "data-analysis",
      "RTB"
    ],
    "summary": "The Real-Time Advertisers Auction dataset provides insights into the dynamics of real-time bidding (RTB) in advertising. Researchers can analyze bidding behaviors, auction outcomes, and the impact of various factors on advertising effectiveness.",
    "use_cases": [
      "Analyzing bidding strategies in real-time auctions",
      "Evaluating the effectiveness of ad placements",
      "Studying the impact of auction dynamics on advertiser behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Real-Time Advertisers Auction dataset?",
      "How can I analyze RTB data?",
      "What insights can be gained from auction data?",
      "What variables are included in the advertiser auction dataset?",
      "How does real-time bidding work in advertising?",
      "What are common use cases for RTB datasets?",
      "What research can be conducted using auction data?",
      "How to preprocess real-time auction data for analysis?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0014,
    "image_url": "/images/datasets/real-time-advertisers-auction.jpg",
    "embedding_text": "The Real-Time Advertisers Auction dataset is a comprehensive collection of data related to real-time bidding (RTB) in the advertising sector. This dataset is structured in a tabular format, consisting of rows that represent individual auction events and columns that capture various attributes of these events, such as bid amounts, advertiser IDs, timestamps, and other relevant variables. The data schema is designed to facilitate analysis of the bidding process, allowing researchers to explore patterns and trends in real-time auctions. The collection methodology typically involves aggregating data from advertising platforms that utilize RTB, ensuring a rich dataset that reflects actual market conditions. Coverage in terms of temporal and geographic dimensions is not explicitly mentioned, but the dataset is likely to encompass a variety of auction events over a specific period and across different regions, depending on the data sources used. Key variables in the dataset may include bid price, winning bid, number of bidders, and time of auction, each measuring critical aspects of the auction process. Data quality is paramount, and while the dataset is expected to be robust, researchers should be aware of potential limitations such as missing values or biases in bidding behavior. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing bid amounts for comparative analysis. This dataset supports a range of research questions, including the effectiveness of different bidding strategies, the influence of auction dynamics on outcomes, and the overall performance of advertising campaigns. Types of analyses that can be conducted include regression analysis to understand the relationships between variables, machine learning models to predict bidding outcomes, and descriptive statistics to summarize the data. Researchers typically leverage this dataset to gain insights into the competitive landscape of online advertising, optimize bidding strategies, and enhance the effectiveness of programmatic advertising campaigns."
  },
  {
    "name": "Diginetica Fashion",
    "description": "Clickstream and purchase data for fashion e-commerce",
    "category": "Fashion & Apparel",
    "url": "https://competitions.codalab.org/competitions/11161",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "clickstream",
      "competition"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "Diginetica Fashion provides clickstream and purchase data specifically for fashion e-commerce. This dataset allows researchers and analysts to explore consumer behavior patterns, analyze purchasing trends, and assess competition within the fashion industry.",
    "use_cases": [
      "Analyzing consumer purchasing behavior in fashion e-commerce",
      "Assessing the impact of marketing campaigns on online sales",
      "Comparing clickstream patterns across different fashion brands"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Diginetica Fashion dataset?",
      "How can I analyze clickstream data for e-commerce?",
      "What insights can be gained from fashion purchase data?",
      "What are the consumer behavior trends in fashion e-commerce?",
      "How does competition affect online fashion sales?",
      "What variables are included in the Diginetica Fashion dataset?",
      "How can I use clickstream data to improve online sales?",
      "What are common preprocessing steps for e-commerce datasets?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0014,
    "embedding_text": "The Diginetica Fashion dataset is a comprehensive collection of clickstream and purchase data tailored for the fashion e-commerce sector. This dataset is structured in a tabular format, consisting of rows that represent individual user interactions and purchases, while the columns include various variables that capture essential information about each interaction. Key variables may include user ID, session ID, timestamps of clicks and purchases, product IDs, categories, and other relevant attributes that help in understanding user behavior and transaction details. The data is typically collected through tracking mechanisms embedded in e-commerce platforms, capturing user interactions as they navigate through the website, along with their subsequent purchase decisions. This methodology ensures a rich dataset that reflects real-time consumer behavior in the online fashion market.\n\nWhile the dataset is robust, it is important to note potential limitations regarding data quality. Issues such as missing values, duplicate entries, or discrepancies in user identification may arise, necessitating common preprocessing steps like data cleaning, normalization, and transformation before analysis. Researchers can address a variety of research questions using this dataset, such as identifying trends in consumer purchasing behavior, evaluating the effectiveness of marketing strategies, and understanding the dynamics of competition among fashion brands. The dataset supports various types of analyses, including regression analysis, machine learning models, and descriptive statistics, allowing for a comprehensive exploration of the factors influencing online sales in the fashion industry.\n\nIn studies, researchers typically leverage the Diginetica Fashion dataset to uncover insights that can inform business strategies, enhance user experience, and optimize marketing efforts. By analyzing clickstream data, they can identify which products attract the most attention, how users navigate through the site, and what factors lead to successful conversions. This dataset serves as a valuable resource for anyone looking to delve into the intricacies of consumer behavior in the fast-paced world of fashion e-commerce."
  },
  {
    "name": "Soso (KDD Cup 2012)",
    "description": "KDD Cup 2012 Track 2 for sponsored search CTR prediction",
    "category": "Advertising",
    "url": "https://www.kaggle.com/competitions/kddcup2012-track2",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "sponsored search",
      "KDD",
      "CTR"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Soso dataset from the KDD Cup 2012 focuses on sponsored search click-through rate (CTR) prediction. It provides a rich source of data for analyzing user interactions with advertisements, enabling researchers to develop models that predict the likelihood of clicks based on various features.",
    "use_cases": [
      "Predicting click-through rates for sponsored advertisements",
      "Analyzing user behavior in response to ads",
      "Developing machine learning models for CTR prediction"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the KDD Cup 2012 Soso dataset?",
      "How can I access the Soso dataset for CTR prediction?",
      "What features are included in the Soso dataset?",
      "What analysis can be performed using the KDD Cup 2012 dataset?",
      "How does the Soso dataset relate to sponsored search advertising?",
      "What are the challenges in predicting CTR using the Soso dataset?",
      "What methodologies are used in analyzing the Soso dataset?",
      "Where can I find information on KDD Cup 2012 datasets?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0013,
    "image_url": "/images/datasets/soso-kdd-cup-2012.png",
    "embedding_text": "The Soso dataset, part of the KDD Cup 2012, is specifically designed for the task of predicting click-through rates (CTR) in the context of sponsored search advertising. This dataset comprises a structured format with rows representing individual ad impressions and columns detailing various features associated with each impression. Key variables within the dataset include user demographics, ad characteristics, and contextual information, which collectively contribute to understanding user interactions with advertisements. The collection methodology for the Soso dataset involves aggregating data from real-world sponsored search campaigns, ensuring that the dataset reflects authentic user behavior and ad performance metrics. Researchers can leverage this dataset to explore a range of research questions, such as identifying factors that influence CTR, evaluating the effectiveness of different ad placements, and developing predictive models that can enhance advertising strategies. Common preprocessing steps for the Soso dataset may include handling missing values, normalizing features, and encoding categorical variables to prepare the data for analysis. The dataset supports various types of analyses, including regression techniques to model CTR, machine learning algorithms for predictive modeling, and descriptive statistics to summarize user behavior patterns. Researchers typically utilize the Soso dataset to validate their models against established benchmarks, assess the impact of different variables on ad performance, and contribute to the broader understanding of consumer behavior in digital advertising. However, it is important to acknowledge potential limitations in data quality, such as biases in user representation or the influence of external factors that may not be captured within the dataset. Overall, the Soso dataset serves as a valuable resource for both academic and industry professionals seeking to advance their knowledge and capabilities in the field of sponsored search advertising."
  },
  {
    "name": "USS (User Satisfaction Simulation)",
    "description": "6,800 dialogues with 5-level satisfaction scale labels across multiple domains",
    "category": "AI & LLM",
    "url": "https://github.com/sunnweiwei/user-satisfaction-simulation",
    "docs_url": null,
    "github_url": "https://github.com/sunnweiwei/user-satisfaction-simulation",
    "tags": [
      "user satisfaction",
      "dialogue",
      "simulation"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The USS (User Satisfaction Simulation) dataset consists of 6,800 dialogues labeled with a 5-level satisfaction scale across various domains. This dataset can be utilized to analyze user satisfaction trends, develop dialogue systems, and enhance user experience in AI applications.",
    "use_cases": [
      "Analyzing user satisfaction trends in dialogues",
      "Developing and testing dialogue systems",
      "Enhancing user experience in AI applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the USS dataset?",
      "How can I analyze user satisfaction from dialogues?",
      "What are the key features of the USS dataset?",
      "In which domains is the USS dataset applicable?",
      "How many dialogues are included in the USS dataset?",
      "What satisfaction scale is used in the USS dataset?",
      "What kind of analyses can be performed with the USS dataset?",
      "Where can I find the USS dataset for research?"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0013,
    "image_url": "/images/datasets/uss-user-satisfaction-simulation.png",
    "embedding_text": "The USS (User Satisfaction Simulation) dataset is a comprehensive collection of 6,800 dialogues that have been meticulously labeled with a 5-level satisfaction scale across multiple domains. This dataset is particularly valuable for researchers and practitioners in the fields of artificial intelligence (AI) and large language models (LLMs), as it provides a rich source of data for analyzing user satisfaction in conversational contexts. The dialogues are structured in a tabular format, where each row represents a unique dialogue instance, and the columns include variables such as dialogue text, satisfaction labels, and possibly other contextual information relevant to the interaction. The satisfaction labels range from low to high, allowing for nuanced analysis of user sentiment and satisfaction levels. The collection methodology for the USS dataset involves simulating dialogues that reflect real-world interactions, ensuring a diverse range of scenarios and user responses. While the specific data sources and collection techniques are not detailed, it is implied that the dataset is generated through controlled simulations designed to capture various aspects of user satisfaction. Coverage in terms of temporal, geographic, and demographic factors is not explicitly mentioned, indicating that the dataset may not be limited to a specific timeframe or region, thus enhancing its applicability across different research contexts. Key variables in the dataset include the dialogue text itself and the associated satisfaction labels, which measure the user's perceived satisfaction level during the interaction. These variables are crucial for understanding user behavior and preferences in dialogue systems. However, like any dataset, the USS dataset may have limitations regarding data quality, such as potential biases in simulated dialogues or the representativeness of the satisfaction scale. Researchers utilizing this dataset may need to perform common preprocessing steps, such as text normalization, tokenization, and encoding of satisfaction labels, to prepare the data for analysis. The USS dataset supports a variety of research questions, including those aimed at understanding factors influencing user satisfaction, evaluating the effectiveness of dialogue systems, and exploring user behavior patterns in conversational AI. Types of analyses that can be conducted with this dataset include regression analysis to identify predictors of satisfaction, machine learning models to classify dialogue outcomes based on satisfaction levels, and descriptive statistics to summarize user satisfaction trends. Researchers typically leverage the USS dataset in studies focused on improving AI-driven dialogue systems, enhancing user experience, and developing more effective user satisfaction measurement tools. Overall, the USS dataset serves as a valuable resource for advancing research in user satisfaction and dialogue systems within the AI and LLM domains.",
    "domain_tags": [
      "AI",
      "LLM"
    ]
  },
  {
    "name": "ConvAI Dataset",
    "description": "4,750 human-to-bot dialogues with thumbs up/down feedback plus quality scores",
    "category": "AI & LLM",
    "url": "http://convai.io/2017/data/dataset_description.pdf",
    "docs_url": "http://convai.io/2017/data/dataset_description.pdf",
    "github_url": null,
    "tags": [
      "chatbot",
      "human feedback",
      "dialogue quality"
    ],
    "best_for": "Learning LLM evaluation, chatbot quality assessment, and dialogue systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ConvAI Dataset consists of 4,750 human-to-bot dialogues that include feedback in the form of thumbs up/down as well as quality scores. This dataset can be utilized to analyze the effectiveness of chatbot interactions and improve dialogue systems through human feedback.",
    "use_cases": [
      "Evaluating chatbot performance based on user feedback",
      "Improving dialogue systems using quality scores"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the ConvAI Dataset?",
      "How can I analyze chatbot dialogues?",
      "What feedback is included in the ConvAI Dataset?",
      "What are the quality scores in the ConvAI Dataset?",
      "How many dialogues are in the ConvAI Dataset?",
      "What types of analyses can be performed on the ConvAI Dataset?"
    ],
    "domain_tags": [
      "AI",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0013,
    "embedding_text": "The ConvAI Dataset is a comprehensive collection of 4,750 human-to-bot dialogues designed to facilitate the study and enhancement of conversational AI systems. This dataset is structured in a tabular format, where each entry represents a dialogue between a human user and a chatbot. The key variables in this dataset include the dialogue text, user feedback in the form of thumbs up or down, and quality scores that assess the effectiveness of the chatbot's responses. These quality scores provide a quantitative measure of dialogue performance, allowing researchers to identify strengths and weaknesses in chatbot interactions. The dialogues were collected through interactions with a chatbot, capturing a wide range of conversational scenarios that reflect real-world usage. While the dataset does not specify temporal or geographic coverage, it serves as a valuable resource for researchers interested in the dynamics of human-computer interaction. The data quality is generally high, but researchers should be aware of potential limitations, such as biases in user feedback or variations in dialogue context that may affect the interpretation of quality scores. Common preprocessing steps for this dataset may include cleaning the dialogue text, normalizing feedback scores, and segmenting dialogues for analysis. Researchers can leverage the ConvAI Dataset to address various research questions, such as understanding user preferences in chatbot interactions, evaluating the impact of feedback on dialogue quality, and developing machine learning models that predict user satisfaction based on dialogue characteristics. The dataset supports a range of analytical approaches, including regression analysis to explore relationships between feedback and dialogue quality, machine learning techniques for predictive modeling, and descriptive analyses to summarize dialogue characteristics. Overall, the ConvAI Dataset is a crucial tool for advancing the field of conversational AI, enabling researchers to develop more effective and user-friendly dialogue systems."
  },
  {
    "name": "Upworthy News Headlines",
    "description": "32,487 headline/image experiments on 538M assignments",
    "category": "Advertising",
    "url": "https://upworthy.natematias.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "A/B testing",
      "headlines",
      "media",
      "experimentation"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "media",
      "experimentation",
      "A/B testing"
    ],
    "summary": "The Upworthy News Headlines dataset comprises 32,487 headline/image experiments conducted on a massive scale of 538 million assignments. This dataset enables researchers and marketers to analyze the effectiveness of various headlines and images in capturing audience attention and engagement, facilitating insights into media strategies and consumer behavior.",
    "use_cases": [
      "Analyzing the effectiveness of different headlines",
      "Studying audience engagement based on image and headline combinations",
      "Evaluating the impact of A/B testing on media strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most effective headlines in the Upworthy dataset?",
      "How do images impact engagement rates in headlines?",
      "Can A/B testing results be generalized across different media?",
      "What patterns can be observed in headline performance?",
      "How does audience engagement vary by headline type?",
      "What insights can be drawn from the Upworthy headline experiments?",
      "How can this dataset inform future media campaigns?",
      "What statistical methods are best for analyzing A/B testing data?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0013,
    "image_url": "/images/datasets/upworthy-news-headlines.png",
    "embedding_text": "The Upworthy News Headlines dataset is a comprehensive collection of 32,487 headline and image experiments that were conducted on an extensive scale, involving 538 million assignments. This dataset is structured in a tabular format, where each row represents a unique experiment, and the columns include key variables such as headline text, image identifiers, engagement metrics, and other relevant attributes. The primary goal of this dataset is to facilitate the analysis of how different headlines and images perform in terms of audience engagement, which is crucial for media outlets and marketers aiming to optimize their content strategies. The collection methodology likely involved systematic A/B testing, where variations of headlines and images were presented to different segments of the audience to measure their effectiveness in real-time. This approach ensures that the data reflects actual user interactions and preferences, providing a rich ground for analysis. While the dataset does not specify temporal or geographic coverage, it is inferred that the experiments were conducted over a significant period and possibly across various demographics, given the scale of the assignments. Key variables in this dataset include the headline text, which measures the wording and phrasing effectiveness, image identifiers that link to visual content, and engagement metrics such as click-through rates and shares, which quantify audience interaction. Researchers utilizing this dataset can address various research questions, such as identifying which types of headlines yield the highest engagement, understanding the interplay between visual content and textual headlines, and exploring the broader implications of A/B testing in media. Common preprocessing steps may involve cleaning the text data, normalizing engagement metrics, and categorizing headlines based on themes or styles for more granular analysis. The dataset supports a range of analytical methods, including regression analysis to predict engagement based on headline characteristics, machine learning techniques for classification of successful headlines, and descriptive statistics to summarize overall performance trends. Researchers typically use this dataset to derive insights that can inform future media campaigns, enhance content creation strategies, and contribute to the broader understanding of consumer behavior in digital media contexts."
  },
  {
    "name": "Criteo Uplift Prediction Dataset",
    "description": "~25 million rows with treatment indicators for benchmarking Individual Treatment Effect (ITE) estimation in advertising",
    "category": "Causal Inference",
    "url": "https://ailab.criteo.com/criteo-uplift-prediction-dataset/",
    "docs_url": "https://ailab.criteo.com/criteo-uplift-prediction-dataset/",
    "github_url": null,
    "tags": [
      "uplift modeling",
      "causal inference",
      "ITE",
      "Criteo"
    ],
    "best_for": "Benchmarking uplift and treatment effect models for advertising",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Criteo Uplift Prediction Dataset contains approximately 25 million rows, providing treatment indicators essential for benchmarking Individual Treatment Effect (ITE) estimation in the advertising domain. Researchers and practitioners can utilize this dataset to analyze the effectiveness of advertising strategies and improve decision-making in marketing campaigns.",
    "use_cases": [
      "Estimating the impact of advertising on consumer behavior",
      "Benchmarking uplift models for marketing strategies",
      "Analyzing treatment effects in advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Uplift Prediction Dataset?",
      "How can I use the Criteo dataset for uplift modeling?",
      "What are treatment indicators in advertising?",
      "What is Individual Treatment Effect estimation?",
      "How does uplift modeling apply to e-commerce?",
      "What analysis can be performed with the Criteo dataset?",
      "What are the key variables in the Criteo Uplift Prediction Dataset?",
      "What are common preprocessing steps for this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "30 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0012,
    "image_url": "/images/logos/criteo.png",
    "embedding_text": "The Criteo Uplift Prediction Dataset is a comprehensive resource for researchers and practitioners interested in causal inference and uplift modeling within the advertising sector. Comprising approximately 25 million rows, the dataset is structured in a tabular format, featuring a variety of columns that include treatment indicators, which are crucial for estimating Individual Treatment Effects (ITE). This dataset serves as a benchmark for assessing the effectiveness of different advertising strategies, allowing for a nuanced understanding of how various treatments influence consumer behavior. The collection methodology for this dataset involves aggregating data from Criteo's extensive advertising platform, which captures user interactions and treatment assignments in real-time. As such, the dataset is rich in detail, providing a robust foundation for analysis. Key variables within the dataset include treatment indicators that denote whether a user was exposed to a particular advertisement, as well as outcome measures that reflect user engagement and conversion rates. These variables are essential for conducting analyses aimed at understanding the causal impacts of advertising interventions. However, researchers should be aware of potential limitations regarding data quality, including issues related to missing values or biases in treatment assignment, which may affect the validity of the results. Common preprocessing steps for utilizing this dataset typically involve cleaning the data, handling missing values, and transforming variables to suit specific analytical needs. The dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, making it versatile for different research questions. Researchers often leverage this dataset to explore questions such as the effectiveness of specific advertising strategies, the differential impact of treatments across various consumer segments, and the overall return on investment for advertising campaigns. By utilizing the Criteo Uplift Prediction Dataset, analysts can gain valuable insights into consumer behavior and optimize marketing strategies based on empirical evidence.",
    "benchmark_usage": [
      "Benchmarking Individual Treatment Effect (ITE) estimation"
    ]
  },
  {
    "name": "Criteo Kaggle CTR Dataset",
    "description": "Standard CTR prediction benchmark with ~45 million records across 7 days, widely used for model comparison",
    "category": "Advertising",
    "url": "https://www.kaggle.com/c/criteo-display-ad-challenge/data",
    "docs_url": "https://www.kaggle.com/c/criteo-display-ad-challenge",
    "github_url": null,
    "tags": [
      "CTR prediction",
      "benchmark",
      "Kaggle",
      "Criteo"
    ],
    "best_for": "Benchmarking CTR prediction models against standard baseline",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "machine-learning",
      "data-analysis"
    ],
    "summary": "The Criteo Kaggle CTR Dataset is a standard benchmark for click-through rate (CTR) prediction, containing approximately 45 million records collected over a span of 7 days. Researchers and data scientists can utilize this dataset to compare various models and algorithms for predicting user engagement in online advertising.",
    "use_cases": [
      "Comparing different machine learning models for CTR prediction",
      "Analyzing user engagement patterns in online advertising",
      "Evaluating the effectiveness of advertising strategies",
      "Benchmarking algorithms for click prediction"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Criteo Kaggle CTR Dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What are the features in the Criteo Kaggle dataset?",
      "Where can I find the Criteo CTR dataset?",
      "What is the size of the Criteo Kaggle dataset?",
      "How is the Criteo dataset structured?",
      "What are common benchmarks for CTR prediction?",
      "What models can be tested on the Criteo dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "7 days",
    "geographic_scope": "Global",
    "size_category": "massive",
    "benchmark_usage": [
      "Model comparison for click-through rate prediction"
    ],
    "model_score": 0.0012,
    "image_url": "/images/datasets/criteo-kaggle-ctr-dataset.png",
    "embedding_text": "The Criteo Kaggle CTR Dataset serves as a pivotal resource for researchers and practitioners in the field of online advertising and machine learning. This dataset comprises approximately 45 million records, meticulously collected over a period of 7 days, making it a substantial benchmark for click-through rate (CTR) prediction tasks. The dataset is structured in a tabular format, featuring a variety of columns that represent different variables relevant to user interactions with online advertisements. Key variables typically include user identifiers, ad identifiers, timestamps, and various categorical features that capture user behavior and ad characteristics. The collection methodology involves aggregating data from real-world user interactions, which provides a rich context for analyzing engagement patterns and predicting future clicks. However, researchers should be aware of potential data quality issues, such as missing values or biases inherent in user behavior, which may affect the accuracy of predictive models. Common preprocessing steps include handling categorical variables through encoding techniques, normalizing numerical features, and addressing any missing data points to ensure robust model training. This dataset is particularly useful for addressing research questions related to user engagement, such as identifying factors that influence click behavior or evaluating the effectiveness of different advertising strategies. It supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, allowing researchers to explore the intricacies of user interactions in the digital advertising landscape. The Criteo dataset is widely recognized for its role in facilitating model comparisons, enabling data scientists to benchmark their algorithms against established standards in CTR prediction. As such, it has become a go-to resource for both academic research and practical applications in the advertising industry."
  },
  {
    "name": "Fliggy Travel",
    "description": "Travel-related data from Alibaba's online travel platform",
    "category": "Travel & Hospitality",
    "url": "https://tianchi.aliyun.com/dataset/113649",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "travel",
      "Alibaba",
      "bookings"
    ],
    "best_for": "Learning travel & hospitality analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "travel"
    ],
    "summary": "Fliggy Travel dataset contains travel-related data sourced from Alibaba's online travel platform. This dataset can be utilized to analyze travel booking trends, consumer preferences, and pricing strategies within the travel and hospitality sector.",
    "use_cases": [
      "Analyzing booking trends over time",
      "Understanding consumer preferences in travel",
      "Evaluating pricing strategies for travel packages"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available in the Fliggy Travel dataset?",
      "How can I analyze travel booking trends using Fliggy data?",
      "What consumer behavior insights can be derived from Alibaba's travel platform?",
      "What are the pricing strategies in the travel industry according to Fliggy?",
      "How does Fliggy Travel data help in understanding travel preferences?",
      "What types of analyses can be performed on travel-related data from Alibaba?"
    ],
    "domain_tags": [
      "travel",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0011,
    "embedding_text": "The Fliggy Travel dataset is a comprehensive collection of travel-related data derived from Alibaba's online travel platform, Fliggy. This dataset is structured in a tabular format, consisting of various rows and columns that capture essential variables related to travel bookings. Each row typically represents an individual booking or transaction, while the columns encompass key variables such as booking dates, travel destinations, customer demographics, pricing details, and other relevant attributes. The data collection methodology employed by Alibaba involves aggregating user interactions, bookings, and transactions on their platform, ensuring a rich dataset that reflects real-world travel behaviors and trends. However, specific details regarding the exact collection methodology and data sources remain proprietary to Alibaba. The dataset is expected to cover a range of travel-related aspects, although explicit temporal and geographic coverage is not provided. Key variables within the dataset measure various dimensions of travel behavior, including booking frequency, customer preferences, and pricing variations across different travel packages. Researchers utilizing this dataset may encounter certain limitations, such as potential biases in user-generated data and the need for careful preprocessing to address missing values or outliers. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. The Fliggy Travel dataset supports a variety of research questions, including inquiries into travel booking trends, consumer preferences, and the effectiveness of pricing strategies. Analysts can employ various types of analyses, such as regression analysis, machine learning techniques, and descriptive statistics, to extract insights from the data. Researchers typically leverage this dataset to conduct studies that explore the dynamics of the travel industry, assess consumer behavior patterns, and evaluate the impact of pricing strategies on booking decisions. Overall, the Fliggy Travel dataset serves as a valuable resource for understanding the intricacies of travel and hospitality, providing a foundation for data-driven decision-making in the e-commerce travel sector."
  },
  {
    "name": "AXA Driver Telematics (Kaggle)",
    "description": "Driving behavior dataset with 50K driver trips characterized by second-by-second GPS coordinates for usage-based insurance",
    "category": "Insurance & Actuarial",
    "url": "https://www.kaggle.com/c/axa-driver-telematics-analysis",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "telematics",
      "UBI",
      "driving-behavior",
      "auto-insurance",
      "GPS-data"
    ],
    "best_for": "Usage-based insurance pricing, driver risk scoring, and behavior analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "insurance",
      "driving-behavior",
      "data-analysis"
    ],
    "summary": "The AXA Driver Telematics dataset contains detailed information on driving behavior, featuring 50,000 trips characterized by second-by-second GPS coordinates. This dataset can be utilized for analyzing driving patterns, developing usage-based insurance models, and enhancing road safety initiatives.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the AXA Driver Telematics dataset?",
      "How can I analyze driving behavior using telematics data?",
      "What insights can be gained from GPS data in insurance?",
      "What are the characteristics of the AXA Driver Telematics dataset?",
      "How does usage-based insurance work with driving data?",
      "What variables are included in the AXA Driver Telematics dataset?",
      "How can telematics data improve auto insurance models?",
      "What are common use cases for driving behavior datasets?"
    ],
    "use_cases": [
      "Analyzing the correlation between driving behavior and accident rates",
      "Developing predictive models for usage-based insurance pricing",
      "Identifying risky driving patterns to enhance safety programs",
      "Evaluating the effectiveness of telematics in reducing insurance claims"
    ],
    "domain_tags": [
      "insurance",
      "automotive"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0011,
    "image_url": "/images/datasets/axa-driver-telematics-kaggle.png",
    "embedding_text": "The AXA Driver Telematics dataset is a comprehensive collection of driving behavior data, consisting of 50,000 trips recorded with second-by-second GPS coordinates. This dataset is particularly valuable for researchers and practitioners in the insurance and automotive sectors, as it provides insights into driving patterns that can inform usage-based insurance (UBI) models. The data structure is organized in a tabular format, where each row represents a unique trip, and columns include variables such as trip duration, distance traveled, speed, acceleration, and GPS coordinates. These variables are crucial for understanding various aspects of driving behavior, including speed patterns, braking habits, and route choices. The collection methodology for this dataset likely involves the use of telematics devices installed in vehicles, which continuously capture and transmit data related to driving activities. While the dataset does not specify temporal or geographic coverage, it is assumed that the data encompasses a diverse range of driving scenarios across different locations. Key variables in the dataset measure critical aspects of driving behavior, such as average speed, frequency of hard braking, and overall trip duration, which can be instrumental in assessing risk profiles for insurance purposes. However, researchers should be aware of potential limitations in data quality, including issues related to GPS accuracy, missing data points, and variations in driving conditions that may not be captured. Common preprocessing steps for this dataset may include cleaning the data to handle missing values, normalizing speed and distance measurements, and segmenting trips into meaningful intervals for analysis. Researchers can address various research questions using this dataset, such as examining the relationship between driving behavior and accident likelihood, evaluating the impact of telematics on insurance premiums, and identifying trends in driver behavior over time. The dataset supports a range of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, making it a versatile resource for studies focused on driving behavior and insurance. Overall, the AXA Driver Telematics dataset serves as a foundational tool for advancing knowledge in the fields of insurance and automotive safety, enabling data-driven decision-making and innovative solutions in usage-based insurance."
  },
  {
    "name": "CSIS Significant Cyber Incidents",
    "description": "Curated list of major cyber attacks with losses exceeding $1 million, maintained by leading security think tank",
    "category": "Cybersecurity",
    "url": "https://www.csis.org/programs/strategic-technologies-program/significant-cyber-incidents",
    "docs_url": "https://www.csis.org/programs/strategic-technologies-program/significant-cyber-incidents",
    "github_url": null,
    "tags": [
      "cyber incidents",
      "major attacks",
      "economic impact"
    ],
    "best_for": "Analyzing high-impact cyber events for economic research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The CSIS Significant Cyber Incidents dataset is a curated list of major cyber attacks that have resulted in financial losses exceeding $1 million. This dataset can be utilized to analyze trends in cyber incidents, assess their economic impact, and inform security strategies.",
    "use_cases": [
      "Analyzing the frequency and types of cyber attacks over time.",
      "Assessing the economic impact of cyber incidents on various sectors.",
      "Identifying patterns in major cyber attacks to improve security measures.",
      "Conducting research on the effectiveness of cybersecurity policies."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the significant cyber incidents listed by CSIS?",
      "How many cyber attacks have resulted in losses over $1 million?",
      "What trends can be observed in major cyber attacks?",
      "What economic impacts are associated with significant cyber incidents?",
      "Who maintains the CSIS Significant Cyber Incidents dataset?",
      "What types of cyber incidents are included in the CSIS dataset?",
      "How can I access the CSIS Significant Cyber Incidents data?",
      "What is the significance of the CSIS dataset in cybersecurity research?"
    ],
    "domain_tags": [
      "cybersecurity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2006-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0011,
    "image_url": "/images/datasets/csis-significant-cyber-incidents.jpeg",
    "embedding_text": "The CSIS Significant Cyber Incidents dataset provides a comprehensive overview of major cyber attacks that have led to financial losses exceeding $1 million. This dataset is maintained by the Center for Strategic and International Studies (CSIS), a leading security think tank, and serves as a critical resource for researchers, policymakers, and cybersecurity professionals. The data structure typically includes rows representing individual cyber incidents, with columns detailing key variables such as the date of the incident, the type of attack, the financial losses incurred, and the entities affected. The collection methodology involves gathering information from various credible sources, including news articles, government reports, and industry publications, ensuring a robust and reliable dataset. While the dataset does not specify temporal or geographic coverage, it encompasses a wide range of incidents from different regions and time periods, reflecting the evolving landscape of cyber threats. Key variables in the dataset measure aspects such as the nature of the attack (e.g., ransomware, data breach), the financial impact, and the sectors affected, providing valuable insights into the trends and patterns of cyber incidents. Data quality is generally high, though researchers should be aware of potential limitations related to underreporting of incidents and varying definitions of what constitutes a significant cyber attack. Common preprocessing steps may include cleaning the data for consistency, categorizing incidents by type, and aggregating financial losses for comparative analysis. Researchers can leverage this dataset to address critical questions regarding the frequency and impact of cyber attacks, the effectiveness of cybersecurity measures, and the economic implications for affected industries. The dataset supports various types of analyses, including regression analysis to identify factors contributing to cyber incidents, machine learning models for predictive analytics, and descriptive statistics to summarize the data. In summary, the CSIS Significant Cyber Incidents dataset is an invaluable tool for understanding the complexities of cyber threats and their economic ramifications, making it essential for advancing research and informing cybersecurity practices."
  },
  {
    "name": "Alibaba Ads Dataset",
    "description": "Advertising dataset from Alibaba for ad targeting and prediction",
    "category": "Advertising",
    "url": "https://tianchi.aliyun.com/dataset/148347",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "targeting",
      "Alibaba"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "advertising"
    ],
    "summary": "The Alibaba Ads Dataset is a comprehensive collection of advertising data from Alibaba, designed for ad targeting and prediction. This dataset allows researchers and practitioners to analyze advertising effectiveness, optimize targeting strategies, and predict consumer behavior based on ad interactions.",
    "use_cases": [
      "Analyzing the effectiveness of different advertising strategies",
      "Predicting consumer responses to targeted ads",
      "Optimizing ad placements based on historical performance",
      "Evaluating the impact of advertising on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Ads Dataset?",
      "How can I use the Alibaba Ads Dataset for ad targeting?",
      "What insights can be gained from the Alibaba Ads Dataset?",
      "Where can I find the Alibaba Ads Dataset?",
      "What variables are included in the Alibaba Ads Dataset?",
      "How does the Alibaba Ads Dataset support advertising research?",
      "What are the limitations of the Alibaba Ads Dataset?",
      "Can I use the Alibaba Ads Dataset for machine learning?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0011,
    "embedding_text": "The Alibaba Ads Dataset is a structured collection of advertising data specifically designed for the purpose of ad targeting and prediction. This dataset typically consists of rows representing individual ad impressions or interactions, with columns capturing various attributes related to the ads, such as ad ID, user demographics, ad content, click-through rates, and conversion metrics. The data structure is conducive to a variety of analyses, enabling researchers to explore the relationships between different variables and their impact on advertising outcomes. The collection methodology for the dataset likely involves aggregating data from Alibaba's advertising platform, where user interactions with ads are logged in real-time. This may include data from various sources such as user profiles, ad performance metrics, and transaction histories, providing a rich context for understanding consumer behavior in response to advertising efforts. Coverage in terms of temporal and geographic dimensions is not explicitly mentioned, but it can be inferred that the dataset encompasses a range of advertising campaigns over a specific period, potentially reflecting seasonal trends and promotional events. Key variables within the dataset may include user demographics (age, gender, location), ad performance indicators (impressions, clicks, conversions), and contextual information about the ads themselves (type of ad, placement, targeting criteria). These variables are essential for measuring the effectiveness of advertising strategies and understanding consumer engagement. However, like any dataset, the Alibaba Ads Dataset may have limitations, such as potential biases in user behavior, incomplete data entries, or variations in data collection practices across different campaigns. Common preprocessing steps that researchers might undertake include cleaning the data to handle missing values, normalizing variables for consistency, and encoding categorical variables for use in machine learning models. The dataset supports a variety of research questions, including inquiries into the effectiveness of different ad formats, the influence of demographic factors on ad engagement, and the overall return on investment for advertising spend. Additionally, it enables various types of analyses, including regression analysis to identify predictors of ad success, machine learning techniques for predictive modeling, and descriptive statistics to summarize ad performance trends. Researchers typically leverage the Alibaba Ads Dataset in studies focused on advertising effectiveness, consumer behavior analysis, and the optimization of marketing strategies, making it a valuable resource for both academic and industry applications."
  },
  {
    "name": "Outbrain Click Prediction Dataset",
    "description": "Content recommendation dataset with 2 billion page views and user engagement data from Outbrain",
    "category": "Advertising",
    "url": "https://www.kaggle.com/c/outbrain-click-prediction/data",
    "docs_url": "https://www.kaggle.com/c/outbrain-click-prediction",
    "github_url": null,
    "tags": [
      "content recommendation",
      "native advertising",
      "Outbrain",
      "engagement"
    ],
    "best_for": "Native advertising and content recommendation CTR prediction",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "user engagement",
      "data analysis"
    ],
    "summary": "The Outbrain Click Prediction Dataset is a comprehensive content recommendation dataset that includes 2 billion page views and user engagement data from Outbrain. Researchers and data scientists can utilize this dataset to analyze user behavior, improve recommendation algorithms, and study the effectiveness of native advertising strategies.",
    "use_cases": [
      "Analyzing user engagement trends over time",
      "Developing and testing recommendation algorithms",
      "Evaluating the performance of native advertising campaigns",
      "Conducting A/B testing on content recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Outbrain Click Prediction Dataset?",
      "How can I access the Outbrain Click Prediction Dataset?",
      "What types of analyses can be performed with the Outbrain dataset?",
      "What are the key features of the Outbrain Click Prediction Dataset?",
      "How does user engagement data in the Outbrain dataset work?",
      "What insights can be gained from analyzing the Outbrain Click Prediction Dataset?",
      "What are the limitations of the Outbrain Click Prediction Dataset?",
      "How is the Outbrain Click Prediction Dataset structured?"
    ],
    "domain_tags": [
      "advertising",
      "digital marketing"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "14 days",
    "geographic_scope": "Global",
    "size_category": "massive",
    "model_score": 0.0011,
    "image_url": "/images/datasets/outbrain-click-prediction-dataset.png",
    "embedding_text": "The Outbrain Click Prediction Dataset is a large-scale dataset that captures user interactions with content recommendations on the Outbrain platform. It consists of 2 billion page views, providing a rich source of data for understanding user engagement and behavior in the context of digital advertising. The dataset is structured in a tabular format, with rows representing individual page views and columns capturing various attributes related to each interaction. Key variables may include user ID, content ID, timestamp of the interaction, and engagement metrics such as clicks and impressions. This dataset is particularly valuable for researchers and data scientists interested in the fields of advertising and user engagement, as it allows for in-depth analysis of how users interact with recommended content. The collection methodology involves aggregating data from real user interactions on the Outbrain platform, ensuring that the dataset reflects actual user behavior. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user engagement patterns. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. The Outbrain Click Prediction Dataset supports a variety of analytical approaches, including regression analysis, machine learning, and descriptive statistics. Researchers can leverage this dataset to address critical research questions related to user engagement, content effectiveness, and the optimization of recommendation systems. By analyzing the data, they can uncover insights into user preferences, improve the accuracy of recommendation algorithms, and evaluate the impact of advertising strategies on user behavior. Overall, the Outbrain Click Prediction Dataset serves as a powerful resource for advancing the understanding of digital marketing and user engagement in the advertising domain."
  },
  {
    "name": "Alpha Vantage",
    "description": "NASDAQ-licensed stock data for 200,000+ tickers with free tier (25 requests/day)",
    "category": "Dataset Aggregators",
    "url": "https://www.alphavantage.co",
    "docs_url": "https://www.alphavantage.co/documentation/",
    "github_url": null,
    "tags": [
      "stocks",
      "free",
      "API",
      "technical indicators"
    ],
    "best_for": "Individual researchers needing free stock data with 50+ technical indicators",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "finance",
      "investing",
      "data analysis"
    ],
    "summary": "Alpha Vantage provides NASDAQ-licensed stock data for over 200,000 tickers, allowing users to access a wide range of financial information through a free tier of 25 requests per day. This dataset is ideal for those interested in analyzing stock performance, technical indicators, and market trends.",
    "use_cases": [
      "Analyzing stock price trends over time",
      "Comparing performance of different stocks",
      "Building predictive models for stock prices",
      "Evaluating technical indicators for trading strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Alpha Vantage stock data?",
      "How to access NASDAQ stock data?",
      "What are the features of Alpha Vantage API?",
      "How to analyze stock data using Alpha Vantage?",
      "What technical indicators can I get from Alpha Vantage?",
      "How to make 25 requests per day with Alpha Vantage?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.001,
    "embedding_text": "Alpha Vantage is a comprehensive dataset that provides licensed stock data from NASDAQ for over 200,000 tickers, making it a valuable resource for financial analysts, data scientists, and researchers interested in the stock market. The dataset is structured in a tabular format, where each row corresponds to a specific stock ticker and contains various columns representing key financial metrics and indicators. These metrics include stock prices, trading volumes, and technical indicators, which are essential for conducting thorough analyses of stock performance and market trends. The collection methodology for Alpha Vantage involves aggregating data from multiple reliable financial sources, ensuring that the information provided is accurate and up-to-date. Users can access this data through a free tier API, which allows for up to 25 requests per day, making it accessible for both casual users and more serious analysts. The key variables within the dataset measure various aspects of stock performance, including open, high, low, and close prices, as well as trading volume and various technical indicators such as moving averages and relative strength index (RSI). While the dataset is robust, users should be aware of potential limitations, such as data latency and the possibility of missing data points during high volatility periods. Common preprocessing steps may include cleaning the data, handling missing values, and normalizing the data for analysis. Researchers can leverage Alpha Vantage to address a variety of research questions, such as identifying trends in stock prices, evaluating the effectiveness of trading strategies, and exploring correlations between different stocks. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for financial research. Typically, researchers use Alpha Vantage in studies focused on market behavior, investment strategies, and the impact of economic events on stock prices, thereby contributing to a deeper understanding of financial markets.",
    "benchmark_usage": [
      "Technical analysis",
      "Stock performance evaluation"
    ]
  },
  {
    "name": "CoAID COVID Misinformation",
    "description": "4,251 news articles and 296K claims about COVID-19 healthcare misinformation. Fact-checked with ground truth labels",
    "category": "Content Moderation",
    "url": "https://github.com/cuilimeng/CoAID",
    "docs_url": null,
    "github_url": "https://github.com/cuilimeng/CoAID",
    "tags": [
      "COVID-19",
      "misinformation",
      "fact-checking",
      "healthcare",
      "fake news"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "misinformation",
      "fact-checking"
    ],
    "summary": "The CoAID COVID Misinformation dataset consists of 4,251 news articles and 296,000 claims related to COVID-19 healthcare misinformation. It provides fact-checked information with ground truth labels, making it a valuable resource for analyzing misinformation in the healthcare sector.",
    "use_cases": [
      "Analyzing the prevalence of COVID-19 misinformation in news articles.",
      "Evaluating the effectiveness of fact-checking methods on healthcare misinformation.",
      "Studying the impact of misinformation on public health responses.",
      "Developing machine learning models to detect misinformation in real-time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the CoAID COVID Misinformation dataset?",
      "How many claims about COVID-19 misinformation are included in the dataset?",
      "What types of articles are contained in the CoAID dataset?",
      "How is the misinformation in the dataset fact-checked?",
      "What are the main topics covered in the CoAID COVID Misinformation dataset?",
      "Can I use the CoAID dataset for machine learning analysis?",
      "What are the key variables in the CoAID COVID Misinformation dataset?",
      "How can researchers utilize the CoAID dataset in their studies?"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/datasets/coaid-covid-misinformation.png",
    "embedding_text": "The CoAID COVID Misinformation dataset is a comprehensive collection of 4,251 news articles and 296,000 claims that focus on misinformation related to COVID-19 healthcare. This dataset is particularly valuable for researchers and practitioners interested in the dynamics of misinformation, especially during a global health crisis. The structure of the dataset includes various columns that categorize the articles and claims, providing a rich schema for analysis. Each article is associated with claims that have been fact-checked, offering ground truth labels that enhance the reliability of the data. The collection methodology involves sourcing articles from various media outlets, ensuring a diverse representation of perspectives on COVID-19 misinformation. The dataset's coverage is primarily focused on the COVID-19 pandemic, and while it does not specify temporal or geographic limitations, it reflects the information landscape during the pandemic period. Key variables in the dataset include the article text, claim details, and fact-checking outcomes, which measure the accuracy of the claims made in the articles. Researchers should be aware of potential limitations in data quality, such as biases in the sources of articles or the subjective nature of fact-checking. Common preprocessing steps may include text normalization, removal of irrelevant content, and categorization of claims for analysis. This dataset supports a variety of research questions, including the analysis of misinformation trends, the effectiveness of fact-checking interventions, and the relationship between misinformation and public health behaviors. It is suitable for various types of analyses, including regression, machine learning, and descriptive statistics. Researchers typically use the CoAID dataset to inform studies on misinformation's impact on public perception and behavior, contributing to the broader understanding of how misinformation affects health communication and policy."
  },
  {
    "name": "CAISO OASIS",
    "description": "California ISO market data including prices, generation, demand, and transmission",
    "category": "Energy",
    "url": "http://oasis.caiso.com/",
    "docs_url": "http://www.caiso.com/market/Pages/MarketProcesses.aspx",
    "github_url": null,
    "tags": [
      "California",
      "prices",
      "wholesale",
      "real-time",
      "day-ahead"
    ],
    "best_for": "Studying wholesale electricity market dynamics and renewable integration",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "market-analysis",
      "pricing"
    ],
    "summary": "The CAISO OASIS dataset provides comprehensive market data from the California Independent System Operator, including critical information on prices, generation, demand, and transmission. Researchers and analysts can utilize this dataset to study market dynamics, assess energy pricing trends, and evaluate the performance of the energy grid.",
    "use_cases": [
      "Analyzing price trends in California's energy market.",
      "Evaluating the impact of renewable energy generation on demand.",
      "Studying transmission efficiency and bottlenecks in the grid.",
      "Forecasting future energy prices based on historical data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the CAISO OASIS dataset?",
      "How can I access California ISO market data?",
      "What types of data are included in the CAISO OASIS dataset?",
      "What are the key variables in the CAISO OASIS dataset?",
      "How does California ISO pricing work?",
      "What insights can be gained from CAISO OASIS data?",
      "How to analyze energy generation and demand in California?",
      "What are the applications of CAISO OASIS market data?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2009-present",
    "geographic_scope": "California",
    "size_category": "medium",
    "model_score": 0.001,
    "embedding_text": "The CAISO OASIS dataset is a vital resource for understanding the dynamics of the California energy market. It encompasses a wide array of data points, including prices, generation metrics, demand figures, and transmission statistics, all of which are crucial for analyzing the performance and efficiency of the energy grid. The dataset is structured in a tabular format, typically consisting of rows representing individual data entries and columns that include variables such as time stamps, price levels, generation capacity, demand metrics, and transmission data. This structured approach allows for straightforward analysis and manipulation using data analysis tools. The data is collected by the California Independent System Operator, which monitors and manages the state's electricity grid, ensuring that supply meets demand while maintaining grid reliability. The methodology for data collection involves real-time monitoring and reporting from various energy producers and consumers across California, providing a comprehensive view of the market landscape. While the dataset is robust, it may have limitations related to data quality, such as missing entries or inconsistencies that can arise from the complexities of energy production and consumption. Researchers often need to perform common preprocessing steps, including data cleaning, normalization, and transformation, to prepare the dataset for analysis. The CAISO OASIS dataset supports a variety of research questions, such as understanding the factors influencing energy prices, assessing the impact of renewable energy sources on traditional generation methods, and evaluating the effectiveness of demand response programs. Analysts can employ various types of analyses, including regression analysis, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically use this dataset in studies focused on energy economics, policy analysis, and market forecasting, making it an essential tool for anyone interested in the energy sector."
  },
  {
    "name": "Stanford Amazon/Beer",
    "description": "Amazon product data and BeerAdvocate reviews from Stanford SNAP",
    "category": "Entertainment & Media",
    "url": "https://snap.stanford.edu/data/#amazon",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "Stanford",
      "beer",
      "Amazon"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Stanford Amazon/Beer dataset combines Amazon product data with BeerAdvocate reviews, providing a rich resource for analyzing consumer preferences and product evaluations in the beer market. Researchers can utilize this dataset to explore trends in product ratings, review sentiments, and the relationship between product features and consumer choices.",
    "use_cases": [
      "Analyzing consumer sentiment in beer reviews",
      "Investigating the impact of product features on ratings",
      "Exploring trends in beer preferences over time",
      "Comparing Amazon ratings with BeerAdvocate reviews"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Stanford Amazon/Beer dataset?",
      "How can I access the Amazon product data and BeerAdvocate reviews?",
      "What insights can be gained from analyzing beer reviews on Amazon?",
      "Are there any studies using the Stanford Amazon/Beer dataset?",
      "What variables are included in the Stanford Amazon/Beer dataset?",
      "How does consumer behavior vary in beer reviews on Amazon?",
      "What are the key features of the Stanford Amazon/Beer dataset?",
      "Can I use the Stanford Amazon/Beer dataset for machine learning projects?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/logos/stanford.png",
    "embedding_text": "The Stanford Amazon/Beer dataset is a comprehensive collection of Amazon product data paired with reviews from BeerAdvocate, curated by Stanford SNAP. This dataset serves as a valuable resource for researchers interested in the intersection of e-commerce and consumer behavior, particularly within the beer market. The dataset is structured in a tabular format, comprising various rows and columns that encapsulate key variables such as product identifiers, review scores, review text, and metadata about the products. Each entry in the dataset represents a unique product review, allowing for detailed analysis of consumer opinions and preferences. The collection methodology involves aggregating data from two prominent sources: Amazon, a leading e-commerce platform, and BeerAdvocate, a well-known beer review site. This dual-source approach enriches the dataset, providing a multifaceted view of consumer interactions with beer products. Researchers can leverage this dataset to address a variety of research questions, such as examining the correlation between product features and consumer ratings, analyzing sentiment in reviews, and identifying trends in beer consumption patterns. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it suitable for both exploratory and confirmatory research. However, potential users should be aware of the data quality and limitations, including the possibility of biased reviews and the varying levels of detail in product descriptions. Common preprocessing steps may include text normalization for review analysis, handling missing values, and feature extraction from product metadata. Overall, the Stanford Amazon/Beer dataset provides a rich foundation for academic inquiry and practical applications in understanding consumer behavior in the beer market."
  },
  {
    "name": "Romania Tenders",
    "description": "Public tender data (2007-2016) from Romania",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/gpreda/public-tenders-romania-20072016",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "tenders",
      "government",
      "Romania"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "government",
      "public procurement",
      "data analysis"
    ],
    "summary": "The Romania Tenders dataset contains public tender data from Romania spanning the years 2007 to 2016. This dataset can be utilized for analyzing government procurement processes, understanding market dynamics, and evaluating the competitiveness of tenders in various sectors.",
    "use_cases": [
      "Analyzing trends in government procurement over time.",
      "Evaluating the competitiveness of bids in various sectors.",
      "Studying the impact of economic conditions on tender values.",
      "Identifying patterns in tender submissions and awards."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What public tenders were issued in Romania from 2007 to 2016?",
      "How can I analyze government procurement trends in Romania?",
      "What are the common characteristics of tenders in Romania?",
      "How do tender values vary across different sectors in Romania?",
      "What is the frequency of tenders issued by the Romanian government?",
      "How can I visualize the distribution of public tenders in Romania?",
      "What insights can be drawn from the Romania Tenders dataset?",
      "How do tender outcomes correlate with bid submissions in Romania?"
    ],
    "domain_tags": [
      "government",
      "public sector",
      "procurement"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2007-2016",
    "geographic_scope": "Romania",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/datasets/romania-tenders.jpg",
    "embedding_text": "The Romania Tenders dataset provides a comprehensive collection of public tender data from Romania, covering a significant period from 2007 to 2016. This dataset is structured in a tabular format, with rows representing individual tenders and columns detailing various attributes such as tender ID, title, description, issuing authority, submission deadlines, and awarded bid amounts. The data is collected from official government sources, ensuring a high degree of reliability and accuracy. However, users should be aware of potential limitations, including missing values or inconsistencies in the data entries, which may require preprocessing steps such as data cleaning and normalization before analysis. Key variables in the dataset include the tender ID, which uniquely identifies each tender, the title and description that provide context about the tender, and the awarded bid amount, which reflects the financial aspect of the procurement process. Researchers can leverage this dataset to address a variety of research questions, such as analyzing trends in government procurement, evaluating the competitiveness of bids, and understanding the distribution of tender values across different sectors. The dataset supports various types of analyses, including descriptive statistics, regression analysis, and machine learning techniques, making it a valuable resource for data scientists and researchers interested in public procurement and market dynamics in Romania. Common preprocessing steps may involve handling missing data, converting categorical variables into numerical formats, and aggregating data for time-series analysis. Overall, the Romania Tenders dataset serves as a vital tool for understanding the intricacies of government procurement in Romania and can facilitate informed decision-making and policy formulation."
  },
  {
    "name": "Yoyi",
    "description": "Computational advertising dataset from Chinese ad platform",
    "category": "Advertising",
    "url": "https://apex.sjtu.edu.cn/datasets/7",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "China",
      "computational ads"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "computational ads"
    ],
    "summary": "The Yoyi dataset is a computational advertising dataset sourced from a Chinese ad platform, providing insights into advertising strategies and consumer interactions in the digital space. Researchers can utilize this dataset to analyze advertising effectiveness, consumer behavior, and optimization of ad placements.",
    "use_cases": [
      "Analyzing the effectiveness of advertising campaigns",
      "Studying consumer behavior in response to ads",
      "Optimizing ad placements based on data insights"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Yoyi computational advertising dataset?",
      "How can I access the Yoyi dataset for advertising analysis?",
      "What insights can be gained from the Yoyi dataset?",
      "What are the key variables in the Yoyi dataset?",
      "How does the Yoyi dataset support computational advertising research?",
      "What methodologies can be applied to the Yoyi dataset?",
      "What are the limitations of the Yoyi dataset?",
      "How is the Yoyi dataset structured?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.001,
    "image_url": "/images/logos/sjtu.edu.png",
    "embedding_text": "The Yoyi dataset represents a significant resource for researchers and practitioners in the field of computational advertising, particularly within the context of the Chinese digital advertising landscape. This dataset is structured in a tabular format, consisting of multiple rows and columns that capture various aspects of advertising interactions. Each row typically represents a unique advertising event or impression, while the columns include key variables such as ad ID, user demographics, impression timestamps, click-through rates, and conversion metrics. The collection methodology for the Yoyi dataset involves aggregating data from a Chinese ad platform, which collects user interactions with ads across various digital channels. This data is invaluable for understanding how different advertising strategies perform in real-time and how they influence consumer behavior. Coverage of the dataset is primarily focused on the Chinese market, providing insights that are particularly relevant to advertisers and researchers interested in this geographic region. The dataset may also include demographic information about users, although specific details on demographic coverage are not explicitly mentioned. Key variables in the dataset measure critical aspects of advertising performance, including user engagement, ad effectiveness, and conversion rates. However, researchers should be aware of potential limitations in data quality, such as biases in user behavior or incomplete data entries, which may affect the reliability of analyses conducted using this dataset. Common preprocessing steps for the Yoyi dataset may include cleaning the data to handle missing values, normalizing variables for consistent analysis, and transforming categorical variables into numerical formats suitable for machine learning algorithms. Researchers can leverage the Yoyi dataset to address various research questions, such as evaluating the impact of specific ad placements on consumer engagement or understanding the relationship between user demographics and ad performance. The dataset supports a range of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, enabling a comprehensive exploration of advertising dynamics. Typically, researchers utilize the Yoyi dataset in studies aimed at improving advertising strategies, enhancing user targeting, and ultimately driving better business outcomes in the competitive landscape of digital advertising."
  },
  {
    "name": "Outbrain Click Prediction",
    "description": "Click prediction based on browsing history from Outbrain",
    "category": "Advertising",
    "url": "https://www.kaggle.com/competitions/outbrain-click-prediction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "click prediction",
      "content",
      "Kaggle"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Outbrain Click Prediction dataset provides insights into user click behavior based on their browsing history. Researchers can utilize this data to build predictive models that forecast user engagement with content, improving advertising strategies and content recommendations.",
    "use_cases": [
      "Building predictive models for user engagement",
      "Analyzing the impact of content on click-through rates",
      "Developing recommendation systems based on user behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Outbrain Click Prediction dataset?",
      "How can I access the Outbrain Click Prediction dataset?",
      "What are the key features of the Outbrain Click Prediction dataset?",
      "What analyses can be performed with the Outbrain Click Prediction dataset?",
      "What is click prediction in advertising?",
      "How does browsing history affect click behavior?",
      "What machine learning techniques can be applied to click prediction?",
      "Where can I find datasets for click prediction?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/outbrain-click-prediction.png",
    "embedding_text": "The Outbrain Click Prediction dataset is a valuable resource for understanding user interactions with online content, particularly in the context of digital advertising. This dataset is structured in a tabular format, consisting of rows that represent individual user interactions and columns that capture various attributes related to those interactions. Key variables typically include user identifiers, content identifiers, browsing history features, and click outcomes, which together provide a comprehensive view of user behavior. The data is collected from Outbrain's content recommendation platform, where users engage with articles and advertisements based on their browsing history. This methodology ensures that the dataset reflects real-world user interactions, making it highly relevant for research in advertising and consumer behavior. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user engagement patterns, which may affect the robustness of analyses conducted using this dataset. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing features to prepare for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, allowing researchers to explore questions such as how browsing history influences click behavior and the effectiveness of different content types in driving user engagement. By leveraging this dataset, researchers can develop predictive models that enhance advertising strategies and improve content recommendations, ultimately leading to more effective engagement with target audiences."
  },
  {
    "name": "MobileRec",
    "description": "19.3M user reviews from 700K users across 10K apps in 48 categories. Google Play app recommendation research",
    "category": "App Stores",
    "url": "https://github.com/mhmaqbool/mobilerec",
    "docs_url": null,
    "github_url": "https://github.com/mhmaqbool/mobilerec",
    "tags": [
      "mobile apps",
      "reviews",
      "Google Play",
      "recommendations",
      "app store"
    ],
    "best_for": "Learning app stores analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "mobile-technology"
    ],
    "summary": "MobileRec is a comprehensive dataset consisting of 19.3 million user reviews from 700,000 users across 10,000 apps in 48 categories, specifically designed for Google Play app recommendation research. This dataset enables researchers and analysts to explore user sentiments, preferences, and trends in mobile app usage.",
    "use_cases": [
      "Sentiment analysis of user reviews to gauge app performance.",
      "Trend analysis to identify popular app categories over time.",
      "Recommendation system development based on user preferences.",
      "Comparative analysis of user satisfaction across different apps."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the user sentiments in MobileRec dataset?",
      "How can I analyze app reviews from MobileRec?",
      "What trends can be identified in mobile app usage using MobileRec?",
      "What categories of apps are most reviewed in the MobileRec dataset?",
      "How do user reviews vary across different app categories in MobileRec?",
      "What insights can be derived from 19.3M reviews in MobileRec?",
      "How can MobileRec be used for app recommendation systems?",
      "What are the common themes in user feedback from MobileRec?"
    ],
    "domain_tags": [
      "technology",
      "mobile-apps"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/mobilerec.png",
    "embedding_text": "The MobileRec dataset is a rich repository of user-generated content, specifically focusing on mobile app reviews collected from the Google Play Store. With a staggering 19.3 million reviews sourced from approximately 700,000 unique users, this dataset encompasses a wide array of 10,000 apps categorized into 48 distinct groups. The data structure is primarily text-based, with each entry representing a user review that may include variables such as review text, rating, app category, and possibly user identifiers, although the exact schema details are not specified. The collection methodology for MobileRec involves scraping publicly available reviews from the Google Play Store, ensuring that the dataset reflects real user experiences and sentiments towards various mobile applications. This comprehensive approach allows for a diverse representation of user opinions across different app categories, making it a valuable resource for researchers and data scientists interested in mobile technology and consumer behavior. The dataset's coverage is extensive in terms of the number of reviews and apps, but specific temporal and geographic dimensions are not explicitly mentioned, which may limit certain types of longitudinal or region-specific analyses. Key variables within the dataset include user ratings, which quantify user satisfaction, and review text, which provides qualitative insights into user experiences. However, researchers should be aware of potential limitations, such as the presence of biased reviews or the influence of fake reviews, which can affect data quality. Common preprocessing steps may involve cleaning the text data to remove noise, normalizing ratings, and possibly conducting sentiment analysis to derive insights from the review content. Researchers can leverage MobileRec to address a variety of research questions, such as understanding user sentiment trends, identifying factors that influence app ratings, and developing predictive models for app recommendations. The dataset supports various types of analyses, including regression analysis to explore relationships between app features and user ratings, machine learning techniques for classification of reviews, and descriptive statistics to summarize user feedback. Overall, MobileRec serves as a critical tool for those studying the dynamics of mobile app usage and user engagement, providing a foundation for innovative research and practical applications in the field of mobile technology."
  },
  {
    "name": "KuaiSAR",
    "description": "5M search actions, 14.6M recommendation events from 25k users",
    "category": "Entertainment & Media",
    "url": "https://kuaisar.github.io/",
    "docs_url": "https://kuaisar.github.io/",
    "github_url": null,
    "tags": [
      "search",
      "recommendations",
      "video",
      "Kuaishou"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "The KuaiSAR dataset comprises 5 million search actions and 14.6 million recommendation events from 25,000 users, primarily focusing on user interactions within the Kuaishou platform. This dataset can be utilized to analyze user behavior, improve recommendation algorithms, and enhance search functionalities in entertainment and media contexts.",
    "use_cases": [
      "Analyzing user search behavior to improve search algorithms.",
      "Developing and testing recommendation systems based on user interactions.",
      "Studying the impact of user engagement on content consumption.",
      "Evaluating the effectiveness of different recommendation strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the search actions in the KuaiSAR dataset?",
      "How many recommendation events are included in the KuaiSAR dataset?",
      "What user interactions does the KuaiSAR dataset capture?",
      "What is the size of the KuaiSAR dataset?",
      "How can the KuaiSAR dataset be used for recommendation system analysis?",
      "What insights can be drawn from the search actions in the KuaiSAR dataset?",
      "What demographic information is available in the KuaiSAR dataset?",
      "How does the KuaiSAR dataset contribute to understanding user behavior in media?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/logos/github.png",
    "embedding_text": "The KuaiSAR dataset is a rich collection of user interaction data from the Kuaishou platform, consisting of 5 million search actions and 14.6 million recommendation events recorded from 25,000 users. This dataset is structured in a tabular format, where each row represents a unique user interaction, and columns include variables such as user ID, action type (search or recommendation), timestamp, and potentially additional metadata related to the content interacted with. The collection methodology likely involves logging user actions directly from the Kuaishou application, capturing real-time interactions as users engage with the platform's content. While the specific temporal and geographic coverage details are not provided, the dataset reflects a snapshot of user behavior over a defined period, which can be critical for understanding trends and patterns in user engagement. Key variables in this dataset include search queries, recommended content, and user identifiers, which facilitate the measurement of user preferences and the effectiveness of recommendation algorithms. Researchers may encounter data quality issues such as missing values or inconsistencies in user behavior, necessitating common preprocessing steps like data cleaning, normalization, and transformation before analysis. The dataset supports various types of analyses, including regression modeling to predict user behavior, machine learning for developing recommendation systems, and descriptive statistics to summarize user engagement patterns. Researchers typically utilize the KuaiSAR dataset to address research questions related to user behavior, the effectiveness of search and recommendation systems, and the overall impact of user interactions on content consumption within the entertainment and media sector. This dataset serves as a valuable resource for both academic research and practical applications in enhancing user experience on digital platforms."
  },
  {
    "name": "Avazu",
    "description": "Dataset for click-through rate prediction on mobile ads",
    "category": "Advertising",
    "url": "https://www.kaggle.com/competitions/avazu-ctr-prediction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "CTR",
      "mobile ads",
      "Kaggle competition"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Avazu dataset is designed for click-through rate prediction on mobile advertisements. It provides a rich source of data for analyzing user interactions with ads, allowing researchers and practitioners to develop models that predict the likelihood of ad clicks based on various features.",
    "use_cases": [
      "Predicting click-through rates for mobile ads",
      "Analyzing user behavior in response to advertisements",
      "Developing machine learning models for ad performance",
      "Evaluating the effectiveness of ad targeting strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Avazu dataset?",
      "How can I use the Avazu dataset for CTR prediction?",
      "Where can I find the Avazu mobile ads dataset?",
      "What features are included in the Avazu dataset?",
      "Is the Avazu dataset available on Kaggle?",
      "What kind of analyses can I perform with the Avazu dataset?",
      "How to preprocess the Avazu dataset for machine learning?",
      "What are common use cases for the Avazu dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/avazu.png",
    "embedding_text": "The Avazu dataset is a comprehensive collection of data specifically designed for the purpose of predicting click-through rates (CTR) in mobile advertising contexts. This dataset is particularly valuable for researchers and data scientists who are interested in understanding user interactions with mobile ads and developing predictive models that can enhance ad targeting and effectiveness. The dataset is structured in a tabular format, consisting of multiple rows and columns that represent various features associated with mobile ad impressions and user interactions. Each row typically corresponds to an individual ad impression, while the columns include key variables such as user demographics, ad features, and contextual information about the ad placement. The collection methodology for the Avazu dataset involves aggregating data from real-world mobile advertising campaigns, capturing user engagement metrics and ad performance indicators. This data is sourced from various mobile ad networks, providing a rich and diverse set of interactions that can be analyzed for insights into consumer behavior. While the dataset does not explicitly mention temporal or geographic coverage, it is assumed to encompass a broad range of user interactions across different demographics and regions, given its application in mobile advertising. Key variables within the dataset measure aspects such as user ID, ad ID, timestamp of the impression, device type, and whether the ad was clicked or not. These variables are crucial for building models that can predict the likelihood of a user clicking on an ad based on their characteristics and the context of the ad delivery. However, researchers should be aware of potential limitations in data quality, including issues related to missing values, biases in user representation, and variations in ad performance across different contexts. Common preprocessing steps for the Avazu dataset may include handling missing data, encoding categorical variables, normalizing numerical features, and splitting the dataset into training and testing subsets for model evaluation. The dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics. Researchers can leverage the Avazu dataset to address critical research questions such as identifying factors that influence ad clicks, evaluating the effectiveness of different ad formats, and optimizing ad targeting strategies based on user behavior. Overall, the Avazu dataset serves as a valuable resource for advancing knowledge in the field of mobile advertising and enhancing the effectiveness of digital marketing strategies."
  },
  {
    "name": "Netflix Prize",
    "description": "100M+ anonymous movie ratings from 480k users",
    "category": "Entertainment & Media",
    "url": "https://www.kaggle.com/datasets/netflix-inc/netflix-prize-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "movies",
      "ratings",
      "recommendations",
      "classic"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "The Netflix Prize dataset consists of over 100 million anonymous movie ratings provided by approximately 480,000 users. This dataset can be utilized to develop and test recommendation algorithms, analyze user preferences, and study trends in movie ratings.",
    "use_cases": [
      "Developing movie recommendation algorithms",
      "Analyzing user rating patterns over time",
      "Studying the impact of user demographics on movie ratings"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Netflix Prize dataset?",
      "How can I access the Netflix Prize movie ratings?",
      "What are the key features of the Netflix Prize dataset?",
      "What types of analyses can be performed on the Netflix Prize dataset?",
      "What insights can be gained from the Netflix Prize ratings?",
      "How to build a recommendation system using the Netflix Prize dataset?",
      "What are the limitations of the Netflix Prize dataset?",
      "How many users contributed to the Netflix Prize dataset?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/datasets/netflix-prize.jpg",
    "embedding_text": "The Netflix Prize dataset is a rich collection of over 100 million anonymous movie ratings, contributed by approximately 480,000 users. This dataset is structured in a tabular format, where each row represents a unique rating instance, and the columns typically include variables such as user ID, movie ID, rating value, and possibly timestamps. The collection methodology involved gathering ratings from users on the Netflix platform, ensuring a diverse range of opinions on various films. The dataset's coverage is extensive in terms of the number of ratings, but it does not specify temporal or geographic dimensions explicitly. Key variables in the dataset include user IDs, which identify the raters, movie IDs, which correspond to the films being rated, and the rating values, which reflect the users' opinions on the movies. These variables allow researchers to measure user preferences, analyze trends in movie ratings, and explore the relationships between different films and user demographics. However, the dataset has known limitations, such as the anonymity of users, which restricts demographic analysis, and the potential for rating biases, as users may not represent the general population. Common preprocessing steps for this dataset may include handling missing values, normalizing ratings, and transforming the data into a suitable format for analysis. Researchers can address various research questions using this dataset, such as understanding how user preferences evolve over time, identifying factors that influence movie ratings, and developing predictive models for recommending films. The types of analyses supported by the Netflix Prize dataset include regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for those interested in the fields of data science and recommendation systems. Researchers typically utilize this dataset to test and benchmark their algorithms, contributing to advancements in recommendation technology and enhancing user experiences in entertainment platforms."
  },
  {
    "name": "Criteo Display Advertising",
    "description": "342GB total with 13 integer features, 26 hashed categorical features",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/ressources/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "CTR prediction",
      "advertising",
      "large-scale"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "advertising",
      "CTR prediction"
    ],
    "summary": "The Criteo Display Advertising dataset is a large-scale dataset designed for predicting click-through rates (CTR) in digital advertising. It contains 342GB of data with 13 integer features and 26 hashed categorical features, making it suitable for various machine learning and regression analyses.",
    "use_cases": [
      "Predicting click-through rates for online ads",
      "Analyzing user engagement with digital advertisements",
      "Evaluating the effectiveness of advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Display Advertising dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What features are included in the Criteo Display Advertising dataset?",
      "Where can I find large-scale advertising datasets?",
      "What are the applications of the Criteo Display Advertising dataset?",
      "How to preprocess the Criteo Display Advertising dataset for analysis?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0009,
    "image_url": "/images/logos/criteo.png",
    "embedding_text": "The Criteo Display Advertising dataset is an extensive collection of data specifically designed for the purpose of click-through rate (CTR) prediction in the realm of digital advertising. This dataset encompasses a total size of 342GB, which is indicative of its massive scale, and it features a combination of 13 integer features alongside 26 hashed categorical features. The structure of the dataset is primarily tabular, making it amenable to a variety of data analysis techniques. Each row in the dataset represents an individual ad impression, while the columns include various features that capture essential characteristics of the ads, users, and contexts in which the ads were displayed. The integer features typically include numerical representations of ad attributes and user interactions, while the hashed categorical features provide a way to encode categorical variables without revealing sensitive information about the users or ads. The collection methodology for this dataset is rooted in real-world advertising scenarios, where data is gathered from online ad placements across various platforms, ensuring that the dataset reflects actual user behavior and engagement with ads. However, it is important to note that the dataset may have limitations regarding data quality, such as potential biases in user interactions or incomplete data for certain features. Common preprocessing steps that researchers might undertake include handling missing values, normalizing numerical features, and decoding hashed categorical features to facilitate analysis. The dataset supports a wide range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for researchers and practitioners in the field of advertising technology. Researchers typically leverage this dataset to address critical research questions related to user engagement, ad effectiveness, and the optimization of advertising strategies. By employing various analytical techniques, they can gain insights into consumer behavior, evaluate the performance of different ad formats, and ultimately enhance the overall efficiency of digital marketing campaigns."
  },
  {
    "name": "Goodreads",
    "description": "Book information and user reviews from Goodreads platform",
    "category": "Entertainment & Media",
    "url": "https://mengtingwan.github.io/data/goodreads.html#datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "books",
      "reviews",
      "recommendations"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The Goodreads dataset contains comprehensive information about books and user-generated reviews from the Goodreads platform. Researchers and analysts can utilize this data to explore trends in reading habits, analyze user preferences, and develop recommendation systems.",
    "use_cases": [
      "Analyzing trends in book popularity over time",
      "Developing a recommendation system based on user reviews",
      "Studying the impact of author reputation on book ratings"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most reviewed books on Goodreads?",
      "How do user ratings correlate with book genres?",
      "What trends can be observed in book recommendations?",
      "How do user reviews vary by author?",
      "What is the average rating for books published in a specific year?",
      "How do demographics influence book preferences?",
      "What are the common themes in user reviews?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0009,
    "image_url": "/images/logos/github.png",
    "embedding_text": "The Goodreads dataset is a rich repository of book-related information, encompassing user reviews, ratings, and various metadata associated with books available on the Goodreads platform. The data structure typically includes rows representing individual books, with columns for key variables such as book title, author, genre, publication date, user ratings, and textual reviews. Each entry provides insights into user engagement with the book, reflecting personal opinions and experiences. The collection methodology primarily involves scraping data from the Goodreads website, where users actively contribute reviews and ratings, ensuring a dynamic and continually updated dataset. This dataset covers a wide range of books across various genres, allowing for diverse analyses related to consumer behavior and preferences in reading. Key variables in the dataset include user ratings, which measure the perceived quality of the book, and textual reviews, which provide qualitative insights into user sentiments. However, researchers should be aware of potential limitations in data quality, such as biased reviews from vocal users or the influence of popular trends on ratings. Common preprocessing steps may include cleaning the text data for analysis, normalizing ratings, and categorizing books by genre or author for more structured insights. The dataset supports a variety of research questions, such as examining the relationship between book characteristics and user ratings, identifying trends in reading preferences over time, and developing predictive models for book recommendations. Analysts can employ various types of analyses, including regression analysis to understand factors influencing ratings, machine learning techniques for recommendation systems, and descriptive statistics to summarize user engagement. Researchers typically use this dataset to explore the dynamics of the book market, assess the impact of social proof on reading choices, and enhance user experience through personalized recommendations."
  },
  {
    "name": "ICPSR",
    "description": "World's largest social science archive - 250,000+ files across 16,000 studies since 1962",
    "category": "Dataset Aggregators",
    "url": "https://www.icpsr.umich.edu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social science",
      "surveys",
      "GSS",
      "ANES",
      "academic"
    ],
    "best_for": "GSS, ANES, World Values Survey - essential for empirical economics research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ICPSR is the world's largest social science archive, containing over 250,000 files across more than 16,000 studies since its inception in 1962. Researchers can utilize this extensive repository for various analyses in social science, including surveys and longitudinal studies.",
    "use_cases": [
      "Analyzing trends in social behavior over time using survey data.",
      "Conducting comparative studies across different demographics using ICPSR datasets."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the ICPSR dataset?",
      "How can I access social science data from ICPSR?",
      "What types of studies are included in the ICPSR archive?",
      "Where can I find survey data in the ICPSR collection?",
      "What is the history of the ICPSR dataset?",
      "How many files are available in the ICPSR archive?",
      "What are the key topics covered by ICPSR studies?",
      "How can ICPSR data be used in academic research?"
    ],
    "domain_tags": [
      "academic",
      "social science"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0008,
    "image_url": "/images/logos/umich.png",
    "embedding_text": "The Inter-university Consortium for Political and Social Research (ICPSR) serves as the world's largest social science archive, providing a comprehensive collection of data files that span over 250,000 datasets across more than 16,000 studies since its establishment in 1962. This extensive repository is invaluable for researchers, educators, and students in the social sciences, offering a wealth of information that can be leveraged for various analytical purposes. The data structure within ICPSR typically consists of tabular formats, where each dataset is organized into rows and columns, with each row representing a unique observation or case and each column corresponding to a specific variable or attribute. The variables included in these datasets often cover a wide range of social science topics, including demographics, attitudes, behaviors, and responses to survey questions. The collection methodology employed by ICPSR involves collaboration with various academic institutions, researchers, and organizations, ensuring that the data is collected using rigorous scientific methods. This includes survey methodologies, observational studies, and experimental designs, all aimed at capturing accurate and reliable information pertinent to social science research. While ICPSR provides a robust archive of data, researchers should be aware of potential limitations, including issues related to data quality, such as missing values, response bias, and the representativeness of samples. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers frequently utilize ICPSR data to address a variety of research questions, ranging from understanding social trends and public opinion to evaluating the effectiveness of policies and interventions. The types of analyses supported by ICPSR datasets are diverse, encompassing descriptive statistics, regression analyses, and even machine learning techniques, depending on the complexity and nature of the data. By tapping into the rich resources available through ICPSR, researchers can conduct meaningful studies that contribute to the advancement of knowledge in the social sciences, making it a cornerstone resource for academic inquiry and exploration."
  },
  {
    "name": "MicroLens",
    "description": "1 billion interactions from 34 million users on 1 million micro-videos",
    "category": "Entertainment & Media",
    "url": "https://github.com/westlake-repl/MicroLens",
    "docs_url": null,
    "github_url": "https://github.com/westlake-repl/MicroLens",
    "tags": [
      "video",
      "micro-video",
      "large-scale",
      "recommendations"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "video-analysis"
    ],
    "summary": "MicroLens is a dataset containing 1 billion interactions from 34 million users on 1 million micro-videos. This rich dataset allows researchers and analysts to explore user engagement patterns, video performance, and recommendation systems in the context of micro-video content.",
    "use_cases": [
      "Analyzing user engagement metrics for micro-videos",
      "Developing recommendation algorithms for video content",
      "Studying the impact of video characteristics on user interactions",
      "Exploring demographic influences on micro-video consumption"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the user engagement patterns in micro-videos?",
      "How can we analyze the performance of micro-videos?",
      "What recommendations can be made based on user interactions with micro-videos?",
      "How do different demographics interact with micro-videos?",
      "What insights can be derived from 1 billion interactions on micro-videos?",
      "How can machine learning be applied to micro-video recommendations?",
      "What are the trends in user behavior on micro-video platforms?",
      "How does video length affect user engagement in micro-videos?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0008,
    "image_url": "/images/datasets/microlens.png",
    "embedding_text": "The MicroLens dataset is a comprehensive collection of 1 billion interactions from 34 million users across 1 million micro-videos, providing a unique opportunity for researchers and data scientists to delve into the dynamics of user engagement within the entertainment and media sector. The dataset is structured in a tabular format, consisting of rows representing individual interactions and columns capturing various attributes such as user ID, video ID, interaction type (e.g., view, like, share), timestamp, and possibly demographic information about the users. This rich schema allows for detailed analysis of user behavior and video performance metrics. The data collection methodology likely involves tracking user interactions on a micro-video platform, where each interaction is logged with precision to ensure accuracy and reliability. The dataset's coverage is extensive, capturing a wide range of user behaviors and preferences, although specific temporal and geographic details are not provided. Key variables in the dataset include user IDs, which help in identifying unique users; video IDs, which correspond to the micro-videos; and interaction types, which measure engagement levels. The quality of the data is expected to be high, given the scale of interactions, but potential limitations may include biases in user demographics or interaction types that could affect the generalizability of findings. Common preprocessing steps may involve cleaning the data to remove duplicates, handling missing values, and normalizing interaction types for analysis. Researchers can leverage this dataset to address various research questions, such as understanding the factors influencing user engagement, the effectiveness of different video formats, and the development of predictive models for user interactions. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for studies focused on micro-video content and user behavior. Overall, MicroLens serves as a valuable asset for anyone looking to explore the intersection of technology, entertainment, and consumer behavior, offering insights that can inform content creation, marketing strategies, and platform development."
  },
  {
    "name": "Indian Grocery (Flipkart Supermart)",
    "description": "Flipkart Supermart transaction and product details",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/aryansingh95/flipkart-grocery-transaction-and-product-details",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "India",
      "Flipkart"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Indian Grocery dataset from Flipkart Supermart contains transaction and product details that provide insights into consumer purchasing behavior in the grocery sector. Researchers can analyze this data to understand trends in grocery shopping, pricing strategies, and consumer preferences in India.",
    "use_cases": [
      "Analyzing consumer purchasing trends in grocery shopping",
      "Examining the impact of pricing on sales volume",
      "Identifying popular products among Indian consumers",
      "Studying seasonal variations in grocery purchases"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the transaction details in the Indian Grocery dataset?",
      "How can I analyze consumer behavior using Flipkart Supermart data?",
      "What product details are available in the Indian Grocery dataset?",
      "What insights can be derived from grocery shopping trends in India?",
      "How does pricing vary across different grocery items?",
      "What are the common products purchased in Flipkart Supermart?",
      "How can I visualize transaction data from the Indian Grocery dataset?",
      "What statistical analyses can be performed on grocery transaction data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0008,
    "image_url": "/images/datasets/indian-grocery-flipkart-supermart.png",
    "embedding_text": "The Indian Grocery dataset from Flipkart Supermart encompasses a comprehensive collection of transaction and product details that are pivotal for understanding consumer behavior in the grocery sector in India. This dataset is structured in a tabular format, containing rows that represent individual transactions and columns that detail various attributes such as product ID, product name, category, price, quantity purchased, and transaction date. The schema is designed to facilitate analysis of purchasing patterns, allowing researchers to explore how different factors influence consumer decisions in the grocery market. Data collection is primarily sourced from Flipkart Supermart's transactional records, which ensures a rich dataset that reflects real-world consumer interactions and preferences. However, it is important to note that while the dataset provides valuable insights, it may also have limitations related to data quality, such as missing values or inconsistencies in product categorization, which researchers need to address during preprocessing. Common preprocessing steps may include cleaning the data to handle missing values, normalizing product categories, and aggregating transaction data for analysis. The dataset supports a variety of analyses, including regression analysis to understand the relationship between price and quantity sold, machine learning models to predict consumer preferences, and descriptive statistics to summarize purchasing behaviors. Researchers typically utilize this dataset to answer critical research questions such as identifying trends in grocery shopping, analyzing the impact of promotional pricing, and understanding consumer demographics in relation to purchasing habits. Overall, the Indian Grocery dataset serves as a valuable resource for both academic and commercial research, providing insights that can inform marketing strategies, inventory management, and consumer engagement initiatives within the retail grocery sector."
  },
  {
    "name": "FERC Form 714",
    "description": "Hourly electricity load data from major U.S. utilities and regional transmission organizations",
    "category": "Energy",
    "url": "https://www.ferc.gov/industries-data/electric/general-information/electric-industry-forms/form-714-annual-electric",
    "docs_url": "https://www.ferc.gov/industries-data/electric/general-information/electric-industry-forms/form-714-annual-electric",
    "github_url": null,
    "tags": [
      "load",
      "demand",
      "hourly",
      "utilities"
    ],
    "best_for": "Hourly demand analysis, load forecasting, and peak demand studies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "data-analysis",
      "load forecasting"
    ],
    "summary": "The FERC Form 714 dataset contains hourly electricity load data collected from major U.S. utilities and regional transmission organizations. This dataset can be utilized to analyze electricity demand patterns, forecast future load requirements, and assess the performance of energy systems.",
    "use_cases": [
      "Analyzing trends in electricity demand over time",
      "Forecasting future electricity load based on historical data",
      "Evaluating the performance of different utilities in managing load",
      "Studying the impact of external factors on electricity consumption patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FERC Form 714 dataset?",
      "How can I access hourly electricity load data?",
      "What insights can be derived from electricity demand data?",
      "What are the key variables in the FERC Form 714?",
      "How do utilities report their load data?",
      "What is the significance of hourly load data in energy analysis?",
      "How can I use FERC Form 714 data for load forecasting?",
      "What methodologies are used in analyzing electricity load data?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2006-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0008,
    "image_url": "/images/logos/ferc.png",
    "embedding_text": "The FERC Form 714 dataset is a comprehensive collection of hourly electricity load data sourced from major utilities and regional transmission organizations across the United States. This dataset is structured in a tabular format, where each row represents a specific hour of electricity load data, and the columns include variables such as utility name, region, load values, and timestamps. The primary purpose of this dataset is to provide insights into electricity demand patterns, which are crucial for energy management and planning. Researchers and analysts can leverage this data to address various research questions related to electricity consumption, such as identifying peak load periods, understanding seasonal variations, and evaluating the impact of policy changes on energy usage. The collection methodology involves reporting by utilities to the Federal Energy Regulatory Commission (FERC), ensuring a standardized approach to data gathering. While the dataset is rich in information, it may have limitations regarding data completeness and accuracy, as it relies on the reporting practices of individual utilities. Common preprocessing steps may include handling missing values, normalizing load data, and aggregating data for specific time frames. This dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a valuable resource for researchers in the energy sector. By utilizing the FERC Form 714 dataset, analysts can gain a deeper understanding of electricity demand dynamics, which can inform decision-making processes in energy policy and infrastructure development."
  },
  {
    "name": "Netflix Viewing Behavior",
    "description": "1.7M episodes/movies watched by 1,060 users over 1 year. Watch patterns, session length, preferences, predictability metrics",
    "category": "Entertainment & Media",
    "url": "https://ieeexplore.ieee.org/document/9500874",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Netflix",
      "streaming",
      "viewing behavior",
      "sessions",
      "video"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "viewing-patterns"
    ],
    "summary": "The Netflix Viewing Behavior dataset contains data on 1.7 million episodes and movies watched by 1,060 users over the course of a year. This dataset allows researchers to analyze viewing patterns, session lengths, user preferences, and predictability metrics associated with streaming content.",
    "use_cases": [
      "Analyzing user engagement and retention based on viewing patterns.",
      "Studying the impact of session length on user satisfaction.",
      "Predicting future viewing behavior based on past preferences.",
      "Exploring trends in content consumption over time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the viewing patterns in the Netflix Viewing Behavior dataset?",
      "How can I analyze session lengths from Netflix user data?",
      "What preferences can be inferred from Netflix viewing behavior?",
      "What metrics are included in the Netflix Viewing Behavior dataset?",
      "How many users contributed to the Netflix Viewing Behavior dataset?",
      "What types of episodes and movies are included in the dataset?",
      "How can I predict user behavior using Netflix viewing data?",
      "What insights can be gained from analyzing 1.7M Netflix episodes?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1 year",
    "size_category": "massive",
    "model_score": 0.0007,
    "image_url": "/images/datasets/netflix-viewing-behavior.png",
    "embedding_text": "The Netflix Viewing Behavior dataset is a comprehensive collection of data reflecting the viewing habits of users on the Netflix platform. With a total of 1.7 million episodes and movies watched by 1,060 users over a span of one year, this dataset provides a rich resource for understanding consumer behavior in the context of streaming media. The data is structured in a tabular format, where each row represents an individual viewing instance, and the columns include variables such as user ID, episode/movie ID, watch time, session length, and various metrics that capture user preferences and predictability. The collection methodology involves tracking user interactions with the Netflix platform, ensuring that the data reflects real-world viewing behavior. This dataset can address a variety of research questions, such as how viewing patterns vary among different demographics, the relationship between session length and user retention, and the predictability of user preferences based on historical data. Researchers typically employ regression analysis, machine learning techniques, and descriptive statistics to extract insights from the dataset. Common preprocessing steps may include data cleaning to handle missing values, normalization of session lengths, and encoding categorical variables for analysis. However, researchers should be aware of potential limitations, such as the lack of demographic information about users and the possibility of biases in viewing behavior based on the sample size. Overall, this dataset serves as a valuable tool for researchers and analysts interested in exploring the dynamics of viewing behavior in the rapidly evolving landscape of streaming media."
  },
  {
    "name": "Fliggy Transfers",
    "description": "Transfer-related data (flights, ground transport) from Fliggy",
    "category": "Travel & Hospitality",
    "url": "https://tianchi.aliyun.com/dataset/140721",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "travel",
      "transfers",
      "transportation"
    ],
    "best_for": "Learning travel & hospitality analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "travel",
      "transportation",
      "e-commerce"
    ],
    "summary": "Fliggy Transfers provides comprehensive transfer-related data, including information on flights and ground transport options available through the Fliggy platform. This dataset can be utilized for analyzing travel patterns, pricing strategies, and consumer behavior in the travel and hospitality sector.",
    "use_cases": [
      "Analyzing consumer preferences in travel transfers",
      "Evaluating pricing strategies for transportation options",
      "Studying the impact of transfer times on travel decisions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available in Fliggy Transfers?",
      "How can I analyze flight transfer options?",
      "What transportation data does Fliggy provide?",
      "What insights can be gained from Fliggy Transfers dataset?",
      "How to use Fliggy Transfers for travel analysis?",
      "What are the key variables in Fliggy Transfers?"
    ],
    "domain_tags": [
      "travel",
      "hospitality"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0007,
    "embedding_text": "Fliggy Transfers is a dataset that encompasses a wide array of transfer-related data, specifically focusing on flights and ground transportation options available through the Fliggy platform. The dataset is structured in a tabular format, which typically includes rows representing individual transfer instances and columns detailing various attributes of these transfers, such as departure and arrival locations, transfer times, pricing information, and types of transportation available. Each variable within the dataset is designed to capture specific aspects of the transfer experience, enabling researchers and analysts to derive meaningful insights into travel behaviors and preferences. The collection methodology for this dataset involves aggregating data from Fliggy's platform, which is known for its comprehensive travel services, including booking flights and ground transport. The data is likely sourced from user interactions, transaction records, and possibly third-party transportation providers, ensuring a rich dataset that reflects real-world travel scenarios. While the dataset does not explicitly mention temporal or geographic coverage, it is reasonable to infer that the data spans various timeframes and locations, given Fliggy's extensive reach in the travel industry. Key variables in the dataset may include transfer duration, cost, type of transport (e.g., bus, taxi, train), and user ratings, which measure the efficiency and satisfaction of the transfer options. However, like any dataset, Fliggy Transfers may have limitations regarding data quality, such as incomplete records or inconsistencies in user-reported information. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing pricing information, and categorizing transport types for easier analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying trends in consumer preferences for different types of transfers, evaluating the impact of transfer times on overall travel satisfaction, and analyzing pricing strategies across different transportation modes. The dataset supports various types of analyses, including regression analysis to understand the relationships between pricing and consumer choices, machine learning techniques for predictive modeling, and descriptive statistics to summarize key trends in travel behavior. Overall, Fliggy Transfers serves as a valuable resource for researchers and analysts interested in the travel and hospitality sector, providing insights that can inform business strategies and enhance the customer experience."
  },
  {
    "name": "UK Gift Shop (Online Retail)",
    "description": "Online retail transactions (2010-2011) from UK gift retailer",
    "category": "Grocery & Supermarkets",
    "url": "http://archive.ics.uci.edu/dataset/352/online+retail",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "UK",
      "UCI",
      "transactions"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The UK Gift Shop dataset contains online retail transaction data from a UK gift retailer for the years 2010 to 2011. This dataset can be utilized to analyze consumer purchasing behavior, pricing strategies, and overall trends in online retail during this period.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Evaluating pricing strategies",
      "Identifying seasonal trends in gift purchases"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "UK online retail transactions dataset",
      "2010-2011 UK gift shop sales data",
      "e-commerce transaction analysis UK",
      "consumer behavior in online retail UK",
      "UK gift retailer transaction dataset",
      "online retail data for analysis",
      "UK e-commerce sales trends 2010-2011"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2010-2011",
    "geographic_scope": "UK",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/logos/uci.png",
    "embedding_text": "The UK Gift Shop dataset is a comprehensive collection of online retail transactions from a UK-based gift retailer, covering the years 2010 to 2011. This dataset is structured in a tabular format, containing rows that represent individual transactions and columns that capture various attributes associated with each transaction. Key variables in this dataset may include transaction ID, product details, customer demographics, purchase amounts, and timestamps, among others. The data collection methodology involved gathering transaction records directly from the retailer's online sales platform, ensuring that the dataset reflects real-world consumer behavior during the specified period. The temporal coverage of the dataset spans two years, providing a snapshot of online retail dynamics in the UK during a time when e-commerce was rapidly evolving. The geographic scope is limited to the UK, making it particularly relevant for studies focused on this market. Researchers can utilize this dataset to explore a variety of research questions, such as how consumer preferences shifted during the holiday seasons, the impact of pricing changes on sales volume, and the overall growth trends in the online retail sector. The dataset supports various types of analyses, including regression analysis to identify factors influencing purchasing decisions, machine learning models to predict future sales trends, and descriptive statistics to summarize consumer behavior patterns. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in transaction records, which may require common preprocessing steps like data cleaning and normalization. Overall, the UK Gift Shop dataset serves as a valuable resource for researchers and analysts interested in the intersection of retail, consumer behavior, and e-commerce, providing insights that can inform business strategies and academic studies alike."
  },
  {
    "name": "Tesco Grocery 1.0",
    "description": "Grocery purchases from Tesco stores via loyalty cards",
    "category": "Grocery & Supermarkets",
    "url": "https://figshare.com/collections/Tesco_Grocery_1_0/4769354",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "loyalty cards",
      "UK",
      "Tesco"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Tesco Grocery 1.0 dataset contains grocery purchase data collected from Tesco stores through loyalty cards. This dataset can be utilized to analyze consumer purchasing behavior, pricing strategies, and market trends within the grocery sector.",
    "use_cases": [
      "Analyzing consumer purchasing behavior",
      "Evaluating the impact of loyalty programs on sales",
      "Studying pricing strategies in the grocery sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the grocery purchase patterns from Tesco loyalty cards?",
      "How can I analyze consumer behavior using Tesco Grocery 1.0?",
      "What insights can be derived from Tesco's grocery purchases?",
      "How do loyalty cards influence grocery shopping habits?",
      "What pricing strategies can be evaluated using Tesco Grocery 1.0?",
      "What demographic trends can be identified in Tesco grocery purchases?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "UK",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/logos/figshare.png",
    "embedding_text": "The Tesco Grocery 1.0 dataset is a comprehensive collection of grocery purchase data derived from Tesco stores, specifically gathered through the usage of loyalty cards. This dataset is structured in a tabular format, consisting of various rows and columns that represent individual transactions, customer identifiers, product details, purchase amounts, and timestamps. Each row corresponds to a unique transaction, while the columns may include variables such as customer ID, product ID, quantity purchased, total spend, and date of purchase. The collection methodology involves the aggregation of data from Tesco's loyalty card program, which tracks customer purchases over time, allowing for a rich analysis of consumer behavior and preferences. The dataset is geographically focused on the UK, reflecting the shopping habits of Tesco's customer base within this region. While the dataset does not explicitly mention temporal coverage, it is assumed to cover a range of dates as transactions are recorded through the loyalty card system. Key variables in this dataset measure aspects such as the frequency of purchases, types of products bought, and total expenditure, providing insights into consumer spending patterns and preferences. However, potential limitations may include data quality issues related to missing values or inaccuracies in transaction records, which are common in large datasets. Researchers may need to perform common preprocessing steps such as data cleaning, normalization, and feature engineering to prepare the dataset for analysis. The Tesco Grocery 1.0 dataset supports various types of analyses, including regression analysis, machine learning modeling, and descriptive statistics, making it a versatile resource for understanding consumer behavior in the grocery sector. Researchers typically utilize this dataset to explore research questions related to purchasing trends, the effectiveness of loyalty programs, and the impact of pricing strategies on consumer choices. Overall, the Tesco Grocery 1.0 dataset serves as a valuable tool for data scientists and researchers interested in the intersection of retail, consumer behavior, and economic analysis."
  },
  {
    "name": "Alibaba Fashion Combo",
    "description": "Fashion item combinations from Alibaba for outfit recommendation",
    "category": "Fashion & Apparel",
    "url": "https://tianchi.aliyun.com/dataset/131519",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "outfit",
      "recommendation"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Fashion Combo dataset contains combinations of fashion items sourced from Alibaba, designed to assist in outfit recommendations. This dataset can be utilized to analyze consumer preferences, generate outfit suggestions, and explore trends in fashion combinations.",
    "use_cases": [
      "Analyzing consumer preferences in fashion",
      "Generating outfit recommendations based on item combinations",
      "Exploring trends in fashion combinations",
      "Studying the impact of outfit recommendations on consumer purchasing behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are popular fashion item combinations on Alibaba?",
      "How can I use Alibaba Fashion Combo for outfit recommendations?",
      "What trends can be identified in Alibaba's fashion offerings?",
      "How do consumer preferences influence fashion combinations?",
      "What data is available for fashion outfit analysis?",
      "Can I analyze outfit recommendations using Alibaba Fashion Combo?",
      "What insights can be drawn from Alibaba's fashion dataset?",
      "How does Alibaba's fashion data support e-commerce research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0007,
    "embedding_text": "The Alibaba Fashion Combo dataset is a valuable resource for researchers and practitioners in the fashion and e-commerce sectors, focusing on the combinations of fashion items available on Alibaba. This dataset is structured in a tabular format, comprising rows that represent individual fashion item combinations and columns that detail various attributes of these items. Key variables may include item categories, colors, styles, and prices, which collectively provide insights into consumer preferences and trends in outfit recommendations. The dataset is collected from Alibaba's extensive inventory, leveraging their API or web scraping techniques to gather a diverse array of fashion items. While specific collection methodologies are not detailed, it is reasonable to assume that the dataset reflects a wide range of fashion offerings available on the platform. The temporal and geographic coverage of the dataset is not explicitly mentioned, suggesting that it encompasses a broad spectrum of items without a specific time frame or regional focus. However, the dataset's medium size indicates a substantial amount of data that can be analyzed for various research purposes. Researchers can utilize this dataset to address questions related to consumer behavior, such as identifying popular combinations of fashion items and understanding the factors that influence outfit choices. Common preprocessing steps may include data cleaning, normalization of item attributes, and the handling of missing values to ensure the dataset is ready for analysis. The dataset supports various types of analyses, including descriptive statistics to summarize item combinations, regression analyses to explore relationships between item attributes and consumer choices, and machine learning techniques to develop recommendation systems. By leveraging the Alibaba Fashion Combo dataset, researchers can gain insights into the dynamics of fashion consumption, explore trends in outfit recommendations, and ultimately contribute to the understanding of consumer behavior in the e-commerce landscape."
  },
  {
    "name": "YouTube User Watch History",
    "description": "1.8M videos watched by 243 users over 1.5 years. Recommendation engine performance, caching research, viewing patterns",
    "category": "Entertainment & Media",
    "url": "https://netsg.cs.sfu.ca/youtubedata/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "YouTube",
      "watch history",
      "recommendations",
      "video",
      "user behavior"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "user behavior",
      "recommendations"
    ],
    "summary": "The YouTube User Watch History dataset contains data on 1.8 million videos watched by 243 users over a period of 1.5 years. It can be utilized to analyze viewing patterns, evaluate recommendation engine performance, and conduct research on caching strategies.",
    "use_cases": [
      "Analyzing user engagement with video content",
      "Evaluating the effectiveness of recommendation algorithms",
      "Studying the impact of caching on video streaming",
      "Identifying trends in user viewing habits over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the YouTube User Watch History dataset?",
      "How can I analyze user behavior from YouTube watch history?",
      "What insights can be gained from YouTube viewing patterns?",
      "How does caching affect video recommendations on YouTube?",
      "What are the key metrics in YouTube watch history analysis?",
      "How many videos were watched in the YouTube User Watch History dataset?",
      "What is the demographic breakdown of users in the YouTube watch history dataset?",
      "What research can be conducted using YouTube watch history data?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1.5 years",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/logos/sfu.png",
    "embedding_text": "The YouTube User Watch History dataset is a rich collection of data that encapsulates the viewing habits of 243 users over a span of 1.5 years, comprising a total of 1.8 million videos watched. This dataset is structured in a tabular format, where each row represents an individual video watched by a user, and the columns include variables such as user ID, video ID, watch duration, timestamps, and possibly other engagement metrics. The collection methodology likely involved tracking user interactions with the YouTube platform, capturing data on each video view, which allows researchers to analyze various aspects of user behavior and video consumption patterns.\n\nKey variables in this dataset include user identifiers, which allow for the segmentation of data by individual users, and video identifiers, which enable the analysis of specific content. Watch duration provides insights into user engagement, while timestamps can be used to study temporal patterns in viewing behavior. The dataset's quality is contingent upon the accuracy of the tracking mechanisms employed by YouTube, and while it offers a wealth of information, researchers should be aware of potential limitations such as biases in user selection and the influence of external factors on viewing habits.\n\nCommon preprocessing steps for this dataset may include cleaning the data to remove any incomplete or erroneous entries, normalizing timestamps for consistency, and aggregating data to derive meaningful metrics such as average watch time per user or total views per video. Researchers can utilize this dataset to address a variety of research questions, such as understanding how different demographics engage with video content, evaluating the performance of recommendation systems, and exploring the effects of caching on user experience.\n\nThe types of analyses supported by this dataset range from descriptive statistics that summarize user behavior to more complex methodologies, including regression analysis and machine learning techniques aimed at predicting user preferences or optimizing recommendation algorithms. Typically, researchers leverage this dataset in studies focused on user engagement, content recommendation strategies, and the overall dynamics of video consumption in the digital age. By analyzing the intricate patterns within this dataset, insights can be gained that not only enhance the understanding of user behavior on platforms like YouTube but also inform the development of more effective recommendation systems and content delivery strategies."
  },
  {
    "name": "Vietnam Supermarket",
    "description": "Sales and inventory snapshot data from Vietnamese supermarket",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/tienanh2003/sales-and-inventory-snapshot-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "supermarket",
      "Vietnam",
      "inventory"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Vietnam Supermarket dataset provides a snapshot of sales and inventory data from a Vietnamese supermarket, offering insights into consumer purchasing behavior and inventory management. Researchers can utilize this data to analyze sales trends, optimize inventory levels, and understand market dynamics in the grocery sector.",
    "use_cases": [
      "Analyzing sales trends over time to identify peak purchasing periods.",
      "Optimizing inventory levels based on sales data to reduce waste.",
      "Understanding consumer behavior through purchasing patterns.",
      "Evaluating the impact of pricing strategies on sales performance."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the sales data from Vietnamese supermarkets?",
      "How can I analyze inventory levels in Vietnam's grocery sector?",
      "What insights can be gained from supermarket sales data in Vietnam?",
      "What are the consumer purchasing trends in Vietnamese supermarkets?",
      "How does inventory management affect sales in Vietnam's grocery stores?",
      "What factors influence pricing in Vietnamese supermarkets?",
      "What are the common challenges in analyzing supermarket sales data?",
      "How can I visualize sales trends from Vietnamese supermarket data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Vietnam",
    "size_category": "medium",
    "model_score": 0.0007,
    "image_url": "/images/datasets/vietnam-supermarket.png",
    "embedding_text": "The Vietnam Supermarket dataset is a comprehensive collection of sales and inventory snapshot data specifically from a Vietnamese supermarket. This dataset is structured in a tabular format, comprising various rows and columns that represent different sales transactions and inventory records. Each row typically corresponds to a unique transaction or inventory entry, while the columns include key variables such as product identifiers, sales amounts, quantities sold, inventory levels, timestamps, and possibly customer demographics. The precise schema of the dataset may vary, but it generally captures essential aspects of supermarket operations. The collection methodology for this dataset likely involves direct data extraction from point-of-sale systems and inventory management software used by the supermarket. This means that the data is sourced from actual sales transactions, providing a real-world snapshot of consumer behavior and inventory dynamics. The dataset is expected to cover a specific geographic area, namely Vietnam, and may reflect the unique characteristics of the local market, including consumer preferences and purchasing habits. However, the temporal coverage is not explicitly mentioned, so it remains unclear whether the data spans a particular time frame or is a one-time snapshot. Key variables in the dataset measure various aspects of supermarket operations, such as sales volume, product turnover rates, and inventory stock levels. These variables are crucial for understanding how products perform over time and how effectively inventory is managed. Researchers utilizing this dataset may encounter certain limitations regarding data quality, such as missing values, inconsistencies in data entry, or variations in product categorization. Common preprocessing steps might include cleaning the data to handle missing values, normalizing product identifiers, and aggregating sales data to analyze trends over specific periods. The Vietnam Supermarket dataset can address a variety of research questions, including the identification of seasonal purchasing trends, the impact of promotional campaigns on sales, and the relationship between inventory levels and sales performance. Analysts can employ various types of analyses, including regression analysis to predict future sales based on historical data, machine learning techniques to uncover patterns in consumer behavior, and descriptive statistics to summarize sales performance. Researchers typically use this dataset in studies focused on retail management, consumer behavior analysis, and inventory optimization strategies, making it a valuable resource for those interested in the grocery sector in Vietnam."
  },
  {
    "name": "Montgomery Liquor",
    "description": "Warehouse and retail liquor sales from Montgomery County, Maryland",
    "category": "Grocery & Supermarkets",
    "url": "https://data.montgomerycountymd.gov/Community-Recreation/Warehouse-and-Retail-Sales/v76h-r7br",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "liquor",
      "retail",
      "government data"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Montgomery Liquor dataset contains information on warehouse and retail liquor sales from Montgomery County, Maryland. This dataset can be utilized to analyze consumer purchasing patterns, evaluate pricing strategies, and assess the impact of government regulations on liquor sales.",
    "use_cases": [
      "Analyzing consumer purchasing patterns in liquor sales",
      "Evaluating the impact of pricing strategies on sales",
      "Assessing the effects of government regulations on liquor distribution",
      "Comparing retail versus warehouse sales performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Montgomery County liquor sales data",
      "warehouse liquor sales Maryland",
      "retail liquor sales analysis",
      "government data on liquor sales",
      "liquor consumer behavior Montgomery",
      "pricing strategies for liquor sales"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Montgomery County, Maryland",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The Montgomery Liquor dataset is a valuable resource for understanding the dynamics of liquor sales within Montgomery County, Maryland. This dataset encompasses warehouse and retail liquor sales, providing a comprehensive view of the market. The data is structured in a tabular format, with rows representing individual sales transactions and columns capturing key variables such as product type, sale price, quantity sold, and date of transaction. The collection methodology for this dataset likely involves aggregating sales data from various retail outlets and warehouses, ensuring a robust representation of the liquor market in the region. The geographic scope is specifically focused on Montgomery County, which allows for localized analysis of consumer behavior and market trends. While the dataset does not explicitly mention temporal coverage, it is essential for users to verify the currency of the data to ensure relevance in their analyses. Key variables within the dataset measure aspects such as sales volume, pricing, and product categories, which are crucial for conducting various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers can leverage this dataset to address critical research questions related to consumer purchasing behavior, the effectiveness of pricing strategies, and the implications of government regulations on liquor sales. Common preprocessing steps may include data cleaning to handle missing values, normalization of price data, and categorization of products for more straightforward analysis. Overall, the Montgomery Liquor dataset serves as a foundational tool for researchers and analysts interested in the retail liquor market, enabling them to derive insights that can inform business strategies and policy decisions."
  },
  {
    "name": "Expedia Hotel",
    "description": "Hotel booking and search data from Expedia",
    "category": "Travel & Hospitality",
    "url": "https://www.kaggle.com/datasets/vijeetnigam26/expedia-hotel",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hotels",
      "bookings",
      "travel search"
    ],
    "best_for": "Learning travel & hospitality analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Expedia Hotel dataset contains comprehensive hotel booking and search data from the Expedia platform. It can be utilized for analyzing consumer behavior in travel bookings, understanding pricing strategies, and optimizing search functionalities.",
    "use_cases": [
      "Analyzing consumer booking patterns over time.",
      "Evaluating the impact of pricing changes on hotel bookings.",
      "Optimizing search algorithms for hotel listings.",
      "Studying the relationship between hotel features and customer satisfaction."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Expedia Hotel dataset?",
      "How can I access hotel booking data from Expedia?",
      "What insights can be gained from Expedia's hotel search data?",
      "Where can I find datasets related to hotel bookings?",
      "What variables are included in the Expedia Hotel dataset?",
      "How does Expedia's hotel data inform travel trends?",
      "What analyses can be performed on hotel booking data?",
      "Is there a dataset for Expedia hotel search and booking?"
    ],
    "domain_tags": [
      "travel",
      "hospitality"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/expedia-hotel.png",
    "embedding_text": "The Expedia Hotel dataset is a rich source of data that encompasses hotel booking and search information from the Expedia platform. This dataset typically includes a variety of variables structured in a tabular format, where each row represents an individual booking or search instance, and columns contain key attributes such as hotel ID, location, price, customer ratings, and booking dates. The data is collected through Expedia's online platform, capturing real-time interactions and transactions made by users. This methodology ensures that the dataset reflects current trends and consumer behaviors in the travel and hospitality sector. However, the dataset may have limitations regarding data quality, such as missing values or inconsistencies in user-generated content, which are common in large-scale datasets. Researchers often need to perform preprocessing steps, including data cleaning, normalization, and handling of missing values, to prepare the data for analysis. The dataset can address a variety of research questions, such as identifying factors that influence hotel bookings, analyzing seasonal trends in travel, and evaluating the effectiveness of marketing strategies. It supports various types of analyses, including regression analysis to understand pricing impacts, machine learning for predictive modeling, and descriptive statistics to summarize booking trends. Researchers typically leverage this dataset in studies focused on consumer behavior, pricing strategies, and market analysis within the travel and hospitality industry, making it a valuable resource for both academic and commercial applications."
  },
  {
    "name": "CMS Medicare & Medicaid Data",
    "description": "Public use files from Centers for Medicare & Medicaid Services including claims data, provider statistics, and program enrollment",
    "category": "Insurance & Actuarial",
    "url": "https://data.cms.gov/",
    "docs_url": "https://www.cms.gov/Research-Statistics-Data-and-Systems/Research-Statistics-Data-and-Systems",
    "github_url": null,
    "tags": [
      "medicare",
      "medicaid",
      "claims-data",
      "healthcare",
      "government-programs"
    ],
    "best_for": "Healthcare policy research, reimbursement modeling, and population health analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "government-programs"
    ],
    "summary": "The CMS Medicare & Medicaid Data consists of public use files from the Centers for Medicare & Medicaid Services, including claims data, provider statistics, and program enrollment information. This dataset can be utilized for various analyses related to healthcare services, policy evaluation, and understanding the dynamics of Medicare and Medicaid programs.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the CMS Medicare & Medicaid Data?",
      "How can I access claims data from Medicare and Medicaid?",
      "What provider statistics are available in the CMS dataset?",
      "What types of program enrollment data does CMS provide?",
      "How can I analyze healthcare data from CMS?",
      "What insights can be gained from Medicare and Medicaid claims data?"
    ],
    "use_cases": [
      "Analyzing trends in healthcare utilization among Medicare beneficiaries.",
      "Evaluating the impact of policy changes on Medicaid enrollment.",
      "Investigating provider performance metrics in Medicare.",
      "Studying the relationship between claims data and patient outcomes."
    ],
    "domain_tags": [
      "healthcare",
      "insurance"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The CMS Medicare & Medicaid Data is a comprehensive dataset that provides public use files from the Centers for Medicare & Medicaid Services (CMS). This dataset includes a variety of information such as claims data, provider statistics, and program enrollment details, which are essential for researchers and analysts in the healthcare sector. The data is structured in a tabular format, consisting of multiple rows and columns that capture various variables related to healthcare services. Each row typically represents an individual claim or enrollment record, while the columns contain key variables such as patient demographics, service dates, procedure codes, and payment amounts. The collection methodology for this dataset involves systematic gathering of data from healthcare providers, insurers, and other relevant entities that participate in Medicare and Medicaid programs. The data is collected through standardized reporting processes to ensure consistency and accuracy. Coverage of the dataset is extensive, although specific temporal and geographic details are not explicitly mentioned in the provided description. However, it is understood that the dataset encompasses a wide range of demographics, reflecting the diverse population served by Medicare and Medicaid. Key variables within the dataset measure critical aspects of healthcare delivery, including the frequency of services utilized, types of procedures performed, and the financial aspects of care provided. Researchers should be aware of potential limitations in data quality, such as missing values or discrepancies in reporting practices among providers, which may affect the reliability of analyses. Common preprocessing steps include data cleaning, normalization of variable formats, and handling of missing data to prepare the dataset for analysis. The CMS Medicare & Medicaid Data can address a variety of research questions, such as examining the effectiveness of healthcare interventions, understanding patient demographics and their impact on service utilization, and evaluating the financial implications of healthcare policies. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for healthcare research. Researchers typically use this dataset in studies aimed at improving healthcare delivery, assessing program effectiveness, and informing policy decisions, thereby contributing to the broader understanding of the healthcare landscape in the United States."
  },
  {
    "name": "Criteo AI Lab Datasets",
    "description": "World's largest public ML dataset - 1TB Click Logs with 4 billion advertising events",
    "category": "Dataset Aggregators",
    "url": "https://ailab.criteo.com/ressources/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "advertising",
      "CTR",
      "recommendations",
      "benchmark"
    ],
    "best_for": "Click-through rate prediction and recommendation systems benchmarks",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "advertising",
      "machine-learning"
    ],
    "summary": "The Criteo AI Lab Datasets is the world's largest public machine learning dataset, comprising 1TB of click logs that capture 4 billion advertising events. Researchers and data scientists can utilize this extensive dataset to analyze user behavior, optimize advertising strategies, and benchmark machine learning models in the context of click-through rate (CTR) prediction and recommendations.",
    "use_cases": [
      "Analyzing click-through rates for advertising campaigns",
      "Developing recommendation systems based on user interactions",
      "Benchmarking machine learning models for advertising",
      "Studying consumer behavior in response to advertisements"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Criteo AI Lab Datasets",
      "public ML dataset for advertising",
      "1TB click logs dataset",
      "advertising events dataset",
      "Criteo click-through rate data",
      "machine learning benchmark datasets",
      "datasets for recommendations"
    ],
    "domain_tags": [
      "advertising",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "benchmark_usage": [
      "Benchmarking machine learning models in advertising"
    ],
    "model_score": 0.0006,
    "embedding_text": "The Criteo AI Lab Datasets represents a significant resource for researchers and practitioners in the fields of machine learning and advertising technology. This dataset is structured in a tabular format, consisting of a vast array of rows and columns that detail user interactions with advertisements. Each row corresponds to an individual advertising event, while the columns capture various attributes such as user ID, ad ID, timestamp, and other contextual information that can influence user behavior. The sheer volume of data\u20141TB encompassing 4 billion events\u2014provides a rich foundation for analysis and model training, allowing for comprehensive insights into advertising effectiveness and user engagement.\n\nThe collection methodology employed by Criteo involves aggregating click logs from their advertising platform, which serves a multitude of online advertisements across diverse domains. This dataset is particularly valuable due to its scale and the breadth of data it encompasses, making it suitable for a wide range of machine learning applications. However, researchers should be aware of potential limitations, such as data quality issues stemming from noise in user interactions or biases in ad placements that may affect the generalizability of findings.\n\nKey variables within the dataset include user identifiers, ad identifiers, timestamps of clicks, and various features that describe the context of each ad impression. These variables allow researchers to measure and analyze critical metrics such as click-through rates (CTR), conversion rates, and user engagement patterns. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing features, and transforming categorical variables into a suitable format for machine learning algorithms.\n\nResearchers typically leverage the Criteo AI Lab Datasets to address a variety of research questions, including but not limited to: What factors drive higher click-through rates for specific advertisements? How do user demographics and behaviors influence ad effectiveness? What machine learning models perform best in predicting user interactions with ads? The dataset supports various types of analyses, including regression analysis, machine learning model training, and descriptive statistics, enabling a comprehensive exploration of advertising dynamics.\n\nIn summary, the Criteo AI Lab Datasets serves as a benchmark for machine learning in the advertising sector, offering a wealth of data that can be harnessed to improve advertising strategies and enhance user experiences. Its extensive size, coupled with the richness of the data, makes it an invaluable asset for both academic research and practical applications in the field of digital marketing."
  },
  {
    "name": "Ukraine Procurement (ProZorro)",
    "description": "Public procurement data from ProZorro system",
    "category": "Auctions & Marketplaces",
    "url": "https://www.kaggle.com/datasets/oleksastepaniuk/prozorro-public-procurement-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "procurement",
      "government",
      "Ukraine"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "public procurement",
      "government data",
      "Ukraine"
    ],
    "summary": "The Ukraine Procurement (ProZorro) dataset contains public procurement data sourced from the ProZorro system, which is designed to enhance transparency and efficiency in government procurement processes in Ukraine. Researchers and analysts can utilize this dataset to explore procurement trends, analyze government spending, and assess the impact of procurement policies.",
    "use_cases": [
      "Analyzing trends in government procurement spending",
      "Evaluating the effectiveness of procurement policies",
      "Investigating supplier competition in public auctions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the ProZorro procurement system?",
      "How can I analyze public procurement data in Ukraine?",
      "What trends can be identified in Ukraine's government procurement?",
      "What are the key variables in the Ukraine Procurement dataset?",
      "How does ProZorro improve transparency in procurement?",
      "What types of analyses can be performed on procurement data?",
      "What are the implications of procurement data for government policy?",
      "How can I access the ProZorro dataset for research?"
    ],
    "domain_tags": [
      "government",
      "public sector"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ukraine",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/ukraine-procurement-prozorro.png",
    "embedding_text": "The Ukraine Procurement (ProZorro) dataset is a comprehensive collection of public procurement data that has been systematically gathered from the ProZorro system, which was established to foster transparency and accountability in government procurement processes in Ukraine. The dataset is structured in a tabular format, consisting of numerous rows and columns that capture various aspects of procurement activities, including details about contracts, suppliers, and procurement procedures. Each row typically represents a unique procurement event, while the columns may include variables such as the procurement ID, supplier information, contract value, procurement method, and dates of the procurement process. This structured approach allows for efficient data analysis and facilitates the extraction of meaningful insights from the dataset. The collection methodology for this dataset involves the systematic recording of procurement data as it is entered into the ProZorro system by government agencies and suppliers. This data is made publicly available to ensure transparency and to enable stakeholders to monitor government spending and procurement practices. The coverage of the dataset is primarily geographic, focusing on procurement activities within Ukraine, and it encompasses a wide range of procurement types and sectors. Key variables in the dataset measure various dimensions of procurement, such as the total value of contracts awarded, the number of bids submitted, and the duration of the procurement process. These variables are crucial for understanding the dynamics of public procurement in Ukraine and for identifying trends over time. However, it is important to note that the dataset may have limitations, including potential inconsistencies in data entry, variations in reporting standards across different agencies, and the possibility of incomplete records. Common preprocessing steps for this dataset may include data cleaning to address missing values, normalization of supplier names, and the transformation of date formats to ensure consistency across the dataset. Researchers can leverage this dataset to address a variety of research questions, such as examining the impact of procurement policies on supplier competition, analyzing the relationship between procurement methods and contract values, and assessing the effectiveness of government spending in achieving desired outcomes. The types of analyses supported by this dataset include descriptive statistics to summarize procurement activities, regression analyses to explore relationships between variables, and machine learning techniques to predict procurement outcomes based on historical data. In summary, the Ukraine Procurement (ProZorro) dataset serves as a valuable resource for researchers, policymakers, and analysts interested in understanding public procurement dynamics in Ukraine and improving the effectiveness of government procurement practices."
  },
  {
    "name": "Criteo Counterfactual Learning",
    "description": "25M logged interactions with counterfactual propensity scores. Gold standard for offline policy evaluation and causal inference in ads",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/criteo-uplift-prediction-dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "causal inference",
      "counterfactual",
      "advertising",
      "uplift modeling",
      "offline evaluation"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "advertising",
      "causal inference",
      "uplift modeling"
    ],
    "summary": "The Criteo Counterfactual Learning dataset consists of 25 million logged interactions that include counterfactual propensity scores. This dataset serves as a gold standard for offline policy evaluation and causal inference in advertising, enabling researchers to analyze the effectiveness of various advertising strategies and improve decision-making processes.",
    "use_cases": [
      "Evaluating the effectiveness of different advertising strategies",
      "Conducting causal inference studies in marketing",
      "Developing uplift models for targeted advertising"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Counterfactual Learning dataset?",
      "How can I use counterfactual propensity scores in advertising?",
      "What are the applications of causal inference in ads?",
      "What insights can be gained from analyzing logged interactions in advertising?",
      "How does uplift modeling work with this dataset?",
      "What are the challenges in offline evaluation of advertising policies?",
      "What types of analyses can be performed with the Criteo dataset?",
      "How can I access the Criteo Counterfactual Learning dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The Criteo Counterfactual Learning dataset is a comprehensive collection of 25 million logged interactions, specifically designed to facilitate advanced research in advertising through the lens of counterfactual propensity scores. This dataset is particularly valuable for researchers and practitioners interested in offline policy evaluation and causal inference, as it provides a robust framework for understanding the impact of various advertising strategies on consumer behavior. The data is structured in a tabular format, with rows representing individual interactions and columns capturing a variety of variables related to these interactions. Key variables may include user identifiers, timestamps, ad impressions, clicks, conversions, and the associated counterfactual propensity scores that estimate the likelihood of a user taking a specific action in response to an ad. The collection methodology for this dataset is grounded in real-world interactions, ensuring that the data reflects genuine user behavior in response to advertising campaigns. This authenticity enhances the dataset's utility for conducting meaningful analyses and deriving actionable insights. Researchers can leverage this dataset to address a range of research questions, such as evaluating the effectiveness of different advertising strategies, understanding the dynamics of consumer engagement, and exploring the intricacies of causal relationships in marketing. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for data scientists and marketers alike. However, users should be aware of potential limitations in data quality, such as missing values or biases inherent in the logged interactions, which may necessitate common preprocessing steps like data cleaning and normalization. Overall, the Criteo Counterfactual Learning dataset stands as a gold standard for those aiming to enhance their understanding of advertising effectiveness through rigorous analytical methodologies.",
    "benchmark_usage": [
      "Offline policy evaluation",
      "Causal inference in advertising"
    ]
  },
  {
    "name": "Avazu Click-Through Rate Dataset",
    "description": "Mobile advertising dataset with 40+ million ad click records from Avazu mobile advertising platform",
    "category": "Advertising",
    "url": "https://www.kaggle.com/c/avazu-ctr-prediction/data",
    "docs_url": "https://www.kaggle.com/c/avazu-ctr-prediction",
    "github_url": null,
    "tags": [
      "mobile ads",
      "CTR",
      "Kaggle",
      "Avazu"
    ],
    "best_for": "Mobile-specific CTR prediction and feature engineering",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "data analysis"
    ],
    "summary": "The Avazu Click-Through Rate Dataset is a comprehensive mobile advertising dataset containing over 40 million ad click records sourced from the Avazu mobile advertising platform. Researchers and analysts can utilize this dataset to explore click-through rates (CTR) in mobile advertising, enabling insights into user engagement and ad performance.",
    "use_cases": [
      "Analyzing user engagement with mobile ads",
      "Evaluating the effectiveness of different ad campaigns",
      "Predicting click-through rates based on ad features",
      "Segmenting users based on click behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Avazu Click-Through Rate Dataset?",
      "How can I analyze mobile advertising data?",
      "What insights can be gained from the Avazu dataset?",
      "Where can I find the Avazu Click-Through Rate Dataset?",
      "What variables are included in the Avazu mobile ads dataset?",
      "How to use the Avazu dataset for CTR analysis?",
      "What are common use cases for the Avazu Click-Through Rate Dataset?",
      "What is the size of the Avazu mobile advertising dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "10 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/avazu-click-through-rate-dataset.png",
    "embedding_text": "The Avazu Click-Through Rate Dataset is a significant resource for researchers and practitioners in the field of mobile advertising. This dataset comprises over 40 million records of ad clicks, providing a rich source of information for analyzing user interactions with mobile advertisements. The data is structured in a tabular format, with rows representing individual ad click events and columns containing various features related to the ads, such as ad ID, user ID, timestamp, and other relevant variables. The dataset enables users to explore the dynamics of click-through rates (CTR) in mobile advertising, which is crucial for understanding user engagement and optimizing ad performance. The collection methodology involves aggregating data from the Avazu mobile advertising platform, which is known for its extensive reach and diverse user base. This makes the dataset particularly valuable for examining trends and patterns in mobile ad interactions across different demographics and user segments. Key variables within the dataset include identifiers for ads and users, timestamps for when clicks occurred, and additional features that may relate to the ad content or the context in which the ad was displayed. These variables are essential for conducting various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically preprocess the data to handle missing values, normalize features, and create training and testing datasets for predictive modeling. Common research questions that can be addressed using this dataset include inquiries into which ad features correlate most strongly with higher CTRs, how user demographics influence click behavior, and what temporal trends can be observed in mobile ad performance. The dataset supports a wide range of analyses, from basic descriptive statistics to more complex predictive modeling techniques, making it a versatile tool for both academic research and practical applications in the advertising industry. Overall, the Avazu Click-Through Rate Dataset serves as a foundational resource for those looking to delve into the intricacies of mobile advertising and user engagement."
  },
  {
    "name": "iPinYou RTB Dataset",
    "description": "Real-time bidding dataset from Chinese DSP with ~35GB of bid requests, impressions, clicks, and conversions with bidding prices",
    "category": "Advertising",
    "url": "https://contest.ipinyou.com/",
    "docs_url": "https://github.com/wnzhang/make-ipinyou-data",
    "github_url": "https://github.com/wnzhang/make-ipinyou-data",
    "tags": [
      "RTB",
      "bidding",
      "conversions",
      "iPinYou"
    ],
    "best_for": "Bid optimization and RTB algorithm research with actual market prices",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "real-time bidding",
      "data analysis"
    ],
    "summary": "The iPinYou RTB Dataset is a comprehensive collection of real-time bidding data from a Chinese Demand-Side Platform (DSP), encompassing approximately 35GB of bid requests, impressions, clicks, and conversions along with associated bidding prices. This dataset can be utilized for analyzing bidding strategies, consumer behavior in digital advertising, and the effectiveness of ad placements.",
    "use_cases": [
      "Analyzing bidding strategies in digital advertising",
      "Evaluating the effectiveness of ad placements",
      "Studying consumer behavior through click and conversion data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the iPinYou RTB Dataset?",
      "How can I analyze bid requests in the iPinYou dataset?",
      "What insights can be gained from the iPinYou RTB Dataset?",
      "What variables are included in the iPinYou RTB Dataset?",
      "How does real-time bidding work in the iPinYou dataset?",
      "What are the common use cases for the iPinYou RTB Dataset?",
      "What kind of analyses can be performed with the iPinYou RTB Dataset?",
      "What are the limitations of the iPinYou RTB Dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "9 days (2013)",
    "geographic_scope": "China",
    "size_category": "large",
    "model_score": 0.0006,
    "image_url": "/images/datasets/ipinyou-rtb-dataset.png",
    "embedding_text": "The iPinYou RTB Dataset is a rich and extensive dataset that captures the dynamics of real-time bidding (RTB) in the digital advertising landscape, specifically sourced from a Chinese Demand-Side Platform (DSP). This dataset contains approximately 35GB of data, which includes a multitude of bid requests, impressions, clicks, and conversions, along with the corresponding bidding prices. The structure of the dataset is primarily tabular, comprising rows that represent individual bid requests and columns that detail various attributes associated with each request, such as timestamps, bid amounts, impression counts, click counts, and conversion rates. The dataset is instrumental for researchers and practitioners interested in understanding the intricacies of digital advertising, particularly in the context of RTB, where advertisers bid in real-time for ad placements based on user profiles and contextual factors.\n\nThe collection methodology for the iPinYou RTB Dataset involves aggregating data from numerous bidding events that occur on the platform, capturing the interactions between advertisers and publishers. This process ensures a comprehensive representation of the bidding landscape, allowing for a detailed analysis of how different variables influence bidding outcomes. Key variables within the dataset include bid price, which reflects the amount advertisers are willing to pay for an impression; click-through rate (CTR), which measures the effectiveness of ads in generating clicks; and conversion rate, which indicates the success of ads in leading to desired actions, such as purchases or sign-ups.\n\nWhile the dataset offers a wealth of information, it is essential to acknowledge certain limitations. Data quality can vary due to factors such as incomplete records or discrepancies in bid reporting. Researchers may need to perform common preprocessing steps, including data cleaning to handle missing values, normalization of bid prices, and transformation of categorical variables into numerical formats for analysis. These preprocessing steps are crucial for ensuring the integrity of the data and the validity of subsequent analyses.\n\nThe iPinYou RTB Dataset supports a variety of analytical approaches, including regression analysis to identify relationships between bidding strategies and outcomes, machine learning techniques for predictive modeling of click and conversion rates, and descriptive statistics to summarize the dataset's characteristics. Researchers typically utilize this dataset to address questions related to the effectiveness of different bidding strategies, the impact of user demographics on bidding behavior, and the overall efficiency of ad spend in achieving marketing objectives. By leveraging the insights derived from this dataset, stakeholders in the advertising industry can make informed decisions to optimize their bidding strategies and enhance the performance of their digital advertising campaigns."
  },
  {
    "name": "Harvard Dataverse Auctions",
    "description": "Auction-related replication datasets from Harvard Dataverse",
    "category": "Advertising",
    "url": "https://dataverse.harvard.edu/dataverse/harvard",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "auctions",
      "replication",
      "academic"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Harvard Dataverse Auctions dataset comprises auction-related replication datasets that are available for academic research. Researchers can utilize this dataset to analyze auction dynamics, consumer behavior, and pricing strategies in various contexts.",
    "use_cases": [
      "Analyzing consumer bidding behavior in auctions",
      "Studying the impact of auction formats on pricing strategies",
      "Evaluating the effectiveness of different auction mechanisms",
      "Investigating the relationship between bidder characteristics and auction outcomes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the auction-related datasets available in Harvard Dataverse?",
      "How can I access the Harvard Dataverse Auctions dataset?",
      "What types of analyses can be performed using auction replication datasets?",
      "What insights can be gained from studying auction dynamics?",
      "Are there any academic studies that utilize the Harvard Dataverse Auctions dataset?",
      "What variables are included in the auction-related datasets from Harvard Dataverse?",
      "How do auction strategies impact consumer behavior?",
      "What is the significance of replication datasets in auction research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/logos/harvard.png",
    "embedding_text": "The Harvard Dataverse Auctions dataset provides a rich resource for researchers interested in the field of auctions and their implications in economic behavior. This dataset is structured in a tabular format, consisting of rows that represent individual auction events and columns that capture various attributes of these auctions. Key variables may include auction ID, item description, starting bid, final bid, bidder information, and timestamps, among others. The data is collected through the Harvard Dataverse, which serves as a repository for academic datasets, ensuring that the information is reliable and sourced from credible academic research. While the specific collection methodology is not detailed, it is common for such datasets to be derived from controlled experiments or observational studies in academic settings. Researchers utilizing this dataset can expect to address a variety of research questions, such as how auction dynamics influence bidder behavior, the effectiveness of different auction formats, and the impact of pricing strategies on consumer participation. The dataset supports various types of analyses, including regression analysis to understand relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize auction outcomes. However, users should be aware of potential limitations in data quality, such as missing values or biases in bidder demographics, which may affect the generalizability of findings. Common preprocessing steps may include cleaning the data to handle missing values, normalizing bid amounts for comparative analysis, and encoding categorical variables for use in machine learning models. Overall, the Harvard Dataverse Auctions dataset is a valuable tool for academic researchers and data scientists looking to explore the complexities of auction systems and their effects on consumer behavior."
  },
  {
    "name": "NBER Public Use Data Archive",
    "description": "Eclectic mix of economic, demographic, and enterprise data from NBER-affiliated research projects",
    "category": "Data Portals",
    "url": "https://www.nber.org/research/data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "research",
      "research",
      "NBER",
      "demographics",
      "enterprise"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NBER Public Use Data Archive provides a diverse collection of economic, demographic, and enterprise data sourced from various research projects affiliated with the National Bureau of Economic Research (NBER). Researchers can utilize this dataset to explore a wide range of economic phenomena, demographic trends, and enterprise behaviors, making it a valuable resource for empirical analysis in economics and social sciences.",
    "use_cases": [
      "Analyzing the impact of economic policies on demographic changes.",
      "Studying enterprise behavior in response to market fluctuations.",
      "Investigating correlations between demographic factors and economic outcomes."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What types of economic data are available in the NBER Public Use Data Archive?",
      "How can I access demographic data from NBER-affiliated research projects?",
      "What enterprise data does the NBER Public Use Data Archive provide?",
      "Are there any datasets related to research on demographics in the NBER archive?",
      "How can I use NBER data for my economic research?",
      "What is the structure of the datasets in the NBER Public Use Data Archive?",
      "What research questions can be answered using NBER data?",
      "What types of analyses can be conducted with the NBER Public Use Data Archive?"
    ],
    "domain_tags": [
      "economics",
      "demographics",
      "enterprise"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/nber-public-use-data-archive.jpg",
    "embedding_text": "The NBER Public Use Data Archive is a comprehensive repository that encompasses a wide array of economic, demographic, and enterprise data derived from research projects affiliated with the National Bureau of Economic Research (NBER). This dataset serves as a vital resource for researchers and analysts looking to delve into empirical studies within the realms of economics and social sciences. The data structure typically consists of tabular formats, where each dataset is organized into rows and columns, with each row representing an individual observation and each column corresponding to a specific variable of interest. The variables included in these datasets may cover a range of topics, such as income levels, employment statistics, demographic characteristics, and enterprise performance metrics. The collection methodology employed by NBER involves rigorous data gathering processes, often utilizing surveys, administrative records, and other reliable sources to ensure the integrity and accuracy of the data. Researchers can expect to find key variables that measure critical aspects of economic and demographic phenomena, such as unemployment rates, population demographics, and business growth indicators. However, it is important to acknowledge that, like any dataset, the NBER Public Use Data Archive may have limitations regarding data quality, including potential biases in self-reported data or gaps in coverage for certain populations or time periods. Common preprocessing steps that researchers may need to undertake include data cleaning, normalization, and handling of missing values to prepare the data for analysis. The versatility of this dataset allows it to address a multitude of research questions, ranging from the evaluation of economic policies to the exploration of demographic shifts over time. Analysts can employ various types of analyses, including regression analysis, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically leverage the NBER Public Use Data Archive in their studies to support hypotheses, validate findings, and contribute to the broader understanding of economic and social dynamics. Overall, this dataset stands out as a crucial tool for those engaged in economic research and analysis, providing a rich foundation for exploring the intricate relationships between economic variables and demographic trends."
  },
  {
    "name": "Census Business Dynamics Statistics",
    "description": "8M+ establishments with firm age data. Job creation/destruction, startups, exits. Longitudinal firm dynamics since 1977",
    "category": "Data Portals",
    "url": "https://www.census.gov/programs-surveys/bds.html",
    "docs_url": "https://www.census.gov/programs-surveys/bds/documentation.html",
    "github_url": null,
    "tags": [
      "Census",
      "firm dynamics",
      "startups",
      "employment",
      "longitudinal"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Census Business Dynamics Statistics dataset provides insights into over 8 million establishments, focusing on firm age data, job creation and destruction, as well as startup and exit trends. This longitudinal dataset, which spans since 1977, allows researchers and analysts to explore the dynamics of firms over time, making it a valuable resource for understanding economic patterns and business cycles.",
    "use_cases": [
      "Analyzing trends in job creation and destruction over time.",
      "Studying the impact of firm age on employment rates.",
      "Investigating startup and exit patterns in various industries.",
      "Exploring longitudinal changes in firm dynamics."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the job creation trends in the Census Business Dynamics Statistics?",
      "How can I analyze firm age data using the Census dataset?",
      "What insights can be derived from the longitudinal firm dynamics data?",
      "What are the startup and exit rates according to the Census Business Dynamics Statistics?",
      "How does firm age impact job creation and destruction?",
      "Where can I find data on over 8 million establishments?",
      "What is the temporal coverage of the Census Business Dynamics Statistics?",
      "How has the business landscape changed since 1977?"
    ],
    "domain_tags": [
      "economics",
      "business",
      "employment"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1977-present",
    "size_category": "massive",
    "model_score": 0.0006,
    "image_url": "/images/datasets/census-business-dynamics-statistics.jpg",
    "embedding_text": "The Census Business Dynamics Statistics dataset is a comprehensive resource that encompasses over 8 million establishments, providing detailed insights into various aspects of firm dynamics, including job creation and destruction, as well as the lifecycle of startups and exits. The dataset is structured in a tabular format, with rows representing individual establishments and columns capturing key variables such as firm age, employment figures, and the status of the firm (active, exited, etc.). This extensive dataset has been collected through rigorous methodologies, utilizing administrative records and surveys conducted by the U.S. Census Bureau, ensuring a high level of data quality and reliability. The temporal coverage of the dataset extends from 1977 to the present, allowing researchers to analyze trends over several decades and understand how economic conditions have influenced business dynamics over time. However, users should be aware of potential limitations, such as variations in reporting standards and the challenges of capturing informal business activities. Key variables within the dataset include firm age, which measures the number of years since establishment, and employment figures, which provide insights into the size and impact of firms within the economy. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing employment figures, and categorizing firms based on age or industry. Researchers typically utilize this dataset to address a variety of research questions, such as examining the correlation between firm age and job creation, analyzing the factors that contribute to business exits, and exploring the overall dynamics of the business landscape. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for economists, business analysts, and policymakers. By leveraging the insights gained from the Census Business Dynamics Statistics, researchers can contribute to a deeper understanding of economic trends, inform policy decisions, and enhance the knowledge base surrounding business dynamics."
  },
  {
    "name": "DrivenData Water Supply Forecasting (2024)",
    "description": "Western US water supply data from Bureau of Reclamation, $500K prize pool for seasonal forecasting",
    "category": "Data Portals",
    "url": "https://www.drivendata.org/competitions/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "climate",
      "forecasting",
      "2024",
      "government data",
      "real-world",
      "time series"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The DrivenData Water Supply Forecasting dataset provides valuable insights into water supply data from the Bureau of Reclamation, specifically for the Western United States. This dataset can be utilized for seasonal forecasting, enabling researchers and data scientists to analyze trends and make predictions regarding water availability, which is crucial for resource management and planning.",
    "use_cases": [
      "Seasonal forecasting of water supply for agricultural planning",
      "Analyzing the impact of climate change on water availability",
      "Developing predictive models for water resource management",
      "Evaluating historical water supply trends and patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the DrivenData Water Supply Forecasting dataset?",
      "How can I access the Bureau of Reclamation water supply data?",
      "What are the key variables in the DrivenData water supply dataset?",
      "What types of analyses can be performed with the water supply forecasting data?",
      "What is the prize pool for the DrivenData water supply forecasting competition?",
      "How is the water supply data collected in the Western US?",
      "What are the applications of seasonal forecasting in water supply management?",
      "What challenges are associated with analyzing water supply data?"
    ],
    "domain_tags": [
      "government data",
      "climate"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2024",
    "geographic_scope": "Western US",
    "size_category": "medium",
    "model_score": 0.0006,
    "embedding_text": "The DrivenData Water Supply Forecasting dataset is a comprehensive collection of water supply data sourced from the Bureau of Reclamation, focusing on the Western United States. This dataset is designed to facilitate seasonal forecasting, which is essential for effective water resource management, especially in regions where water scarcity is a pressing concern. The dataset likely includes a variety of variables related to water supply, such as historical water levels, precipitation data, and possibly temperature readings, structured in a time-series format that allows for the analysis of changes over time. Researchers and data scientists can leverage this dataset to address critical research questions related to water availability, the impact of climate variability on water resources, and the development of predictive models that can inform policy and planning decisions. The data collection methodology involves systematic gathering of information from various sources, ensuring a robust dataset that reflects real-world conditions. However, users should be aware of potential limitations in data quality, such as gaps in historical records or inconsistencies in measurement techniques. Common preprocessing steps may include data cleaning to handle missing values, normalization of data for comparative analysis, and feature engineering to extract meaningful insights from the raw data. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for researchers interested in environmental science, resource management, and climate studies. By engaging with this dataset, users can contribute to a deeper understanding of water supply dynamics and develop strategies to mitigate the impacts of water scarcity in the face of ongoing climate challenges."
  },
  {
    "name": "Indian Sales",
    "description": "Sales forecasting dataset for small basket items in India",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/girishvutukuri/sales-forecasting-for-small-basket",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "India",
      "forecasting",
      "retail"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Indian Sales dataset is designed for sales forecasting of small basket items in India, providing valuable insights into retail trends. Researchers and analysts can utilize this dataset to predict sales patterns, optimize inventory, and enhance marketing strategies.",
    "use_cases": [
      "Sales trend analysis",
      "Inventory optimization",
      "Marketing strategy development"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Indian Sales dataset?",
      "How can I use the Indian Sales dataset for forecasting?",
      "What insights can be gained from the Indian Sales dataset?",
      "Where can I find sales forecasting datasets for India?",
      "What are the key variables in the Indian Sales dataset?",
      "How does the Indian Sales dataset support retail analysis?",
      "What types of analyses can be performed with the Indian Sales dataset?",
      "What is the significance of small basket items in retail sales forecasting?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/indian-sales.png",
    "embedding_text": "The Indian Sales dataset is a comprehensive resource tailored for sales forecasting, particularly focusing on small basket items within the Indian retail sector. This dataset is structured in a tabular format, comprising various rows and columns that capture essential variables related to sales transactions. Each row typically represents a unique sales transaction, while the columns encompass key variables such as item identifiers, sales amounts, transaction dates, and possibly customer demographics. The data collection methodology for this dataset involves aggregating sales data from retail outlets across India, ensuring a diverse representation of consumer behavior and purchasing patterns. Although the specific collection methods are not detailed, it is common for such datasets to utilize point-of-sale systems and transaction logs to gather accurate sales information. The dataset's coverage is primarily geographic, focusing on the Indian market, which is characterized by a rich tapestry of consumer preferences and shopping habits. While temporal coverage is not explicitly mentioned, sales forecasting datasets often span multiple time periods, allowing for seasonal analysis and trend identification. Key variables in the dataset may include sales volume, item category, pricing information, and promotional activities, each measuring different aspects of the retail environment. However, potential limitations may arise from data quality issues, such as missing values or inconsistencies in transaction reporting, which are common in large-scale retail datasets. Researchers utilizing this dataset may need to perform common preprocessing steps, including data cleaning, normalization, and feature engineering, to prepare the data for analysis. The Indian Sales dataset supports a variety of research questions, such as understanding the impact of pricing strategies on sales performance, identifying seasonal trends in consumer purchasing behavior, and evaluating the effectiveness of marketing campaigns. Analysts can employ various analytical techniques, including regression analysis, machine learning models, and descriptive statistics, to extract meaningful insights from the data. Typically, researchers leverage this dataset to inform business decisions, enhance sales forecasting accuracy, and develop targeted marketing strategies, ultimately contributing to improved operational efficiency within the retail sector. By analyzing the sales patterns of small basket items, stakeholders can gain a deeper understanding of consumer preferences, enabling them to tailor their offerings and optimize inventory management."
  },
  {
    "name": "Ipinyou RTB",
    "description": "Real-time bidding (RTB) dataset for CTR prediction",
    "category": "Advertising",
    "url": "https://github.com/wnzhang/make-ipinyou-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "RTB",
      "advertising",
      "CTR",
      "programmatic"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "data science",
      "machine learning"
    ],
    "summary": "The Ipinyou RTB dataset is designed for real-time bidding applications, specifically focusing on click-through rate (CTR) prediction. Researchers and practitioners can utilize this dataset to develop and evaluate algorithms that optimize ad placements and improve advertising effectiveness.",
    "use_cases": [
      "Developing predictive models for CTR",
      "Analyzing bidding strategies in advertising",
      "Evaluating the effectiveness of ad placements"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Ipinyou RTB dataset?",
      "How can I use the Ipinyou RTB dataset for CTR prediction?",
      "What variables are included in the Ipinyou RTB dataset?",
      "Where can I find the Ipinyou RTB dataset?",
      "What are the applications of the Ipinyou RTB dataset?",
      "How is the Ipinyou RTB dataset structured?",
      "What research questions can be addressed with the Ipinyou RTB dataset?",
      "What preprocessing is needed for the Ipinyou RTB dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/ipinyou-rtb.png",
    "embedding_text": "The Ipinyou RTB dataset is a comprehensive resource for researchers and practitioners interested in the field of real-time bidding (RTB) and click-through rate (CTR) prediction. This dataset is structured in a tabular format, consisting of rows and columns that represent various aspects of online advertising transactions. Each row corresponds to a unique ad impression, while the columns include key variables such as ad ID, user ID, timestamp, bid price, and whether the ad was clicked. The dataset is designed to facilitate the analysis of user behavior and ad performance in a programmatic advertising context.\n\nThe collection methodology for the Ipinyou RTB dataset involves gathering data from real-time bidding platforms, where advertisers bid for ad placements in real-time auctions. This data is typically collected over a specified period, capturing a wide range of ad impressions and user interactions. The dataset may include information on various demographic factors, although specific demographic coverage is not explicitly mentioned. Researchers should be aware of potential limitations in data quality, such as missing values or biases in user behavior that could affect the accuracy of predictive models.\n\nKey variables in the dataset include the ad ID, which identifies the specific advertisement being displayed, the user ID, which represents the individual viewing the ad, and the timestamp, indicating when the impression occurred. The bid price reflects the amount an advertiser is willing to pay for the impression, while the click variable indicates whether the ad was clicked by the user. These variables are crucial for measuring the effectiveness of different advertising strategies and understanding user engagement with ads.\n\nCommon preprocessing steps for the Ipinyou RTB dataset may include handling missing values, normalizing bid prices, and encoding categorical variables for use in machine learning algorithms. Researchers often perform exploratory data analysis to identify patterns and trends in the data, which can inform the development of predictive models. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing researchers to explore the relationships between ad characteristics, bidding strategies, and user behavior.\n\nIn studies, researchers typically use the Ipinyou RTB dataset to address research questions related to ad effectiveness, user engagement, and the optimization of bidding strategies. By analyzing the dataset, they can develop algorithms that predict CTR based on historical data, evaluate the impact of different ad placements, and assess the overall performance of advertising campaigns. The insights gained from this dataset can help improve the efficiency of programmatic advertising and enhance the decision-making processes of advertisers."
  },
  {
    "name": "EU Open Data Portal",
    "description": "2 million datasets from 205 catalogues across 36 European countries",
    "category": "Dataset Aggregators",
    "url": "https://data.europa.eu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "EU",
      "Europe",
      "Eurostat",
      "open data"
    ],
    "best_for": "European economic data including comprehensive Eurostat statistics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EU Open Data Portal provides access to 2 million datasets sourced from 205 catalogues across 36 European countries. This extensive collection allows users to explore a wide range of topics, conduct analyses, and derive insights related to various sectors in Europe.",
    "use_cases": [
      "Analyzing economic indicators across European countries",
      "Comparing demographic data from various regions in Europe",
      "Conducting research on environmental statistics in the EU",
      "Exploring public health data trends in different European nations"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on the EU Open Data Portal?",
      "How can I access datasets from 36 European countries?",
      "What types of data are included in the EU Open Data Portal?",
      "Where can I find open data related to Eurostat?",
      "What are the categories of datasets available on the EU Open Data Portal?",
      "How many datasets does the EU Open Data Portal offer?",
      "What is the significance of the EU Open Data Portal for researchers?",
      "How can I utilize the datasets from the EU Open Data Portal in my analysis?"
    ],
    "domain_tags": [
      "government",
      "economics",
      "social sciences"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "geographic_scope": "Europe",
    "model_score": 0.0006,
    "image_url": "/images/logos/europa.png",
    "embedding_text": "The EU Open Data Portal serves as a comprehensive repository of datasets, aggregating information from 205 distinct catalogues across 36 European countries. This portal is designed to facilitate access to a vast array of data, encompassing various sectors such as economics, health, environment, and social sciences. The datasets are structured in a tabular format, allowing for easy manipulation and analysis using standard data analysis tools. Each dataset typically consists of multiple rows and columns, where rows represent individual data entries and columns correspond to different variables or attributes related to the data. The collection methodology involves sourcing data from official statistics, governmental organizations, and other credible institutions, ensuring a high level of data quality and reliability. However, users should be aware of potential limitations, such as data completeness and the frequency of updates, which may vary across datasets. Researchers can leverage this portal to address a multitude of research questions, ranging from economic trends and demographic shifts to public health concerns and environmental changes. The datasets support various types of analyses, including descriptive statistics, regression analysis, and machine learning applications. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. Overall, the EU Open Data Portal is an invaluable resource for researchers, policymakers, and anyone interested in exploring the rich tapestry of data available across Europe."
  },
  {
    "name": "Google Play Store Dataset",
    "description": "2.3M apps with ratings, reviews, categories, sizes, installs. Android app marketplace data",
    "category": "App Stores",
    "url": "https://www.kaggle.com/datasets/gauthamp10/google-playstore-apps",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Google Play",
      "Android",
      "apps",
      "ratings",
      "Kaggle"
    ],
    "best_for": "Learning app stores analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "mobile-apps"
    ],
    "summary": "The Google Play Store Dataset contains data on 2.3 million Android applications, including their ratings, reviews, categories, sizes, and install counts. This dataset allows researchers and analysts to explore trends in mobile app performance, user preferences, and market dynamics within the Android app ecosystem.",
    "use_cases": [
      "Analyzing trends in app ratings over time.",
      "Comparing app performance across different categories.",
      "Investigating the relationship between app size and install counts.",
      "Exploring user sentiment through reviews."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Google Play Store Dataset?",
      "How many apps are included in the Google Play Store Dataset?",
      "What types of data are available in the Google Play Store Dataset?",
      "What insights can be derived from analyzing app ratings and reviews?",
      "How can I access the Google Play Store Dataset?",
      "What categories of apps are included in the Google Play Store Dataset?",
      "What are the common sizes of apps in the Google Play Store Dataset?",
      "How can I use the Google Play Store Dataset for market analysis?"
    ],
    "domain_tags": [
      "retail",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/datasets/google-play-store-dataset.jpg",
    "embedding_text": "The Google Play Store Dataset is a comprehensive collection of data pertaining to 2.3 million Android applications available on the Google Play Store. This dataset encompasses a wide array of variables, including app ratings, reviews, categories, sizes, and install counts, providing a rich resource for researchers and analysts interested in the mobile app marketplace. The data is structured in a tabular format, with each row representing a unique application and columns detailing various attributes such as app name, developer, rating, number of reviews, category, size, and total installs. This structure facilitates straightforward analysis and manipulation using data analysis tools and programming languages such as Python and R.\n\nThe collection methodology for this dataset typically involves web scraping techniques or API access to gather information directly from the Google Play Store. This ensures that the data is both current and relevant, reflecting the latest trends and user interactions within the app marketplace. However, it is important to note that the dataset may have limitations regarding data quality, as it relies on user-generated content for reviews and ratings, which can be subjective and may not always represent the true performance of an app.\n\nKey variables within the dataset include app ratings, which measure user satisfaction on a scale, and the number of reviews, which indicates the level of user engagement. Categories provide insight into the type of app, such as games, productivity, or social media, while size and install counts can be used to assess app popularity and market penetration. Researchers can leverage this dataset to address a variety of research questions, such as how app ratings correlate with install counts, what factors influence user reviews, and how different categories of apps perform relative to one another.\n\nCommon preprocessing steps for this dataset may include cleaning the data to handle missing values, normalizing ratings, and categorizing reviews for sentiment analysis. The dataset supports various types of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize app performance metrics.\n\nIn studies, researchers typically use the Google Play Store Dataset to explore consumer behavior in the mobile app market, evaluate the impact of app features on user satisfaction, and identify trends in app development and user preferences. The insights gained from this dataset can inform marketing strategies, app development decisions, and overall understanding of the mobile app ecosystem."
  },
  {
    "name": "IPUMS",
    "description": "Harmonized microdata from US Census (1850-present), ACS, CPS, and 103+ countries' censuses",
    "category": "Dataset Aggregators",
    "url": "https://www.ipums.org",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "census",
      "microdata",
      "harmonized",
      "demographics"
    ],
    "best_for": "Harmonized census microdata across time and countries - free for academic research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The IPUMS dataset provides harmonized microdata from the US Census and other international censuses, allowing researchers to analyze demographic trends over time. It enables users to explore various aspects of population data, including age, gender, and socioeconomic status across different countries and time periods.",
    "use_cases": [
      "Analyzing demographic shifts in the US population over the last century.",
      "Comparing socioeconomic indicators across different countries using harmonized data.",
      "Investigating the impact of immigration on labor market outcomes.",
      "Studying trends in household composition and family structures."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the IPUMS dataset?",
      "How can I access harmonized microdata from the US Census?",
      "What types of demographic data are available in IPUMS?",
      "How does IPUMS facilitate cross-national comparisons of census data?",
      "What are the key features of the IPUMS dataset?",
      "In what ways can researchers utilize IPUMS for demographic analysis?",
      "What time periods does the IPUMS dataset cover?",
      "What are the limitations of using IPUMS for research?"
    ],
    "domain_tags": [
      "demographics",
      "sociology",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0006,
    "image_url": "/images/logos/ipums.png",
    "embedding_text": "The IPUMS dataset is a comprehensive collection of harmonized microdata derived from the US Census and various international census sources, spanning from 1850 to the present. This dataset is structured in a tabular format, consisting of rows representing individual respondents and columns that capture a wide array of demographic variables, such as age, sex, race, marital status, education level, and occupation. The data schema is designed to facilitate easy access and analysis, with consistent variable definitions across different time periods and geographic locations. Researchers can leverage this dataset to address a multitude of research questions related to population dynamics, social structures, and economic conditions. The collection methodology involves meticulous data harmonization processes that ensure comparability across diverse census datasets from over 103 countries. This harmonization allows for robust cross-national analyses and enables researchers to draw meaningful conclusions about global demographic trends. Coverage of the dataset includes a rich temporal dimension, with data available from the mid-19th century to contemporary times, allowing for longitudinal studies that can reveal shifts in demographic patterns over time. The geographic scope encompasses the United States and numerous other countries, providing a broad context for comparative research. Key variables within the dataset are meticulously documented, detailing what each variable measures and how it can be utilized in analysis. However, researchers should be aware of certain limitations, such as potential inconsistencies in data collection methods across different countries and time periods, which may affect the comparability of some variables. Common preprocessing steps may include data cleaning, handling missing values, and recoding variables to align with specific research needs. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for scholars in sociology, economics, and public policy. Researchers typically use IPUMS to explore questions related to demographic changes, socioeconomic disparities, and the effects of policy interventions on population groups, thereby contributing to a deeper understanding of societal trends and challenges."
  },
  {
    "name": "Harvard Dataverse",
    "description": "Global network of 120+ Dataverse installations hosting 75,000+ datasets with free 1TB storage",
    "category": "Dataset Aggregators",
    "url": "https://dataverse.harvard.edu",
    "docs_url": "https://guides.dataverse.org",
    "github_url": "https://github.com/IQSS/dataverse",
    "tags": [
      "social science",
      "replication",
      "DOI",
      "academic"
    ],
    "best_for": "Social science data with support for Stata, SPSS, and R formats",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "social science",
      "academic research",
      "data sharing"
    ],
    "summary": "The Harvard Dataverse is a global network that hosts over 75,000 datasets across more than 120 installations. It provides researchers with free access to 1TB of storage, facilitating the sharing and replication of academic data.",
    "use_cases": [
      "Analyzing trends in social science research",
      "Conducting replication studies using datasets from Harvard Dataverse",
      "Exploring academic datasets for educational purposes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the Harvard Dataverse?",
      "How can I access datasets on social science from Harvard Dataverse?",
      "What is the process for submitting a dataset to Harvard Dataverse?",
      "Are there any datasets related to replication studies in Harvard Dataverse?",
      "How does Harvard Dataverse support academic research?",
      "What types of data can be found in the Harvard Dataverse?"
    ],
    "domain_tags": [
      "education",
      "research",
      "social science"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/harvard-dataverse.png",
    "embedding_text": "The Harvard Dataverse serves as a comprehensive repository for datasets, primarily focusing on social science research. With over 75,000 datasets available, it provides a rich resource for researchers looking to explore a variety of topics within the social sciences. The data structure typically consists of rows representing individual datasets and columns detailing various attributes such as dataset title, authors, publication date, and keywords. Each dataset may contain multiple variables that measure different aspects of the research topic, allowing for diverse analyses. The collection methodology involves contributions from researchers worldwide, ensuring a broad range of data sources and perspectives. However, users should be aware of potential data quality issues, including variations in dataset documentation and completeness. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers can utilize the Harvard Dataverse to address various research questions, such as examining the impact of social policies, understanding demographic trends, or investigating replication in academic studies. The datasets support a wide array of analytical techniques, including regression analysis, machine learning, and descriptive statistics. By leveraging the resources available in the Harvard Dataverse, researchers can enhance their studies, contribute to the academic community, and promote transparency and reproducibility in research.",
    "benchmark_usage": [
      "Data sharing",
      "Replication studies"
    ]
  },
  {
    "name": "Brazilian Store Chain",
    "description": "Sales data from Brazilian retail chain",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/marcio486/sales-data-for-a-chain-of-brazilian-stores",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "Brazil",
      "chain stores"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Brazilian Store Chain dataset contains sales data from a retail chain in Brazil, providing insights into consumer purchasing behavior and sales trends. Researchers can analyze this data to understand market dynamics, optimize pricing strategies, and enhance inventory management.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Optimizing pricing strategies",
      "Forecasting sales trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Brazilian retail sales data",
      "sales data for grocery stores in Brazil",
      "Brazilian chain store sales analysis",
      "consumer behavior in Brazilian supermarkets",
      "retail data analysis Brazil",
      "grocery sales trends in Brazil"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/brazilian-store-chain.png",
    "embedding_text": "The Brazilian Store Chain dataset is a comprehensive collection of sales data from a prominent retail chain in Brazil, structured in a tabular format. This dataset typically includes various rows representing individual sales transactions, with columns detailing key variables such as transaction ID, product ID, quantity sold, sale price, date of transaction, and possibly customer demographics. The data collection methodology likely involves point-of-sale systems that capture sales transactions in real-time, ensuring a rich dataset for analysis. The geographic coverage is specifically focused on Brazil, providing insights into the local retail market dynamics. While the dataset does not explicitly mention temporal coverage, it is essential for researchers to understand the time frame of the data to contextualize their analyses. Key variables within the dataset measure aspects such as sales volume, revenue generated, and customer purchasing behavior, which can be instrumental in addressing various research questions related to consumer behavior and market trends. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in transaction records, which may necessitate common preprocessing steps like data cleaning and normalization. The dataset supports a range of analyses, including regression analysis to identify factors influencing sales, machine learning models for predictive analytics, and descriptive statistics to summarize sales trends. Researchers typically utilize this dataset to explore questions such as how pricing strategies affect sales volume, the impact of promotional campaigns on consumer behavior, and the identification of seasonal trends in grocery purchases. Overall, the Brazilian Store Chain dataset serves as a valuable resource for understanding the intricacies of the retail sector in Brazil, offering numerous opportunities for insightful analyses and research."
  },
  {
    "name": "Netflix Engagement Reports",
    "description": "Hours viewed for every Netflix title (original and licensed) watched >50K hours. First public streaming metrics since 2021",
    "category": "Entertainment & Media",
    "url": "https://about.netflix.com/en/news/what-we-watched-a-netflix-engagement-report",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Netflix",
      "streaming",
      "engagement",
      "viewership",
      "hours watched"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "media",
      "viewership"
    ],
    "summary": "The Netflix Engagement Reports dataset provides detailed metrics on the hours viewed for every Netflix title that has been watched for more than 50,000 hours. This dataset serves as the first public streaming metrics released since 2021, allowing researchers and analysts to explore viewer engagement trends and patterns across various Netflix titles.",
    "use_cases": [
      "Analyzing viewer engagement trends over time",
      "Comparing performance of original vs licensed Netflix content",
      "Investigating factors influencing hours viewed",
      "Exploring demographic differences in viewership patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the hours viewed for Netflix titles?",
      "How can I analyze Netflix engagement metrics?",
      "What streaming metrics are available for Netflix since 2021?",
      "Which Netflix titles have over 50K hours viewed?",
      "What is the viewership trend for Netflix originals?",
      "How does licensed content perform compared to original content on Netflix?",
      "What insights can be derived from Netflix engagement reports?",
      "How to visualize Netflix viewership data?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/netflix-engagement-reports.png",
    "embedding_text": "The Netflix Engagement Reports dataset is a comprehensive collection of streaming metrics that captures the viewing hours for every Netflix title, both original and licensed, that has surpassed 50,000 hours of viewership. This dataset is particularly significant as it marks the first public release of streaming metrics by Netflix since 2021, providing valuable insights into viewer engagement and content performance on the platform. The data is structured in a tabular format, where each row represents a unique Netflix title, and the columns include variables such as title name, type (original or licensed), and total hours viewed. This structure allows for straightforward analysis and visualization of the data. The collection methodology for this dataset involves aggregating viewership data directly from Netflix's streaming service, ensuring a high level of accuracy and relevance. However, it is important to note that while the dataset provides extensive metrics, it may not capture all titles or account for variations in viewing habits across different demographics or regions. Researchers and analysts can utilize this dataset to explore a variety of research questions, such as identifying trends in viewer engagement over time, comparing the performance of original content versus licensed titles, and understanding the factors that influence viewership patterns. Common preprocessing steps may include cleaning the data for any inconsistencies, normalizing the hours viewed for comparative analysis, and potentially enriching the dataset with additional demographic or contextual information. The dataset supports various types of analyses, including descriptive statistics to summarize the data, regression analysis to identify relationships between variables, and machine learning techniques to predict future viewing trends based on historical data. Researchers typically employ this dataset in studies focused on consumer behavior, content strategy, and the evolving landscape of digital media consumption, making it a valuable resource for anyone interested in the entertainment and media industry."
  },
  {
    "name": "Patreon Creator Data",
    "description": "279K+ active creators with membership tiers and patron counts. Creator economy platform metrics from Graphtreon",
    "category": "Creator Economy",
    "url": "https://graphtreon.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Patreon",
      "creators",
      "memberships",
      "subscriptions",
      "creator economy"
    ],
    "best_for": "Learning creator economy analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "creator-economy",
      "subscriptions",
      "digital-platforms"
    ],
    "summary": "The Patreon Creator Data dataset contains information on over 279,000 active creators, including their membership tiers and patron counts. This dataset provides valuable insights into the creator economy, allowing researchers and analysts to explore trends in subscriptions and memberships across various creators on the platform.",
    "use_cases": [
      "Analyzing trends in creator memberships over time",
      "Comparing patron counts across different creator categories",
      "Exploring the relationship between membership tiers and patron engagement",
      "Identifying successful strategies for monetization in the creator economy"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the active creators on Patreon?",
      "How many membership tiers do creators typically have?",
      "What is the average patron count for creators on Patreon?",
      "What trends can be observed in the creator economy?",
      "How do different creators structure their memberships?",
      "What metrics are available for analyzing Patreon creators?",
      "How does the patron count vary across different categories of creators?",
      "What insights can be drawn from the Patreon Creator Data?"
    ],
    "domain_tags": [
      "digital-content",
      "entertainment",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/logos/graphtreon.png",
    "embedding_text": "The Patreon Creator Data dataset is a comprehensive collection of information on over 279,000 active creators utilizing the Patreon platform, which is a well-known service for creators to monetize their content through memberships. This dataset includes various metrics such as membership tiers, patron counts, and other relevant data that can be leveraged to understand the dynamics of the creator economy. The data is structured in a tabular format, with each row representing an individual creator and columns detailing key variables such as creator ID, membership tier details, and the number of patrons associated with each tier. This structure allows for straightforward analysis and manipulation using data analysis tools such as pandas in Python.\n\nThe collection methodology for this dataset involves aggregating data from Graphtreon, a platform that tracks creator metrics on Patreon. Graphtreon compiles publicly available data from creators' profiles, ensuring that the dataset reflects real-time statistics on creator activities and patron interactions. While the dataset provides a wealth of information, it is important to note that data quality may vary based on the accuracy of the information provided by creators on their profiles. Additionally, there may be limitations regarding the completeness of data for creators who do not publicly disclose their membership details.\n\nKey variables in this dataset include the number of patrons, which measures the level of support each creator receives, and the structure of membership tiers, which indicates how creators choose to monetize their content. These variables are crucial for understanding the economic viability of different creator strategies and can inform research questions related to the effectiveness of various membership models.\n\nCommon preprocessing steps for this dataset may involve cleaning the data to handle any inconsistencies or missing values, as well as normalizing the data for comparative analysis. Researchers can utilize this dataset to address a variety of research questions, such as examining the impact of membership tier structures on patron retention or analyzing the correlation between creator engagement and patron counts.\n\nThe types of analyses supported by this dataset include regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize the data. Researchers typically use this dataset to gain insights into the creator economy, identify trends, and develop strategies for content monetization, making it a valuable resource for anyone interested in the intersection of technology and economics in the digital content space."
  },
  {
    "name": "Brazilian Drugs (ANVISA)",
    "description": "Sales data for controlled substances reported by ANVISA",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/tiagoacardoso/venda-medicamentos-controlados-anvisa",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "pharmaceuticals",
      "Brazil",
      "regulated"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Brazilian Drugs dataset contains sales data for controlled substances reported by ANVISA, providing insights into the pharmaceutical market in Brazil. Researchers can analyze trends in drug sales, consumer behavior, and the impact of regulations on the market.",
    "use_cases": [
      "Analyzing the impact of regulatory changes on drug sales.",
      "Investigating consumer purchasing patterns for pharmaceuticals.",
      "Evaluating pricing strategies in the Brazilian drug market."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends for controlled substances in Brazil?",
      "How do regulations affect pharmaceutical sales in Brazil?",
      "What consumer behaviors can be observed in the Brazilian drug market?",
      "What is the impact of pricing on drug sales in Brazil?",
      "How does the sales data for pharmaceuticals vary by region in Brazil?",
      "What are the most commonly sold controlled substances in Brazil?"
    ],
    "domain_tags": [
      "retail",
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/brazilian-drugs-anvisa.jpeg",
    "embedding_text": "The Brazilian Drugs dataset, provided by ANVISA, is a comprehensive collection of sales data for controlled substances in Brazil. This dataset is structured in a tabular format, consisting of rows representing individual sales transactions and columns detailing various attributes of each transaction. Key variables may include the type of substance sold, quantity, price, date of sale, and location of the sale, among others. The data is collected through mandatory reporting by pharmaceutical companies to ANVISA, ensuring a level of reliability and compliance with regulatory standards. However, researchers should be aware of potential limitations in data quality, such as incomplete reporting or variations in data entry practices across different companies.\n\nThe dataset covers the Brazilian market specifically, making it a valuable resource for understanding the dynamics of drug sales within this geographic context. While the temporal coverage is not explicitly mentioned, researchers can utilize the dataset to analyze trends over time if they have access to historical data. The demographic coverage is primarily focused on the consumer base in Brazil, which may vary by region and socioeconomic status.\n\nKey variables in this dataset measure aspects such as sales volume, pricing strategies, and regulatory compliance. Researchers can conduct various types of analyses, including regression analysis to identify factors influencing sales, machine learning models to predict future sales trends, and descriptive statistics to summarize the data. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing prices for inflation, and categorizing substances into relevant groups for analysis.\n\nThis dataset supports a range of research questions, such as examining the effects of pricing on consumer purchasing behavior, assessing the impact of regulatory changes on market dynamics, and exploring the relationship between sales and demographic factors. Researchers typically use this dataset to inform policy decisions, develop marketing strategies, and enhance understanding of the pharmaceutical landscape in Brazil. Overall, the Brazilian Drugs dataset serves as a critical tool for both academic research and industry analysis, providing insights into the complex interplay between regulation, market forces, and consumer behavior in the pharmaceutical sector."
  },
  {
    "name": "Rossmann Store Sales",
    "description": "1,115 Rossmann drug stores historical sales data",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/rossmann-store-sales",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "Germany",
      "Kaggle competition"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Rossmann Store Sales dataset contains historical sales data from 1,115 Rossmann drug stores in Germany. This dataset can be used for various analyses, including sales forecasting, understanding consumer behavior, and optimizing inventory management.",
    "use_cases": [
      "Sales forecasting for retail stores",
      "Analyzing the impact of promotions on sales",
      "Understanding consumer purchasing behavior",
      "Optimizing inventory management based on sales patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical sales trends for Rossmann stores?",
      "How do seasonal changes affect sales in drug stores?",
      "What factors influence sales performance in retail?",
      "How can sales data be used to predict future performance?",
      "What is the impact of promotions on sales in Rossmann stores?",
      "How do different store locations perform in terms of sales?",
      "What are the common sales patterns observed in grocery stores?",
      "How can machine learning be applied to retail sales data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/rossmann-store-sales.png",
    "embedding_text": "The Rossmann Store Sales dataset is a comprehensive collection of historical sales data from 1,115 Rossmann drug stores located across Germany. This dataset is structured in a tabular format, containing rows that represent individual sales transactions and columns that capture various attributes related to each transaction. Key variables in the dataset include store identifiers, sales amounts, customer counts, promotional activities, and dates of transactions. The data is collected from the operational systems of Rossmann stores, ensuring a high level of accuracy and relevance to real-world retail scenarios. However, like any dataset, it may have limitations, such as missing values or anomalies due to human error or system glitches during data entry. Researchers and analysts typically engage in several preprocessing steps to clean and prepare the data for analysis, including handling missing values, normalizing sales figures, and transforming date formats for time-series analysis. The dataset supports a range of analytical techniques, including regression analysis to identify trends and relationships, machine learning models for predictive analytics, and descriptive statistics to summarize sales performance. Common research questions that can be addressed using this dataset include the impact of seasonal variations on sales, the effectiveness of marketing promotions, and the correlation between store location characteristics and sales performance. By leveraging this dataset, researchers can gain valuable insights into consumer behavior and retail dynamics, ultimately aiding in strategic decision-making for businesses operating in the grocery and supermarket sector."
  },
  {
    "name": "Store Item Demand",
    "description": "50 items across 10 different stores over 5 years",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/demand-forecasting-kernels-only",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "demand forecasting",
      "Kaggle competition",
      "time series"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "time series analysis"
    ],
    "topic_tags": [
      "demand forecasting",
      "retail analytics",
      "time series analysis"
    ],
    "summary": "The Store Item Demand dataset contains demand data for 50 different items across 10 stores over a span of 5 years. This dataset can be utilized for demand forecasting, enabling analysts to predict future sales trends and optimize inventory management.",
    "use_cases": [
      "Forecasting future demand for grocery items",
      "Analyzing sales trends over time",
      "Optimizing inventory levels based on predicted demand"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the demand for items in grocery stores?",
      "How can I forecast sales using time series data?",
      "What are the trends in store item demand over 5 years?",
      "How does demand vary across different stores?",
      "What factors influence grocery item sales?",
      "How can I analyze time series data for retail?",
      "What machine learning techniques can be applied to demand forecasting?",
      "Where can I find datasets for grocery demand forecasting?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "5 years",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/store-item-demand.jpg",
    "embedding_text": "The Store Item Demand dataset is a comprehensive collection of demand data for 50 distinct grocery items across 10 different stores over a period of 5 years. This dataset is structured in a tabular format, where each row represents a specific item-store combination for a particular time period, allowing for detailed analysis of sales trends and demand patterns. The key variables in this dataset include item identifiers, store identifiers, timestamps, and demand quantities, which collectively provide a robust framework for understanding consumer behavior in the grocery sector. Researchers and analysts can leverage this dataset to explore various research questions, such as identifying seasonal trends in item demand, comparing sales performance across different stores, and assessing the impact of external factors on grocery sales. The collection methodology for this dataset typically involves aggregating sales data from point-of-sale systems across multiple retail locations, ensuring a diverse representation of consumer purchasing patterns. However, it is important to note that the dataset may have limitations, such as missing values or inconsistencies in item categorization, which could affect the accuracy of analyses. Common preprocessing steps include handling missing data, normalizing demand figures, and transforming timestamps into a suitable format for time series analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning algorithms, and descriptive statistics, making it a valuable resource for both academic research and practical applications in retail management. By utilizing this dataset, researchers can gain insights into demand forecasting, enabling retailers to optimize inventory levels, improve customer satisfaction, and enhance overall operational efficiency.",
    "benchmark_usage": [
      "Demand forecasting",
      "Sales trend analysis"
    ]
  },
  {
    "name": "JD.com Search",
    "description": "170,000 users' real search queries (2021-2022) from JD.com",
    "category": "E-Commerce",
    "url": "https://github.com/rucliujn/JDsearch",
    "docs_url": null,
    "github_url": "https://github.com/rucliujn/JDsearch",
    "tags": [
      "search queries",
      "e-commerce search",
      "China"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The JD.com Search dataset contains 170,000 real search queries from users of JD.com, a major e-commerce platform in China, collected between 2021 and 2022. This dataset can be utilized to analyze consumer behavior, search trends, and e-commerce dynamics in the Chinese market.",
    "use_cases": [
      "Analyzing consumer search behavior",
      "Identifying trends in e-commerce searches",
      "Studying the impact of search queries on sales",
      "Exploring seasonal variations in search patterns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "JD.com search queries dataset",
      "real user search data from JD.com",
      "e-commerce search queries in China",
      "search behavior analysis JD.com",
      "consumer search trends JD.com",
      "2021 2022 JD.com search data"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "temporal_coverage": "2021-2022",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/jdcom-search.png",
    "embedding_text": "The JD.com Search dataset is a rich collection of 170,000 real search queries submitted by users on JD.com, one of China's leading e-commerce platforms, during the years 2021 and 2022. This dataset is structured in a tabular format, where each row represents an individual search query made by a user, and the columns capture various attributes of these queries. The primary variable is the search query itself, which provides insights into what products or services users are interested in at a given time. Additional columns may include timestamps, user demographics, or other relevant metadata, although specific details on these additional variables are not provided in the description. The collection methodology for this dataset likely involved tracking user interactions on the JD.com platform, capturing search queries as they occur in real-time. This approach ensures that the data reflects actual user behavior, making it a valuable resource for understanding consumer preferences and trends in the e-commerce sector. The dataset's temporal coverage spans two years, allowing researchers to analyze changes in search behavior over time, such as shifts in consumer interests or the impact of external events on shopping patterns. Geographically, the dataset is focused on China, providing insights specific to this market, which is crucial for businesses and researchers interested in the dynamics of e-commerce in the region. While the dataset offers a wealth of information, it is important to consider potential limitations related to data quality. For instance, the dataset may be biased towards certain demographics if the user base of JD.com does not represent the entire population of online shoppers in China. Additionally, the dataset may not capture the full context of each search query, such as the intent behind the search or the subsequent actions taken by users. Common preprocessing steps for this dataset might include cleaning the search query text to remove irrelevant characters, normalizing the text for consistency, and possibly categorizing queries into broader product or service categories for analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying popular product categories, understanding seasonal trends in consumer behavior, or examining the relationship between search queries and sales performance. The dataset supports various types of analyses, including descriptive statistics to summarize search behavior, regression analysis to explore relationships between variables, and machine learning techniques to predict future search trends or consumer preferences. Overall, the JD.com Search dataset serves as a valuable resource for researchers and practitioners in the fields of e-commerce, marketing, and consumer behavior, providing insights that can inform business strategies and enhance understanding of the rapidly evolving online shopping landscape in China."
  },
  {
    "name": "Alibaba Cloud Theme",
    "description": "Themed dataset related to Alibaba Cloud services",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/9716",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cloud",
      "Alibaba",
      "themed"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce"
    ],
    "summary": "The Alibaba Cloud Theme dataset is a themed collection of data related to Alibaba Cloud services, primarily focusing on e-commerce applications. Researchers and analysts can utilize this dataset to explore various aspects of cloud services in the e-commerce sector, including consumer behavior and pricing strategies.",
    "use_cases": [
      "Analyzing consumer behavior in e-commerce using Alibaba Cloud services",
      "Evaluating pricing strategies for cloud services in the e-commerce sector",
      "Identifying trends in cloud service usage among e-commerce businesses"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Cloud Theme dataset?",
      "How can I analyze Alibaba Cloud services in e-commerce?",
      "What insights can be gained from the Alibaba Cloud Theme dataset?",
      "What are the key variables in the Alibaba Cloud Theme dataset?",
      "How does Alibaba Cloud impact consumer behavior?",
      "What pricing strategies can be derived from the Alibaba Cloud Theme dataset?",
      "What analyses can be performed using the Alibaba Cloud Theme dataset?",
      "What are the limitations of the Alibaba Cloud Theme dataset?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "embedding_text": "The Alibaba Cloud Theme dataset is a specialized collection of data that focuses on the various services offered by Alibaba Cloud, particularly in the context of e-commerce. This dataset is structured in a tabular format, comprising rows and columns that represent different variables related to cloud services. Each row typically corresponds to a specific instance or observation, while the columns represent key attributes such as service type, pricing, usage metrics, and consumer engagement statistics. The collection methodology for this dataset involves aggregating data from various sources, including Alibaba Cloud's own service usage reports, customer feedback, and market research studies. This ensures a comprehensive view of how Alibaba Cloud services are utilized within the e-commerce sector. Coverage in terms of temporal and geographic aspects is not explicitly mentioned, but the dataset is designed to provide insights relevant to current trends in e-commerce. Key variables within the dataset may include service categories, pricing models, customer demographics, and usage statistics, each measuring different facets of cloud service performance and consumer interaction. Data quality is a critical aspect, and while the dataset aims for high accuracy, known limitations may include potential biases in self-reported data and variations in service usage across different regions. Common preprocessing steps might involve cleaning the data to remove duplicates, handling missing values, and normalizing pricing information for comparative analysis. Researchers can leverage this dataset to address a variety of research questions, such as how Alibaba Cloud services influence consumer purchasing decisions, what pricing strategies are most effective in attracting e-commerce businesses, and how cloud service usage varies across different e-commerce sectors. The types of analyses supported by this dataset include regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize key trends. Typically, researchers use the Alibaba Cloud Theme dataset in studies aimed at understanding the intersection of cloud technology and e-commerce, providing valuable insights that can inform business strategies and policy decisions."
  },
  {
    "name": "Rakuten SIGIR",
    "description": "E-commerce dataset for SIGIR workshop from Rakuten",
    "category": "E-Commerce",
    "url": "https://sigir-ecom.github.io/ecom2018/data-task.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "SIGIR",
      "e-commerce",
      "search"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "search"
    ],
    "summary": "The Rakuten SIGIR dataset is an e-commerce dataset specifically designed for research in search and information retrieval. It provides insights into consumer behavior and search patterns within the e-commerce domain, making it suitable for various analyses related to user interactions and product searches.",
    "use_cases": [
      "Analyzing consumer search behavior",
      "Evaluating search algorithms",
      "Studying product recommendation systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Rakuten SIGIR dataset?",
      "How can I access the Rakuten SIGIR dataset?",
      "What types of analyses can be performed with the Rakuten SIGIR dataset?",
      "What are the key variables in the Rakuten SIGIR dataset?",
      "What insights can be gained from the Rakuten SIGIR dataset?",
      "How is the Rakuten SIGIR dataset structured?",
      "What are the limitations of the Rakuten SIGIR dataset?",
      "In what research contexts is the Rakuten SIGIR dataset commonly used?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "embedding_text": "The Rakuten SIGIR dataset is a comprehensive e-commerce dataset that serves as a valuable resource for researchers and practitioners in the fields of information retrieval and e-commerce analytics. It is structured in a tabular format, consisting of rows and columns that represent various aspects of user interactions with the e-commerce platform. Each row typically corresponds to a unique user interaction, while the columns capture key variables such as user ID, search queries, product IDs, timestamps, and other relevant metadata. This structure allows for a detailed analysis of search behaviors and patterns, enabling researchers to draw meaningful insights from the data.\n\nThe collection methodology for the Rakuten SIGIR dataset involves gathering data from user interactions on the Rakuten e-commerce platform during a specific period. This data is sourced from real user activities, ensuring that the dataset reflects authentic consumer behavior in an online shopping environment. While the exact collection methodology is not detailed, it is common for such datasets to employ logging mechanisms that capture user actions in real time, allowing for a rich dataset that can be used for various analytical purposes.\n\nIn terms of coverage, the dataset is expected to provide a snapshot of user interactions over a defined temporal range, although specific years or timeframes are not explicitly mentioned. Similarly, the geographic scope is not specified, but it can be inferred that the dataset primarily reflects interactions from users within the regions where Rakuten operates. This aspect is crucial for understanding the demographic and cultural factors that may influence consumer behavior in e-commerce.\n\nKey variables in the dataset include user identifiers, search queries, product identifiers, timestamps, and potentially additional metadata that describes the context of each interaction. These variables are instrumental in measuring aspects such as search effectiveness, user engagement, and product visibility. Researchers can utilize these variables to explore various research questions, such as how different search queries impact product discovery or how user behavior varies across different product categories.\n\nData quality is an essential consideration when working with the Rakuten SIGIR dataset. As with any dataset derived from user interactions, there may be limitations related to data completeness, accuracy, and potential biases in user behavior. Common preprocessing steps may include cleaning the data to handle missing values, normalizing search queries, and aggregating interactions to create meaningful metrics for analysis.\n\nThe dataset supports a wide range of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use this dataset to address questions related to search optimization, user experience, and the effectiveness of recommendation systems. By leveraging the insights gained from the Rakuten SIGIR dataset, researchers can contribute to the development of more effective search algorithms and enhance the overall user experience in e-commerce platforms. Overall, the Rakuten SIGIR dataset is a rich resource for understanding the dynamics of search in the e-commerce landscape, providing a foundation for innovative research and practical applications in the field."
  },
  {
    "name": "Amazon Reviews (2023)",
    "description": "571M reviews (1996-2023), 33 categories, 48M items - comprehensive Amazon review dataset",
    "category": "E-Commerce",
    "url": "https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "Amazon",
      "large-scale",
      "sentiment"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "sentiment-analysis"
    ],
    "summary": "The Amazon Reviews (2023) dataset contains 571 million reviews spanning from 1996 to 2023 across 33 categories and 48 million items. This comprehensive dataset allows researchers and practitioners to analyze consumer sentiment, trends in product reviews, and the impact of reviews on purchasing behavior.",
    "use_cases": [
      "Sentiment analysis of consumer reviews to gauge public opinion on products.",
      "Trend analysis to identify shifts in consumer preferences over time.",
      "Comparative analysis of review patterns across different product categories.",
      "Predictive modeling to forecast product performance based on review data."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common sentiments expressed in Amazon reviews?",
      "How do product categories differ in terms of review volume?",
      "What trends can be identified in Amazon reviews over time?",
      "How do review scores correlate with sales data?",
      "What are the most reviewed items on Amazon?",
      "How does sentiment vary across different categories of products?",
      "What insights can be drawn from the analysis of large-scale Amazon reviews?",
      "How can machine learning be applied to predict product ratings based on reviews?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "temporal_coverage": "1996-2023",
    "size_category": "massive",
    "model_score": 0.0005,
    "image_url": "/images/datasets/amazon-reviews-2023.png",
    "embedding_text": "The Amazon Reviews (2023) dataset is a vast collection of 571 million reviews that spans from 1996 to 2023, covering 33 distinct product categories and encompassing 48 million individual items. This dataset is structured in a tabular format, where each row represents a unique review, and the columns include variables such as review text, star rating, product ID, category, and timestamp. The collection methodology involves scraping publicly available review data from the Amazon platform, ensuring a comprehensive representation of consumer feedback over a significant temporal range. The dataset's coverage is extensive, providing insights into consumer behavior across multiple years, allowing for longitudinal studies on trends and shifts in sentiment. Key variables include the review text, which measures consumer sentiment and opinion, and the star rating, which quantifies the overall satisfaction level of the reviewer. While the dataset is rich in information, researchers should be aware of potential limitations, such as the presence of biased reviews or the impact of fake reviews, which may skew results. Common preprocessing steps might include text normalization, sentiment scoring, and filtering out irrelevant or duplicate entries to enhance data quality. Researchers can leverage this dataset to address various research questions, such as understanding the factors that influence consumer ratings, analyzing the relationship between review sentiment and product sales, or exploring the dynamics of consumer feedback over time. The dataset supports various types of analyses, including regression analysis to identify predictors of review scores, machine learning techniques for sentiment classification, and descriptive statistics to summarize review trends. Overall, the Amazon Reviews dataset serves as a valuable resource for researchers and practitioners interested in e-commerce, consumer behavior, and data-driven insights into product performance."
  },
  {
    "name": "Walmart Sales",
    "description": "General sales data including CPI and unemployment rate",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/yasserh/walmart-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Walmart",
      "macro",
      "sales"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Walmart Sales dataset contains general sales data along with macroeconomic indicators such as the Consumer Price Index (CPI) and unemployment rates. This dataset can be utilized to analyze sales trends, understand consumer behavior, and evaluate the impact of economic factors on retail performance.",
    "use_cases": [
      "Analyzing the impact of economic indicators on sales performance",
      "Evaluating seasonal sales trends",
      "Understanding consumer purchasing behavior in relation to macroeconomic factors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Walmart Sales dataset?",
      "How does CPI affect Walmart sales?",
      "What trends can be observed in Walmart sales data?",
      "What is the relationship between unemployment rates and grocery sales?",
      "How can I analyze Walmart sales data?",
      "What variables are included in the Walmart Sales dataset?",
      "What insights can be derived from Walmart's sales data?",
      "How does Walmart's sales performance compare to economic indicators?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/walmart-sales.jpg",
    "embedding_text": "The Walmart Sales dataset is a comprehensive collection of sales data that includes various macroeconomic indicators, specifically the Consumer Price Index (CPI) and unemployment rates. This dataset is structured in a tabular format, consisting of rows and columns that represent different sales transactions and their associated variables. Key variables within this dataset may include sales figures, product categories, timestamps of transactions, and the corresponding CPI and unemployment rate at the time of each sale. The data collection methodology typically involves aggregating sales data from Walmart's retail operations, alongside gathering macroeconomic data from reliable financial and governmental sources. The dataset is designed to provide insights into how external economic factors influence retail sales, making it a valuable resource for researchers and analysts in the field of economics and retail studies. Researchers often utilize this dataset to explore a variety of research questions, such as the correlation between unemployment rates and consumer spending at Walmart, or how fluctuations in the CPI affect pricing strategies and sales volume. The types of analyses supported by this dataset include regression analysis, machine learning models, and descriptive statistics, allowing for a multifaceted examination of sales trends and consumer behavior. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing sales figures for inflation, and transforming categorical variables for analytical purposes. While the dataset is rich in information, researchers should be aware of potential limitations, such as data quality issues arising from incomplete records or discrepancies in macroeconomic data. Overall, the Walmart Sales dataset serves as a crucial tool for understanding the dynamics of retail sales in the context of broader economic trends."
  },
  {
    "name": "Federal Procurement Data System (FPDS)",
    "description": "Comprehensive U.S. government contract database with 50+ million unclassified actions and 200+ data elements per transaction",
    "category": "Defense Economics",
    "url": "https://www.fpds.gov/",
    "docs_url": "https://www.fpds.gov/wiki/index.php/V1.4_Atom_Feed_FAQ",
    "github_url": null,
    "tags": [
      "procurement",
      "contracts",
      "government",
      "acquisition"
    ],
    "best_for": "Analyzing U.S. defense contracting patterns and vendor relationships",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Federal Procurement Data System (FPDS) is a comprehensive database that contains over 50 million unclassified actions related to U.S. government contracts. Researchers and analysts can utilize this dataset to explore trends in government procurement, analyze spending patterns, and assess the impact of federal contracts on various sectors.",
    "use_cases": [
      "Analyzing government spending trends over time.",
      "Assessing the impact of federal contracts on local economies.",
      "Evaluating the effectiveness of government procurement policies.",
      "Identifying patterns in contractor performance and compliance."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Federal Procurement Data System?",
      "How can I access the FPDS dataset?",
      "What types of data are included in the FPDS?",
      "What are the key variables in the FPDS?",
      "How does the FPDS support analysis of government contracts?",
      "What insights can be gained from the FPDS data?",
      "What is the significance of procurement data in defense economics?",
      "How can I analyze trends in federal procurement using FPDS?"
    ],
    "domain_tags": [
      "defense",
      "government",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2004-present",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0005,
    "image_url": "/images/logos/fpds.png",
    "embedding_text": "The Federal Procurement Data System (FPDS) serves as a vital resource for understanding the landscape of U.S. government contracting. This dataset encompasses a vast array of over 50 million unclassified actions, providing a detailed view of federal procurement activities. Each entry in the FPDS is structured in a tabular format, comprising numerous rows and columns that represent various transactions and their associated data elements. The dataset includes more than 200 data elements per transaction, capturing key variables such as contract amounts, award dates, contractor information, and the specific goods or services procured. The collection methodology for FPDS involves systematic reporting from federal agencies, ensuring that the data reflects a comprehensive view of government spending. This data is crucial for researchers and policymakers who seek to analyze trends in government procurement, evaluate the impact of contracts on economic sectors, and assess compliance with federal regulations. While the FPDS provides extensive coverage of procurement actions, it is important to note that the dataset is limited to unclassified actions, which may exclude certain sensitive contracts. Additionally, the quality of the data may vary based on the reporting practices of different agencies, necessitating common preprocessing steps such as data cleaning and normalization before analysis. Researchers can leverage FPDS data to address a variety of research questions, including the effectiveness of procurement strategies, the identification of spending patterns across different sectors, and the evaluation of contractor performance. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and practical inquiries into defense economics and government spending. Overall, the FPDS is an indispensable asset for those looking to gain insights into the dynamics of federal procurement and its implications for the economy."
  },
  {
    "name": "Office Supplies (DMDA 2023)",
    "description": "Office supply sales for DMDA 2023 workshop challenge",
    "category": "Grocery & Supermarkets",
    "url": "https://sites.google.com/view/dmdaworkshop2023/data-challenge",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "office supplies",
      "forecasting",
      "workshop"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Office Supplies dataset for DMDA 2023 workshop challenge includes sales data related to office supplies. This dataset can be utilized for forecasting sales trends, analyzing consumer behavior, and developing pricing strategies.",
    "use_cases": [
      "Forecasting sales trends",
      "Analyzing consumer purchasing behavior",
      "Developing pricing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends for office supplies in DMDA 2023?",
      "How can we forecast future office supply sales?",
      "What factors influence consumer behavior in office supply purchases?",
      "What pricing strategies can be derived from the office supplies dataset?",
      "How does the office supplies market perform during workshops?",
      "What insights can be gained from analyzing office supply sales data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/office-supplies-dmda-2023.png",
    "embedding_text": "The Office Supplies dataset for DMDA 2023 workshop challenge is a structured collection of sales data specifically focused on office supplies. This dataset is organized in a tabular format, comprising various rows and columns that represent different variables associated with office supply sales. Key variables may include product categories, sales figures, timestamps of transactions, and possibly demographic information about the consumers. The collection methodology for this dataset likely involves gathering sales data from retail transactions during the DMDA 2023 workshop, which serves as a controlled environment for analyzing office supply sales. Given the nature of the dataset, it is essential to consider its temporal coverage, which, while not explicitly mentioned, can be inferred to encompass the period surrounding the workshop event. The geographic scope is also unspecified, indicating that the dataset may be applicable to a broad range of locations or specific to the event's hosting area. The dataset is expected to be of medium size, providing a sufficient volume of data for analysis without overwhelming the user. Researchers and data scientists can leverage this dataset to address various research questions, such as identifying sales trends over time, understanding consumer behavior in the context of office supplies, and developing effective pricing strategies based on historical sales data. Common preprocessing steps may include cleaning the data to remove any inconsistencies, handling missing values, and transforming variables into suitable formats for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic research and practical business applications. Overall, the Office Supplies dataset serves as a valuable tool for those looking to explore the dynamics of office supply sales and consumer behavior in a workshop setting."
  },
  {
    "name": "Spotify Music Streaming Sessions (MSSD)",
    "description": "150M+ listening sessions with skips, track features, and playlist context. The largest public music streaming behavior dataset",
    "category": "Entertainment & Media",
    "url": "https://paperswithcode.com/dataset/mssd",
    "docs_url": "https://arxiv.org/abs/1901.09851",
    "github_url": null,
    "tags": [
      "Spotify",
      "streaming",
      "sessions",
      "skips",
      "large-scale"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "music",
      "streaming",
      "behavioral-analysis"
    ],
    "summary": "The Spotify Music Streaming Sessions (MSSD) dataset contains over 150 million listening sessions, providing insights into user behavior, including skips, track features, and playlist context. Researchers can utilize this dataset to analyze music consumption patterns, user engagement, and the impact of various track features on listening behavior.",
    "use_cases": [
      "Analyzing user engagement with different music tracks",
      "Studying the impact of playlist context on listening behavior",
      "Investigating patterns of skips in music streaming",
      "Exploring relationships between track features and user preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Spotify Music Streaming Sessions dataset?",
      "How can I analyze user behavior in music streaming?",
      "What insights can be derived from Spotify listening sessions?",
      "How does track feature influence skips in music?",
      "What are the characteristics of large-scale music streaming datasets?",
      "Where can I find datasets on music streaming behavior?",
      "What are common analyses performed on music streaming data?",
      "How to access the Spotify Music Streaming Sessions dataset?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0005,
    "image_url": "/images/datasets/spotify-music-streaming-sessions-mssd.png",
    "embedding_text": "The Spotify Music Streaming Sessions (MSSD) dataset is an extensive collection of over 150 million listening sessions, making it one of the largest public datasets available for analyzing music streaming behavior. This dataset includes a variety of data points such as skips, track features, and playlist context, allowing researchers to delve into the intricacies of user engagement and preferences in the realm of music streaming. The data is structured in a tabular format, with each row representing an individual listening session. Key variables in the dataset include user identifiers, track identifiers, timestamps, and various features of the tracks such as genre, tempo, and acoustic characteristics. Additionally, the dataset captures user interactions, including whether a track was skipped or played to completion, which provides valuable insights into user behavior and preferences. The collection methodology for this dataset involves aggregating data from Spotify's streaming service, where user interactions are logged in real-time. This ensures that the dataset reflects current trends and behaviors in music consumption. However, it is important to note that while the dataset is vast, it may have limitations regarding data quality, such as potential biases in user demographics or incomplete data for certain tracks. Researchers often need to perform common preprocessing steps, including data cleaning, normalization, and feature engineering, to prepare the dataset for analysis. The MSSD supports a wide range of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers can address various research questions, such as how different track features influence user engagement, the impact of playlist context on listening behavior, and patterns of skips in music streaming. By leveraging this dataset, researchers can gain insights into the dynamics of music consumption, explore user preferences, and contribute to the understanding of the evolving landscape of music streaming. Overall, the Spotify Music Streaming Sessions dataset serves as a rich resource for those interested in the intersection of technology and music, providing a foundation for innovative research and analysis in the entertainment and media sectors."
  },
  {
    "name": "Ecuador Grocery (Favorita)",
    "description": "Unit sales data with store/item metadata and oil prices from Ecuador",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "sales forecasting",
      "Ecuador",
      "Kaggle"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "grocery",
      "sales forecasting",
      "Ecuador"
    ],
    "summary": "The Ecuador Grocery (Favorita) dataset contains unit sales data along with store and item metadata, as well as oil prices from Ecuador. This dataset can be utilized for sales forecasting and understanding consumer behavior in the grocery sector.",
    "use_cases": [
      "Analyzing the impact of oil prices on grocery sales",
      "Forecasting future sales trends based on historical data",
      "Understanding consumer purchasing behavior in Ecuadorian grocery stores"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the unit sales data for grocery items in Ecuador?",
      "How do oil prices affect grocery sales in Ecuador?",
      "What metadata is available for stores and items in the Ecuador Grocery dataset?",
      "Can I use this dataset for sales forecasting?",
      "What trends can be analyzed in the grocery sector of Ecuador?",
      "How can I access the Ecuador Grocery dataset on Kaggle?",
      "What are the key variables in the Ecuador Grocery dataset?",
      "What insights can be gained from analyzing grocery sales data in Ecuador?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ecuador",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/ecuador-grocery-favorita.jpg",
    "embedding_text": "The Ecuador Grocery (Favorita) dataset is a comprehensive collection of unit sales data that includes detailed metadata for stores and items, as well as oil prices specific to Ecuador. This dataset is structured in a tabular format, containing rows that represent individual sales transactions and columns that encompass various variables such as store identifiers, item identifiers, sales quantities, prices, and oil price data. The collection methodology for this dataset involves aggregating sales data from grocery stores across Ecuador, ensuring a diverse representation of the grocery market. The dataset captures a wide range of items sold in these stores, allowing for in-depth analysis of consumer purchasing patterns and sales trends. Key variables in the dataset include unit sales figures, which measure the quantity of each item sold, store identifiers that link sales to specific locations, and oil prices that may influence consumer behavior and pricing strategies. While the dataset provides valuable insights, it is important to consider potential limitations such as data quality issues, including missing values or inconsistencies in reporting. Common preprocessing steps may include cleaning the data to handle missing values, normalizing prices, and aggregating sales data to a suitable time frame for analysis. Researchers can leverage this dataset to address various research questions, such as the relationship between oil prices and grocery sales, seasonal trends in consumer purchasing behavior, and the effectiveness of promotional strategies in driving sales. The dataset supports a variety of analytical techniques, including regression analysis, machine learning models for sales forecasting, and descriptive statistics to summarize sales performance. Researchers typically use this dataset in studies focused on retail economics, consumer behavior analysis, and market trend forecasting, making it a valuable resource for those interested in the grocery sector in Ecuador."
  },
  {
    "name": "LIAR Fact-Checking",
    "description": "12.8K fact-checked political statements with speaker metadata and 6-way truthfulness labels. Politifact benchmark",
    "category": "Content Moderation",
    "url": "https://www.cs.ucsb.edu/~william/data/liar_dataset.zip",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fact-checking",
      "politics",
      "misinformation",
      "NLP",
      "Politifact"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "politics",
      "fact-checking",
      "misinformation"
    ],
    "summary": "The LIAR Fact-Checking dataset contains 12.8K fact-checked political statements, each accompanied by speaker metadata and 6-way truthfulness labels. It serves as a benchmark for evaluating the performance of fact-checking algorithms and natural language processing models in identifying misinformation in political discourse.",
    "use_cases": [
      "Evaluating the accuracy of political statements made by public figures.",
      "Training machine learning models to classify misinformation in political contexts.",
      "Analyzing trends in political discourse and the prevalence of misinformation over time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LIAR Fact-Checking dataset?",
      "How can I access the LIAR Fact-Checking dataset?",
      "What types of analyses can be performed with the LIAR Fact-Checking dataset?",
      "What are the truthfulness labels in the LIAR Fact-Checking dataset?",
      "What speaker metadata is included in the LIAR Fact-Checking dataset?",
      "How does the LIAR Fact-Checking dataset compare to other fact-checking datasets?",
      "What research questions can be explored using the LIAR Fact-Checking dataset?",
      "What are the common use cases for the LIAR Fact-Checking dataset?"
    ],
    "domain_tags": [
      "politics",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Used as a benchmark for fact-checking algorithms and NLP models."
    ],
    "model_score": 0.0005,
    "embedding_text": "The LIAR Fact-Checking dataset is a comprehensive resource designed to facilitate research in the domain of misinformation and political discourse. It consists of 12,800 fact-checked political statements, each meticulously annotated with speaker metadata and classified into six distinct truthfulness categories. This dataset serves as a benchmark for evaluating the performance of various natural language processing (NLP) models and algorithms aimed at detecting misinformation. The data structure is organized in a tabular format, where each row represents a unique political statement, and the columns include key variables such as the statement text, speaker information, and the assigned truthfulness label. The truthfulness labels range from 'true' to 'pants on fire,' providing a nuanced understanding of the accuracy of each statement. The collection methodology involves sourcing statements from a variety of public figures, ensuring a diverse representation of political discourse. Researchers can leverage this dataset to address critical questions regarding the prevalence of misinformation in political communication, the effectiveness of fact-checking efforts, and the impact of speaker credibility on public perception. Common preprocessing steps may include text normalization, tokenization, and the removal of irrelevant metadata to prepare the data for analysis. The dataset supports various types of analyses, including regression, machine learning classification tasks, and descriptive statistics, enabling researchers to explore patterns and correlations within the data. Overall, the LIAR Fact-Checking dataset is an invaluable tool for scholars, data scientists, and practitioners interested in understanding and combating misinformation in the political arena."
  },
  {
    "name": "FCC Spectrum Auctions",
    "description": "87+ auctions (1994-present) with round-by-round bidding data. Complete bid histories, reserve prices, winners. Auction theory empirics",
    "category": "Auctions & Marketplaces",
    "url": "https://www.fcc.gov/auctions-summary",
    "docs_url": "https://www.fcc.gov/auction/",
    "github_url": null,
    "tags": [
      "spectrum auctions",
      "FCC",
      "bidding",
      "auction theory",
      "telecommunications"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "auctions",
      "market analysis",
      "telecommunications"
    ],
    "summary": "The FCC Spectrum Auctions dataset provides comprehensive round-by-round bidding data from over 87 auctions conducted since 1994. Researchers and analysts can utilize this dataset to explore auction dynamics, analyze bidding strategies, and study the implications of auction theory in telecommunications.",
    "use_cases": [
      "Analyzing bidding patterns and strategies in telecommunications auctions.",
      "Studying the impact of reserve prices on auction outcomes.",
      "Exploring the relationship between auction design and bidder behavior.",
      "Evaluating the effectiveness of different auction formats in spectrum allocation."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What data is available in the FCC Spectrum Auctions dataset?",
      "How can I analyze bidding strategies using FCC auction data?",
      "What insights can be gained from the complete bid histories in the FCC Spectrum Auctions?",
      "What are the reserve prices and winners in the FCC Spectrum Auctions?",
      "How does auction theory apply to the FCC Spectrum Auctions data?",
      "What time period does the FCC Spectrum Auctions dataset cover?",
      "What types of analyses can be performed with FCC auction data?",
      "Where can I find round-by-round bidding data for FCC auctions?"
    ],
    "domain_tags": [
      "telecommunications",
      "auctions"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1994-present",
    "size_category": "medium",
    "model_score": 0.0005,
    "image_url": "/images/datasets/fcc-spectrum-auctions.jpg",
    "embedding_text": "The FCC Spectrum Auctions dataset is a rich resource for researchers interested in the dynamics of auction markets, particularly in the telecommunications sector. This dataset encompasses over 87 auctions that have taken place from 1994 to the present, providing a detailed account of round-by-round bidding data. The structure of the dataset is primarily tabular, with rows representing individual bids and columns capturing various attributes such as bid amounts, bidder identities, auction identifiers, and timestamps. Key variables include bid amounts, which measure the financial commitment of bidders, and auction identifiers, which allow for the organization of data by specific auction events. The dataset's collection methodology involves aggregating data from official FCC auction records, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, such as missing data for certain auctions or discrepancies in bidder reporting. Common preprocessing steps may include cleaning the data to handle missing values, normalizing bid amounts for inflation, and categorizing bidders based on their bidding behavior. This dataset supports a variety of analytical approaches, including regression analysis to identify factors influencing bidding behavior, machine learning techniques for predictive modeling, and descriptive statistics to summarize auction outcomes. Researchers typically use this dataset to address questions related to auction efficiency, the impact of regulatory changes on bidding strategies, and the overall effectiveness of different auction formats. By leveraging the insights gained from this dataset, analysts can contribute to a deeper understanding of auction theory and its practical applications in the telecommunications industry."
  },
  {
    "name": "Criteo Terabyte",
    "description": "342GB, 45M samples with 13 integer features and 26 hashed categorical features for CTR prediction",
    "category": "Advertising",
    "url": "https://huggingface.co/datasets/criteo/CriteoClickLogs",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "CTR",
      "advertising",
      "large-scale",
      "benchmark"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Criteo Terabyte dataset is a large-scale collection designed for click-through rate (CTR) prediction, containing 342GB of data with 45 million samples. It features 13 integer variables and 26 hashed categorical variables, making it suitable for various predictive modeling tasks in advertising.",
    "use_cases": [
      "Predicting click-through rates for online advertisements",
      "Benchmarking machine learning models for advertising",
      "Analyzing user behavior in e-commerce",
      "Evaluating the effectiveness of advertising strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Terabyte dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What features are included in the Criteo Terabyte dataset?",
      "Where can I find large-scale advertising datasets?",
      "What are the common preprocessing steps for CTR prediction datasets?",
      "How many samples does the Criteo Terabyte dataset have?",
      "What types of analyses can be performed on the Criteo dataset?",
      "What is the size of the Criteo Terabyte dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "benchmark_usage": [
      "CTR prediction",
      "large-scale model evaluation"
    ],
    "model_score": 0.0005,
    "image_url": "/images/datasets/criteo-terabyte.png",
    "embedding_text": "The Criteo Terabyte dataset is a comprehensive resource for researchers and practitioners in the field of advertising and machine learning. It consists of 342GB of data, encompassing 45 million samples, which are structured in a tabular format. The dataset includes 13 integer features and 26 hashed categorical features, specifically designed to facilitate click-through rate (CTR) prediction tasks. This dataset is particularly valuable for those looking to benchmark machine learning models in a large-scale environment. The data is collected from real-world advertising interactions, providing a rich source of information for understanding user behavior and optimizing advertising strategies. The key variables within the dataset measure various aspects of user interactions with ads, including user demographics, ad placements, and contextual information, although specific details about the variables are not disclosed due to privacy concerns associated with hashed categorical features. Researchers utilizing this dataset can expect to engage in various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Common preprocessing steps may involve handling missing values, normalizing numerical features, and decoding hashed categorical variables, which are essential for preparing the data for effective model training. The dataset supports a range of research questions, such as identifying factors that influence click-through rates, evaluating the performance of different advertising strategies, and exploring trends in consumer behavior over time. Overall, the Criteo Terabyte dataset serves as a benchmark for large-scale advertising studies, providing insights that can drive the development of more effective advertising models and strategies."
  },
  {
    "name": "NetEase Music (INFORMS)",
    "description": "Data from NetEase Cloud Music for INFORMS competition",
    "category": "Entertainment & Media",
    "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3554826",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "music",
      "streaming",
      "INFORMS",
      "China"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "music",
      "streaming",
      "data analysis"
    ],
    "summary": "The NetEase Music dataset contains data from the NetEase Cloud Music platform, specifically curated for the INFORMS competition. Researchers can utilize this dataset to analyze trends in music streaming behavior, user preferences, and the impact of various factors on music consumption in China.",
    "use_cases": [
      "Analyzing user preferences in music streaming",
      "Studying the impact of demographics on music consumption",
      "Exploring trends in music genres over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NetEase Music dataset?",
      "How can I access NetEase Cloud Music data?",
      "What insights can be gained from music streaming data?",
      "What are the trends in music consumption in China?",
      "How does user behavior vary in music streaming?",
      "What variables are included in the NetEase Music dataset?",
      "How is the data structured in the NetEase Music dataset?",
      "What analyses can be performed with the NetEase Music data?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0005,
    "embedding_text": "The NetEase Music dataset is a comprehensive collection of data sourced from the NetEase Cloud Music platform, specifically designed for the INFORMS competition. This dataset is structured in a tabular format, comprising various rows and columns that represent different variables related to music streaming behavior. Key variables may include user demographics, song attributes, streaming frequency, and genre preferences, among others. The collection methodology involves aggregating data from user interactions on the NetEase platform, ensuring a rich dataset that reflects real-world music consumption patterns. While the dataset provides valuable insights into music streaming trends, researchers should be aware of potential limitations such as data quality issues, including missing values or biases in user representation. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying factors influencing music preferences, analyzing the relationship between demographics and streaming habits, and exploring the evolution of music genres over time. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and industry research. Typically, researchers utilize the NetEase Music dataset in studies focused on consumer behavior, market trends in the music industry, and the impact of digital platforms on music consumption.",
    "image_url": "/images/logos/ssrn.png"
  },
  {
    "name": "Fashion-MNIST",
    "description": "70,000 28x28 grayscale images of 10 fashion categories from Zalando",
    "category": "Fashion & Apparel",
    "url": "https://github.com/zalandoresearch/fashion-mnist",
    "docs_url": null,
    "github_url": "https://github.com/zalandoresearch/fashion-mnist",
    "tags": [
      "image classification",
      "benchmark",
      "deep learning"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "Fashion-MNIST is a dataset consisting of 70,000 grayscale images categorized into 10 distinct fashion categories sourced from Zalando. It serves as a benchmark for image classification tasks, particularly in the field of deep learning, allowing researchers and practitioners to develop and evaluate their algorithms.",
    "use_cases": [
      "Developing image classification models",
      "Benchmarking deep learning algorithms",
      "Evaluating model performance in fashion recognition"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Fashion-MNIST dataset?",
      "How can I use Fashion-MNIST for image classification?",
      "What are the categories in the Fashion-MNIST dataset?",
      "Where can I find the Fashion-MNIST dataset?",
      "What are common benchmarks for Fashion-MNIST?",
      "How many images are in the Fashion-MNIST dataset?",
      "What is the resolution of images in Fashion-MNIST?",
      "What types of machine learning models can be applied to Fashion-MNIST?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "image",
    "size_category": "medium",
    "benchmark_usage": [
      "Image classification benchmark"
    ],
    "model_score": 0.0005,
    "image_url": "/images/datasets/fashion-mnist.png",
    "embedding_text": "The Fashion-MNIST dataset is a collection of 70,000 grayscale images, each measuring 28x28 pixels, representing 10 different categories of fashion items. The categories include T-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots. This dataset is widely used in the machine learning community as a benchmark for evaluating image classification algorithms, particularly those based on deep learning techniques. The images are organized in a structured format, where each image corresponds to a specific label indicating its category. The dataset is designed to serve as a more challenging replacement for the original MNIST dataset, which consists of handwritten digits. The collection methodology involves sourcing images from Zalando, a prominent online fashion retailer, ensuring a diverse representation of fashion items. While the dataset does not explicitly mention temporal or geographic coverage, it reflects contemporary fashion trends as of its creation, making it relevant for current research in e-commerce and consumer behavior. Key variables in the dataset include the image pixel values, which are normalized grayscale values ranging from 0 to 255, and the corresponding labels that categorize each image. Researchers often preprocess the dataset by normalizing pixel values, augmenting images for better model generalization, and splitting the data into training, validation, and test sets. The Fashion-MNIST dataset supports various types of analyses, including regression, machine learning, and descriptive statistics. It allows researchers to explore questions related to fashion item recognition, consumer preferences, and the effectiveness of different classification algorithms. Common research questions include how well different models can classify fashion items, the impact of image preprocessing techniques on model performance, and the exploration of transfer learning approaches using pre-trained models. Overall, Fashion-MNIST serves as an essential resource for both academic research and practical applications in the field of image recognition and deep learning."
  },
  {
    "name": "Tencent Social Ads",
    "description": "Social ad CTR prediction dataset from Tencent",
    "category": "Advertising",
    "url": "https://algo.qq.com/index.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social ads",
      "CTR",
      "Tencent",
      "China"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "data science",
      "machine learning"
    ],
    "summary": "The Tencent Social Ads dataset is designed for predicting click-through rates (CTR) for social advertisements. Researchers and practitioners can leverage this dataset to develop models that enhance ad targeting and effectiveness in social media marketing.",
    "use_cases": [
      "Predicting click-through rates for social ads",
      "Analyzing the effectiveness of advertising strategies",
      "Optimizing ad placements based on user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Tencent Social Ads dataset?",
      "How can I predict CTR using Tencent Social Ads?",
      "Where can I find social ad datasets?",
      "What variables are included in the Tencent Social Ads dataset?",
      "How does Tencent's social ad data compare to other datasets?",
      "What machine learning techniques can be applied to social ad CTR prediction?",
      "What insights can be gained from analyzing Tencent Social Ads?",
      "How is CTR measured in social advertising?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/qq.png",
    "embedding_text": "The Tencent Social Ads dataset is a comprehensive collection of data aimed at predicting click-through rates (CTR) for social advertisements. This dataset is particularly valuable for researchers and practitioners in the field of advertising and data science, as it provides insights into user interactions with ads on social media platforms. The data is structured in a tabular format, consisting of multiple rows and columns that capture various attributes related to social ads. Each row typically represents an individual ad impression, while the columns include key variables such as ad ID, user demographics, ad content features, and engagement metrics like clicks and impressions. The collection methodology for this dataset involves aggregating data from Tencent's advertising platform, which is one of the largest in China, thus ensuring a rich source of information for analysis. The dataset covers a range of user interactions over time, although specific temporal coverage details are not explicitly mentioned. It is important to note that while the dataset is robust, there may be limitations related to data quality, such as potential biases in user demographics or incomplete engagement records. Common preprocessing steps for this dataset may include handling missing values, normalizing numerical features, and encoding categorical variables to prepare the data for analysis. Researchers can utilize this dataset to address various research questions, such as identifying factors that influence ad engagement, comparing the performance of different ad formats, or developing predictive models for CTR based on historical data. The types of analyses supported by this dataset include regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic research and practical applications in the advertising industry. Overall, the Tencent Social Ads dataset serves as a critical tool for enhancing the understanding of social advertising dynamics and improving ad targeting strategies."
  },
  {
    "name": "SOA Mortality Tables",
    "description": "Society of Actuaries mortality tables and experience studies used as industry standards for life insurance pricing",
    "category": "Insurance & Actuarial",
    "url": "https://mort.soa.org/",
    "docs_url": "https://www.soa.org/resources/experience-studies/",
    "github_url": null,
    "tags": [
      "mortality-tables",
      "life-insurance",
      "actuarial",
      "experience-studies",
      "annuities"
    ],
    "best_for": "Life insurance product development, reserving, and mortality assumption setting",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "insurance",
      "actuarial science",
      "mortality studies"
    ],
    "summary": "The SOA Mortality Tables provide standardized mortality rates and life expectancy data, essential for actuaries in the life insurance industry. These tables enable professionals to price life insurance products accurately and assess risk associated with mortality.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the Society of Actuaries mortality tables?",
      "How are mortality tables used in life insurance pricing?",
      "What is the significance of experience studies in actuarial science?",
      "Where can I find mortality tables for life insurance?",
      "What data is included in the SOA Mortality Tables?",
      "How do actuaries use mortality tables for risk assessment?",
      "What are the key variables in the SOA Mortality Tables?",
      "What methodologies are used to create mortality tables?"
    ],
    "use_cases": [
      "Analyzing mortality trends over time to inform life insurance pricing strategies.",
      "Evaluating the impact of demographic changes on life expectancy and insurance products.",
      "Conducting risk assessments for new insurance policies based on mortality data.",
      "Comparing different mortality tables to assess their applicability in various actuarial models."
    ],
    "domain_tags": [
      "insurance",
      "actuarial",
      "finance"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/soa.png",
    "embedding_text": "The SOA Mortality Tables are a critical resource for actuaries and professionals in the life insurance industry, providing essential data on mortality rates and life expectancy. These tables are structured in a tabular format, typically containing rows that represent different age groups and columns that detail various mortality rates and life expectancies. The key variables measured in these tables include age, gender, and mortality rates, which are crucial for determining the risk associated with life insurance policies. The data is collected through rigorous methodologies, often involving extensive experience studies and demographic analyses to ensure accuracy and reliability. While the tables serve as industry standards, it is important to note that data quality can vary based on the sources used, and known limitations may include changes in population health trends and external factors that could influence mortality rates. Common preprocessing steps may involve cleaning the data for inconsistencies and adjusting for demographic shifts over time. Researchers and actuaries utilize these tables to address various research questions, such as evaluating the long-term sustainability of life insurance products, understanding the implications of demographic changes on mortality, and assessing the financial viability of insurance offerings. The analyses supported by the SOA Mortality Tables can range from descriptive statistics to more complex regression analyses and machine learning models, allowing for a comprehensive understanding of mortality trends and their implications in the insurance sector. Overall, the SOA Mortality Tables are indispensable for actuaries, providing a foundation for informed decision-making in life insurance pricing and risk management.",
    "benchmark_usage": [
      "Industry standards for life insurance pricing",
      "Risk assessment in actuarial studies"
    ]
  },
  {
    "name": "Stack Overflow Developer Survey",
    "description": "49K+ annual responses with salaries, tech adoption, and developer analytics",
    "category": "Labor Markets",
    "url": "https://survey.stackoverflow.co/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "developers",
      "salaries",
      "survey",
      "tech"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Stack Overflow Developer Survey is an extensive dataset comprising over 49,000 annual responses from developers worldwide, focusing on salaries, technology adoption, and various developer analytics. Researchers and analysts can utilize this dataset to explore trends in developer compensation, technology usage, and the overall landscape of the software development profession.",
    "use_cases": [
      "Analyzing salary trends across different programming languages and technologies.",
      "Examining the relationship between developer experience and job satisfaction.",
      "Investigating the impact of educational background on salary and job roles.",
      "Exploring technology adoption trends among developers over time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the salary trends for developers according to the Stack Overflow Developer Survey?",
      "How does technology adoption vary among different developer demographics?",
      "What insights can be drawn from the Stack Overflow Developer Survey regarding job satisfaction?",
      "What are the most popular programming languages among developers in the Stack Overflow Developer Survey?",
      "How do salaries differ by geographic location in the Stack Overflow Developer Survey?",
      "What factors influence developer salaries based on the Stack Overflow Developer Survey data?",
      "What are the common career paths for developers as indicated by the Stack Overflow Developer Survey?",
      "How has the technology landscape changed over the years according to the Stack Overflow Developer Survey?"
    ],
    "domain_tags": [
      "technology",
      "labor markets"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/stackoverflow.png",
    "embedding_text": "The Stack Overflow Developer Survey dataset is a rich collection of responses from developers, providing insights into various aspects of the software development industry. The dataset is structured in a tabular format, consisting of rows representing individual survey responses and columns capturing key variables such as salary, technology usage, job roles, and demographic information. Each response includes details about the respondent's programming languages, frameworks, tools, and their respective levels of expertise, as well as their geographic location and years of experience. This comprehensive structure allows for a multifaceted analysis of the developer community. The data is collected annually through an online survey distributed via Stack Overflow, a popular platform for developers. The survey is designed to gather information on developers' salaries, technology adoption, and other relevant analytics, making it a valuable resource for researchers and industry professionals alike. The coverage of the dataset is global, as it includes responses from developers in various countries, although specific geographic details may not always be disclosed. The demographic coverage includes a diverse range of developers, from beginners to experienced professionals, providing a broad perspective on the industry. Key variables within the dataset measure aspects such as salary, job satisfaction, technology preferences, and demographic characteristics, allowing for in-depth analysis of trends and patterns. However, researchers should be aware of potential limitations, such as response bias and the self-reported nature of the data, which may affect the accuracy of certain variables. Common preprocessing steps may include cleaning the data to handle missing values, normalizing salary figures for comparison, and categorizing responses for easier analysis. The dataset supports various types of analyses, including regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize key findings. Researchers typically use this dataset to address questions related to salary disparities, technology adoption trends, and the overall dynamics of the developer workforce, making it an essential tool for understanding the evolving landscape of software development."
  },
  {
    "name": "Innerwear Data",
    "description": "Data scraped from Victoria's Secret and other innerwear retailers",
    "category": "Fashion & Apparel",
    "url": "https://www.kaggle.com/datasets/PromptCloudHQ/innerwear-data-from-victorias-secret-and-others",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "retail",
      "scraped data"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Innerwear Data consists of information scraped from Victoria's Secret and other innerwear retailers, providing insights into the fashion and retail sectors. This dataset can be utilized for various analyses related to consumer behavior, pricing strategies, and market trends in the innerwear industry.",
    "use_cases": [
      "Analyzing consumer preferences in innerwear",
      "Studying pricing strategies of innerwear retailers",
      "Evaluating market trends in the fashion industry",
      "Comparing product offerings across different retailers"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Innerwear data from Victoria's Secret",
      "scraped data on innerwear retailers",
      "fashion retail datasets",
      "consumer behavior in innerwear",
      "pricing analysis of innerwear",
      "Victoria's Secret market data",
      "data on innerwear sales trends",
      "fashion and apparel data"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/innerwear-data.jpg",
    "embedding_text": "The Innerwear Data is a comprehensive dataset that has been meticulously scraped from various innerwear retailers, including prominent brands such as Victoria's Secret. This dataset is structured in a tabular format, consisting of multiple rows and columns that capture various attributes related to innerwear products. Key variables in this dataset may include product names, categories, prices, sizes, colors, and other relevant features that characterize the innerwear offerings from different retailers. The collection methodology involves web scraping techniques, which allow for the extraction of data from online retail platforms, ensuring that the dataset reflects current market offerings and trends. However, it is important to note that the quality of the data may vary depending on the scraping process and the source websites. Known limitations include potential inaccuracies in pricing, missing values for certain product attributes, and the possibility of outdated information if the dataset is not regularly updated. Common preprocessing steps for this dataset may involve cleaning the data to handle missing values, normalizing price formats, and categorizing products for easier analysis. Researchers can leverage this dataset to address various research questions, such as understanding consumer preferences in the innerwear market, analyzing pricing strategies employed by different retailers, and evaluating market trends within the fashion and apparel industry. The dataset supports various types of analyses, including regression analysis to identify factors influencing pricing, machine learning algorithms for predictive modeling, and descriptive statistics to summarize product offerings and consumer behavior. Typically, researchers utilize this dataset in studies focused on e-commerce, consumer behavior analysis, and market research, making it a valuable resource for anyone interested in the intersection of fashion and retail."
  },
  {
    "name": "Amazon Last Mile",
    "description": "9,184 historical routes across 5 US metro areas",
    "category": "Logistics & Supply Chain",
    "url": "https://registry.opendata.aws/amazon-last-mile-challenges/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "routing",
      "last-mile",
      "Amazon",
      "AWS"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "logistics",
      "supply-chain"
    ],
    "summary": "The Amazon Last Mile dataset contains 9,184 historical routes across five major US metro areas, providing insights into last-mile delivery logistics. Researchers can analyze routing efficiency, delivery times, and operational challenges faced by Amazon in urban environments.",
    "use_cases": [
      "Analyzing the efficiency of delivery routes in urban settings.",
      "Studying the impact of traffic patterns on delivery times.",
      "Evaluating operational challenges in last-mile logistics.",
      "Comparing Amazon's delivery strategies across different metro areas."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical last-mile delivery routes used by Amazon?",
      "How does Amazon optimize its delivery routes in urban areas?",
      "What factors influence last-mile delivery efficiency?",
      "Can I analyze Amazon's routing data for logistics research?",
      "What insights can be gained from Amazon's last-mile delivery routes?",
      "How many routes does Amazon operate in major US metro areas?",
      "What are the common challenges in last-mile logistics for Amazon?",
      "How can I access the Amazon Last Mile dataset for analysis?"
    ],
    "domain_tags": [
      "logistics",
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "US",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/opendata.png",
    "embedding_text": "The Amazon Last Mile dataset is a comprehensive collection of 9,184 historical delivery routes utilized by Amazon across five major metropolitan areas in the United States. This dataset is structured in a tabular format, with each row representing a unique delivery route and various columns detailing key attributes such as route distance, estimated delivery time, start and end locations, and timestamps. The data is particularly valuable for researchers and analysts interested in logistics and supply chain optimization, as it provides a rich source of information on the last-mile delivery process, which is often the most complex and costly segment of the supply chain. The collection methodology for this dataset is based on Amazon's operational data, capturing real-world delivery scenarios and the challenges faced in urban environments. While the specific data sources are not disclosed, it is understood that the data is derived from Amazon's logistics operations, reflecting actual delivery practices and routes taken by drivers. The dataset covers five major US metro areas, allowing for comparative analysis across different urban contexts. However, it is important to note that the dataset may have limitations in terms of data quality, such as potential inaccuracies in route details or variations in traffic conditions that could affect delivery times. Common preprocessing steps may include data cleaning to address any inconsistencies, normalization of route distances, and the categorization of delivery times for more straightforward analysis. Researchers can utilize this dataset to address a variety of research questions, such as examining the factors that influence delivery efficiency, the impact of urban infrastructure on last-mile logistics, and the effectiveness of different routing strategies employed by Amazon. The types of analyses supported by this dataset include regression analysis to identify key predictors of delivery performance, machine learning models to forecast delivery times, and descriptive statistics to summarize the characteristics of the delivery routes. Overall, the Amazon Last Mile dataset serves as an essential resource for those studying logistics, e-commerce, and urban transportation, enabling a deeper understanding of the complexities involved in last-mile delivery operations."
  },
  {
    "name": "Dressipi Fashion (RecSys 2022)",
    "description": "Session interactions and item features from styling service",
    "category": "Fashion & Apparel",
    "url": "http://www.recsyschallenge.com/2022/dataset.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "RecSys",
      "styling",
      "sessions"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Dressipi Fashion dataset contains session interactions and item features from a styling service, providing insights into consumer behavior in the fashion industry. Researchers can analyze user interactions with fashion items to enhance recommendation systems and understand styling preferences.",
    "use_cases": [
      "Analyzing consumer preferences in fashion styling.",
      "Developing and testing recommendation algorithms.",
      "Studying session behavior of users on fashion platforms."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Dressipi Fashion dataset?",
      "How can I analyze session interactions in fashion?",
      "What features are included in the Dressipi dataset?",
      "How does the Dressipi dataset help in building recommendation systems?",
      "What insights can be gained from the Dressipi Fashion dataset?",
      "Where can I find session interaction data for fashion?",
      "What are the applications of the Dressipi Fashion dataset in research?"
    ],
    "domain_tags": [
      "fashion",
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/recsyschallenge.png",
    "embedding_text": "The Dressipi Fashion dataset is a rich collection of session interactions and item features derived from a styling service, specifically designed to enhance understanding of consumer behavior within the fashion industry. This dataset is structured in a tabular format, comprising rows that represent individual user sessions and columns that detail various item features, session attributes, and user interactions. Key variables in the dataset may include session IDs, timestamps, user demographics (if available), item IDs, item categories, and interaction types (such as views, clicks, or purchases). Each variable is meticulously crafted to capture the nuances of user engagement with fashion items, enabling researchers to draw meaningful insights from the data. The collection methodology for this dataset likely involves tracking user interactions on the styling service platform, where data is logged in real-time as users browse and engage with different fashion items. This approach ensures that the dataset reflects actual user behavior, providing a reliable foundation for analysis. However, researchers should be aware of potential limitations in data quality, such as incomplete session records or biases in user demographics, which could affect the generalizability of findings. Common preprocessing steps may include data cleaning to handle missing values, normalization of item features, and encoding categorical variables for analysis. The dataset supports a variety of analytical approaches, including regression analysis to identify factors influencing user engagement, machine learning techniques for building predictive models, and descriptive statistics to summarize user behavior patterns. Researchers typically leverage the Dressipi Fashion dataset to address critical research questions related to consumer preferences, the effectiveness of recommendation systems, and the overall impact of styling services on purchasing decisions. By analyzing this dataset, scholars and practitioners can gain valuable insights into the dynamics of the fashion market, ultimately contributing to the development of more effective marketing strategies and personalized shopping experiences."
  },
  {
    "name": "AWS Open Data Registry",
    "description": "300+ petabytes across hundreds of datasets - Common Crawl, satellite imagery, genomics",
    "category": "Dataset Aggregators",
    "url": "https://registry.opendata.aws",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cloud",
      "petabyte",
      "Common Crawl",
      "satellite",
      "genomics"
    ],
    "best_for": "Large-scale data - Common Crawl (300B+ web pages), satellite imagery, genomics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The AWS Open Data Registry provides access to over 300 petabytes of diverse datasets, including Common Crawl, satellite imagery, and genomics data. Researchers and developers can leverage this extensive repository for various applications, such as machine learning, data analysis, and scientific research.",
    "use_cases": [
      "Analyzing web data trends using Common Crawl datasets.",
      "Utilizing satellite imagery for environmental monitoring.",
      "Conducting genomic research with available genomics datasets."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the AWS Open Data Registry?",
      "How can I access Common Crawl data from AWS?",
      "What types of satellite imagery datasets are hosted on AWS?",
      "Are there genomics datasets available in the AWS Open Data Registry?",
      "What is the size of the datasets in the AWS Open Data Registry?",
      "How can I use AWS Open Data for machine learning projects?",
      "What are the common applications of datasets in the AWS Open Data Registry?",
      "Where can I find information on data quality for AWS Open Data?"
    ],
    "domain_tags": [
      "cloud",
      "satellite",
      "genomics"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0004,
    "image_url": "/images/logos/opendata.png",
    "embedding_text": "The AWS Open Data Registry is a comprehensive platform that hosts over 300 petabytes of data across a multitude of datasets, including well-known resources such as Common Crawl, satellite imagery, and genomics data. This registry serves as a vital resource for researchers, developers, and data scientists looking to leverage large-scale datasets for various analytical purposes. The data structure within the registry is diverse, accommodating a wide range of formats and schemas that cater to different research needs. Users can find datasets with varying rows and columns, each containing specific variables that measure distinct attributes relevant to their fields of study. For instance, Common Crawl datasets typically include web page data, metadata, and crawl information, while satellite imagery datasets may contain spatial data, timestamps, and image resolution details. Genomics datasets often feature sequences, annotations, and associated metadata that are crucial for biological research. The collection methodology for these datasets varies significantly. Common Crawl data is collected through automated web crawling processes, capturing vast amounts of web content over time. Satellite imagery is sourced from various satellite missions and providers, ensuring a wide range of spatial and temporal coverage. Genomics data is often derived from research institutions and biobanks, reflecting the latest advancements in genetic research. Although the AWS Open Data Registry offers a wealth of information, users should be aware of potential data quality issues and limitations. Some datasets may have incomplete records, outdated information, or inconsistencies that could affect analysis outcomes. Therefore, common preprocessing steps, such as data cleaning, normalization, and transformation, are essential before conducting any analyses. Researchers can utilize the AWS Open Data Registry to address a variety of research questions, including those related to web trends, environmental changes, and genetic variations. The datasets support multiple types of analyses, including regression analysis, machine learning applications, and descriptive statistics. By employing these datasets, researchers can uncover insights, develop predictive models, and contribute to scientific knowledge across various domains. Overall, the AWS Open Data Registry stands as a pivotal resource for anyone looking to harness the power of big data in their research and development endeavors."
  },
  {
    "name": "TalkingData AdTracking Fraud Detection",
    "description": "200 million clicks with 0.25% positive fraud class for mobile ad fraud detection benchmarking",
    "category": "Fraud Detection",
    "url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/data",
    "docs_url": "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection",
    "github_url": null,
    "tags": [
      "fraud detection",
      "mobile",
      "imbalanced",
      "TalkingData"
    ],
    "best_for": "Developing and benchmarking ad fraud detection models",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "fraud detection",
      "mobile advertising"
    ],
    "summary": "The TalkingData AdTracking Fraud Detection dataset consists of 200 million clicks, with a mere 0.25% classified as positive fraud cases. This dataset is designed for benchmarking mobile ad fraud detection algorithms, providing a rich resource for researchers and practitioners in the field of fraud detection.",
    "use_cases": [
      "Developing machine learning models to detect ad fraud",
      "Benchmarking existing fraud detection algorithms",
      "Analyzing the characteristics of fraudulent versus legitimate clicks",
      "Evaluating the effectiveness of fraud detection techniques"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the TalkingData AdTracking Fraud Detection dataset?",
      "How can I use the TalkingData dataset for mobile ad fraud detection?",
      "What are the characteristics of the clicks in the TalkingData dataset?",
      "What percentage of the dataset represents fraudulent activity?",
      "What methodologies can be applied to analyze the TalkingData dataset?",
      "Are there any known limitations of the TalkingData AdTracking dataset?",
      "How can I benchmark my fraud detection algorithms using this dataset?",
      "What types of analyses can be performed on the TalkingData dataset?"
    ],
    "domain_tags": [
      "advertising",
      "technology"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "4 days",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/talkingdata-adtracking-fraud-detection.jpg",
    "embedding_text": "The TalkingData AdTracking Fraud Detection dataset is a substantial collection of 200 million clicks, specifically curated for the purpose of mobile ad fraud detection benchmarking. This dataset is particularly valuable for researchers and data scientists who are focused on understanding and mitigating fraudulent activities in mobile advertising. The structure of the dataset is tabular, consisting of rows representing individual clicks and columns that capture various attributes associated with each click. Key variables in the dataset include identifiers for the clicks, timestamps, user demographics, device information, and the classification of each click as either fraudulent or legitimate. The dataset's classification indicates that only 0.25% of the clicks are labeled as positive fraud cases, which highlights the imbalanced nature of the data and presents unique challenges for analysis and model training. The collection methodology employed by TalkingData involves aggregating click data from mobile advertising campaigns, ensuring a diverse representation of user interactions across different devices and platforms. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user demographics that could affect the generalizability of findings. Common preprocessing steps may include handling imbalanced classes, normalizing data, and feature engineering to enhance model performance. The dataset supports a variety of analyses, including regression, machine learning, and descriptive statistics, allowing researchers to explore questions related to the patterns of fraudulent behavior, the effectiveness of different detection algorithms, and the overall impact of fraud on advertising metrics. By leveraging this dataset, researchers can contribute to the development of more robust fraud detection systems, ultimately improving the integrity of mobile advertising ecosystems. The insights gained from analyzing the TalkingData AdTracking dataset can inform best practices in fraud prevention and detection, making it a critical resource for those working in the intersection of technology and economics.",
    "benchmark_usage": [
      "Benchmarking mobile ad fraud detection algorithms"
    ]
  },
  {
    "name": "Multi-Region MMM eCommerce Dataset",
    "description": "Marketing mix modeling data for 100 brands across Google, Meta, and TikTok channels in multiple regions",
    "category": "Marketing Mix",
    "url": "https://figshare.com/articles/dataset/Multi-Region_MMM_Data/21629584",
    "docs_url": "https://figshare.com/articles/dataset/Multi-Region_MMM_Data/21629584",
    "github_url": null,
    "tags": [
      "MMM",
      "marketing mix",
      "ecommerce",
      "multi-channel"
    ],
    "best_for": "Marketing mix modeling with cross-channel and cross-region data",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "marketing",
      "consumer-behavior"
    ],
    "summary": "The Multi-Region MMM eCommerce Dataset provides comprehensive marketing mix modeling data for 100 brands across major digital advertising platforms such as Google, Meta, and TikTok. This dataset enables researchers and analysts to evaluate the effectiveness of marketing strategies and optimize budget allocations across multiple channels.",
    "use_cases": [
      "Evaluating the effectiveness of different marketing channels",
      "Optimizing marketing budgets across platforms",
      "Analyzing consumer behavior in response to marketing efforts"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Multi-Region MMM eCommerce Dataset?",
      "How can I analyze marketing mix modeling data?",
      "What brands are included in the Multi-Region MMM eCommerce Dataset?",
      "What channels are covered in the marketing mix dataset?",
      "How does marketing impact eCommerce sales?",
      "What variables are included in the MMM dataset?",
      "How can I use this dataset for regression analysis?",
      "What insights can be gained from multi-channel marketing data?"
    ],
    "domain_tags": [
      "retail",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "varies",
    "geographic_scope": "Multiple regions",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/figshare.png",
    "embedding_text": "The Multi-Region MMM eCommerce Dataset is a rich resource designed for marketing mix modeling, encompassing data from 100 brands across key digital advertising platforms such as Google, Meta, and TikTok. This dataset is structured in a tabular format, containing rows that represent individual marketing campaigns or time periods for each brand, with columns that capture various metrics related to marketing spend, channel performance, and sales outcomes. Key variables in the dataset include marketing spend by channel, impressions, clicks, conversion rates, and sales figures, which collectively enable a comprehensive analysis of marketing effectiveness. The data is likely collected through a combination of direct input from brands, third-party analytics tools, and public data sources, ensuring a robust dataset for analysis. However, users should be aware of potential limitations, such as data quality issues stemming from inconsistent reporting practices across brands or channels. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and transforming variables to suit specific analytical models. Researchers can leverage this dataset to address critical research questions such as the impact of marketing spend on sales performance, the effectiveness of different advertising channels, and the interplay between consumer behavior and marketing strategies. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and commercial research. Typically, researchers use this dataset to derive actionable insights that inform marketing strategies, optimize resource allocation, and ultimately drive sales growth in the competitive eCommerce landscape."
  },
  {
    "name": "Amazon AWS Open Data",
    "description": "Registry of Open Data with analysis-ready datasets",
    "category": "Data Portals",
    "url": "https://registry.opendata.aws/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "AWS",
      "open data",
      "cloud",
      "various"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Amazon AWS Open Data is a comprehensive registry that provides access to a variety of analysis-ready datasets hosted on Amazon Web Services. Users can leverage this data for various analytical purposes, including data science projects, machine learning applications, and research across multiple domains.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the Amazon AWS Open Data registry?",
      "How can I access open data on AWS?",
      "What types of analysis can be performed with AWS open datasets?",
      "Are there any datasets related to cloud computing on AWS?",
      "What is the purpose of the Amazon AWS Open Data registry?",
      "How do I find analysis-ready datasets on AWS?",
      "What are the benefits of using AWS for open data?",
      "Can I use AWS open data for machine learning projects?"
    ],
    "domain_tags": [
      "cloud",
      "data portals"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0004,
    "embedding_text": "The Amazon AWS Open Data initiative serves as a vital resource for researchers, developers, and data enthusiasts looking to access a wide array of datasets that are ready for analysis. This registry is designed to facilitate the discovery and utilization of datasets that are hosted on Amazon Web Services, allowing users to tap into the power of cloud computing for their data needs. The datasets available through this registry cover various domains and are structured to support a range of analytical tasks. Each dataset typically consists of rows and columns, where rows represent individual records and columns correspond to specific variables or attributes of the data. The variables may include numerical values, categorical data, and text fields, depending on the nature of the dataset. The collection methodology for these datasets varies, as they are sourced from multiple contributors, including academic institutions, government agencies, and private organizations. This diversity in data sources enriches the registry, providing users with a broad spectrum of information for their analyses. While the specific temporal and geographic coverage of the datasets is not explicitly mentioned, users can expect a variety of datasets that may span different time periods and regions, depending on the individual dataset's focus. Key variables within the datasets can measure a range of phenomena, from economic indicators to environmental metrics, enabling researchers to explore numerous research questions. However, users should be aware of potential limitations in data quality, such as missing values or inconsistencies, which may require common preprocessing steps like data cleaning and normalization before analysis. Researchers often utilize the Amazon AWS Open Data registry to address various research questions, including those related to trends in consumer behavior, pricing models, and other economic analyses. The datasets support a variety of analytical techniques, including regression analysis, machine learning, and descriptive statistics, making them versatile tools for data-driven research. Overall, the Amazon AWS Open Data registry stands out as a significant platform for accessing open data, fostering innovation, and supporting a wide range of analytical endeavors."
  },
  {
    "name": "NeurIPS Competition Data",
    "description": "Top-tier conference with competitions and benchmarks",
    "category": "Data Portals",
    "url": "https://nips.cc/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "NeurIPS",
      "ML",
      "benchmarks"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NeurIPS Competition Data is a collection of datasets used in various machine learning competitions held at the NeurIPS conference. This data can be utilized for benchmarking algorithms and evaluating machine learning models in a competitive setting.",
    "use_cases": [
      "Benchmarking machine learning algorithms",
      "Evaluating model performance in competitions",
      "Developing new machine learning models",
      "Conducting comparative analysis of algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the NeurIPS Competition Data?",
      "How can I access NeurIPS competition datasets?",
      "What benchmarks are available in NeurIPS data?",
      "What machine learning competitions are associated with NeurIPS?",
      "How to use NeurIPS data for ML model evaluation?",
      "What types of algorithms can be benchmarked with NeurIPS data?",
      "Where can I find datasets from NeurIPS competitions?",
      "What are the challenges in using NeurIPS competition data?"
    ],
    "domain_tags": [
      "technology",
      "education"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/nips.png",
    "embedding_text": "The NeurIPS Competition Data is a vital resource for researchers and practitioners in the field of machine learning. This dataset is structured to support various types of analyses, including regression, machine learning model evaluation, and descriptive statistics. It typically consists of multiple rows and columns, where each row represents a unique instance or observation, and columns represent different features or variables pertinent to the competition tasks. The data is collected through rigorous methodologies employed during the NeurIPS competitions, which often involve contributions from a diverse range of participants, including academic researchers and industry professionals. While specific details about the data collection methodology may vary by competition, the overarching goal is to create a robust and comprehensive dataset that accurately reflects the challenges posed in machine learning tasks. The coverage of the dataset is broad, encompassing various domains and applications, although specific temporal or geographic coverage is not explicitly mentioned. Key variables within the dataset typically measure performance metrics, feature values, and other relevant attributes that are essential for evaluating machine learning models. Researchers often encounter data quality issues, such as missing values or inconsistencies, which necessitate common preprocessing steps like data cleaning, normalization, and feature selection. The NeurIPS Competition Data is particularly useful for addressing research questions related to algorithm performance, model generalization, and the effectiveness of different machine learning approaches. Researchers leverage this dataset to conduct comparative analyses, benchmark new algorithms, and explore innovative solutions to complex problems in machine learning. Overall, the NeurIPS Competition Data serves as a foundational tool for advancing the state of the art in machine learning research and development.",
    "benchmark_usage": [
      "Evaluating machine learning models against established benchmarks"
    ]
  },
  {
    "name": "Data Mining Cup",
    "description": "Industry-sponsored data mining competitions",
    "category": "Data Portals",
    "url": "https://www.data-mining-cup.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "data mining",
      "industry",
      "competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Data Mining Cup is a series of industry-sponsored competitions that focus on data mining techniques and applications. Participants are provided with datasets to solve real-world problems, allowing them to apply their data analysis skills in a competitive environment.",
    "use_cases": [
      "Analyzing competition datasets to improve data mining skills",
      "Developing predictive models based on provided data",
      "Comparing different data mining techniques in a competitive setting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Data Mining Cup?",
      "How can I participate in data mining competitions?",
      "What datasets are used in the Data Mining Cup?",
      "What skills are needed for data mining competitions?",
      "Where can I find data mining challenges?",
      "What are the benefits of participating in data mining competitions?"
    ],
    "domain_tags": [
      "industry"
    ],
    "size_category": "medium",
    "model_score": 0.0004,
    "embedding_text": "The Data Mining Cup is an esteemed platform for aspiring data scientists and analysts, offering a unique opportunity to engage in industry-sponsored competitions that emphasize the application of data mining techniques. Participants are typically provided with structured datasets that vary in complexity and size, allowing for a hands-on approach to solving real-world problems. The data structure often consists of rows representing individual observations and columns corresponding to various features or variables relevant to the competition's objectives. These datasets are meticulously curated from diverse sources, ensuring a rich tapestry of information that reflects actual industry challenges. The collection methodology may involve partnerships with businesses that provide anonymized data or simulated datasets designed to mimic real-world scenarios. While specific details about the temporal and geographic coverage of the datasets are not explicitly mentioned, participants can expect a range of data that spans various industries and timeframes, reflecting contemporary issues in data mining. Key variables within the datasets typically measure critical aspects relevant to the competition, such as customer behavior, transaction details, or product features, depending on the specific challenge presented. However, participants should be aware of potential data quality issues, including missing values or biases inherent in the datasets, which may necessitate common preprocessing steps such as data cleaning, normalization, and feature selection. The Data Mining Cup encourages participants to explore a variety of research questions, including predictive modeling, classification, and clustering, thereby supporting a wide array of analyses ranging from regression techniques to machine learning algorithms. Researchers and practitioners alike utilize the insights gained from these competitions to enhance their understanding of data mining applications, refine their analytical skills, and contribute to the broader field of data science. By participating in the Data Mining Cup, individuals not only gain practical experience but also have the chance to showcase their talents in a competitive environment, fostering a community of innovation and collaboration in the realm of data mining.",
    "data_modality": "tabular"
  },
  {
    "name": "DrivenData",
    "description": "Data science competitions for social impact",
    "category": "Data Portals",
    "url": "https://www.drivendata.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social impact",
      "competitions",
      "non-profit"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "DrivenData hosts data science competitions aimed at solving social issues through innovative data-driven solutions. Participants can engage with real-world datasets, develop models, and contribute to impactful projects that benefit non-profit organizations.",
    "use_cases": [
      "Developing predictive models for non-profit initiatives",
      "Analyzing data to improve social programs",
      "Creating machine learning solutions for community challenges"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the current data science competitions on DrivenData?",
      "How can I participate in DrivenData competitions?",
      "What datasets are available for social impact projects?",
      "What skills are needed for DrivenData competitions?",
      "How does DrivenData support non-profit organizations?",
      "What types of data science challenges does DrivenData offer?",
      "Are there resources for beginners on DrivenData?",
      "What is the impact of DrivenData competitions on social issues?"
    ],
    "domain_tags": [
      "non-profit"
    ],
    "size_category": "medium",
    "model_score": 0.0004,
    "embedding_text": "DrivenData is a platform dedicated to hosting data science competitions that focus on social impact. The competitions are designed to engage data scientists, analysts, and enthusiasts in solving pressing social issues through the application of data science techniques. Participants are provided with datasets that often originate from real-world social challenges, allowing them to apply their skills in a meaningful context. The data structure typically includes a variety of rows and columns, representing different variables pertinent to the competition's theme. These variables may include demographic information, outcome measures, and other relevant metrics that participants can analyze to derive insights. The collection methodology for the datasets used in DrivenData competitions varies, but it often involves collaboration with non-profit organizations, government agencies, or community groups that provide the necessary data. This ensures that the data is not only relevant but also reflects the complexities of the social issues being addressed. While specific temporal and geographic coverage details are not always explicitly mentioned, the datasets generally encompass a wide range of social issues that can be analyzed from multiple perspectives. Key variables in these datasets are designed to measure outcomes related to the competitions, such as the effectiveness of interventions, demographic impacts, and other indicators of social change. However, users should be aware of potential limitations in data quality, including missing values, biases in data collection, or variations in data reporting standards across different organizations. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the datasets for analysis. Researchers and participants in DrivenData competitions can address a variety of research questions, such as evaluating the impact of specific programs, identifying factors that contribute to social issues, and developing predictive models that can inform policy decisions. The types of analyses supported by the data include regression analysis, machine learning applications, and descriptive statistics, allowing participants to explore the data from different analytical angles. DrivenData serves as a valuable resource for researchers and practitioners alike, providing a unique opportunity to leverage data science for social good while fostering a community of like-minded individuals committed to making a difference.",
    "data_modality": "mixed"
  },
  {
    "name": "UCI Machine Learning Repository",
    "description": "688 curated benchmark datasets since 1987 - gold standard for ML research",
    "category": "Dataset Aggregators",
    "url": "https://archive.ics.uci.edu",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "benchmarks",
      "classic",
      "ML",
      "academic"
    ],
    "best_for": "Benchmark datasets with extensive documentation for reproducible ML research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The UCI Machine Learning Repository is a comprehensive collection of 688 curated benchmark datasets that have been made available since 1987. These datasets serve as a gold standard for machine learning research, providing researchers and practitioners with a reliable source of data for testing algorithms and models across various domains.",
    "use_cases": [
      "Testing machine learning algorithms",
      "Benchmarking model performance",
      "Academic research in machine learning",
      "Educational purposes for data science courses"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the UCI Machine Learning Repository?",
      "How can I access benchmark datasets for machine learning?",
      "What types of data are included in the UCI Machine Learning Repository?",
      "Where can I find curated datasets for academic research?",
      "What is the history of the UCI Machine Learning Repository?",
      "How do I use UCI datasets for machine learning experiments?",
      "What are the most popular datasets in the UCI Machine Learning Repository?",
      "How can I contribute to the UCI Machine Learning Repository?"
    ],
    "domain_tags": [
      "academic"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/uci.png",
    "embedding_text": "The UCI Machine Learning Repository is a well-established resource that has been pivotal in the field of machine learning since its inception in 1987. It hosts a diverse array of 688 curated benchmark datasets that researchers and practitioners utilize for various purposes, including testing and validating machine learning algorithms. The datasets vary in structure, typically presented in a tabular format with rows representing individual observations and columns corresponding to different variables or features. Each dataset is accompanied by detailed documentation outlining its schema, including key variables, their definitions, and the types of measurements they encompass. The collection methodology for these datasets is rigorous, often involving contributions from academic institutions and researchers who ensure the data's relevance and quality. While the repository does not explicitly mention temporal or geographic coverage for most datasets, it is understood that they span various domains and contexts, making them suitable for a wide range of research questions. Researchers often employ these datasets to address critical inquiries in machine learning, such as evaluating the effectiveness of different algorithms, exploring data patterns, and conducting comparative analyses. Common preprocessing steps may include data cleaning, normalization, and feature selection, which are essential for preparing the datasets for analysis. The UCI Machine Learning Repository supports various types of analyses, including regression, classification, and clustering, making it a versatile tool for both novice and experienced data scientists. As a gold standard in the field, it continues to be a valuable asset for academic research, educational purposes, and practical applications in machine learning.",
    "benchmark_usage": [
      "Commonly used for benchmarking machine learning models"
    ]
  },
  {
    "name": "Google Cloud Public Datasets",
    "description": "20+ petabytes across 200+ datasets with 1TB free BigQuery queries monthly",
    "category": "Dataset Aggregators",
    "url": "https://cloud.google.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cloud",
      "BigQuery",
      "climate",
      "blockchain",
      "COVID"
    ],
    "best_for": "SQL-based analysis of climate, blockchain, and Google Trends data",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Google Cloud Public Datasets offer a vast collection of over 20 petabytes of data across more than 200 datasets, providing users with 1TB of free BigQuery queries each month. This extensive resource allows researchers and data scientists to explore a variety of topics, including climate change, blockchain technology, and the impacts of COVID-19, enabling them to perform complex analyses and derive insights from large-scale data.",
    "use_cases": [
      "Analyzing climate data to study environmental changes over time.",
      "Using blockchain datasets to explore cryptocurrency trends.",
      "Investigating the economic impacts of COVID-19 through public health data.",
      "Performing machine learning analyses on large datasets for predictive modeling."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the available datasets in Google Cloud Public Datasets?",
      "How can I access Google Cloud Public Datasets?",
      "What types of data are included in Google Cloud Public Datasets?",
      "What is the size of Google Cloud Public Datasets?",
      "How can I use BigQuery with Google Cloud Public Datasets?",
      "What are the benefits of using Google Cloud Public Datasets for research?",
      "Are there any restrictions on using Google Cloud Public Datasets?",
      "What topics can be explored using Google Cloud Public Datasets?"
    ],
    "domain_tags": [
      "cloud",
      "data science",
      "big data",
      "environment"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0004,
    "image_url": "/images/datasets/google-cloud-public-datasets.png",
    "embedding_text": "The Google Cloud Public Datasets initiative provides an extensive repository of data designed to facilitate research and analysis across various domains. With over 20 petabytes of data available, this resource encompasses more than 200 datasets, making it a significant asset for data scientists, researchers, and analysts. The datasets are structured in a tabular format, which allows for easy querying and manipulation using Google BigQuery, a powerful data warehouse solution. Users are granted 1TB of free BigQuery queries each month, enabling them to perform substantial analyses without incurring costs. The datasets cover a wide range of topics, including but not limited to climate science, blockchain technology, and the socio-economic impacts of the COVID-19 pandemic. Each dataset is meticulously curated, ensuring that the data is relevant and of high quality, although users should be aware of potential limitations such as missing values or biases inherent in the data collection process. Researchers typically utilize these datasets to address a variety of research questions, from understanding trends in climate change to analyzing the effects of public health policies during the pandemic. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. The Google Cloud Public Datasets serve as a foundational tool for both descriptive and inferential analyses, including regression modeling and machine learning applications, thereby empowering users to derive meaningful insights from vast amounts of data."
  },
  {
    "name": "EU TED Procurement",
    "description": "800K+ procurement notices annually. All EU public contracts above thresholds. Structured XML since 2006. Cross-country procurement research",
    "category": "Auctions & Marketplaces",
    "url": "https://ted.europa.eu/TED/browse/browseByMap.do",
    "docs_url": "https://simap.ted.europa.eu/web/simap/standard-forms-for-public-procurement",
    "github_url": null,
    "tags": [
      "EU",
      "procurement",
      "tenders",
      "government",
      "cross-country"
    ],
    "best_for": "Learning auctions & marketplaces analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "procurement",
      "government",
      "EU"
    ],
    "summary": "The EU TED Procurement dataset contains over 800,000 procurement notices annually, covering all public contracts above specified thresholds across the European Union. This structured XML dataset, available since 2006, is invaluable for cross-country procurement research and analysis.",
    "use_cases": [
      "Analyzing trends in public procurement across EU countries",
      "Comparing procurement practices between different EU member states",
      "Evaluating the impact of procurement policies on market competition",
      "Studying the frequency and types of tenders issued by government agencies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest EU procurement notices?",
      "How can I analyze government tenders in the EU?",
      "What procurement data is available for cross-country comparisons?",
      "Where can I find structured XML data on EU public contracts?",
      "What are the trends in EU procurement over the years?",
      "How do procurement thresholds vary across EU countries?",
      "What insights can be gained from EU public contracts data?",
      "How to access the EU TED Procurement dataset?"
    ],
    "domain_tags": [
      "government",
      "public sector",
      "procurement"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2006-present",
    "geographic_scope": "European Union",
    "size_category": "massive",
    "model_score": 0.0004,
    "image_url": "/images/logos/europa.png",
    "embedding_text": "The EU TED Procurement dataset is a comprehensive collection of procurement notices that provides insights into public contracts across the European Union. With over 800,000 notices published annually, this dataset serves as a critical resource for researchers, policymakers, and analysts interested in understanding government procurement processes. The data is structured in XML format, allowing for efficient parsing and analysis. Each notice typically includes key variables such as contract value, issuing authority, contract type, and award criteria, which are essential for evaluating procurement practices and trends. The dataset has been compiled since 2006, ensuring a rich temporal coverage that allows for longitudinal studies of procurement behavior and policy impacts. Researchers can utilize this dataset to address various research questions, such as the effectiveness of procurement strategies, the influence of regulatory changes on tendering processes, and the identification of market trends in public contracting. Common preprocessing steps may include data cleaning to handle missing values, normalization of contract values for inflation, and categorization of contract types for comparative analysis. The dataset supports various types of analyses, including regression analysis to identify factors influencing contract awards, machine learning techniques for predictive modeling of procurement outcomes, and descriptive statistics to summarize procurement activities. Overall, the EU TED Procurement dataset is an invaluable tool for advancing knowledge in the field of public procurement and enhancing transparency in government spending."
  },
  {
    "name": "Meta (Facebook) Research",
    "description": "1.1B+ public FB/IG posts with engagement metrics",
    "category": "Social & Web",
    "url": "https://fort.fb.com/researcher-datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "social media",
      "Facebook",
      "Instagram",
      "engagement"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "social media",
      "engagement"
    ],
    "summary": "The Meta (Facebook) Research dataset contains over 1.1 billion public posts from Facebook and Instagram, along with engagement metrics. This dataset can be utilized to analyze social media trends, user engagement patterns, and the impact of content on audience interaction.",
    "use_cases": [
      "Analyzing user engagement trends on social media",
      "Comparing engagement metrics between Facebook and Instagram",
      "Identifying factors that influence post engagement",
      "Studying the impact of content type on audience interaction"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the engagement metrics for public Facebook posts?",
      "How do Instagram posts compare in engagement to Facebook posts?",
      "What trends can be identified in social media engagement?",
      "How does post content affect user interaction on Facebook?",
      "What are the most engaging types of posts on Instagram?",
      "How can public social media data inform marketing strategies?",
      "What demographic insights can be gained from analyzing Facebook and Instagram posts?",
      "How can engagement metrics be used to measure social media effectiveness?"
    ],
    "domain_tags": [
      "social media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0004,
    "image_url": "/images/logos/fb.png",
    "embedding_text": "The Meta (Facebook) Research dataset is a comprehensive collection of over 1.1 billion public posts sourced from Facebook and Instagram, encompassing a wide array of engagement metrics. This dataset is structured in a tabular format, where each row represents an individual post, and columns include variables such as post content, engagement metrics (likes, shares, comments), timestamps, and potentially user demographics. The collection methodology involves scraping publicly available posts from these platforms, ensuring that the data reflects real user interactions and behaviors. However, it is important to note that the dataset may have limitations regarding data quality, including potential biases in the types of posts that are publicly available and the engagement metrics that can be accurately captured. Common preprocessing steps may include cleaning the text data, normalizing engagement metrics, and filtering out posts that do not meet certain criteria for analysis. Researchers can leverage this dataset to address a variety of research questions, such as understanding the factors that drive engagement on social media, analyzing trends in user interactions over time, and comparing the effectiveness of different content types across platforms. The dataset supports various types of analyses, including regression analysis to identify predictors of engagement, machine learning models to classify post types, and descriptive statistics to summarize engagement trends. Typically, researchers use this dataset to inform marketing strategies, enhance social media campaigns, and contribute to academic discussions surrounding social media dynamics and user behavior."
  },
  {
    "name": "Ukraine eCommerce (Fozzy)",
    "description": "E-commerce sales data from Fozzy Group retail chain in Ukraine",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "Ukraine",
      "sales"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Ukraine eCommerce dataset from Fozzy provides comprehensive sales data from the Fozzy Group retail chain in Ukraine, focusing on grocery and supermarket sales. This dataset can be utilized for various analyses related to consumer behavior, pricing strategies, and market trends in the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns in the grocery sector",
      "Evaluating the impact of pricing changes on sales",
      "Identifying seasonal trends in e-commerce sales",
      "Comparing sales performance across different product categories"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the sales trends in Ukraine's e-commerce sector?",
      "How does consumer behavior vary in grocery shopping?",
      "What pricing strategies are effective in the Ukrainian retail market?",
      "What are the seasonal patterns in e-commerce sales?",
      "How does the Fozzy Group's sales data compare to other retailers?",
      "What factors influence online grocery shopping in Ukraine?",
      "What demographic trends can be observed in the sales data?",
      "How can regression analysis be applied to this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ukraine",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/ukraine-ecommerce-fozzy.jpg",
    "embedding_text": "The Ukraine eCommerce dataset from Fozzy is a rich source of e-commerce sales data specifically from the Fozzy Group retail chain, which operates within the grocery and supermarket sector in Ukraine. This dataset is structured in a tabular format, comprising multiple rows and columns that capture various sales metrics over time. Each row typically represents a unique transaction or sales record, while the columns include key variables such as product identifiers, sales amounts, transaction dates, and possibly customer demographics. The data collection methodology employed by Fozzy Group likely involves systematic recording of sales transactions through their point-of-sale systems, ensuring a high level of accuracy and detail in the data captured. However, it is essential to consider potential limitations in data quality, such as missing values or inconsistencies in product categorization, which may arise from the complexities of retail operations. Researchers and analysts can leverage this dataset to address a variety of research questions, including examining consumer behavior trends, assessing the effectiveness of pricing strategies, and identifying seasonal fluctuations in sales. The dataset supports various types of analyses, including regression analysis to model relationships between different variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize sales performance. Common preprocessing steps may involve cleaning the data to handle missing values, transforming variables for analysis, and aggregating sales data to derive insights at different levels, such as daily, weekly, or monthly sales trends. Overall, this dataset serves as a valuable resource for researchers and practitioners interested in exploring the dynamics of the e-commerce landscape in Ukraine, particularly within the grocery retail sector."
  },
  {
    "name": "Kaggle Datasets",
    "description": "50,000+ public datasets with free GPU notebooks and active ML community of 23M members",
    "category": "Dataset Aggregators",
    "url": "https://www.kaggle.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "ML",
      "competitions",
      "notebooks",
      "community"
    ],
    "best_for": "Applied machine learning with financial, economic, and pricing datasets",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Kaggle Datasets is a comprehensive collection of over 50,000 public datasets that cater to a wide range of data science and machine learning needs. Users can leverage these datasets for various analyses, model training, and competitions, supported by an active community of 23 million members who share insights and resources.",
    "use_cases": [
      "Training machine learning models",
      "Conducting data analysis for competitions",
      "Exploring data science techniques",
      "Collaborating with the ML community"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the best datasets for machine learning?",
      "How can I access public datasets on Kaggle?",
      "What types of datasets are available for data analysis?",
      "Where can I find datasets for competitions?",
      "How to use Kaggle notebooks with datasets?",
      "What is the size of the Kaggle datasets collection?",
      "What are the most popular datasets on Kaggle?",
      "How to contribute to Kaggle datasets?"
    ],
    "domain_tags": [
      "ML",
      "Data Science"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/kaggle-datasets.png",
    "embedding_text": "Kaggle Datasets represents a vast repository of over 50,000 public datasets, designed to facilitate the needs of data scientists and machine learning practitioners. The datasets encompass a wide array of topics, providing a rich resource for exploration and analysis. The data structure typically consists of tabular formats, where each dataset is organized into rows and columns, with variables representing different attributes of the data. Users can find datasets related to various domains, including but not limited to finance, healthcare, and social sciences. The collection methodology involves contributions from users and organizations worldwide, ensuring a diverse range of data sources and types. However, the quality of the datasets can vary, and users are encouraged to assess the data quality and limitations before proceeding with their analyses. Common preprocessing steps may include data cleaning, normalization, and transformation, which are essential for preparing the data for machine learning models or statistical analyses. Researchers can utilize these datasets to address a multitude of research questions, such as predicting outcomes, identifying trends, or exploring relationships between variables. The datasets support various types of analyses, including regression, machine learning, and descriptive statistics, making them versatile tools for both novice and experienced data scientists. Furthermore, the active Kaggle community provides a platform for collaboration, knowledge sharing, and competition, enhancing the overall experience of using these datasets in research and practical applications."
  },
  {
    "name": "Twitch Streaming Dataset",
    "description": "16 days of viewer counts, stream metadata, game categories from Oct 2017. Live streaming platform dynamics",
    "category": "Entertainment & Media",
    "url": "https://github.com/mingt2019/Twitch-Dataset",
    "docs_url": null,
    "github_url": "https://github.com/mingt2019/Twitch-Dataset",
    "tags": [
      "Twitch",
      "live streaming",
      "viewership",
      "gaming"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "live streaming",
      "gaming",
      "viewership"
    ],
    "summary": "The Twitch Streaming Dataset contains viewer counts and stream metadata collected over 16 days in October 2017. This dataset allows researchers and analysts to explore the dynamics of live streaming on the Twitch platform, focusing on viewer engagement and game category popularity.",
    "use_cases": [
      "Analyzing viewer engagement trends across different game categories.",
      "Comparing viewer counts between popular and less popular streams.",
      "Investigating the impact of stream metadata on viewer retention.",
      "Exploring the relationship between streaming frequency and viewer counts."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the viewer counts for different games on Twitch?",
      "How does stream metadata affect viewer engagement on Twitch?",
      "What trends can be observed in Twitch viewership over 16 days?",
      "How do game categories influence viewer counts on Twitch?",
      "What is the distribution of viewers across different Twitch streams?",
      "How can I analyze Twitch streaming data for research purposes?",
      "What insights can be gained from the Twitch Streaming Dataset?",
      "How does the Twitch platform's dynamics change over time?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2017",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/twitch-streaming-dataset.png",
    "embedding_text": "The Twitch Streaming Dataset is a rich collection of data that captures the dynamics of live streaming on the popular platform, Twitch. This dataset spans a period of 16 days in October 2017, providing a snapshot of viewer counts, stream metadata, and game categories. The data is structured in a tabular format, consisting of rows that represent individual streams and columns that capture various attributes such as viewer counts, stream titles, game categories, and other metadata relevant to each stream. Researchers and analysts can leverage this dataset to gain insights into the behavior of viewers on Twitch, exploring how different factors influence viewer engagement and the popularity of various game categories.\n\nThe collection methodology for this dataset likely involved scraping data directly from the Twitch platform or utilizing an API provided by Twitch, which allows for the extraction of real-time data related to streams and viewer counts. As such, the dataset is expected to reflect a diverse range of viewer interactions and streaming behaviors during the specified time frame. However, it is important to note that the dataset may have limitations in terms of data quality, such as potential missing values or inaccuracies in viewer counts due to fluctuations in streaming activity or technical issues during data collection.\n\nKey variables in the dataset include viewer counts, which measure the number of viewers watching a particular stream at any given time, and stream metadata, which encompasses details such as the title of the stream, the game being played, and other relevant attributes. These variables are crucial for understanding the factors that drive viewer engagement on the platform. Common preprocessing steps may include cleaning the data to handle missing values, normalizing viewer counts for comparative analysis, and categorizing streams based on game types for more targeted insights.\n\nResearchers can utilize the Twitch Streaming Dataset to address a variety of research questions, such as examining how different game categories impact viewer counts, identifying trends in viewer engagement over time, and analyzing the relationship between stream metadata and viewer retention. The dataset supports various types of analyses, including regression analysis to identify predictors of viewer counts, machine learning techniques for clustering streams based on viewer behavior, and descriptive statistics to summarize viewer engagement patterns.\n\nOverall, the Twitch Streaming Dataset serves as a valuable resource for researchers interested in the intersection of technology, entertainment, and media. By analyzing this dataset, researchers can uncover insights into the dynamics of live streaming, contributing to a deeper understanding of viewer behavior and the factors that drive engagement on platforms like Twitch."
  },
  {
    "name": "EIA-860 Generator Inventory",
    "description": "Annual survey of all U.S. electric generators including capacity, technology, ownership, and location",
    "category": "Energy",
    "url": "https://www.eia.gov/electricity/data/eia860/",
    "docs_url": "https://www.eia.gov/electricity/data/eia860/",
    "github_url": null,
    "tags": [
      "generators",
      "capacity",
      "technology",
      "annual"
    ],
    "best_for": "Understanding the U.S. electricity generation fleet composition and trends",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "electricity",
      "infrastructure"
    ],
    "summary": "The EIA-860 Generator Inventory dataset provides comprehensive annual data on all electric generators in the U.S., detailing their capacity, technology, ownership, and geographical location. Researchers and analysts can utilize this dataset to assess trends in energy production, evaluate the impact of different technologies, and inform policy decisions related to energy infrastructure.",
    "use_cases": [
      "Analyzing trends in renewable energy capacity across the U.S.",
      "Evaluating the impact of ownership structures on generator performance.",
      "Assessing the distribution of different generator technologies geographically.",
      "Investigating the relationship between generator capacity and energy output."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the EIA-860 Generator Inventory?",
      "How can I access the EIA-860 dataset?",
      "What types of generators are included in the EIA-860 data?",
      "What information does the EIA-860 provide about electric generators?",
      "How is the EIA-860 dataset structured?",
      "What are the key variables in the EIA-860 Generator Inventory?",
      "What methodologies are used to collect data for the EIA-860?",
      "What analyses can be performed using the EIA-860 dataset?"
    ],
    "domain_tags": [
      "energy",
      "infrastructure"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2001-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/logos/eia.png",
    "embedding_text": "The EIA-860 Generator Inventory dataset is a vital resource for understanding the landscape of electric generation in the United States. This dataset is structured in a tabular format, containing rows that represent individual electric generators and columns that capture various attributes such as generator capacity, technology type, ownership details, and geographic location. The data is collected annually through a comprehensive survey conducted by the Energy Information Administration (EIA), which ensures that it reflects the most current state of electric generation infrastructure across the nation. The key variables in the dataset include generator capacity measured in megawatts, technology type (such as solar, wind, natural gas, coal, etc.), ownership information (public, private, or cooperative), and location data that specifies the state and county of each generator. These variables are crucial for measuring the performance and distribution of energy generation capabilities across different technologies and regions. While the dataset is robust, researchers should be aware of potential limitations, such as reporting inconsistencies or variations in data collection methodologies across different states. Common preprocessing steps may include cleaning the data for missing values, standardizing technology classifications, and aggregating data for regional analyses. The EIA-860 dataset supports a variety of research questions, such as examining the growth of renewable energy sources, analyzing the impact of regulatory changes on generator ownership, and exploring the geographic distribution of energy resources. It is particularly useful for conducting descriptive analyses, regression modeling, and machine learning applications aimed at predicting future energy trends. Researchers typically leverage this dataset to inform policy decisions, guide investment strategies in energy infrastructure, and contribute to academic studies focused on energy economics and sustainability."
  },
  {
    "name": "FRED (Federal Reserve Economic Data)",
    "description": "816,000+ US macroeconomic time series from 100+ sources with free API",
    "category": "Dataset Aggregators",
    "url": "https://fred.stlouisfed.org",
    "docs_url": "https://fred.stlouisfed.org/docs/api/fred/",
    "github_url": null,
    "tags": [
      "macro",
      "economics",
      "time series",
      "Fed",
      "API"
    ],
    "best_for": "US macroeconomic research - Paul Krugman called it 'the most amazing economics website'",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "macroeconomics",
      "time series analysis",
      "economic indicators"
    ],
    "summary": "The FRED (Federal Reserve Economic Data) dataset provides access to over 816,000 US macroeconomic time series collected from more than 100 sources. This extensive dataset allows users to analyze economic trends, perform time series forecasting, and develop economic models using a free API.",
    "use_cases": [
      "Analyzing trends in US GDP over time",
      "Forecasting unemployment rates using historical data",
      "Studying the impact of monetary policy on inflation",
      "Comparing economic indicators across different states"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FRED dataset?",
      "How can I access US macroeconomic time series data?",
      "What types of economic indicators are available in FRED?",
      "How do I use the FRED API?",
      "What are the sources of data in FRED?",
      "What analyses can be performed with FRED data?",
      "Where can I find time series data for US economics?",
      "What is the significance of the FRED dataset in economic research?"
    ],
    "domain_tags": [
      "economics",
      "finance"
    ],
    "data_modality": "time-series",
    "size_category": "massive",
    "geographic_scope": "United States",
    "model_score": 0.0004,
    "embedding_text": "The FRED (Federal Reserve Economic Data) dataset is a comprehensive repository of over 816,000 macroeconomic time series data points, sourced from more than 100 reputable organizations and institutions. This dataset is structured in a tabular format, where each row typically represents a unique time series, and the columns include variables such as the series identifier, title, frequency, units, and the actual data values over time. The data is meticulously collected from various authoritative sources, including government agencies, international organizations, and academic institutions, ensuring a high level of reliability and credibility. Researchers and analysts often utilize FRED to explore a wide range of economic phenomena, from inflation and employment trends to GDP growth and interest rates. Key variables within the dataset measure essential economic indicators, such as consumer price indices, employment figures, and production outputs, providing a rich landscape for economic analysis. However, users should be aware of potential limitations, including data gaps for certain series and variations in data collection methodologies across different sources. Common preprocessing steps may involve handling missing values, normalizing data, and transforming time series for analysis. FRED supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for economists and data scientists alike. Researchers typically leverage this dataset to address critical research questions related to economic trends, policy impacts, and forecasting future economic conditions, thereby contributing valuable insights to the field of economics."
  },
  {
    "name": "BLS JOLTS",
    "description": "Monthly job openings, hires, separations by industry since 2000. Bureau of Labor Statistics time series",
    "category": "Labor Markets",
    "url": "https://www.bls.gov/jlt/data.htm",
    "docs_url": "https://www.bls.gov/jlt/jltover.htm",
    "github_url": null,
    "tags": [
      "jobs",
      "labor",
      "openings",
      "hires",
      "BLS"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The BLS JOLTS dataset provides monthly statistics on job openings, hires, and separations across various industries in the United States since the year 2000. This dataset can be utilized to analyze labor market trends, understand employment dynamics, and inform policy decisions related to workforce development.",
    "use_cases": [
      "Analyzing trends in job openings and hires over time",
      "Comparing job separation rates across different industries",
      "Investigating the impact of economic policies on labor market dynamics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the monthly job openings reported by BLS?",
      "How many hires were made in the last year according to JOLTS?",
      "What industries have the highest job separations?",
      "How do job openings vary by industry?",
      "What trends can be observed in hires and separations over time?",
      "How does the BLS JOLTS data inform labor market policies?",
      "What is the historical data on job openings since 2000?",
      "How can I access the BLS JOLTS dataset?"
    ],
    "domain_tags": [
      "labor markets"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2000-present",
    "size_category": "medium",
    "model_score": 0.0004,
    "image_url": "/images/datasets/bls-jolts.png",
    "embedding_text": "The BLS JOLTS (Job Openings and Labor Turnover Survey) dataset is a comprehensive time series collection that provides vital insights into the dynamics of the labor market in the United States. This dataset includes monthly data on job openings, hires, and separations, allowing researchers and analysts to explore employment trends across various industries since the year 2000. The data is structured in a tabular format, with rows representing individual months and columns detailing key variables such as the number of job openings, hires, and separations, categorized by industry. Each entry in the dataset provides a snapshot of labor market activity, enabling users to assess fluctuations and patterns over time. The collection methodology involves surveys conducted by the Bureau of Labor Statistics (BLS), which collects data from a representative sample of establishments across the country. This ensures that the dataset reflects a broad spectrum of industries and employment situations. The coverage of the dataset is extensive, spanning over two decades, and focuses on the entire United States, making it a valuable resource for understanding national labor market trends. Key variables within the dataset include the number of job openings, which measures the demand for labor; hires, which indicate the rate of employment growth; and separations, which encompass both voluntary and involuntary departures from jobs. These variables are critical for analyzing labor market health and can inform various research questions, such as the relationship between job openings and unemployment rates or the effects of economic downturns on hiring practices. While the BLS JOLTS dataset is a robust resource, users should be aware of certain limitations. Data quality can vary based on the response rates of surveyed establishments, and there may be discrepancies in reporting practices across different industries. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and aggregating data to different time frames for trend analysis. Researchers typically employ this dataset for a variety of analyses, including regression modeling to predict employment trends, machine learning applications to classify industry behaviors, and descriptive statistics to summarize labor market conditions. By leveraging the BLS JOLTS dataset, analysts can gain a deeper understanding of workforce dynamics, identify emerging trends, and contribute to informed policy-making in the realm of labor economics.",
    "geographic_scope": "United States"
  },
  {
    "name": "AEA Data and Code Repository",
    "description": "Replication packages for all AEA publications since 2019 with DOI-assigned packages",
    "category": "Dataset Aggregators",
    "url": "https://www.openicpsr.org/openicpsr/search/aea/studies",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "replication",
      "AEA",
      "economics",
      "reproducibility"
    ],
    "best_for": "Verifying published empirical work with data, code, and documentation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The AEA Data and Code Repository provides replication packages for all AEA publications since 2019, which include datasets and code necessary for reproducing the results of these studies. Researchers can utilize this repository to verify findings, conduct further analysis, and build upon existing research in economics.",
    "use_cases": [
      "Verifying findings from AEA publications",
      "Conducting further analysis on economic data",
      "Building upon existing research in economics"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the AEA Data and Code Repository?",
      "How can I access replication packages for AEA publications?",
      "What types of data are included in the AEA Data and Code Repository?",
      "What is the significance of DOI-assigned packages in the AEA repository?",
      "How can I use AEA data for my research?",
      "Are there any limitations to the data provided in the AEA repository?",
      "What is the purpose of replication packages in economics?",
      "How do I cite the AEA Data and Code Repository in my work?"
    ],
    "domain_tags": [
      "economics"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The AEA Data and Code Repository serves as a comprehensive resource for researchers in the field of economics, providing access to replication packages associated with all publications from the American Economic Association (AEA) since 2019. This repository is designed to facilitate reproducibility in research, a critical aspect of scientific inquiry that ensures findings can be verified and built upon by other scholars. The data structure within the repository typically includes a variety of datasets and corresponding code files, which are essential for replicating the analyses presented in AEA publications. Each replication package is meticulously organized, often containing a README file that outlines the contents, data structure, and instructions for use. The datasets may vary in terms of rows and columns, depending on the specific study, but they generally include key variables that measure various economic phenomena, such as consumer behavior, market dynamics, and policy impacts. The collection methodology for the data included in the repository typically involves rigorous data gathering techniques, often utilizing existing datasets from reputable sources, surveys, or experimental data collected by researchers. While the repository aims to provide high-quality data, users should be aware of potential limitations, such as missing data points or specific contextual factors that may not be fully captured in the datasets. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. Researchers can leverage the AEA Data and Code Repository to address a wide range of research questions, from exploring causal relationships in economic models to conducting descriptive analyses of economic trends. The repository supports various types of analyses, including regression analyses, machine learning applications, and descriptive statistics, making it a versatile tool for both novice and experienced researchers. By utilizing the replication packages, researchers can not only verify the findings of previous studies but also explore new avenues of inquiry, contributing to the ongoing discourse in the field of economics.",
    "benchmark_usage": [
      "Replication of AEA publications"
    ]
  },
  {
    "name": "AirBnb (Inside Airbnb)",
    "description": "6M+ listings, 190M+ reviews with pricing and amenities",
    "category": "Real Estate",
    "url": "http://insideairbnb.com/explore",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Airbnb",
      "rentals",
      "pricing",
      "global"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The AirBnb dataset, sourced from Inside Airbnb, contains over 6 million listings and more than 190 million reviews, providing insights into pricing, amenities, and rental trends across various locations globally. Researchers and analysts can utilize this extensive dataset to explore market dynamics, consumer preferences, and pricing strategies in the short-term rental industry.",
    "use_cases": [
      "Analyzing pricing trends in the short-term rental market",
      "Examining consumer behavior based on review data",
      "Assessing the impact of amenities on rental success",
      "Exploring geographic differences in Airbnb listings"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average prices for Airbnb listings in different cities?",
      "How do amenities affect rental prices?",
      "What trends can be observed in Airbnb reviews over time?",
      "How does the number of reviews correlate with pricing?",
      "What are the most common amenities offered in Airbnb listings?",
      "How does the geographic location influence rental prices?",
      "What demographic factors can be inferred from the data?",
      "How do seasonal trends affect Airbnb pricing?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/insideairbnb.png",
    "embedding_text": "The AirBnb dataset from Inside Airbnb is a comprehensive collection of data that includes over 6 million listings and more than 190 million reviews, making it a valuable resource for researchers and analysts interested in the short-term rental market. The dataset is structured in a tabular format, with rows representing individual listings and reviews, and columns containing key variables such as listing ID, price, number of reviews, and various amenities offered. This structure allows for easy manipulation and analysis using data analysis tools such as Python's pandas library. The data is collected from publicly available information on the Airbnb platform, ensuring a wide coverage of listings across different geographic locations. However, it is important to note that the dataset may have limitations in terms of data quality, as it relies on user-generated content, which can vary in accuracy and completeness. Common preprocessing steps may include cleaning the data to handle missing values, normalizing price data, and categorizing amenities for analysis. Researchers can leverage this dataset to address various research questions, such as how pricing strategies vary across different regions, the relationship between the number of reviews and pricing, and the impact of amenities on rental success. The dataset supports a range of analyses, including regression analysis to understand pricing dynamics, machine learning techniques for predictive modeling, and descriptive statistics to summarize trends in consumer behavior. Overall, the AirBnb dataset is an essential tool for those looking to explore the intricacies of the short-term rental market, providing insights that can inform business strategies and academic research alike."
  },
  {
    "name": "NYC Shopping",
    "description": "Large sales dataset from New York City retail",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/pigment/big-sales-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "NYC",
      "retail",
      "large-scale"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The NYC Shopping dataset is a comprehensive collection of sales data from retail establishments in New York City, focusing on grocery and supermarket transactions. This dataset can be utilized for various analyses, including understanding consumer purchasing patterns, evaluating pricing strategies, and conducting market research.",
    "use_cases": [
      "Analyzing consumer purchasing trends",
      "Evaluating the impact of pricing on sales",
      "Conducting market segmentation analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NYC Shopping dataset?",
      "How can I analyze retail sales in NYC?",
      "What trends can be identified in NYC grocery sales?",
      "What are the key variables in the NYC Shopping dataset?",
      "How does consumer behavior vary in NYC supermarkets?",
      "What insights can be drawn from large-scale retail data in NYC?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "New York City",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/nyc-shopping.png",
    "embedding_text": "The NYC Shopping dataset is a large-scale sales dataset that captures a wide array of retail transactions specifically from grocery and supermarket establishments in New York City. This dataset is structured in a tabular format, consisting of numerous rows and columns that represent individual sales transactions, customer demographics, product details, and time stamps. Each row typically corresponds to a single transaction, while the columns may include variables such as transaction ID, store location, product categories, quantities sold, prices, and timestamps of the sales. The dataset is designed to provide a comprehensive view of retail sales dynamics in one of the largest urban markets in the United States. The collection methodology for this dataset likely involves aggregating sales data from various retail partners, utilizing point-of-sale systems and transaction logs to ensure accuracy and completeness. The data sources may include direct contributions from grocery chains, independent supermarkets, and possibly public records that track retail activity in the city. Coverage of this dataset is geographically focused on New York City, capturing the diverse shopping behaviors of its residents and visitors. While specific temporal coverage is not mentioned, the dataset is expected to include a range of dates to reflect seasonal trends and changes in consumer behavior over time. Key variables within the dataset measure critical aspects of retail sales, including sales volume, revenue generated, and customer demographics, which can provide insights into purchasing patterns and preferences. However, like any large dataset, there may be limitations regarding data quality, such as missing values, inconsistencies in data entry, or variations in how sales are recorded across different retailers. Researchers using the NYC Shopping dataset may need to perform common preprocessing steps, including data cleaning, normalization, and handling of missing values, to prepare the data for analysis. This dataset supports a variety of research questions, such as how pricing strategies affect consumer purchasing decisions, what demographic factors influence shopping behavior, and how sales vary across different neighborhoods in New York City. Analysts can employ various types of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize sales trends. Researchers typically use this dataset in studies focused on retail economics, consumer behavior analysis, and market research, leveraging the rich data to draw actionable insights that can inform business strategies and policy decisions."
  },
  {
    "name": "Google Trends Datastore",
    "description": "Search interest data for nowcasting. Economic indicators, demand prediction, event detection",
    "category": "Social & Web",
    "url": "https://googletrends.github.io/data/",
    "docs_url": "https://developers.google.com/search/apis/trends",
    "github_url": null,
    "tags": [
      "search trends",
      "nowcasting",
      "research",
      "time-series"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Google Trends Datastore provides search interest data that can be utilized for nowcasting economic indicators, predicting demand, and detecting events. Researchers can analyze trends over time to gain insights into consumer behavior and market dynamics.",
    "use_cases": [
      "Analyzing consumer interest in products over time",
      "Predicting economic trends based on search behavior",
      "Detecting emerging events or trends in various sectors"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Google Trends Datastore?",
      "How can I access search interest data for economic indicators?",
      "What are the applications of Google Trends data in demand prediction?",
      "How does Google Trends data help in event detection?",
      "What insights can be derived from analyzing search trends?",
      "In what ways can nowcasting be performed using Google Trends data?",
      "What types of analyses can be conducted with Google Trends data?",
      "How can researchers utilize Google Trends for market research?"
    ],
    "domain_tags": [
      "retail",
      "healthcare",
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Google Trends Datastore is a rich repository of search interest data that serves as a valuable resource for researchers and analysts looking to understand consumer behavior and market dynamics. This dataset is structured in a tabular format, comprising rows that represent distinct time intervals and columns that capture various search terms and their corresponding interest levels. Each entry in the dataset reflects the volume of searches for specific keywords, allowing for a detailed analysis of trends over time. The primary variables include search terms, interest over time, and geographical breakdowns of search interest, which can be aggregated or disaggregated based on the research needs. The data is collected from Google\u2019s search engine, which aggregates user queries to provide insights into what people are searching for at any given moment. This methodology ensures a broad coverage of topics and trends, making it a versatile tool for economic analysis. However, researchers should be aware of certain limitations, such as potential biases in search behavior and the fact that not all search queries are captured. Common preprocessing steps include normalizing the data, handling missing values, and possibly aggregating search interest by week or month to facilitate analysis. This dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use Google Trends data to address research questions related to consumer interest, market trends, and economic indicators. By analyzing this data, one can uncover patterns that may indicate shifts in consumer preferences, identify emerging trends, and make informed predictions about future market behavior. The Google Trends Datastore thus stands as a critical tool for those engaged in economic research, market analysis, and related fields.",
    "benchmark_usage": [
      "Nowcasting economic indicators",
      "Demand prediction",
      "Event detection"
    ]
  },
  {
    "name": "FakeNewsNet",
    "description": "23K news articles labeled fake/real with social context. Includes PolitiFact and GossipCop sources",
    "category": "Content Moderation",
    "url": "https://github.com/KaiDMML/FakeNewsNet",
    "docs_url": null,
    "github_url": "https://github.com/KaiDMML/FakeNewsNet",
    "tags": [
      "fake news",
      "misinformation",
      "social media",
      "fact-checking"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "FakeNewsNet is a dataset containing 23,000 news articles that are labeled as either fake or real, providing valuable insights into the landscape of misinformation. This dataset includes articles from reputable sources such as PolitiFact and GossipCop, making it a useful resource for analyzing the spread of fake news and its social context.",
    "use_cases": [
      "Analyzing the prevalence of misinformation in news articles.",
      "Studying the social context surrounding fake news dissemination.",
      "Developing machine learning models for fake news detection.",
      "Conducting sentiment analysis on real vs. fake news articles."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FakeNewsNet dataset?",
      "How can I access the FakeNewsNet dataset?",
      "What types of articles are included in FakeNewsNet?",
      "What sources are used in the FakeNewsNet dataset?",
      "How is the FakeNewsNet dataset structured?",
      "What research can be conducted using the FakeNewsNet dataset?",
      "What are the labels in the FakeNewsNet dataset?",
      "How many articles are in the FakeNewsNet dataset?"
    ],
    "domain_tags": [
      "media",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/fakenewsnet.png",
    "embedding_text": "The FakeNewsNet dataset is a comprehensive collection of 23,000 news articles that have been meticulously labeled as either fake or real, making it a crucial resource for researchers and practitioners interested in the dynamics of misinformation. The dataset is structured in a tabular format, with each row representing a unique article and columns containing various attributes such as article text, publication date, source, and the assigned label (fake or real). The articles are sourced from reputable fact-checking organizations, specifically PolitiFact and GossipCop, known for their rigorous standards in evaluating the veracity of news content. This dual-source approach enhances the credibility of the dataset and provides a diverse range of articles spanning different topics and styles. The key variables within the dataset include the article's title, body text, label, and source, which collectively facilitate a variety of analyses aimed at understanding the characteristics and spread of misinformation. Researchers can leverage this dataset to address critical questions regarding the factors that contribute to the virality of fake news, the effectiveness of fact-checking interventions, and the role of social media in shaping public perception. Common preprocessing steps may involve text normalization, tokenization, and the removal of stop words to prepare the data for further analysis. The dataset supports various analytical techniques, including regression analysis, machine learning classification tasks, and descriptive statistics, enabling a multifaceted exploration of the data. Researchers typically utilize the FakeNewsNet dataset in studies focused on misinformation detection, the impact of fake news on public opinion, and the development of algorithms designed to identify and mitigate the spread of false information. Overall, the FakeNewsNet dataset serves as a vital tool in the ongoing battle against misinformation, providing a rich foundation for empirical research and practical applications in the field of content moderation."
  },
  {
    "name": "PJM Data Miner",
    "description": "Comprehensive market data from PJM, the largest U.S. regional transmission organization",
    "category": "Energy",
    "url": "https://dataminer2.pjm.com/",
    "docs_url": "https://www.pjm.com/markets-and-operations/data-dictionary",
    "github_url": null,
    "tags": [
      "PJM",
      "Eastern US",
      "prices",
      "wholesale",
      "capacity"
    ],
    "best_for": "Analyzing the largest U.S. electricity market covering 13 states",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "market-analysis",
      "pricing"
    ],
    "summary": "The PJM Data Miner provides comprehensive market data from PJM, the largest regional transmission organization in the U.S. This dataset allows users to analyze wholesale electricity prices and capacity metrics, facilitating insights into energy market trends and dynamics.",
    "use_cases": [
      "Analyzing price trends in the wholesale electricity market",
      "Evaluating capacity metrics for energy supply planning",
      "Comparing regional electricity prices",
      "Studying the impact of market changes on energy prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the PJM Data Miner?",
      "How can I analyze wholesale electricity prices?",
      "What data does PJM provide for market analysis?",
      "What are the capacity metrics available in PJM Data Miner?",
      "How does PJM influence energy prices in the Eastern US?",
      "What trends can be identified using PJM market data?",
      "What are the key variables in PJM Data Miner?",
      "How can I visualize PJM market data?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1999-present",
    "geographic_scope": "Eastern US",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/pjm.png",
    "embedding_text": "The PJM Data Miner is a robust dataset that encompasses a wide array of market data from the PJM Interconnection, which is recognized as the largest regional transmission organization in the United States. This dataset is structured in a tabular format, consisting of rows and columns that capture various market-related variables. Each row typically represents a specific time period, while the columns include key variables such as electricity prices, capacity metrics, and other relevant market indicators. The data is collected through PJM's operational systems, which aggregate information from numerous sources within the energy market, ensuring a comprehensive view of market dynamics. Coverage of the dataset is primarily focused on the Eastern United States, reflecting the geographical scope of PJM's operations. While the dataset does not explicitly mention temporal coverage, it is designed to provide insights into current and historical market conditions, making it a valuable resource for researchers and analysts alike. Key variables within the dataset include wholesale electricity prices, which measure the cost of electricity in the market, and capacity metrics, which indicate the available supply of electricity. These variables are crucial for understanding market trends and making informed decisions regarding energy supply and demand. Data quality is a significant consideration, as the dataset is derived from operational systems that may have inherent limitations, such as data entry errors or inconsistencies in reporting. Researchers utilizing this dataset may need to perform common preprocessing steps, including data cleaning and normalization, to ensure accurate analysis. The PJM Data Miner supports a variety of research questions, such as examining the impact of regulatory changes on electricity prices, analyzing the relationship between capacity and market prices, and identifying trends in energy consumption. The dataset is suitable for various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing researchers to derive meaningful insights from the data. Typically, researchers use the PJM Data Miner in studies focused on energy economics, market behavior, and policy analysis, leveraging the rich dataset to inform their findings and contribute to the broader understanding of the energy market."
  },
  {
    "name": "Common Crawl",
    "description": "250TB/month web crawl. 9.5 PB archive since 2008. Product listings, pricing, economic text at web scale",
    "category": "Social & Web",
    "url": "https://commoncrawl.org/",
    "docs_url": "https://commoncrawl.org/the-data/get-started/",
    "github_url": null,
    "tags": [
      "web crawl",
      "text",
      "pricing",
      "petabyte-scale"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "Common Crawl is a vast dataset consisting of a monthly web crawl that captures a wide array of web content, including product listings and pricing information. Researchers and data scientists can leverage this dataset to analyze trends in e-commerce, consumer behavior, and pricing strategies at a large scale.",
    "use_cases": [
      "Analyzing trends in online pricing over time.",
      "Studying consumer behavior through web-scraped product listings.",
      "Investigating the impact of web content on economic indicators.",
      "Building machine learning models to predict pricing strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Common Crawl dataset?",
      "How can I access the Common Crawl data?",
      "What types of data are included in the Common Crawl archive?",
      "What are the applications of the Common Crawl dataset in economic research?",
      "How does Common Crawl support analysis of web-scale pricing?",
      "What insights can be gained from analyzing product listings in Common Crawl?",
      "What is the size of the Common Crawl dataset?",
      "How frequently is the Common Crawl dataset updated?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/commoncrawl.png",
    "embedding_text": "Common Crawl is a comprehensive dataset that encompasses a monthly web crawl, capturing a staggering 250 terabytes of data each month, resulting in a cumulative archive of 9.5 petabytes since its inception in 2008. This dataset is particularly valuable for researchers and analysts interested in e-commerce, consumer behavior, and pricing dynamics at a web scale. The data structure of Common Crawl is primarily text-based, consisting of web pages that include various elements such as product listings, pricing information, and economic text. Each entry in the dataset can be thought of as a row, with columns representing different variables such as URLs, HTML content, and metadata about the pages. The collection methodology involves automated web crawling, where bots systematically browse the internet to capture and store web content. This approach enables the dataset to cover a wide range of topics and industries, although the specific geographic and temporal coverage is not explicitly detailed in the dataset description. Key variables within the dataset include product names, prices, and descriptions, which can be utilized to measure trends in consumer preferences and pricing strategies. However, users should be aware of potential data quality issues, such as incomplete or outdated information, as the dataset relies on the state of the web at the time of crawling. Common preprocessing steps may include cleaning HTML content, extracting relevant fields, and normalizing pricing data to facilitate analysis. Researchers can address a variety of research questions using Common Crawl, such as examining how online pricing varies across different product categories or how consumer behavior shifts in response to economic changes. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Typically, researchers utilize Common Crawl in studies focused on market trends, competitive analysis, and the impact of digital content on economic factors, making it a vital resource for anyone looking to explore the intersection of technology and economics."
  },
  {
    "name": "Redfin Housing Market Data",
    "description": "Downloadable housing market data: home prices, sales, inventory, listings by metro/city/zip. Updated weekly from MLS",
    "category": "Real Estate",
    "url": "https://www.redfin.com/news/data-center/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "housing prices",
      "large-scale",
      "real-world",
      "time series"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "real estate",
      "housing market",
      "data analysis"
    ],
    "summary": "The Redfin Housing Market Data provides downloadable datasets that include comprehensive information on home prices, sales, inventory, and listings categorized by metro, city, and zip code. This data is updated weekly from Multiple Listing Services (MLS), making it a valuable resource for analyzing trends in the housing market over time.",
    "use_cases": [
      "Analyzing trends in home prices over time.",
      "Comparing housing market conditions across different metro areas.",
      "Investigating the relationship between inventory levels and sales prices.",
      "Forecasting future housing market trends using time series analysis."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the current housing market trend in my city?",
      "How have home prices changed over the last year?",
      "What is the inventory level of homes for sale in my area?",
      "How do housing prices vary by zip code?",
      "What are the average sales prices in different metro areas?",
      "How does the number of listings correlate with home prices?",
      "What are the seasonal trends in the housing market?",
      "How can I analyze housing market data using Python?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/redfin-housing-market-data.png",
    "embedding_text": "The Redfin Housing Market Data is a rich dataset that offers a comprehensive view of the housing market, capturing various dimensions such as home prices, sales, inventory, and listings. This data is structured in a tabular format, typically comprising rows that represent individual transactions or listings, with columns detailing key variables such as the sale price, listing price, property type, square footage, number of bedrooms and bathrooms, and the date of sale or listing. The dataset is updated weekly, ensuring that users have access to the most current information available from Multiple Listing Services (MLS). This frequent updating is crucial for researchers and analysts who need to track rapid changes in the housing market, especially in dynamic urban areas. The collection methodology involves aggregating data from multiple MLS sources, which allows for a broad coverage of various geographic regions, including metropolitan areas, cities, and zip codes. While specific temporal and geographic coverage details are not explicitly mentioned, the dataset's structure suggests it can be used to analyze trends over time and across different locations. Key variables in the dataset measure critical aspects of the housing market, such as pricing trends, sales volume, and inventory levels. These variables are essential for understanding market dynamics and can be used to answer various research questions, such as how economic factors influence housing prices or what seasonal patterns exist in home sales. However, users should be aware of potential limitations in data quality, such as inconsistencies in reporting across different MLS systems or missing data points. Common preprocessing steps may include cleaning the data to handle missing values, normalizing price data to account for inflation, and transforming variables for analysis. Researchers typically utilize this dataset for a variety of analyses, including regression modeling to predict future prices, machine learning applications to classify properties, and descriptive statistics to summarize market conditions. The versatility of this dataset makes it a valuable tool for anyone interested in the real estate sector, from academic researchers to industry professionals, enabling them to derive insights that can inform decision-making and policy development in the housing market."
  },
  {
    "name": "Italian Grocers",
    "description": "Receipt-level sales data from Italian grocery stores",
    "category": "Grocery & Supermarkets",
    "url": "https://data.mendeley.com/datasets/s8dgbs3rng/1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Italy",
      "receipts"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Italian Grocers dataset consists of receipt-level sales data collected from various Italian grocery stores. This dataset can be utilized to analyze consumer purchasing patterns, pricing strategies, and overall sales performance within the grocery sector.",
    "use_cases": [
      "Analyzing consumer purchasing trends over time",
      "Evaluating the impact of pricing changes on sales",
      "Identifying popular products among consumers",
      "Assessing the effectiveness of promotional campaigns"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "What is the Italian Grocers dataset?",
      "How can I analyze grocery sales data from Italy?",
      "What insights can be gained from receipt-level sales data?",
      "What are the purchasing trends in Italian grocery stores?",
      "How do pricing strategies affect sales in grocery stores?",
      "Where can I find sales data for Italian grocery stores?",
      "What variables are included in the Italian Grocers dataset?",
      "How can I use receipt data for consumer behavior analysis?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Italy",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/mendeley.png",
    "embedding_text": "The Italian Grocers dataset is a comprehensive collection of receipt-level sales data sourced from various grocery stores across Italy. This dataset is structured in a tabular format, where each row represents a unique transaction or receipt, and the columns capture key variables such as item descriptions, quantities, prices, transaction dates, and store identifiers. The data is meticulously collected from point-of-sale systems in grocery stores, ensuring a high level of detail in the recorded transactions. Researchers and analysts can leverage this dataset to explore a wide range of research questions related to consumer behavior, pricing strategies, and sales performance in the grocery sector. Key variables in the dataset may include product categories, total transaction amounts, discounts applied, and customer demographics if available. However, users should be aware of potential limitations, such as missing data for certain transactions or variations in data collection practices across different stores. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing product names, and aggregating sales data to analyze trends over specific time periods. The dataset supports various types of analyses, including descriptive statistics to summarize sales patterns, regression analyses to understand the relationship between pricing and sales volume, and machine learning techniques to predict future purchasing behaviors. Researchers typically utilize this dataset to inform their studies on consumer purchasing patterns, evaluate the effectiveness of marketing strategies, and develop insights that can drive business decisions in the retail sector."
  },
  {
    "name": "Walmart (M5)",
    "description": "Hierarchical sales data for 3,049 products across 10 stores",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/competitions/m5-forecasting-accuracy",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "forecasting",
      "hierarchical",
      "Walmart",
      "M5 competition"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Walmart (M5) dataset contains hierarchical sales data for 3,049 products across 10 stores, allowing for detailed analysis of sales patterns and forecasting. Researchers can utilize this dataset to develop predictive models for sales forecasting and understand consumer purchasing behavior in the grocery and supermarket sector.",
    "use_cases": [
      "Sales forecasting for grocery products",
      "Analyzing consumer purchasing patterns",
      "Evaluating the impact of promotions on sales",
      "Comparative analysis of sales across different stores"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Walmart (M5) dataset?",
      "How can I access the Walmart (M5) sales data?",
      "What types of analyses can be performed with Walmart (M5) dataset?",
      "What are the key variables in the Walmart (M5) dataset?",
      "How does the Walmart (M5) dataset support forecasting?",
      "What is the structure of the Walmart (M5) dataset?",
      "What are common preprocessing steps for the Walmart (M5) dataset?",
      "What research questions can be addressed using the Walmart (M5) dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/walmart-m5.jpg",
    "embedding_text": "The Walmart (M5) dataset is a comprehensive collection of hierarchical sales data encompassing 3,049 products across 10 different stores. This dataset is structured in a tabular format, where each row represents sales data for a specific product in a specific store, allowing for a detailed examination of sales trends and patterns. Key variables within the dataset include product identifiers, store identifiers, sales figures, and potentially other attributes related to the products and stores, which are crucial for understanding sales dynamics. The collection methodology for this dataset typically involves aggregating sales data from various Walmart stores, ensuring that it reflects real-world purchasing behavior over a defined period. However, specific details regarding the data collection process and sources are not provided, which may limit the understanding of its origins and reliability. Researchers utilizing the Walmart (M5) dataset can explore a variety of research questions, such as how sales vary by product category, the effects of seasonal trends on grocery sales, and the influence of store location on consumer behavior. Common preprocessing steps may include handling missing values, normalizing sales figures, and transforming categorical variables into numerical formats suitable for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for data scientists and researchers in the field of retail analytics. By leveraging this dataset, researchers can develop predictive models for sales forecasting, analyze consumer purchasing patterns, and evaluate the effectiveness of marketing strategies, contributing valuable insights to the grocery and supermarket industry."
  },
  {
    "name": "Mexican Grocery",
    "description": "Data from a Mexican grocery store",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/martinezjosegpe/grocery-store",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Mexico",
      "retail"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Mexican Grocery dataset contains data collected from a Mexican grocery store, providing insights into consumer purchasing patterns and retail dynamics in the grocery sector. Researchers can utilize this dataset to analyze consumer behavior, pricing strategies, and inventory management within the context of the Mexican retail market.",
    "use_cases": [
      "Analyzing consumer purchasing trends in Mexican grocery stores",
      "Evaluating pricing strategies and their impact on sales",
      "Investigating inventory turnover rates",
      "Studying the effects of promotions on consumer behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available from a Mexican grocery store?",
      "How can I analyze grocery purchasing patterns in Mexico?",
      "What insights can be gained from retail data in Mexico?",
      "What are the pricing strategies in Mexican grocery stores?",
      "How does consumer behavior vary in grocery shopping in Mexico?",
      "What variables are included in the Mexican Grocery dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Mexico",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/mexican-grocery.png",
    "embedding_text": "The Mexican Grocery dataset provides a comprehensive view of consumer behavior and retail dynamics within the grocery sector in Mexico. This dataset is structured in a tabular format, typically comprising rows that represent individual transactions or customer interactions and columns that capture various attributes related to these transactions. Key variables may include transaction ID, product categories, prices, quantities purchased, and timestamps, among others. The collection methodology likely involves direct data capture from point-of-sale systems in the grocery store, ensuring a rich dataset that reflects real-world shopping behaviors. However, researchers should be aware of potential limitations in data quality, such as missing values or inaccuracies in transaction recording, which may affect analyses. Common preprocessing steps might include data cleaning, normalization of prices, and handling of missing data to prepare the dataset for analysis. Researchers can utilize this dataset to address a variety of research questions, such as examining the relationship between pricing strategies and sales volume, understanding seasonal trends in grocery purchases, or exploring demographic influences on buying behavior. The dataset supports various types of analyses, including regression analysis to model relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize key findings. Overall, the Mexican Grocery dataset serves as a valuable resource for understanding the intricacies of grocery retail in Mexico, offering insights that can inform business strategies and academic research alike."
  },
  {
    "name": "NATO Defence Expenditure",
    "description": "Standardized defense spending data for all NATO members enabling alliance burden-sharing analysis and 2% GDP target tracking",
    "category": "Defense Economics",
    "url": "https://www.nato.int/cps/en/natohq/topics_49198.htm",
    "docs_url": "https://www.nato.int/nato_static_fl2014/assets/pdf/2024/3/pdf/240314-def-exp-2023-en.pdf",
    "github_url": null,
    "tags": [
      "NATO",
      "alliance",
      "burden-sharing",
      "Europe"
    ],
    "best_for": "Analyzing NATO burden-sharing and 2% GDP spending targets",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NATO Defence Expenditure dataset provides standardized defense spending data for all NATO members, facilitating analysis of alliance burden-sharing and tracking the 2% GDP target. Researchers can utilize this dataset to assess defense spending trends and compare expenditures across member nations.",
    "use_cases": [
      "Analyzing trends in defense spending among NATO members over time.",
      "Comparing individual NATO member expenditures against the 2% GDP target.",
      "Assessing the impact of defense spending on alliance cohesion.",
      "Evaluating the effectiveness of burden-sharing within NATO."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NATO Defence Expenditure dataset?",
      "How can I analyze NATO defense spending data?",
      "What are the trends in NATO member defense expenditures?",
      "How does NATO burden-sharing work?",
      "What is the 2% GDP target for NATO members?",
      "Where can I find standardized defense spending data for NATO?",
      "What variables are included in NATO Defence Expenditure data?",
      "How can this dataset help in defense economics research?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1949-present",
    "geographic_scope": "Europe",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/nato.png",
    "embedding_text": "The NATO Defence Expenditure dataset is a comprehensive collection of standardized defense spending data for all NATO member countries. This dataset is structured in a tabular format, typically consisting of rows representing individual NATO member countries and columns detailing various aspects of their defense expenditures. Key variables may include total defense spending, percentage of GDP allocated to defense, and year-on-year changes in spending. The data is collected from official government reports and NATO publications, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, such as variations in how different countries report their defense expenditures and the impact of exchange rate fluctuations on comparative analyses. Common preprocessing steps may involve cleaning the data for missing values, normalizing expenditures to a common currency, and aggregating data across years for longitudinal studies. This dataset supports a variety of analyses, including regression analysis to explore relationships between defense spending and economic indicators, machine learning models to predict future spending trends, and descriptive statistics to summarize spending patterns. Researchers typically use this dataset to address important questions regarding the effectiveness of NATO's burden-sharing mechanisms, the implications of defense spending on national security, and the alignment of member countries with NATO's strategic objectives. By providing a clear picture of defense expenditures, the dataset enables informed discussions on policy-making and resource allocation within the alliance.",
    "benchmark_usage": [
      "Burden-sharing analysis",
      "GDP target tracking"
    ]
  },
  {
    "name": "Bandcamp Music Sales",
    "description": "Music sales data (digital/physical) from Bandcamp platform",
    "category": "Entertainment & Media",
    "url": "https://components.one/datasets/bandcamp-sales",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "music",
      "sales",
      "independent artists"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Bandcamp Music Sales dataset provides insights into music sales data from the Bandcamp platform, covering both digital and physical sales. Researchers and analysts can use this dataset to explore trends in independent music sales, consumer preferences, and the economic impact of digital platforms on the music industry.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Comparing sales performance of different genres",
      "Evaluating the impact of pricing strategies on sales",
      "Studying consumer behavior in music purchasing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in Bandcamp music sales?",
      "How do independent artists perform on Bandcamp?",
      "What factors influence music sales on Bandcamp?",
      "What is the average price of music sold on Bandcamp?",
      "How does Bandcamp compare to other music sales platforms?",
      "What demographics are most likely to purchase music on Bandcamp?",
      "What types of music are most popular on Bandcamp?",
      "How do physical and digital sales compare on Bandcamp?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/components.png",
    "embedding_text": "The Bandcamp Music Sales dataset is a comprehensive collection of sales data sourced from the Bandcamp platform, which is known for its support of independent artists. This dataset typically includes a variety of variables that capture essential aspects of music sales, such as the number of units sold, sales revenue, artist information, genre categorization, and sales channels (digital versus physical). The data is structured in a tabular format, where each row represents a unique sale transaction, and columns correspond to different attributes of the sale. Key variables may include artist name, album title, sale date, sale type (digital or physical), and price, among others. This structure allows for detailed analysis and insights into the dynamics of music sales on the platform. The collection methodology involves aggregating sales data directly from the Bandcamp platform, ensuring that the dataset reflects real-time sales activity and trends. However, it is important to note that the dataset may have limitations, such as potential biases in artist representation or variations in data completeness based on the sales channels used. Researchers utilizing this dataset can address a variety of research questions, such as identifying trends in music sales over time, understanding consumer preferences for different genres, and evaluating the effectiveness of various pricing strategies. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a valuable resource for both academic and industry research. Common preprocessing steps may include data cleaning to handle missing values, normalization of sales figures, and categorization of genres or artist types. Overall, the Bandcamp Music Sales dataset serves as a vital tool for exploring the intersection of music, economics, and consumer behavior in the digital age, providing insights that can inform both artists and industry stakeholders."
  },
  {
    "name": "Zenodo",
    "description": "CERN-operated research data repository with DOI citations, accepts all file types up to 50GB",
    "category": "Dataset Aggregators",
    "url": "https://zenodo.org",
    "docs_url": "https://help.zenodo.org",
    "github_url": null,
    "tags": [
      "DOI",
      "CERN",
      "open science",
      "preservation"
    ],
    "best_for": "Publishing and preserving research data with persistent DOI citations",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Zenodo is a research data repository operated by CERN that facilitates the sharing and preservation of research data across various disciplines. It accepts all file types up to 50GB and provides DOI citations for easy referencing, making it an essential resource for researchers looking to archive and disseminate their work.",
    "use_cases": [
      "Researchers can use Zenodo to archive their datasets and ensure they are preserved for future use.",
      "Academics can cite their datasets using DOIs provided by Zenodo to enhance the credibility of their research.",
      "Data scientists can explore various datasets available on Zenodo for machine learning and data analysis projects.",
      "Educators can utilize Zenodo to find open datasets for teaching purposes."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Zenodo and how does it function as a research data repository?",
      "How can I access datasets from Zenodo?",
      "What types of files can be uploaded to Zenodo?",
      "How does Zenodo support open science initiatives?",
      "What are the benefits of using Zenodo for data preservation?",
      "How can I cite datasets from Zenodo using DOI?",
      "What is the maximum file size allowed for uploads on Zenodo?",
      "What types of research data are available on Zenodo?"
    ],
    "domain_tags": [
      "open science",
      "research",
      "data preservation"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/zenodo.png",
    "embedding_text": "Zenodo is a comprehensive research data repository operated by CERN, designed to facilitate the sharing, preservation, and citation of research data across a multitude of disciplines. The platform is particularly notable for its support of open science initiatives, allowing researchers to upload a wide variety of file types, with a maximum size limit of 50GB per file. This flexibility makes Zenodo an invaluable resource for researchers who need to archive large datasets, software, or any other digital artifacts associated with their research projects. The repository employs a robust DOI (Digital Object Identifier) system, which provides a permanent link to datasets, ensuring that they can be easily cited in academic publications and referenced by other researchers. The data structure within Zenodo is designed to accommodate a diverse range of research outputs, including but not limited to datasets, publications, presentations, and software. Each entry in the repository typically includes metadata that describes the content, authorship, and access conditions, allowing users to quickly ascertain the relevance and applicability of the data to their own research needs. Zenodo's collection methodology is built on the principles of open access and data sharing, encouraging researchers to contribute their work and make it available to the broader scientific community. The platform aggregates data from various sources, including institutional repositories, individual researchers, and collaborative projects, thereby creating a rich tapestry of research outputs that span multiple disciplines. While Zenodo does not explicitly mention temporal or geographic coverage in its description, the diversity of datasets available suggests a wide-ranging applicability across different time periods and locations. Key variables within the datasets hosted on Zenodo can vary significantly depending on the nature of the research being conducted. For instance, datasets may include quantitative measurements, qualitative observations, or even multimedia files, each contributing to a comprehensive understanding of the research topic. However, researchers should be aware of potential limitations in data quality, as datasets may vary in terms of completeness, accuracy, and consistency. Common preprocessing steps that researchers might undertake when utilizing data from Zenodo include data cleaning, normalization, and transformation to ensure compatibility with analytical tools and methodologies. Zenodo supports a variety of analyses, including descriptive statistics, regression modeling, and machine learning applications, making it a versatile tool for researchers across different fields. Typical research questions that can be addressed using data from Zenodo range from exploratory analyses of trends and patterns to hypothesis-driven investigations that seek to establish causal relationships. Overall, Zenodo serves as a pivotal platform for researchers looking to enhance the visibility and impact of their work while contributing to the collective knowledge base of the scientific community."
  },
  {
    "name": "IMF Data",
    "description": "International macroeconomic forecasts, BOP, and financial statistics for 195 countries",
    "category": "Dataset Aggregators",
    "url": "https://data.imf.org",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "macro",
      "international",
      "forecasts",
      "BOP",
      "IFS"
    ],
    "best_for": "World Economic Outlook forecasts and International Financial Statistics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The IMF Data provides comprehensive international macroeconomic forecasts, balance of payments (BOP), and financial statistics for 195 countries. Researchers and analysts can utilize this dataset to explore economic trends, perform comparative analyses, and forecast future economic conditions.",
    "use_cases": [
      "Analyzing economic trends across different countries",
      "Forecasting future economic conditions based on historical data",
      "Comparative analysis of financial statistics among nations",
      "Studying the impact of macroeconomic factors on global economies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the international macroeconomic forecasts for 195 countries?",
      "How can I analyze balance of payments data from the IMF?",
      "What financial statistics are available for different countries?",
      "Where can I find IMF data for economic forecasting?",
      "What variables are included in the IMF dataset?",
      "How do I access financial statistics from the IMF?",
      "What trends can be identified in international economic data?",
      "How does BOP data vary across countries?"
    ],
    "domain_tags": [
      "finance",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The IMF Data is a rich dataset that encompasses international macroeconomic forecasts, balance of payments (BOP), and financial statistics for 195 countries, making it an invaluable resource for researchers and analysts in the fields of economics and finance. The data is structured in a tabular format, consisting of rows representing different countries and columns that capture various economic indicators and statistics. Key variables within the dataset include GDP, inflation rates, current account balances, and other financial metrics that provide insights into the economic health of nations. The collection methodology of the IMF Data involves rigorous data gathering from national statistical agencies, central banks, and other reliable sources, ensuring a high level of data quality. However, researchers should be aware of potential limitations, such as discrepancies in data reporting practices across countries and the availability of certain indicators over time. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and aggregating data to specific time periods or geographic regions. The dataset supports a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics, allowing researchers to address a wide range of research questions related to economic performance, policy impacts, and international financial stability. Researchers typically use this dataset to conduct empirical studies that assess the relationships between macroeconomic variables, evaluate the effectiveness of economic policies, and forecast future trends based on historical data patterns. Overall, the IMF Data serves as a foundational tool for understanding global economic dynamics and informing policy decisions.",
    "geographic_scope": "195 countries"
  },
  {
    "name": "Chicago Rideshare Data",
    "description": "Trip-level data for all Transportation Network Provider (Uber/Lyft) trips in Chicago since 2018. Includes ~57 million trips annually with origins, destinations, and fares.",
    "category": "Transportation Economics & Technology",
    "url": "https://data.cityofchicago.org/Transportation/Transportation-Network-Providers-Trips/m6dm-c72p",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "rideshare",
      "Chicago",
      "trip-data",
      "urban-mobility",
      "Uber",
      "Lyft"
    ],
    "best_for": "Ridesharing market analysis, platform competition, and urban transportation research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban-mobility",
      "data-analysis"
    ],
    "summary": "The Chicago Rideshare Data provides trip-level information for all Uber and Lyft rides in Chicago since 2018, encompassing approximately 57 million trips annually. This dataset allows researchers and analysts to explore urban mobility patterns, fare structures, and the overall impact of rideshare services on transportation economics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Chicago Rideshare Data?",
      "How many trips does the Chicago Rideshare Data include?",
      "What variables are available in the Chicago Rideshare Data?",
      "How can I analyze rideshare trends in Chicago?",
      "What insights can be gained from Uber and Lyft trip data?",
      "Where can I find trip-level data for rideshare services in Chicago?",
      "What are the origins and destinations in the Chicago Rideshare Data?",
      "How does the fare structure work in the Chicago Rideshare Data?"
    ],
    "use_cases": [
      "Analyzing the impact of rideshare services on public transportation usage.",
      "Studying fare variations based on time of day and location.",
      "Exploring trip patterns and demand hotspots in Chicago.",
      "Evaluating the economic implications of rideshare services on local businesses."
    ],
    "domain_tags": [
      "transportation",
      "urban-planning",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018-present",
    "geographic_scope": "Chicago",
    "size_category": "massive",
    "model_score": 0.0003,
    "image_url": "/images/logos/cityofchicago.png",
    "embedding_text": "The Chicago Rideshare Data is a comprehensive dataset that captures trip-level information for all rides provided by Transportation Network Providers (TNPs) such as Uber and Lyft in the city of Chicago. Since its inception in 2018, this dataset has grown to include approximately 57 million trips annually, making it a rich resource for researchers and analysts interested in urban mobility and transportation economics. The structure of the dataset is tabular, consisting of rows representing individual trips and columns that detail various attributes of each trip. Key variables typically include trip origin and destination coordinates, fare amounts, trip duration, and timestamps, among others. These variables allow for a detailed analysis of travel patterns, fare dynamics, and the overall performance of rideshare services in urban settings. The data is collected through the rideshare companies' operational systems, ensuring a high level of accuracy and granularity. However, researchers should be aware of potential limitations, such as data privacy concerns and the exclusion of non-TNP transportation methods, which may affect the comprehensiveness of analyses. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing fare amounts, and aggregating trips by time intervals for trend analysis. This dataset supports a variety of analytical approaches, including regression analysis, machine learning applications, and descriptive statistics. Researchers can leverage this data to address questions related to urban mobility, such as understanding how rideshare services influence traffic congestion, identifying peak demand times, and evaluating the economic impact on local businesses. Overall, the Chicago Rideshare Data serves as a vital tool for examining the evolving landscape of urban transportation and the role of technology in shaping travel behaviors."
  },
  {
    "name": "Hate Speech Data Catalogue",
    "description": "50+ hate speech datasets across languages compiled at hatespeechdata.com. Meta-resource for content moderation research",
    "category": "Content Moderation",
    "url": "https://hatespeechdata.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hate speech",
      "catalogue",
      "multilingual",
      "meta-dataset"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Hate Speech Data Catalogue is a comprehensive collection of over 50 datasets that focus on hate speech across various languages. This meta-resource serves as an essential tool for researchers in content moderation, enabling them to analyze and understand hate speech patterns and trends in different linguistic contexts.",
    "use_cases": [
      "Analyzing hate speech trends across different languages",
      "Developing content moderation algorithms",
      "Conducting comparative studies on hate speech in various cultures",
      "Training machine learning models for hate speech detection"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the Hate Speech Data Catalogue?",
      "How can I access multilingual hate speech datasets?",
      "What is the purpose of the Hate Speech Data Catalogue?",
      "Where can I find resources for content moderation research?",
      "What types of hate speech data are included in the catalogue?",
      "How can I use the Hate Speech Data Catalogue for my research?",
      "What languages are covered in the hate speech datasets?",
      "What are the key features of the Hate Speech Data Catalogue?"
    ],
    "domain_tags": [
      "content moderation"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Hate Speech Data Catalogue is a meticulously curated collection that encompasses over 50 datasets focused on the phenomenon of hate speech across multiple languages. This catalogue serves as a vital meta-resource for researchers engaged in content moderation, providing a centralized repository of datasets that can be utilized for various analytical purposes. The data structure typically consists of rows representing individual instances of hate speech, with columns detailing various attributes such as the source of the data, the language of the text, and the type of hate speech being analyzed. Key variables may include the text of the speech itself, labels indicating the type of hate speech (e.g., racist, sexist), and metadata such as the date of collection and the platform from which the data was sourced. The collection methodology for these datasets varies, often involving the aggregation of data from social media platforms, online forums, and other digital communication channels where hate speech is prevalent. Researchers can expect a diverse range of data sources, reflecting the multifaceted nature of hate speech across different cultural and linguistic contexts. However, it is important to note that the quality of the data may vary, with potential limitations including biases in data collection, the subjective nature of hate speech categorization, and the challenges of accurately capturing context in textual data. Common preprocessing steps may involve cleaning the text data, normalizing language variations, and encoding categorical variables for analysis. The Hate Speech Data Catalogue can address a variety of research questions, such as identifying trends in hate speech over time, examining the impact of social media policies on hate speech prevalence, and exploring the relationship between hate speech and societal factors. The datasets support a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making them versatile tools for researchers in the field. Typically, researchers utilize the Hate Speech Data Catalogue to inform their studies on content moderation, develop algorithms for hate speech detection, and conduct comparative analyses across different languages and cultures, ultimately contributing to the broader understanding of hate speech and its implications in society."
  },
  {
    "name": "YouTube-8M",
    "description": "8M videos with video-level features for large-scale video understanding. Google Research benchmark for video classification",
    "category": "Entertainment & Media",
    "url": "https://research.google.com/youtube8m/",
    "docs_url": "https://research.google.com/youtube8m/download.html",
    "github_url": null,
    "tags": [
      "YouTube",
      "video",
      "classification",
      "benchmark",
      "Google Research"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "YouTube-8M is a dataset consisting of 8 million videos that provide video-level features aimed at facilitating large-scale video understanding. This dataset serves as a benchmark for video classification tasks, enabling researchers and practitioners to evaluate and develop new algorithms for video analysis.",
    "use_cases": [
      "Developing video classification algorithms",
      "Evaluating machine learning models for video analysis",
      "Benchmarking video understanding techniques"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the YouTube-8M dataset?",
      "How can I use YouTube-8M for video classification?",
      "What features are included in the YouTube-8M dataset?",
      "What are the benchmarks available for video classification?",
      "How does YouTube-8M support large-scale video understanding?",
      "What research has been conducted using YouTube-8M?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "benchmark_usage": [
      "Video classification benchmark"
    ],
    "model_score": 0.0003,
    "image_url": "/images/logos/google.png",
    "embedding_text": "The YouTube-8M dataset is a comprehensive collection of 8 million YouTube videos, designed to advance the field of video understanding through large-scale video classification. Each video in the dataset is associated with a variety of video-level features, including but not limited to visual and audio characteristics, which are essential for training machine learning models. The dataset is structured in a way that facilitates easy access to these features, allowing researchers to focus on developing and testing their algorithms without the need to gather and preprocess raw video data themselves. The schema of the dataset typically includes columns for video IDs, labels, and various extracted features that represent the content of the videos. These features can encompass aspects such as frame-level visual data, audio signals, and metadata related to the videos, providing a rich source of information for analysis.\n\nThe collection methodology for YouTube-8M involves automated processes that extract features from the videos available on the YouTube platform. This includes leveraging existing algorithms for feature extraction that can analyze both the visual and auditory components of the videos. The dataset is particularly valuable for researchers interested in the intersection of computer vision and natural language processing, as it allows for the exploration of how visual content can be classified and understood in relation to its audio counterpart.\n\nIn terms of coverage, the dataset is vast, encompassing a wide range of video content from various domains within entertainment and media. However, specific temporal or geographic coverage details are not explicitly mentioned, which means that the dataset can be considered to have a broad applicability across different contexts. The demographic coverage is also diverse, reflecting the global nature of YouTube's audience, although specific demographic variables are not included in the dataset.\n\nKey variables within the YouTube-8M dataset include the video ID, which serves as a unique identifier for each video, and the associated labels that categorize the videos into various classes. These labels are crucial for training classification models, as they provide the ground truth against which model predictions can be evaluated. The dataset's quality is generally high, given its automated feature extraction process; however, known limitations include potential biases in the video content and the inherent challenges of accurately labeling such a large volume of videos.\n\nCommon preprocessing steps for utilizing the YouTube-8M dataset may include normalizing the feature values, handling missing data, and splitting the dataset into training, validation, and test sets for model evaluation. Researchers typically employ this dataset to address a variety of research questions related to video classification, such as how different features contribute to classification accuracy or how to improve model performance through advanced machine learning techniques.\n\nThe types of analyses supported by the YouTube-8M dataset include regression analyses, machine learning model training and evaluation, and descriptive statistics to summarize the characteristics of the video content. Researchers often use this dataset in studies aimed at benchmarking new algorithms against established methods, thereby contributing to the ongoing development of effective video understanding technologies. Overall, YouTube-8M serves as a vital resource for advancing research in video classification and understanding, providing a platform for innovation in the field."
  },
  {
    "name": "OpenStreetMap Road Network",
    "description": "Open-source global road network data including road types, speeds, and connectivity. Downloadable via Overpass API or pre-processed extracts.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.openstreetmap.org/",
    "docs_url": "https://wiki.openstreetmap.org/wiki/Main_Page",
    "github_url": "https://github.com/openstreetmap",
    "tags": [
      "road-network",
      "open-data",
      "global",
      "GIS",
      "routing"
    ],
    "best_for": "Road network analysis, routing, accessibility studies, and urban form research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OpenStreetMap Road Network dataset provides comprehensive open-source data on global road networks, detailing various road types, speeds, and connectivity. This dataset can be utilized for routing applications, urban planning, and transportation analysis.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the OpenStreetMap Road Network dataset?",
      "How can I access OpenStreetMap road data?",
      "What types of roads are included in the OpenStreetMap dataset?",
      "What are the connectivity features of the OpenStreetMap Road Network?",
      "How can I use OpenStreetMap data for routing?",
      "What are the benefits of using open-source road network data?",
      "Where can I find global road network data?",
      "What is the Overpass API for accessing OpenStreetMap data?"
    ],
    "use_cases": [
      "Analyzing traffic patterns in urban areas",
      "Developing routing algorithms for navigation systems",
      "Studying the impact of road connectivity on economic activities",
      "Evaluating infrastructure needs in transportation planning"
    ],
    "domain_tags": [
      "transportation",
      "urban planning"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/openstreetmap-road-network.png",
    "embedding_text": "The OpenStreetMap Road Network dataset is a rich source of open-source global road network data, encompassing various attributes such as road types, speeds, and connectivity. This dataset is structured in a tabular format, consisting of rows that represent individual road segments and columns that detail various characteristics of these segments, including road type (e.g., highway, residential), speed limits, and connectivity to other roads. The collection methodology for this dataset is based on crowdsourced contributions from users around the world, making it a dynamic and continuously updated resource. The data can be accessed through the Overpass API, which allows users to query specific geographic areas and obtain tailored extracts of the road network data. The coverage of this dataset is global, providing insights into road networks across different countries and regions, although it does not specify temporal coverage or demographic details. Key variables within the dataset include road type, which categorizes the roads based on their function and usage; speed limits, which indicate the maximum allowable speed for vehicles on each road segment; and connectivity measures, which assess how well different roads are linked to each other. These variables are essential for various analyses, including transportation modeling, urban planning, and infrastructure development. However, users should be aware of potential limitations in data quality, such as inconsistencies in road classifications or incomplete data in less populated areas. Common preprocessing steps may include cleaning the data to remove duplicates, standardizing road type classifications, and integrating the dataset with other geographic information systems (GIS) for enhanced analysis. Researchers and practitioners typically use this dataset to address a range of questions related to transportation efficiency, urban development, and economic impacts of road infrastructure. Analyses supported by this dataset include regression modeling to predict traffic flows, machine learning applications for route optimization, and descriptive statistics to summarize road network characteristics. Overall, the OpenStreetMap Road Network dataset serves as a foundational resource for anyone interested in transportation economics and technology, providing the necessary data to drive informed decision-making and innovative research in the field.",
    "geographic_scope": "global"
  },
  {
    "name": "Drone Delivery",
    "description": "Drone delivery logistics and operations dataset",
    "category": "Logistics & Supply Chain",
    "url": "https://tianchi.aliyun.com/dataset/89726",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "drones",
      "delivery",
      "autonomous"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Drone Delivery dataset provides insights into the logistics and operations involved in drone-based delivery systems. Researchers and practitioners can analyze the data to optimize delivery routes, assess operational efficiency, and explore the impact of autonomous delivery on supply chain dynamics.",
    "use_cases": [
      "Optimizing delivery routes for drones",
      "Assessing operational efficiency of drone logistics",
      "Analyzing consumer behavior related to drone deliveries"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Drone Delivery dataset?",
      "How can I analyze drone logistics data?",
      "What insights can be gained from drone delivery operations?",
      "What are the key variables in the drone delivery dataset?",
      "How does drone delivery impact supply chain efficiency?",
      "What analyses can be performed on drone delivery data?",
      "What are the challenges in drone delivery logistics?",
      "How can I visualize drone delivery operations?"
    ],
    "domain_tags": [
      "logistics",
      "supply chain"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0003,
    "embedding_text": "The Drone Delivery dataset is a comprehensive collection of data related to the logistics and operations of drone-based delivery systems. This dataset is structured in a tabular format, consisting of rows and columns that capture various aspects of drone delivery operations. Each row represents a unique delivery instance, while the columns include variables such as delivery time, distance traveled, payload weight, and operational costs. These variables are crucial for understanding the efficiency and effectiveness of drone delivery systems. The data collection methodology typically involves gathering information from real-world drone delivery operations, including data from GPS tracking systems, delivery management software, and operational logs maintained by logistics companies. This dataset may also incorporate data from surveys or interviews with stakeholders involved in drone delivery, providing a holistic view of the operational landscape. While the dataset does not specify temporal or geographic coverage, it is assumed to encompass a range of delivery scenarios across various locations, reflecting the growing trend of drone usage in logistics. Key variables in the dataset measure critical aspects of drone delivery, such as the time taken for deliveries, the distance covered, and the weight of the packages delivered. These variables enable researchers to analyze performance metrics and identify areas for improvement in drone logistics. However, like many datasets, the Drone Delivery dataset may have limitations, including potential biases in data collection, variations in operational practices across different regions, and the evolving nature of drone technology. Common preprocessing steps for this dataset may include cleaning the data to remove outliers, normalizing variables for comparative analysis, and transforming categorical variables into numerical formats for machine learning applications. Researchers can leverage this dataset to address various research questions, such as how drone delivery affects supply chain efficiency, what factors influence delivery times, and how consumer behavior varies with the adoption of drone technology. The types of analyses supported by this dataset include regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize operational performance. Researchers typically use this dataset in studies focused on optimizing logistics operations, evaluating the impact of autonomous delivery systems, and exploring consumer acceptance of drone delivery services. Overall, the Drone Delivery dataset serves as a valuable resource for those interested in the intersection of technology and logistics, providing insights that can drive innovation in the field."
  },
  {
    "name": "EPA CEMS (Continuous Emissions Monitoring)",
    "description": "Hourly emissions and generation data from U.S. power plants since 1995",
    "category": "Energy",
    "url": "https://campd.epa.gov/",
    "docs_url": "https://www.epa.gov/power-sector/about-continuous-emissions-monitoring-system-cems",
    "github_url": null,
    "tags": [
      "emissions",
      "hourly",
      "CO2",
      "SO2",
      "NOx"
    ],
    "best_for": "Analyzing power plant emissions patterns and environmental impacts",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EPA CEMS dataset provides hourly emissions and generation data from U.S. power plants, starting from 1995. Researchers and analysts can utilize this dataset to study trends in emissions over time, assess the impact of regulatory changes, and evaluate the performance of power plants in terms of emissions and energy generation.",
    "use_cases": [
      "Analyzing trends in CO2 emissions from power plants over time.",
      "Evaluating the effectiveness of environmental regulations on emissions.",
      "Comparing emissions across different types of power generation.",
      "Assessing the relationship between energy generation and emissions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the EPA CEMS dataset?",
      "How can I access hourly emissions data from U.S. power plants?",
      "What types of emissions are measured in the EPA CEMS dataset?",
      "What years does the EPA CEMS dataset cover?",
      "How is the data in the EPA CEMS dataset structured?",
      "What can I analyze using the EPA CEMS dataset?",
      "Where can I find generation data for U.S. power plants?",
      "What are the limitations of the EPA CEMS dataset?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "1995-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/epa.png",
    "embedding_text": "The EPA CEMS (Continuous Emissions Monitoring System) dataset is a comprehensive collection of hourly emissions and generation data from U.S. power plants, providing critical insights into the environmental impact of energy production. This dataset has been collected since 1995, making it a valuable resource for researchers, policymakers, and environmental analysts interested in understanding the dynamics of emissions in the energy sector. The data structure typically includes rows representing individual hourly records for each power plant, with columns detailing key variables such as CO2, SO2, and NOx emissions, as well as energy generation metrics. Each entry in the dataset captures the emissions levels and generation output for specific power plants, allowing for detailed temporal analysis of emissions trends. The collection methodology involves continuous monitoring of emissions through sophisticated instrumentation installed at power plants, ensuring high data quality and reliability. However, users should be aware of potential limitations, such as gaps in data reporting or variations in monitoring practices across different states and plants. Common preprocessing steps may include handling missing values, normalizing emissions data, and aggregating data to different time intervals for analysis. Researchers can leverage this dataset to address a variety of research questions, including the impact of regulatory changes on emissions, the effectiveness of different energy sources in minimizing emissions, and the correlation between energy generation and environmental outcomes. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for understanding the intersection of energy production and environmental health. Overall, the EPA CEMS dataset serves as a foundational resource for studies aimed at evaluating the sustainability of energy practices and informing future energy policy decisions."
  },
  {
    "name": "Brazil Medical",
    "description": "Medicine sales data in Brazil",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/tgomesjuliana/brazil-medicine-sales",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "pharmaceuticals",
      "Brazil",
      "healthcare"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "pharmaceuticals",
      "consumer-behavior"
    ],
    "summary": "The Brazil Medical dataset provides insights into medicine sales across Brazil, capturing various aspects of the pharmaceutical market. Researchers and analysts can utilize this data to explore trends in healthcare consumption, pricing strategies, and the overall dynamics of the grocery and supermarket sector related to medical products.",
    "use_cases": [
      "Analyzing trends in medicine sales over time",
      "Evaluating the impact of pricing on consumer purchasing behavior",
      "Investigating the distribution of pharmaceutical products across different regions in Brazil"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Brazil medicine sales data",
      "pharmaceutical sales trends in Brazil",
      "healthcare consumer behavior Brazil",
      "grocery sales data Brazil",
      "analysis of medicine pricing in Brazil",
      "Brazil healthcare market dataset"
    ],
    "domain_tags": [
      "retail",
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Brazil",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/datasets/brazil-medical.jpg",
    "embedding_text": "The Brazil Medical dataset is a comprehensive collection of medicine sales data specifically focused on the Brazilian market. This dataset is structured in a tabular format, consisting of rows that represent individual sales transactions and columns that capture various attributes related to each sale. Key variables may include product identifiers, sales volume, pricing information, transaction dates, and possibly demographic data of consumers, although the exact schema would depend on the specific implementation of the dataset. The collection methodology for this dataset likely involves aggregating sales data from various grocery and supermarket chains across Brazil, possibly supplemented by industry reports or surveys that provide insights into consumer behavior and market trends. Coverage of the dataset is geographically limited to Brazil, making it particularly relevant for researchers and analysts interested in the Brazilian healthcare and retail sectors. The dataset may also capture temporal aspects, although specific time frames are not mentioned in the provided description. Key variables in the dataset measure aspects such as sales volume, pricing, and potentially consumer demographics, allowing for a multifaceted analysis of the pharmaceutical market. However, like many datasets, it may have limitations regarding data quality, such as missing values or inconsistencies in reporting across different retailers. Common preprocessing steps that researchers might undertake include data cleaning to address missing or erroneous entries, normalization of pricing data, and transformation of categorical variables into a suitable format for analysis. The dataset can support a variety of research questions, such as examining the correlation between pricing strategies and sales volume, understanding seasonal trends in medicine consumption, or analyzing the impact of economic factors on healthcare purchasing behavior. Types of analyses that can be performed using this dataset include regression analysis to model relationships between variables, machine learning techniques for predictive analytics, and descriptive statistics to summarize key trends and patterns within the data. Researchers typically utilize this dataset in studies focused on healthcare economics, consumer behavior analysis, and market research, making it a valuable resource for understanding the dynamics of the pharmaceutical industry in Brazil."
  },
  {
    "name": "SIPRI Military Expenditure Database",
    "description": "Comprehensive annual military spending data covering all countries since 1949 in local currency, constant/current USD, and GDP shares",
    "category": "Defense Economics",
    "url": "https://www.sipri.org/databases/milex",
    "docs_url": "https://www.sipri.org/databases/milex/sources-and-methods",
    "github_url": null,
    "tags": [
      "military spending",
      "defense budgets",
      "international",
      "SIPRI"
    ],
    "best_for": "Cross-country defense spending analysis and burden-sharing studies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SIPRI Military Expenditure Database provides comprehensive annual data on military spending across all countries since 1949, allowing researchers and analysts to explore trends in defense budgets in various currencies, including local currency, constant/current USD, and GDP shares. This dataset can be utilized for economic analysis, policy evaluation, and comparative studies in defense economics.",
    "use_cases": [
      "Analyzing trends in military spending over time across different countries.",
      "Comparing military expenditure as a share of GDP among nations.",
      "Evaluating the impact of military budgets on national economies.",
      "Investigating correlations between military spending and other economic indicators."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the SIPRI Military Expenditure Database?",
      "How can I access military spending data for all countries since 1949?",
      "What are the trends in military spending over the decades?",
      "How does military expenditure relate to GDP shares?",
      "What currencies are used in the SIPRI Military Expenditure Database?",
      "What are the key variables in the SIPRI Military Expenditure Database?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1949-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0003,
    "image_url": "/images/logos/sipri.png",
    "embedding_text": "The SIPRI Military Expenditure Database is a vital resource for researchers and policymakers interested in defense economics. It offers a structured dataset that includes annual military spending data for all countries, dating back to 1949. The data is organized in a tabular format, with rows representing individual countries and years, while columns include variables such as military expenditure in local currency, military expenditure in constant and current USD, and military spending as a share of GDP. This schema allows for straightforward analysis and comparison across different nations and time periods. The data is collected through a combination of national government reports, international organizations, and SIPRI's own research, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as discrepancies in reporting standards between countries and the challenges of converting currencies over time. Common preprocessing steps may include normalizing data for inflation, converting currencies to a common standard, and handling missing values. Researchers can use this dataset to address a variety of questions, such as examining the relationship between military spending and economic growth, assessing the impact of defense budgets on international relations, or exploring trends in military expenditure in response to global conflicts. The dataset supports various types of analyses, including regression analysis, machine learning techniques, and descriptive statistics, making it a versatile tool for understanding the dynamics of military spending in a global context. Overall, the SIPRI Military Expenditure Database is an essential dataset for anyone studying the intersection of defense and economics, providing a wealth of information for informed decision-making and policy development."
  },
  {
    "name": "PUDL (Public Utility Data Liberation)",
    "description": "Cleaned and integrated dataset combining EIA, FERC, and EPA energy data into a unified database",
    "category": "Energy",
    "url": "https://catalyst.coop/pudl/",
    "docs_url": "https://catalystcoop-pudl.readthedocs.io/",
    "github_url": "https://github.com/catalyst-cooperative/pudl",
    "tags": [
      "integrated",
      "cleaned",
      "EIA",
      "FERC",
      "EPA"
    ],
    "best_for": "Research requiring integrated energy data without extensive cleaning",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "data-integration",
      "environmental-data"
    ],
    "summary": "The PUDL dataset is a cleaned and integrated collection of energy data sourced from the EIA, FERC, and EPA, providing a unified database for analysis. Researchers can utilize this dataset to explore various aspects of energy production, consumption, and regulation in the United States.",
    "use_cases": [
      "Analyzing trends in energy production and consumption over time.",
      "Evaluating the impact of regulatory changes on energy markets.",
      "Conducting comparative studies of emissions data across different energy sources.",
      "Performing machine learning analyses to predict energy demand."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the PUDL dataset?",
      "How can I access the PUDL energy data?",
      "What types of analyses can be performed with PUDL?",
      "What data sources are included in the PUDL dataset?",
      "How is the PUDL dataset structured?",
      "What are the key variables in the PUDL dataset?",
      "What are the limitations of the PUDL dataset?",
      "How can PUDL be used for energy research?"
    ],
    "domain_tags": [
      "energy",
      "environment",
      "data-science"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1994-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/pudl-public-utility-data-liberation.png",
    "embedding_text": "The PUDL (Public Utility Data Liberation) dataset is a comprehensive and meticulously cleaned and integrated collection of energy data that combines information from the U.S. Energy Information Administration (EIA), the Federal Energy Regulatory Commission (FERC), and the Environmental Protection Agency (EPA). This dataset serves as a unified database, allowing researchers and analysts to access a wide range of energy-related information in a structured format. The data is organized in a tabular structure, consisting of rows and columns that represent various observations and variables related to energy production, consumption, and regulatory compliance. Key variables within the dataset include metrics such as energy generation by source, emissions data, and utility performance indicators, which provide valuable insights into the energy landscape in the United States. The collection methodology involves aggregating data from multiple authoritative sources, ensuring that the information is both reliable and relevant for research purposes. While the dataset is designed to be comprehensive, users should be aware of potential limitations, such as data gaps or inconsistencies that may arise from the integration of disparate data sources. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers can leverage the PUDL dataset to address a variety of research questions, such as examining the relationships between energy production and environmental impact, evaluating the effectiveness of energy policies, and exploring trends in energy consumption patterns. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and industry research. By utilizing the PUDL dataset, researchers typically aim to contribute to the understanding of energy systems, inform policy decisions, and drive innovations in energy technology and sustainability."
  },
  {
    "name": "EIA-923 Power Plant Operations",
    "description": "Monthly power plant fuel consumption, generation, and emissions data for all U.S. generators",
    "category": "Energy",
    "url": "https://www.eia.gov/electricity/data/eia923/",
    "docs_url": "https://www.eia.gov/electricity/data/eia923/",
    "github_url": null,
    "tags": [
      "power plants",
      "generation",
      "fuel",
      "emissions",
      "monthly"
    ],
    "best_for": "Analyzing power plant operations, fuel mix trends, and emissions patterns",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EIA-923 Power Plant Operations dataset provides comprehensive monthly data on fuel consumption, generation, and emissions for all power plants across the United States. Researchers and analysts can utilize this dataset to study trends in energy production, assess environmental impacts, and evaluate the efficiency of power generation methods.",
    "use_cases": [
      "Analyzing trends in fuel consumption across different power plants.",
      "Evaluating the environmental impact of power generation methods.",
      "Comparing emissions data between various types of power plants.",
      "Assessing the efficiency of energy production over time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the EIA-923 Power Plant Operations dataset?",
      "How can I access monthly power plant fuel consumption data?",
      "What types of emissions data are included in the EIA-923 dataset?",
      "What are the key variables in the EIA-923 dataset?",
      "How does power generation vary across different U.S. states?",
      "What trends can be observed in monthly power plant operations?",
      "How can I analyze fuel consumption patterns in U.S. power plants?",
      "What research questions can be addressed using the EIA-923 dataset?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2001-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/eia.png",
    "embedding_text": "The EIA-923 Power Plant Operations dataset is a crucial resource for understanding the dynamics of energy production in the United States. It encompasses a wide array of data points related to power plant operations, specifically focusing on monthly fuel consumption, generation output, and emissions metrics. The dataset is structured in a tabular format, consisting of rows that represent individual power plants and columns that capture various operational variables. Key variables include the type of fuel used, the amount of electricity generated, and the emissions produced, which are essential for evaluating the environmental impact of different energy sources. Researchers can leverage this dataset to conduct a variety of analyses, including regression analysis to identify trends over time, machine learning applications for predictive modeling, and descriptive statistics to summarize the operational characteristics of power plants. The dataset is collected through rigorous methodologies employed by the Energy Information Administration (EIA), ensuring a high level of data quality. However, users should be aware of potential limitations, such as reporting inconsistencies or gaps in data for certain plants. Common preprocessing steps may include data cleaning to handle missing values, normalization of emissions data, and aggregation of monthly figures for comparative studies. This dataset supports a range of research questions, such as examining the relationship between fuel types and emissions, analyzing the impact of regulatory changes on power generation, and exploring the geographical distribution of power plants and their operational efficiencies. Overall, the EIA-923 dataset serves as a foundational tool for researchers, policymakers, and industry analysts aiming to understand and improve the energy landscape in the United States."
  },
  {
    "name": "SIPRI Arms Transfers Database",
    "description": "Most comprehensive public source on international transfers of major conventional weapons since 1950",
    "category": "Defense Economics",
    "url": "https://www.sipri.org/databases/armstransfers",
    "docs_url": "https://www.sipri.org/databases/armstransfers/sources-and-methods",
    "github_url": null,
    "tags": [
      "arms trade",
      "weapons transfers",
      "international",
      "SIPRI"
    ],
    "best_for": "Analyzing global arms trade patterns and supplier-recipient relationships",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SIPRI Arms Transfers Database is the most comprehensive public source on international transfers of major conventional weapons since 1950. Researchers and analysts can use this dataset to examine trends in arms trade, analyze the impact of military expenditures on global security, and study the relationships between countries involved in arms transfers.",
    "use_cases": [
      "Analyzing trends in global arms transfers over time",
      "Studying the relationship between arms transfers and conflict",
      "Evaluating the impact of arms trade on national security policies",
      "Comparing arms transfer patterns between different countries"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the SIPRI Arms Transfers Database?",
      "How can I access the SIPRI Arms Transfers Database?",
      "What data is included in the SIPRI Arms Transfers Database?",
      "What are the trends in international arms transfers since 1950?",
      "How do arms transfers affect international relations?",
      "What methodologies are used in the SIPRI Arms Transfers Database?",
      "What are the key variables in the SIPRI Arms Transfers Database?",
      "How can I analyze arms trade data from SIPRI?"
    ],
    "domain_tags": [
      "defense",
      "international relations"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1950-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/sipri.png",
    "embedding_text": "The SIPRI Arms Transfers Database serves as a vital resource for understanding the dynamics of international arms trade, providing a detailed account of major conventional weapons transfers since 1950. This dataset is structured in a tabular format, consisting of rows that represent individual arms transfer events, and columns that include key variables such as the exporting and importing countries, the type of weaponry involved, the quantity of arms transferred, and the year of the transfer. The collection methodology employed by the Stockholm International Peace Research Institute (SIPRI) involves extensive research and verification processes, utilizing a variety of data sources including government reports, international organizations, and media coverage to ensure the accuracy and reliability of the information presented. While the database offers comprehensive coverage of arms transfers, it is important to note that it primarily focuses on major conventional weapons, which may limit its applicability to studies involving smaller arms or non-conventional weaponry. Researchers often engage with this dataset to address a range of research questions, such as examining the geopolitical implications of arms transfers, assessing the impact of military aid on conflict outcomes, and exploring the economic dimensions of the arms trade. Common preprocessing steps may include data cleaning to handle inconsistencies, normalization of country names, and temporal alignment of transfer events. The dataset supports various types of analyses, including descriptive statistics to summarize trends, regression analyses to explore relationships between variables, and machine learning techniques for predictive modeling. Overall, the SIPRI Arms Transfers Database is an essential tool for scholars and practitioners in the fields of defense economics and international relations, facilitating a deeper understanding of the complex interplay between arms trade and global security dynamics."
  },
  {
    "name": "Facebook URL Shares",
    "description": "38M URLs with 10T exposure numbers, fact-checking flags, interaction types (2017-2019). Social Science One initiative",
    "category": "Social & Web",
    "url": "https://socialscience.one/our-facebook-partnership",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Facebook",
      "URL sharing",
      "misinformation",
      "fact-checking",
      "social media"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "social-media",
      "misinformation",
      "data-analysis"
    ],
    "summary": "The Facebook URL Shares dataset contains 38 million URLs along with their exposure numbers and fact-checking flags, spanning the years 2017 to 2019. This dataset is valuable for analyzing social media interactions and understanding the spread of misinformation.",
    "use_cases": [
      "Analyzing the impact of fact-checking on URL sharing behavior.",
      "Studying the spread of misinformation across social media platforms.",
      "Investigating interaction types associated with shared URLs.",
      "Evaluating trends in social media exposure over time."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the exposure numbers for URLs shared on Facebook?",
      "How does fact-checking influence URL sharing on social media?",
      "What types of interactions are recorded in the Facebook URL Shares dataset?",
      "How can I analyze misinformation trends using Facebook URL data?",
      "What is the significance of the Social Science One initiative in social media research?",
      "How can I access the Facebook URL Shares dataset for my research?",
      "What years does the Facebook URL Shares dataset cover?",
      "What variables are included in the Facebook URL Shares dataset?"
    ],
    "domain_tags": [
      "social-media",
      "research",
      "data-science"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2017-2019",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/facebook-url-shares.jpg",
    "embedding_text": "The Facebook URL Shares dataset is a comprehensive collection of 38 million URLs that have been shared on the Facebook platform, accompanied by 10 trillion exposure numbers, fact-checking flags, and various interaction types recorded between the years 2017 and 2019. This dataset is part of the Social Science One initiative, which aims to facilitate research into social media's impact on society, particularly concerning misinformation and public discourse. The data is structured in a tabular format, with each row representing a unique URL and its associated metrics. Key columns in the dataset include the URL itself, exposure numbers indicating how many times the URL was viewed, fact-checking flags that denote whether the URL has been subject to verification, and interaction types that categorize how users engaged with the content. The collection methodology involves aggregating data from Facebook's API, ensuring a rich dataset that reflects real-world social media dynamics. Researchers can leverage this dataset to explore various research questions, such as the effectiveness of fact-checking in curbing misinformation, the correlation between URL exposure and user engagement, and the overall trends in social media interactions over the specified temporal coverage. The dataset's quality is generally high, but researchers should be aware of potential limitations, such as biases in user engagement and the representativeness of the URLs included. Common preprocessing steps may include cleaning the data to remove duplicates, handling missing values, and transforming categorical variables for analysis. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile resource for social scientists and data analysts alike. Researchers typically use this dataset to conduct studies that assess the impact of social media on public opinion, analyze the spread of misinformation, and evaluate the effectiveness of interventions aimed at improving information quality on platforms like Facebook."
  },
  {
    "name": "MSOM Pharma Manufacturing (2024)",
    "description": "Continuous pharmaceutical manufacturing data from MSD. Real production processes for operations management research",
    "category": "Logistics & Supply Chain",
    "url": "https://pubsonline.informs.org/page/msom/data-driven-challenge",
    "docs_url": "https://pubsonline.informs.org/doi/10.1287/msom.2024.0860",
    "github_url": null,
    "tags": [
      "manufacturing",
      "operations",
      "INFORMS",
      "2024",
      "real-world",
      "competition"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The MSOM Pharma Manufacturing dataset consists of continuous data from real pharmaceutical manufacturing processes at MSD, aimed at operations management research. Researchers can utilize this dataset to analyze production efficiency, process optimization, and operational strategies within the pharmaceutical industry.",
    "use_cases": [
      "Analyzing production efficiency in pharmaceutical manufacturing.",
      "Optimizing operational strategies based on real-world data.",
      "Conducting research on supply chain logistics specific to the pharmaceutical industry."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the MSOM Pharma Manufacturing dataset?",
      "How can I access continuous pharmaceutical manufacturing data from MSD?",
      "What insights can be gained from real production processes in the pharmaceutical sector?",
      "What are the applications of operations management research using this dataset?",
      "How does MSD's manufacturing data contribute to logistics and supply chain studies?",
      "What variables are included in the MSOM Pharma Manufacturing dataset?",
      "How can I analyze pharmaceutical manufacturing efficiency with this data?",
      "What research questions can be explored using the MSOM Pharma Manufacturing dataset?"
    ],
    "domain_tags": [
      "healthcare",
      "pharmaceutical"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2024",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The MSOM Pharma Manufacturing dataset is a comprehensive collection of continuous data derived from the real-world production processes of MSD, a prominent player in the pharmaceutical industry. This dataset is specifically designed for operations management research, providing insights into various aspects of pharmaceutical manufacturing. The data structure typically includes rows representing individual production runs or batches, with columns detailing key variables such as production time, yield, resource utilization, and operational parameters. Each variable is meticulously measured to ensure accurate representation of the manufacturing processes, allowing researchers to delve into the intricacies of production efficiency and effectiveness.\n\nThe collection methodology for this dataset involves direct observation and recording of production activities within MSD's manufacturing facilities. This approach ensures that the data reflects actual operational conditions, making it a valuable resource for researchers seeking to understand real-world manufacturing dynamics. The dataset is expected to cover a range of production scenarios, although specific temporal and geographic coverage details are not explicitly mentioned. As such, researchers should be aware of potential limitations regarding the generalizability of findings based on the dataset's context.\n\nKey variables within the dataset may include metrics such as production cycle times, batch sizes, defect rates, and resource allocation. These variables are crucial for measuring the efficiency and productivity of manufacturing processes, enabling researchers to identify patterns and correlations that can inform operational improvements. However, it is essential to acknowledge potential data quality issues, such as missing values or inconsistencies, which may arise from the complexities of real-world manufacturing environments. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis.\n\nResearchers can leverage the MSOM Pharma Manufacturing dataset to address various research questions, such as identifying factors that influence production efficiency, evaluating the impact of operational changes on output quality, and exploring the relationship between resource utilization and manufacturing costs. The dataset supports a range of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics, allowing for a multifaceted exploration of the data.\n\nIn studies, researchers typically use this dataset to benchmark performance against industry standards, assess the effectiveness of different manufacturing strategies, and develop predictive models that can enhance decision-making processes within pharmaceutical operations. By utilizing the insights gained from this dataset, researchers can contribute to the advancement of knowledge in the field of operations management, particularly within the context of the pharmaceutical supply chain."
  },
  {
    "name": "LaDe Last-Mile Delivery",
    "description": "10.6M+ packages, 619k trajectories with GPS data",
    "category": "Logistics & Supply Chain",
    "url": "https://arxiv.org/html/2306.10675v2",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "last-mile",
      "GPS",
      "trajectories",
      "large-scale"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The LaDe Last-Mile Delivery dataset contains over 10.6 million packages and 619,000 trajectories enriched with GPS data. This dataset allows researchers and analysts to explore patterns in last-mile delivery logistics, optimize routes, and understand the dynamics of package movement in urban environments.",
    "use_cases": [
      "Analyzing delivery route efficiency using GPS trajectories.",
      "Studying the impact of urban geography on last-mile delivery times.",
      "Optimizing logistics operations based on package movement patterns."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LaDe Last-Mile Delivery dataset?",
      "How can I analyze GPS data from last-mile delivery?",
      "What insights can be gained from 10.6M packages in logistics?",
      "What are the trajectories available in the LaDe dataset?",
      "How does last-mile delivery impact logistics efficiency?",
      "What variables are included in the LaDe Last-Mile Delivery dataset?",
      "How can GPS data improve last-mile delivery strategies?",
      "What analysis can be performed on large-scale delivery data?"
    ],
    "domain_tags": [
      "logistics",
      "supply chain"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/logos/arxiv.png",
    "embedding_text": "The LaDe Last-Mile Delivery dataset is a comprehensive collection of over 10.6 million packages and 619,000 trajectories, all enriched with GPS data, making it a valuable resource for researchers and practitioners in the logistics and supply chain domain. The dataset is structured in a tabular format, where each row represents a unique package delivery instance, and the columns include variables such as package ID, delivery time, GPS coordinates, and trajectory information. This structure allows for easy manipulation and analysis using data analysis tools and programming languages such as Python and R.\n\nThe data collection methodology involves tracking the movement of packages through GPS-enabled devices, capturing precise location data at various points during the delivery process. This approach not only provides a rich dataset for analysis but also ensures that the information is accurate and reflective of real-world delivery scenarios. The dataset's large scale enables researchers to conduct extensive analyses, uncovering trends and patterns that may not be visible in smaller datasets.\n\nKey variables in the dataset include the package ID, which uniquely identifies each package, timestamps that indicate when the package was picked up and delivered, and GPS coordinates that provide the exact location of the package at different stages of its journey. These variables are crucial for measuring delivery times, analyzing route efficiency, and understanding the factors that influence last-mile delivery performance.\n\nDespite its strengths, the dataset may have limitations, such as potential inaccuracies in GPS data due to signal loss or urban canyons, where tall buildings obstruct satellite signals. Researchers should be aware of these limitations and consider appropriate preprocessing steps, such as filtering out erroneous GPS points or normalizing delivery times based on traffic conditions.\n\nThe LaDe dataset supports a variety of analyses, including regression analysis to identify factors influencing delivery times, machine learning models to predict delivery efficiency, and descriptive statistics to summarize the data. Researchers can explore questions such as how urban geography affects delivery times, the impact of package volume on logistics efficiency, and the optimization of delivery routes based on historical data.\n\nIn summary, the LaDe Last-Mile Delivery dataset is an invaluable asset for those studying logistics and supply chain management. Its extensive coverage of package deliveries, combined with detailed GPS trajectory information, provides a rich foundation for research and analysis, enabling practitioners to enhance last-mile delivery strategies and improve overall logistics operations."
  },
  {
    "name": "CASdatasets R Package",
    "description": "Collection of 40+ actuarial datasets for P&C insurance including freMTPL, ausautoBI, and loss triangles for teaching and research",
    "category": "Insurance & Actuarial",
    "url": "https://cran.r-project.org/package=CASdatasets",
    "docs_url": "https://freakonometrics.github.io/CASdatasets/",
    "github_url": "https://github.com/freakonometrics/CASdatasets",
    "tags": [
      "actuarial",
      "P&C-insurance",
      "loss-triangles",
      "claims-data",
      "teaching-datasets"
    ],
    "best_for": "Learning actuarial methods, pricing model development, and reserving exercises",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The CASdatasets R Package is a comprehensive collection of over 40 actuarial datasets specifically designed for property and casualty (P&C) insurance. These datasets, including freMTPL, ausautoBI, and various loss triangles, serve as valuable resources for teaching and research in the field of actuarial science.",
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What datasets are included in the CASdatasets R Package?",
      "How can I use the CASdatasets for teaching actuarial science?",
      "What types of loss triangles are available in the CASdatasets?",
      "Where can I find actuarial datasets for P&C insurance?",
      "What is the freMTPL dataset?",
      "How can I analyze claims data using CASdatasets?",
      "What are the applications of the CASdatasets R Package in research?",
      "What actuarial datasets are suitable for beginners?"
    ],
    "use_cases": [
      "Analyzing claims data for P&C insurance.",
      "Teaching actuarial concepts using real datasets.",
      "Conducting research on loss triangles.",
      "Evaluating the performance of insurance models."
    ],
    "domain_tags": [
      "insurance",
      "actuarial"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/casdatasets-r-package.png",
    "embedding_text": "The CASdatasets R Package is a robust repository of over 40 actuarial datasets tailored for the property and casualty (P&C) insurance sector. This package includes notable datasets such as freMTPL, which focuses on motor third-party liability insurance, and ausautoBI, which provides insights into Australian automobile bodily injury claims. Additionally, it encompasses various loss triangles that are essential for understanding the development of insurance claims over time. The datasets are structured in a tabular format, typically consisting of rows representing individual claims or observations, and columns that detail various attributes such as claim amounts, dates, and other relevant variables. The collection methodology for these datasets involves aggregating data from reputable sources within the insurance industry, ensuring a high level of data integrity and relevance for both teaching and research purposes. While the specific temporal and geographic coverage of the datasets is not explicitly mentioned, they are designed to be applicable in a wide range of actuarial contexts. Key variables within these datasets measure critical aspects of insurance claims, including the frequency and severity of claims, which are vital for actuaries in assessing risk and setting premiums. Users should be aware of potential limitations in data quality, such as missing values or biases inherent in the data collection process. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers can leverage the CASdatasets R Package to address a variety of research questions, such as the impact of different factors on claim amounts or the development patterns of claims over time. The datasets support various types of analyses, including regression analysis, machine learning models, and descriptive statistics, making them versatile tools for both academic and practical applications in the field of actuarial science. Typically, researchers utilize these datasets to validate models, conduct simulations, and enhance their understanding of insurance dynamics, thereby contributing to the advancement of knowledge in the actuarial domain."
  },
  {
    "name": "Wikipedia Full Database Dump",
    "description": "Complete Wikipedia content and metadata in SQL/XML format, includes all revisions and edit history",
    "category": "Social & Web",
    "url": "https://dumps.wikimedia.org/",
    "docs_url": "https://en.wikipedia.org/wiki/Wikipedia:Database_download",
    "github_url": null,
    "tags": [
      "database dump",
      "SQL",
      "large-scale",
      "text",
      "real-world",
      "encyclopedic"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Wikipedia Full Database Dump contains the complete content and metadata of Wikipedia in SQL/XML format, including all revisions and edit history. This dataset allows users to analyze the evolution of Wikipedia entries, study editing behaviors, and explore the vast knowledge contained within the encyclopedia.",
    "use_cases": [
      "Analyzing trends in Wikipedia content over time.",
      "Studying the behavior of Wikipedia editors and their contributions.",
      "Exploring the reliability and accuracy of information on Wikipedia.",
      "Conducting sentiment analysis on Wikipedia articles."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is included in the Wikipedia Full Database Dump?",
      "How can I access the complete Wikipedia content and metadata?",
      "What format is the Wikipedia database dump available in?",
      "What types of analyses can be performed using Wikipedia data?",
      "How does Wikipedia's edit history contribute to research?",
      "What are the benefits of using a large-scale database like Wikipedia for analysis?",
      "In what ways can the Wikipedia database dump be utilized in data science projects?",
      "What insights can be gained from analyzing Wikipedia's revisions and edits?"
    ],
    "domain_tags": [
      "social",
      "web"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/wikimedia.png",
    "embedding_text": "The Wikipedia Full Database Dump is a comprehensive dataset that encapsulates the entirety of Wikipedia's content and metadata, structured in SQL/XML format. This dataset includes all revisions and edit histories, providing a rich resource for researchers and data scientists interested in the dynamics of knowledge creation and dissemination. The data structure consists of numerous rows and columns, where each entry corresponds to a Wikipedia article, and the associated columns contain variables such as article titles, content, edit timestamps, user information, and revision details. The collection methodology involves the systematic extraction of data from Wikipedia's servers, ensuring that the dataset reflects the most current and historical versions of articles. As a result, this dataset offers a unique opportunity to analyze the temporal evolution of knowledge, track changes in information accuracy, and understand the collaborative nature of content creation on the platform. Key variables within the dataset include article titles, which identify the subject matter; content, which provides the textual information; and metadata such as edit timestamps and user identifiers, which measure the frequency and nature of contributions. While the dataset is extensive, it is important to note that the quality of the data may vary, as it reflects the collaborative editing process of Wikipedia, which can lead to inconsistencies and biases in the information presented. Common preprocessing steps may involve cleaning the text data, normalizing formats, and filtering out irrelevant revisions to focus on significant changes. Researchers can leverage this dataset to address a variety of research questions, such as examining the impact of editorial policies on content accuracy, exploring the relationship between user engagement and article quality, and identifying patterns in the types of information that are most frequently updated. The dataset supports various types of analyses, including regression analysis to identify trends, machine learning for predictive modeling, and descriptive statistics to summarize content characteristics. In studies, researchers typically utilize the Wikipedia Full Database Dump to gain insights into the collaborative nature of knowledge production, assess the reliability of information, and explore the social dynamics of online communities."
  },
  {
    "name": "Metacritic Video Games",
    "description": "Video game reviews and metadata from Metacritic",
    "category": "Entertainment & Media",
    "url": "https://tianchi.aliyun.com/dataset/144719",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "video games",
      "reviews",
      "ratings"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "video games",
      "reviews",
      "ratings"
    ],
    "summary": "The Metacritic Video Games dataset contains a comprehensive collection of video game reviews and associated metadata sourced from Metacritic. This dataset allows users to analyze trends in video game ratings, compare reviews across different platforms, and explore consumer sentiment regarding various titles.",
    "use_cases": [
      "Analyzing the correlation between critic and user ratings.",
      "Identifying trends in video game ratings over time.",
      "Comparing ratings across different gaming platforms.",
      "Exploring the impact of game genre on overall ratings."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest video game ratings on Metacritic?",
      "How do user reviews compare to critic reviews for video games?",
      "What trends can be observed in video game ratings over the years?",
      "Which video game genres receive the highest ratings?",
      "How do platform differences affect video game reviews?",
      "What is the average rating of video games released in a specific year?",
      "How do user scores vary by demographic factors?",
      "What are the most reviewed video games on Metacritic?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Metacritic Video Games dataset is a rich repository of video game reviews and metadata, primarily sourced from Metacritic, a well-known aggregator of video game reviews. This dataset is structured in a tabular format, with rows representing individual video game titles and columns containing key variables such as game title, release date, platform, critic score, user score, and review count. Each entry in the dataset provides insights into the reception of video games across various platforms, allowing researchers and analysts to explore the landscape of video game ratings comprehensively. The collection methodology involves aggregating reviews from both critics and users, ensuring a balanced perspective on each title's performance. The dataset captures a wide range of video games, making it a valuable resource for understanding trends in the gaming industry. Key variables include critic scores, which measure the aggregated opinions of professional reviewers, and user scores, reflecting the sentiments of the gaming community. These scores can be analyzed to identify patterns and correlations, such as the relationship between critic and user ratings. However, it is essential to acknowledge potential limitations in data quality, such as the influence of review bombing or the subjective nature of reviews, which may skew the results. Common preprocessing steps may include handling missing values, normalizing scores, and categorizing games by genre or platform for more granular analysis. Researchers can leverage this dataset to address various research questions, such as examining the impact of game genre on ratings, analyzing trends in user sentiment over time, or comparing the reception of games across different platforms. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and industry analysis. Overall, the Metacritic Video Games dataset serves as a foundational resource for anyone interested in exploring the dynamics of video game ratings and reviews."
  },
  {
    "name": "SIPRI Arms Industry Database",
    "description": "Financial data on the 100 largest defense companies worldwide including revenue, profits, and employment",
    "category": "Defense Economics",
    "url": "https://www.sipri.org/databases/armsindustry",
    "docs_url": "https://www.sipri.org/databases/armsindustry/sources-and-methods",
    "github_url": null,
    "tags": [
      "defense industry",
      "contractors",
      "companies",
      "SIPRI"
    ],
    "best_for": "Analyzing defense industry concentration and contractor performance",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "defense industry",
      "financial analysis",
      "company performance"
    ],
    "summary": "The SIPRI Arms Industry Database provides comprehensive financial data on the 100 largest defense companies globally, including key metrics such as revenue, profits, and employment figures. Researchers and analysts can utilize this dataset to assess trends in the defense sector, evaluate company performance, and study the economic impact of defense spending.",
    "use_cases": [
      "Analyzing trends in defense spending over time.",
      "Comparing financial performance among leading defense contractors.",
      "Evaluating the employment impact of the defense industry.",
      "Studying the correlation between defense spending and economic indicators."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the largest defense companies by revenue?",
      "How do defense contractors' profits compare globally?",
      "What employment trends exist in the defense industry?",
      "What financial metrics are available for SIPRI's Arms Industry Database?",
      "How has the revenue of defense companies changed over the years?",
      "What is the economic impact of the defense industry?",
      "Which companies are the top contractors in the defense sector?",
      "What data does SIPRI provide on global defense spending?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2002-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The SIPRI Arms Industry Database is a pivotal resource for understanding the financial landscape of the global defense sector. This dataset encompasses detailed financial data on the 100 largest defense companies worldwide, offering insights into various key performance indicators such as revenue, profits, and employment figures. The data is structured in a tabular format, with rows representing individual companies and columns detailing specific financial metrics. Key variables include total revenue, net profits, and employee counts, which collectively provide a comprehensive overview of each company's financial health and operational scale. Researchers and analysts often leverage this dataset to explore significant research questions related to the defense industry's economic impact, trends in military spending, and the competitive landscape among defense contractors. The collection methodology for the SIPRI Arms Industry Database involves rigorous data gathering from publicly available financial reports, industry publications, and other reputable sources, ensuring a high level of data quality and reliability. However, users should be aware of potential limitations, such as variations in reporting standards across different countries and companies, which may affect comparability. Common preprocessing steps may include data cleaning to handle missing values, standardizing financial metrics for comparative analysis, and aggregating data to derive insights at a macroeconomic level. The dataset supports a variety of analyses, including descriptive statistics to summarize company performance, regression analyses to identify relationships between financial metrics and external economic factors, and machine learning techniques for predictive modeling. Researchers typically utilize the SIPRI Arms Industry Database to conduct in-depth studies on the defense sector, contributing valuable insights to discussions on military economics, public policy, and international relations."
  },
  {
    "name": "Polish Grocery",
    "description": "Yearly sales data (2018) from Polish grocery shop",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/agatii/total-sale-2018-yearly-data-of-grocery-shop",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Poland",
      "yearly data"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Polish Grocery dataset contains yearly sales data from a Polish grocery shop for the year 2018. This dataset can be utilized to analyze consumer purchasing patterns, assess pricing strategies, and evaluate sales performance in the grocery sector.",
    "use_cases": [
      "Analyzing consumer purchasing trends over the year.",
      "Evaluating the impact of pricing changes on sales volume.",
      "Identifying seasonal variations in grocery sales.",
      "Comparing sales performance across different product categories."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the yearly sales trends in Polish grocery shops?",
      "How do pricing strategies affect grocery sales in Poland?",
      "What consumer behaviors can be observed from Polish grocery sales data?",
      "What products are most popular in Polish grocery stores?",
      "How does seasonality impact grocery sales in Poland?",
      "What demographic factors influence grocery shopping in Poland?",
      "How can regression analysis be applied to grocery sales data?",
      "What insights can be derived from analyzing yearly sales data in the grocery sector?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018",
    "geographic_scope": "Poland",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/polish-grocery.jpg",
    "embedding_text": "The Polish Grocery dataset is a comprehensive collection of yearly sales data from a grocery shop located in Poland, specifically covering the year 2018. This dataset is structured in a tabular format, consisting of rows representing individual sales transactions and columns that capture various attributes such as product categories, sales amounts, quantities sold, and potentially other relevant metrics. The data collection methodology typically involves direct sales records from the grocery shop, ensuring that the dataset reflects actual consumer behavior and purchasing patterns within the specified time frame. However, it is essential to note that the dataset may have limitations regarding data quality, such as missing values or inconsistencies in recording sales, which are common challenges in retail datasets. Researchers and analysts can employ common preprocessing steps such as data cleaning, normalization, and transformation to prepare the dataset for analysis. The key variables in this dataset allow for the measurement of sales performance, consumer preferences, and pricing effectiveness, making it a valuable resource for various research questions. Analysts can explore how different factors influence sales, assess the impact of promotional strategies, and identify trends over the year. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, enabling researchers to derive meaningful insights into consumer behavior and market dynamics in the grocery sector. Typically, this dataset is utilized in studies focusing on retail economics, consumer behavior analysis, and pricing strategy evaluation, providing a foundation for understanding the complexities of grocery sales in Poland."
  },
  {
    "name": "Stack Overflow Data Dump",
    "description": "Full Q&A archive + annual developer survey (49K+ responses). Salaries, tech adoption, developer analytics",
    "category": "Social & Web",
    "url": "https://archive.org/details/stackexchange",
    "docs_url": "https://survey.stackoverflow.co/",
    "github_url": null,
    "tags": [
      "developers",
      "salaries",
      "tech",
      "survey",
      "Q&A"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Stack Overflow Data Dump is a comprehensive archive that includes a full question and answer dataset along with an annual developer survey featuring over 49,000 responses. This dataset provides insights into salaries, technology adoption, and developer analytics, making it a valuable resource for understanding trends in the software development industry.",
    "use_cases": [
      "Analyzing salary trends among developers",
      "Investigating technology adoption rates",
      "Examining developer demographics",
      "Conducting sentiment analysis on developer Q&A"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What insights can be gained from the Stack Overflow Data Dump?",
      "How do developer salaries vary across different technologies?",
      "What are the latest trends in tech adoption according to the developer survey?",
      "How can I analyze developer analytics from the Stack Overflow dataset?",
      "What demographic information is available in the Stack Overflow Data Dump?",
      "How does the Stack Overflow survey data inform software development practices?",
      "What are the key variables in the Stack Overflow Data Dump?",
      "How can I use the Stack Overflow Data Dump for regression analysis?"
    ],
    "domain_tags": [
      "technology",
      "software development"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/archive.png",
    "embedding_text": "The Stack Overflow Data Dump is a rich dataset that encompasses a full archive of questions and answers from the Stack Overflow platform, alongside an annual developer survey that collects responses from over 49,000 developers. This dataset is structured in a tabular format, containing various rows and columns that represent different variables related to developer experiences, salaries, technology usage, and more. The data is collected through user interactions on the Stack Overflow website, where developers post questions and provide answers, as well as through structured survey responses gathered annually. The dataset covers a wide range of topics relevant to the software development community, including programming languages, frameworks, tools, and developer demographics. Key variables in the dataset include salary information, technology adoption rates, and responses to various survey questions that gauge developer satisfaction and preferences. However, like any dataset, it has its limitations, including potential biases in self-reported data and the representativeness of the sample. Researchers often need to perform common preprocessing steps such as cleaning the data, handling missing values, and normalizing responses to prepare it for analysis. The dataset supports various types of analyses, including regression analysis to understand salary determinants, machine learning applications to predict trends, and descriptive statistics to summarize developer behaviors. Researchers typically utilize this dataset to address questions related to salary disparities, technology trends, and the overall landscape of the software development industry, making it an invaluable resource for both academic and industry research."
  },
  {
    "name": "French Motor TPL (freMTPL2)",
    "description": "French motor third-party liability insurance dataset with 678K policies and claims - the standard benchmark for insurance ML papers",
    "category": "Insurance & Actuarial",
    "url": "https://cran.r-project.org/package=CASdatasets",
    "docs_url": "https://freakonometrics.github.io/CASdatasets/",
    "github_url": null,
    "tags": [
      "motor-insurance",
      "claims-frequency",
      "pricing",
      "benchmark",
      "actuarial"
    ],
    "best_for": "Insurance pricing models, GLM vs ML comparisons, and actuarial research",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "insurance",
      "actuarial",
      "data-analysis"
    ],
    "summary": "The French Motor TPL (freMTPL2) dataset provides a comprehensive collection of motor third-party liability insurance policies and claims, encompassing 678,000 records. This dataset serves as a standard benchmark for machine learning applications in insurance, allowing researchers and practitioners to explore claims frequency, pricing strategies, and actuarial analyses.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the French Motor TPL dataset?",
      "How can I analyze claims frequency in motor insurance?",
      "What are the key variables in the freMTPL2 dataset?",
      "How is the French Motor TPL dataset used in machine learning?",
      "What preprocessing steps are needed for the freMTPL2 dataset?",
      "What insights can be derived from motor insurance claims data?",
      "What benchmarks exist for insurance machine learning studies?",
      "How does the freMTPL2 dataset support actuarial analysis?"
    ],
    "use_cases": [
      "Analyzing claims frequency trends",
      "Developing pricing models for motor insurance",
      "Benchmarking machine learning algorithms in insurance",
      "Conducting actuarial assessments of risk"
    ],
    "domain_tags": [
      "insurance",
      "actuarial"
    ],
    "data_modality": "tabular",
    "geographic_scope": "France",
    "size_category": "large",
    "benchmark_usage": [
      "Standard benchmark for insurance ML papers"
    ],
    "model_score": 0.0002,
    "image_url": "/images/logos/r-project.png",
    "embedding_text": "The French Motor TPL (freMTPL2) dataset is a rich resource for researchers and practitioners in the field of insurance and actuarial science, providing a detailed collection of motor third-party liability insurance policies and claims. With a total of 678,000 records, this dataset is recognized as a standard benchmark for machine learning applications in the insurance sector. The data is structured in a tabular format, consisting of various rows and columns that capture essential variables related to insurance policies and claims. Key variables within the dataset include policyholder information, claim amounts, claim frequencies, and various attributes of the insured vehicles, which collectively facilitate in-depth analyses of risk and pricing strategies. The collection methodology for the freMTPL2 dataset is grounded in rigorous data gathering practices, ensuring that the information is both comprehensive and reliable. Researchers typically utilize this dataset to address a range of research questions, such as understanding the factors influencing claims frequency, developing predictive models for pricing, and conducting actuarial assessments of risk. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for data scientists and actuaries alike. However, users should be aware of potential data quality issues and limitations, which may necessitate common preprocessing steps such as data cleaning, normalization, and handling of missing values. Overall, the freMTPL2 dataset serves as a vital resource for advancing knowledge in the field of motor insurance and actuarial science, enabling users to derive meaningful insights and contribute to the development of innovative solutions in the industry."
  },
  {
    "name": "Dominicks Soft Drinks",
    "description": "Weekly scanner data on soft drink purchases from Dominick's Finer Foods",
    "category": "Grocery & Supermarkets",
    "url": "https://www.chicagobooth.edu/research/kilts/research-data/dominicks",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "scanner data",
      "soft drinks",
      "Chicago Booth"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Dominicks Soft Drinks dataset contains weekly scanner data on soft drink purchases from Dominick's Finer Foods. This dataset can be utilized to analyze consumer purchasing patterns, pricing strategies, and market trends in the soft drink industry.",
    "use_cases": [
      "Analyzing trends in soft drink sales over time.",
      "Evaluating the impact of promotional pricing on sales volume.",
      "Studying consumer preferences for different soft drink brands.",
      "Investigating seasonal variations in soft drink purchases."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Dominicks Soft Drinks dataset?",
      "How can I analyze soft drink purchases from Dominick's Finer Foods?",
      "What insights can be gained from weekly scanner data on soft drinks?",
      "Where can I find data on soft drink purchases in Chicago?",
      "What are the trends in soft drink consumption at Dominick's?",
      "How does pricing affect soft drink sales at grocery stores?",
      "What consumer behaviors can be analyzed using scanner data?",
      "What variables are included in the Dominicks Soft Drinks dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Chicago",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/dominicks-soft-drinks.jpg",
    "embedding_text": "The Dominicks Soft Drinks dataset provides a comprehensive view of soft drink purchasing behavior through weekly scanner data collected from Dominick's Finer Foods, a well-known grocery chain. The dataset is structured in a tabular format, consisting of rows that represent individual transactions and columns that capture various attributes related to each purchase. Key variables typically include transaction date, product identifier, quantity purchased, price per unit, and possibly customer demographics, although specific demographic data is not explicitly mentioned in the description. The collection methodology involves using scanner technology at the point of sale, which records every transaction in real-time, ensuring high accuracy and reliability of the data. This dataset is particularly valuable for researchers and analysts interested in consumer behavior, pricing strategies, and market dynamics within the soft drink sector. The geographic scope is limited to the Chicago area, providing insights specific to this market, while the temporal coverage is not explicitly stated. However, the weekly nature of the data suggests a focus on short-term trends and patterns. Researchers can utilize this dataset to explore a variety of research questions, such as how pricing changes influence consumer purchasing decisions, what factors drive brand loyalty among soft drink consumers, and how seasonal events impact sales. Common preprocessing steps may include cleaning the data to handle missing values, normalizing prices to account for inflation, and aggregating sales data to analyze trends over different time periods. The dataset supports various types of analyses, including regression analysis to model the relationship between price and quantity sold, machine learning techniques for predictive modeling, and descriptive statistics to summarize purchasing patterns. Overall, the Dominicks Soft Drinks dataset serves as a rich resource for academic research, market analysis, and business strategy development in the retail food sector."
  },
  {
    "name": "Iowa Liquor",
    "description": "Monthly Class E liquor sales data with volume and pricing from Iowa",
    "category": "Grocery & Supermarkets",
    "url": "https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "liquor",
      "government data",
      "Iowa"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Iowa Liquor dataset provides monthly Class E liquor sales data, including volume and pricing information from the state of Iowa. Researchers and analysts can utilize this dataset to explore trends in liquor sales, analyze pricing strategies, and understand consumer purchasing behavior in the grocery and supermarket sector.",
    "use_cases": [
      "Analyzing seasonal trends in liquor sales.",
      "Evaluating the impact of pricing strategies on sales volume.",
      "Studying consumer behavior patterns in liquor purchases.",
      "Comparing liquor sales across different months."
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the monthly liquor sales trends in Iowa?",
      "How do pricing changes affect liquor sales in Iowa?",
      "What is the volume of Class E liquor sold in Iowa each month?",
      "What government data is available on liquor sales in Iowa?",
      "How does consumer behavior vary in liquor purchases in Iowa?",
      "What are the key factors influencing liquor sales in Iowa?",
      "How can I analyze Iowa's liquor sales data using Python?",
      "What insights can be derived from Iowa's monthly liquor sales data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Iowa",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/iowa.png",
    "embedding_text": "The Iowa Liquor dataset is a comprehensive collection of monthly Class E liquor sales data, specifically detailing the volume and pricing of liquor sold in the state of Iowa. This dataset is structured in a tabular format, with rows representing individual monthly records and columns capturing key variables such as sales volume, pricing, and possibly other relevant metrics. The data is collected from government sources, ensuring a level of reliability and accuracy that is crucial for research and analysis. The dataset covers a specific geographic area, namely Iowa, making it particularly useful for studies focused on regional consumer behavior and market trends in the liquor industry. While the exact temporal coverage is not specified, the monthly nature of the data allows for time-series analyses that can reveal trends over time. Key variables within the dataset measure important aspects of liquor sales, including the total volume sold and the pricing per unit, which can be critical for understanding market dynamics. However, researchers should be aware of potential limitations in data quality, such as reporting inconsistencies or gaps in data collection. Common preprocessing steps may include data cleaning to handle missing values, normalization of pricing data, and aggregation of sales figures for broader analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying seasonal trends in liquor sales, evaluating the effects of pricing changes on consumer purchasing behavior, and analyzing the overall market dynamics within the retail sector. The dataset supports various types of analyses, including regression analysis to model relationships between pricing and sales volume, machine learning techniques for predictive modeling, and descriptive statistics to summarize the data. Overall, the Iowa Liquor dataset serves as a valuable resource for researchers, analysts, and anyone interested in exploring the intricacies of liquor sales and consumer behavior in Iowa."
  },
  {
    "name": "FEMA NFIP Claims & Policies",
    "description": "National Flood Insurance Program data with 2M+ claims since 1978 and policy-level information for flood risk modeling",
    "category": "Insurance & Actuarial",
    "url": "https://www.fema.gov/openfema-data-page/fima-nfip-redacted-claims-v2",
    "docs_url": "https://www.fema.gov/about/openfema/data-sets",
    "github_url": null,
    "tags": [
      "flood-insurance",
      "catastrophe",
      "claims-data",
      "natural-disasters",
      "property-insurance"
    ],
    "best_for": "Catastrophe modeling, flood risk analysis, and climate-related insurance research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The FEMA NFIP Claims & Policies dataset contains over 2 million claims related to the National Flood Insurance Program since 1978, along with detailed policy-level information. This dataset can be utilized for flood risk modeling, insurance pricing analysis, and understanding the impact of natural disasters on property insurance.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the FEMA NFIP Claims & Policies dataset?",
      "How can I access the National Flood Insurance Program data?",
      "What types of claims are included in the FEMA NFIP dataset?",
      "What is the temporal coverage of the FEMA NFIP Claims & Policies dataset?",
      "How can flood risk modeling be performed using FEMA data?",
      "What are the key variables in the FEMA NFIP dataset?",
      "How has the National Flood Insurance Program evolved since 1978?",
      "What insights can be gained from analyzing flood insurance claims data?"
    ],
    "use_cases": [
      "Analyzing trends in flood insurance claims over time.",
      "Modeling flood risk to inform policy pricing.",
      "Assessing the impact of natural disasters on property values.",
      "Evaluating the effectiveness of the National Flood Insurance Program."
    ],
    "domain_tags": [
      "insurance",
      "natural-disasters"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1978-present",
    "size_category": "large",
    "model_score": 0.0002,
    "image_url": "/images/datasets/fema-nfip-claims-policies.png",
    "embedding_text": "The FEMA NFIP Claims & Policies dataset is a comprehensive collection of data related to the National Flood Insurance Program, which has been operational since 1978. This dataset includes over 2 million claims, providing a rich source of information for researchers and analysts interested in flood risk modeling and the insurance industry. The data is structured in a tabular format, with rows representing individual claims and policies, and columns containing various attributes such as claim amounts, policy details, dates of claims, and geographic identifiers. The key variables in this dataset measure aspects such as the amount claimed, the type of coverage, the location of the insured property, and the circumstances surrounding each claim. Researchers can leverage this dataset to explore a variety of research questions, including the frequency and severity of flood-related claims, the effectiveness of flood insurance in mitigating financial losses, and the relationship between flood risk and property values. The dataset's temporal coverage spans several decades, allowing for longitudinal studies that can reveal trends and changes in flood risk over time. However, users should be aware of potential limitations in data quality, such as inconsistencies in reporting or variations in policy coverage over the years. Common preprocessing steps may include cleaning the data to handle missing values, normalizing claim amounts for inflation, and aggregating data to analyze trends over specific time periods. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the insurance sector. Researchers typically use this dataset to inform policy decisions, enhance risk assessment models, and contribute to the broader understanding of how natural disasters impact communities and economies."
  },
  {
    "name": "Israeli Grocery",
    "description": "Grocery purchase data from Israel",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/arielpazsawicki/kimonaim",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "grocery",
      "Israel",
      "purchases"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Israeli Grocery dataset contains detailed grocery purchase data from Israel, providing insights into consumer behavior and purchasing patterns. Researchers can analyze this data to understand trends in grocery shopping, pricing strategies, and the impact of various factors on consumer choices.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Studying the impact of pricing on sales",
      "Evaluating the effectiveness of promotional strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the grocery purchase trends in Israel?",
      "How do consumer behaviors vary in grocery shopping?",
      "What factors influence grocery prices in Israel?",
      "What is the demographic breakdown of grocery purchases in Israel?",
      "How can grocery purchase data inform marketing strategies?",
      "What are the seasonal variations in grocery purchases in Israel?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Israel",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/israeli-grocery.png",
    "embedding_text": "The Israeli Grocery dataset is a comprehensive collection of grocery purchase data from Israel, structured in a tabular format that allows for easy manipulation and analysis. The dataset typically includes rows representing individual transactions, with columns capturing various attributes such as product categories, prices, quantities purchased, and possibly demographic information about the consumers. The collection methodology for this dataset may involve direct surveys, point-of-sale data from grocery stores, or aggregated data from market research firms, ensuring a rich source of information for analysis. While the exact temporal coverage is not specified, the dataset is likely to encompass a range of time periods to capture trends and changes in consumer behavior over time. The geographic scope is explicitly limited to Israel, making it particularly relevant for studies focused on this region's grocery market. Key variables within the dataset may include product identifiers, purchase dates, consumer demographics, and payment methods, each measuring different aspects of the grocery shopping experience. Researchers should be aware of potential limitations in data quality, such as missing values or biases in consumer reporting, which could affect the reliability of analyses. Common preprocessing steps might involve cleaning the data, handling missing values, and transforming variables for analysis. This dataset supports a variety of research questions, including those related to consumer behavior, pricing strategies, and market trends. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically use this dataset to inform studies on consumer preferences, evaluate marketing effectiveness, and understand the dynamics of the grocery retail sector in Israel."
  },
  {
    "name": "Turkish Drugs",
    "description": "Drug sales data from Turkey",
    "category": "Grocery & Supermarkets",
    "url": "https://www.kaggle.com/datasets/emrahaydemr/drug-sales-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "pharmaceuticals",
      "Turkey",
      "sales"
    ],
    "best_for": "Learning grocery & supermarkets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Turkish Drugs dataset contains drug sales data from Turkey, providing insights into the pharmaceutical market within the country. Researchers and analysts can use this dataset to explore sales trends, consumer behavior, and market dynamics in the Turkish pharmaceutical sector.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Comparing sales across different drug categories",
      "Investigating the impact of pricing strategies on sales",
      "Studying consumer purchasing behavior in the pharmaceutical sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the drug sales trends in Turkey?",
      "How do pharmaceutical sales vary across different regions in Turkey?",
      "What factors influence drug sales in the Turkish market?",
      "What is the impact of pricing on drug sales in Turkey?",
      "How does consumer behavior affect pharmaceutical sales in Turkey?",
      "What are the seasonal patterns in drug sales in Turkey?"
    ],
    "domain_tags": [
      "retail",
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Turkey",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/turkish-drugs.jpg",
    "embedding_text": "The Turkish Drugs dataset is a comprehensive collection of drug sales data from Turkey, structured in a tabular format that includes various rows and columns representing different sales transactions. Each row typically corresponds to a specific sale, while the columns contain key variables such as drug name, sales volume, price, date of sale, and possibly demographic information about the consumers. The dataset is likely collected from various sources, including pharmacies, hospitals, and retail outlets, ensuring a wide coverage of the pharmaceutical market in Turkey. However, the exact collection methodology and data sources are not specified, which may pose challenges in assessing the reliability and validity of the data. The dataset focuses on the Turkish market, and while it provides valuable insights into drug sales, it may have limitations in terms of data quality, such as missing values or inconsistencies in reporting. Common preprocessing steps may include cleaning the data to handle missing entries, normalizing prices for inflation, and aggregating sales data to analyze trends over time. Researchers can utilize this dataset to address various research questions, such as examining sales trends, understanding consumer behavior, and evaluating the impact of pricing strategies on drug sales. The dataset supports a range of analyses, including regression analysis to identify factors influencing sales, machine learning techniques for predictive modeling, and descriptive statistics to summarize the data. Typically, researchers use this dataset to gain insights into the dynamics of the pharmaceutical market in Turkey, helping to inform business strategies, policy decisions, and further academic research in the field."
  },
  {
    "name": "YFCC100M",
    "description": "100M Flickr photos/videos with metadata under Creative Commons. Yahoo/Flickr dataset for multimedia research",
    "category": "Entertainment & Media",
    "url": "https://multimediacommons.wordpress.com/yfcc100m-core-dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Flickr",
      "photos",
      "video",
      "multimedia",
      "Creative Commons"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "multimedia research",
      "data analysis"
    ],
    "summary": "The YFCC100M dataset contains 100 million Flickr photos and videos, complete with metadata, available under Creative Commons licensing. This extensive dataset is ideal for multimedia research, allowing users to explore various aspects of visual content, metadata analysis, and user interactions.",
    "use_cases": [
      "Analyzing user engagement with multimedia content",
      "Studying trends in visual media over time",
      "Exploring the relationship between metadata and content popularity",
      "Developing machine learning models for image and video classification"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the YFCC100M dataset?",
      "Where can I find 100M Flickr photos and videos?",
      "What metadata is included in the YFCC100M dataset?",
      "How can I use the YFCC100M dataset for multimedia research?",
      "What are the Creative Commons licenses for YFCC100M?",
      "What types of analyses can be performed on the YFCC100M dataset?",
      "What is the significance of the YFCC100M dataset in data science?",
      "How to access the YFCC100M dataset for research purposes?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/datasets/yfcc100m.png",
    "embedding_text": "The YFCC100M dataset is a comprehensive collection of 100 million photos and videos sourced from Flickr, enriched with extensive metadata that is freely available under Creative Commons licensing. This dataset serves as a valuable resource for researchers and practitioners in the field of multimedia research, offering a rich tapestry of visual content that can be analyzed for various insights. The data structure consists of a vast number of rows, each representing an individual photo or video, accompanied by columns that detail key variables such as user-uploaded tags, descriptions, timestamps, and other metadata attributes that provide context to the visual content. The collection methodology involves aggregating user-generated content from the Flickr platform, ensuring a diverse representation of multimedia artifacts that span a wide range of topics and themes.\n\nCoverage of the dataset is extensive in terms of the sheer volume of content, but specific temporal and geographic dimensions are not explicitly detailed in the dataset description. Key variables within the dataset include user engagement metrics, such as view counts and comment counts, as well as descriptive elements like titles and tags that users assign to their uploads. These variables are crucial for measuring the popularity and relevance of multimedia content, offering insights into user behavior and content trends.\n\nData quality is generally high, as the dataset is derived from a well-established platform; however, limitations may arise from inconsistencies in user-generated metadata and potential biases in the types of content that are uploaded. Common preprocessing steps may include cleaning the metadata, filtering out irrelevant content, and normalizing tags for more effective analysis. Researchers can leverage the YFCC100M dataset to address a variety of research questions, such as examining the dynamics of visual culture, understanding the impact of metadata on content discoverability, and exploring the evolution of multimedia trends over time.\n\nThe dataset supports a range of analytical approaches, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically utilize this dataset to conduct studies that investigate the interplay between visual content and social media dynamics, develop algorithms for content classification, and analyze the effects of user engagement on multimedia consumption patterns. Overall, the YFCC100M dataset stands as a pivotal resource for advancing knowledge in multimedia research, enabling a deeper understanding of how images and videos shape and reflect contemporary culture."
  },
  {
    "name": "OECD Data",
    "description": "Harmonized indicators for 38 member countries - gold standard for advanced economy comparisons",
    "category": "Dataset Aggregators",
    "url": "https://data-explorer.oecd.org",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "OECD",
      "international",
      "harmonized",
      "SDMX",
      "API"
    ],
    "best_for": "Harmonized cross-country comparisons for advanced economies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OECD Data provides harmonized indicators for 38 member countries, serving as a gold standard for advanced economy comparisons. Researchers and analysts can utilize this dataset to conduct comparative studies on economic performance and policy effectiveness across different nations.",
    "use_cases": [
      "Comparative analysis of economic indicators across OECD member countries",
      "Policy effectiveness evaluation using harmonized data",
      "Trend analysis of economic performance over time",
      "International benchmarking of economic policies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the harmonized indicators provided by OECD?",
      "How can I access OECD data for advanced economy comparisons?",
      "What countries are included in the OECD Data?",
      "What types of analyses can be performed with OECD indicators?",
      "How does OECD data support international economic research?",
      "What is the significance of SDMX in OECD data?",
      "How can I use OECD data for policy analysis?",
      "What are the key variables measured in OECD Data?"
    ],
    "domain_tags": [
      "economics",
      "international relations"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/oecd.png",
    "embedding_text": "The OECD Data is a comprehensive dataset that provides harmonized indicators for 38 member countries, making it an invaluable resource for researchers and analysts interested in comparative economic studies. This dataset is structured in a tabular format, consisting of rows representing different countries and columns that capture various economic indicators. The key variables included in the dataset measure aspects such as GDP, employment rates, inflation, and other critical economic metrics that are essential for understanding the economic landscape of advanced economies. The data is collected through rigorous methodologies that ensure high quality and reliability, drawing from national statistics offices and international organizations. However, users should be aware of potential limitations, such as differences in data collection methods across countries, which may affect comparability. Common preprocessing steps may include normalization of data, handling of missing values, and transformation of variables to facilitate analysis. Researchers typically use this dataset to address a range of research questions, including the impact of economic policies on growth, the relationship between employment and inflation, and the comparative analysis of economic resilience during crises. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing for a robust exploration of economic phenomena. Overall, the OECD Data serves as a critical tool for understanding economic trends and informing policy decisions within the context of international economics.",
    "geographic_scope": "OECD member countries"
  },
  {
    "name": "LIAR",
    "description": "12.8K fact-checked political statements with speaker metadata",
    "category": "Content Moderation",
    "url": "https://sites.cs.ucsb.edu/~william/software.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fact-checking",
      "politics",
      "misinformation"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "fact-checking",
      "politics",
      "misinformation"
    ],
    "summary": "The LIAR dataset contains 12.8K fact-checked political statements, providing a rich resource for analyzing the accuracy of political discourse. Researchers can utilize this dataset to study patterns of misinformation and the effectiveness of fact-checking in political communication.",
    "use_cases": [
      "Analyzing the accuracy of political statements",
      "Studying the impact of misinformation on public opinion",
      "Evaluating the effectiveness of fact-checking organizations",
      "Investigating trends in political discourse"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LIAR dataset?",
      "How can I access the LIAR dataset?",
      "What types of analyses can be performed with the LIAR dataset?",
      "What are the key features of the LIAR dataset?",
      "How is the LIAR dataset structured?",
      "What research questions can the LIAR dataset help answer?",
      "What are the limitations of the LIAR dataset?",
      "How can fact-checking be analyzed using the LIAR dataset?"
    ],
    "domain_tags": [
      "politics",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The LIAR dataset is a comprehensive collection of 12.8K fact-checked political statements, designed to facilitate research in the realms of political communication and misinformation. This dataset is structured in a tabular format, with each row representing a unique political statement. Key variables within the dataset include the statement text, the speaker's identity, the context of the statement, and the fact-checking outcome, which categorizes the statement as true, false, or misleading. The collection methodology involves aggregating statements from various political speeches, debates, and public communications, ensuring a diverse representation of political discourse. Researchers can leverage this dataset to address critical research questions related to the prevalence of misinformation in political contexts, the effectiveness of fact-checking interventions, and the overall impact of political statements on public perception. The dataset supports various analytical approaches, including regression analysis and machine learning techniques, enabling users to explore correlations between speaker characteristics and the accuracy of their statements. However, it is important to note that the dataset may have limitations in terms of coverage and potential biases in speaker representation. Common preprocessing steps may include text normalization, tokenization, and the removal of non-essential metadata to prepare the data for analysis. Overall, the LIAR dataset serves as a vital resource for scholars and practitioners interested in understanding the dynamics of political communication and the role of fact-checking in mitigating misinformation."
  },
  {
    "name": "Alibaba Industrial Dump (150GB)",
    "description": "Large-scale industrial dataset from Alibaba (150GB)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/81505",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "large-scale",
      "industrial",
      "Alibaba"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce"
    ],
    "summary": "The Alibaba Industrial Dump is a large-scale dataset containing various industrial data sourced from Alibaba. Researchers and analysts can utilize this dataset to explore trends in e-commerce, analyze consumer behavior, and develop pricing strategies.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Developing pricing models",
      "Identifying trends in industrial data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Alibaba Industrial Dump dataset?",
      "How can I access the Alibaba Industrial Dump dataset?",
      "What types of data are included in the Alibaba Industrial Dump?",
      "What analyses can be performed with the Alibaba Industrial Dump?",
      "What are the key variables in the Alibaba Industrial Dump dataset?",
      "What is the size of the Alibaba Industrial Dump dataset?",
      "What are the applications of the Alibaba Industrial Dump dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "model_score": 0.0002,
    "embedding_text": "The Alibaba Industrial Dump is a comprehensive dataset that provides insights into various aspects of industrial operations and e-commerce activities. This dataset, which spans approximately 150GB, is structured in a tabular format, consisting of numerous rows and columns that capture a wide array of variables relevant to industrial transactions and consumer interactions. Each row in the dataset represents a unique transaction or event, while the columns include variables such as product categories, transaction amounts, timestamps, and customer demographics. The data is collected from Alibaba's extensive e-commerce platform, which serves millions of users and businesses, thereby ensuring a rich and diverse dataset. The collection methodology involves aggregating data from various sources within the Alibaba ecosystem, ensuring that the dataset is both extensive and representative of current market trends. While the exact temporal and geographic coverage is not specified, the dataset is likely to reflect a broad range of transactions across different time periods and regions, given Alibaba's global reach. Key variables in the dataset may include product identifiers, sales figures, user ratings, and demographic information about buyers, which can be instrumental in measuring consumer preferences and market dynamics. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in user-reported data, which may necessitate common preprocessing steps like data cleaning and normalization. The dataset supports a variety of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for researchers and data scientists. Typical research questions that can be addressed using this dataset include understanding consumer behavior trends, evaluating the effectiveness of marketing strategies, and predicting future sales patterns. Overall, the Alibaba Industrial Dump serves as a valuable tool for those looking to delve into the intricacies of e-commerce and industrial data analysis."
  },
  {
    "name": "M5Product",
    "description": "5 modalities (image, text, table, video, audio), 6M+ samples for multimodal learning",
    "category": "E-Commerce",
    "url": "https://xiaodongsuper.github.io/M5Product_dataset/index.html",
    "docs_url": "https://xiaodongsuper.github.io/M5Product_dataset/index.html",
    "github_url": null,
    "tags": [
      "multimodal",
      "product data",
      "images",
      "video"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The M5Product dataset consists of over 6 million samples across five modalities: image, text, table, video, and audio, designed for multimodal learning in the e-commerce domain. Researchers and practitioners can leverage this dataset to develop and evaluate models that analyze product data, enhancing understanding of consumer behavior and improving product recommendations.",
    "use_cases": [
      "Developing multimodal models for product recommendation systems",
      "Analyzing consumer behavior through various data modalities",
      "Creating visualizations to understand product trends",
      "Training models for automated product categorization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the M5Product dataset?",
      "How can I access the M5Product dataset for multimodal learning?",
      "What types of data are included in the M5Product dataset?",
      "What are the applications of the M5Product dataset in e-commerce?",
      "How many samples are in the M5Product dataset?",
      "What modalities are covered in the M5Product dataset?",
      "Can the M5Product dataset be used for machine learning?",
      "What are the key features of the M5Product dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0002,
    "embedding_text": "The M5Product dataset is a comprehensive collection designed for multimodal learning, encompassing over 6 million samples that span five distinct modalities: image, text, table, video, and audio. This dataset is particularly valuable in the e-commerce sector, where understanding product data through various forms of media can significantly enhance consumer engagement and decision-making processes. The data structure includes a variety of variables that capture different aspects of products, such as images for visual representation, textual descriptions for detailed information, tables for structured data, and videos and audio for demonstrations or advertisements. Each sample within the dataset is meticulously organized, allowing researchers to explore relationships between different modalities and their impact on consumer behavior. The collection methodology for the M5Product dataset involves aggregating data from multiple sources, ensuring a rich and diverse dataset that reflects real-world e-commerce scenarios. This diversity is crucial for training robust models that can generalize well across different types of products and consumer interactions. While the dataset does not specify temporal or geographic coverage, it is designed to be applicable across various contexts within the e-commerce landscape. Key variables within the dataset include product images, descriptions, pricing information, and multimedia content, which collectively measure aspects such as product appeal, consumer interest, and potential sales performance. Researchers utilizing the M5Product dataset can expect to encounter high-quality data, although they should be aware of potential limitations such as data sparsity in certain modalities or variations in data quality across different sources. Common preprocessing steps may include normalization of text data, resizing of images, and alignment of different modalities to ensure compatibility for analysis. The dataset supports a wide range of research questions, including inquiries into how different modalities influence consumer purchasing decisions, the effectiveness of various marketing strategies, and the development of predictive models for sales forecasting. Analysts can employ various techniques, including regression analysis, machine learning algorithms, and descriptive statistics, to extract insights from the data. Researchers typically use the M5Product dataset to build and validate models that enhance product recommendations, improve user experience, and drive sales growth in the competitive e-commerce environment."
  },
  {
    "name": "JD-pretrain-data",
    "description": "Encoded search queries and item data for intent detection",
    "category": "E-Commerce",
    "url": "https://github.com/jdcomsearch/jd-pretrain-data",
    "docs_url": null,
    "github_url": "https://github.com/jdcomsearch/jd-pretrain-data",
    "tags": [
      "search",
      "intent detection",
      "embeddings"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The JD-pretrain-data dataset consists of encoded search queries and item data specifically designed for intent detection in e-commerce applications. Researchers and practitioners can utilize this dataset to improve search algorithms and enhance user experience by better understanding consumer intent.",
    "use_cases": [
      "Improving search algorithms for e-commerce platforms",
      "Analyzing consumer behavior based on search queries",
      "Developing intent detection models",
      "Enhancing recommendation systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is JD-pretrain-data?",
      "Where can I find encoded search queries for intent detection?",
      "How can I use JD-pretrain-data for e-commerce research?",
      "What are the applications of intent detection in e-commerce?",
      "What type of data does JD-pretrain-data include?",
      "How does JD-pretrain-data support search algorithms?",
      "What insights can I gain from analyzing JD-pretrain-data?",
      "What are the key variables in JD-pretrain-data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/jd-pretrain-data.png",
    "embedding_text": "The JD-pretrain-data dataset is a valuable resource for researchers and practitioners in the field of e-commerce, focusing on the analysis of encoded search queries and item data that are crucial for intent detection. This dataset is structured in a tabular format, consisting of rows and columns that represent various search queries and associated item data. Each row typically corresponds to a unique search query, while the columns may include variables such as query text, item identifiers, and encoded representations of the search intent. The data collection methodology involves gathering search queries from e-commerce platforms, where users input their queries to find specific products or services. This data is then encoded to facilitate machine learning applications, particularly in understanding user intent and improving search functionalities. The dataset's coverage is primarily focused on e-commerce contexts, without specific temporal or geographic limitations explicitly mentioned. Key variables in the dataset measure aspects such as the frequency of search queries, the relevance of items to those queries, and the encoded features that represent user intent. While the dataset provides valuable insights, it is important to note potential limitations, such as biases in the search queries based on user demographics or the specific e-commerce platform from which the data was collected. Common preprocessing steps may include normalizing query text, handling missing values, and transforming categorical variables into numerical formats suitable for analysis. Researchers can address various research questions using this dataset, such as identifying patterns in consumer search behavior, evaluating the effectiveness of different search algorithms, and developing predictive models for user intent. The types of analyses supported by the JD-pretrain-data include regression analysis, machine learning model training, and descriptive analytics to summarize user behavior. Typically, researchers leverage this dataset to enhance search algorithms, improve user experience on e-commerce platforms, and contribute to the broader understanding of consumer behavior in digital marketplaces."
  },
  {
    "name": "Alibaba Clickstream 2018",
    "description": "Clickstream data from Alibaba platforms (2018)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/56",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "clickstream",
      "Alibaba",
      "user behavior"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Clickstream 2018 dataset contains clickstream data from various Alibaba platforms, capturing user interactions and behaviors throughout the year 2018. Researchers can utilize this dataset to analyze consumer behavior patterns, optimize marketing strategies, and enhance user experience on e-commerce platforms.",
    "use_cases": [
      "Analyzing user navigation paths on Alibaba platforms",
      "Identifying trends in consumer purchasing behavior",
      "Optimizing product placement based on user interactions",
      "Evaluating the effectiveness of marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Clickstream 2018 dataset?",
      "How can I access clickstream data from Alibaba?",
      "What insights can be gained from Alibaba's user behavior data?",
      "What variables are included in the Alibaba Clickstream dataset?",
      "How does clickstream data inform e-commerce strategies?",
      "What are the common analyses performed on Alibaba's clickstream data?",
      "What year does the Alibaba Clickstream dataset cover?",
      "What are the key features of the Alibaba Clickstream 2018 dataset?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Alibaba Clickstream 2018 dataset is a rich source of clickstream data that captures user interactions across various Alibaba platforms throughout the year 2018. This dataset is structured in a tabular format, consisting of numerous rows and columns that represent different user interactions, including clicks, page views, and timestamps. Each row corresponds to an individual user session, while the columns contain key variables such as user ID, session ID, product ID, time spent on each page, and the sequence of clicks made by the user. The collection methodology for this dataset involves tracking user behavior through cookies and session identifiers, ensuring that a comprehensive view of user interactions is captured. However, it is important to note that while the dataset provides valuable insights into user behavior, it may also have limitations regarding data quality, such as incomplete sessions or missing values due to users navigating away from the platform before completing their interactions. Researchers typically preprocess this data to handle missing values, normalize timestamps, and aggregate user interactions to derive meaningful insights. Common preprocessing steps may include filtering out incomplete sessions, aggregating click data by user or session, and transforming categorical variables into numerical formats for analysis. The dataset supports a variety of analyses, including regression analysis to identify factors influencing purchasing decisions, machine learning techniques for predictive modeling, and descriptive statistics to summarize user behavior trends. Researchers can leverage this dataset to address several research questions, such as understanding the impact of user navigation patterns on conversion rates, identifying the most popular products based on click frequency, and evaluating the effectiveness of targeted marketing campaigns. Overall, the Alibaba Clickstream 2018 dataset serves as a vital resource for researchers and practitioners in the e-commerce domain, providing insights that can drive strategic decision-making and enhance user engagement on digital platforms."
  },
  {
    "name": "DB1B Airline Origin and Destination Survey",
    "description": "10% random sample of all US airline tickets with origin, destination, fare, and itinerary details. Quarterly since 1993. The gold standard for airline pricing research.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.transtats.bts.gov/DatabaseInfo.asp?QO_VQ=EFI",
    "docs_url": "https://www.transtats.bts.gov/Fields.asp?gnoession_VQ=FHK",
    "github_url": null,
    "tags": [
      "airlines",
      "fares",
      "routes",
      "BTS",
      "pricing"
    ],
    "best_for": "Airline pricing research, competition analysis, and route economics",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "economics",
      "data-analysis"
    ],
    "summary": "The DB1B Airline Origin and Destination Survey provides a 10% random sample of all US airline tickets, including details such as origin, destination, fare, and itinerary. This dataset is invaluable for researchers and analysts studying airline pricing trends and consumer behavior in the aviation sector.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the DB1B Airline Origin and Destination Survey?",
      "How can I access the DB1B Airline Origin and Destination dataset?",
      "What variables are included in the DB1B Airline dataset?",
      "What research can be conducted using the DB1B Airline dataset?",
      "How does the DB1B dataset support airline pricing analysis?",
      "What are the limitations of the DB1B Airline Origin and Destination Survey?",
      "How is the DB1B dataset structured?",
      "What time period does the DB1B dataset cover?"
    ],
    "use_cases": [
      "Analyzing fare trends over time to identify pricing strategies.",
      "Studying the impact of route changes on consumer behavior.",
      "Evaluating the effects of economic factors on airline ticket prices.",
      "Conducting regression analysis to predict future fare changes."
    ],
    "domain_tags": [
      "transportation",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "Quarterly since 1993",
    "geographic_scope": "United States",
    "size_category": "medium",
    "benchmark_usage": [
      "Airline pricing research",
      "Consumer behavior analysis"
    ],
    "model_score": 0.0002,
    "embedding_text": "The DB1B Airline Origin and Destination Survey is a comprehensive dataset that offers a 10% random sample of all airline tickets sold in the United States. This dataset includes critical variables such as origin and destination airports, fare amounts, and itinerary details, making it a gold standard for airline pricing research. The structure of the data is tabular, with rows representing individual airline tickets and columns capturing essential attributes such as departure and arrival locations, ticket prices, and travel itineraries. Researchers and analysts can leverage this dataset to explore various dimensions of airline economics, including fare trends, route profitability, and consumer purchasing patterns. The data is collected quarterly and has been consistently available since 1993, providing a rich temporal coverage that allows for longitudinal studies on pricing strategies and market dynamics in the airline industry. The geographic scope is specifically focused on the United States, making it particularly relevant for researchers interested in domestic air travel. Key variables in the dataset measure aspects such as ticket prices, travel routes, and passenger demographics, which can be instrumental in addressing research questions related to pricing elasticity, market competition, and consumer behavior. However, users should be aware of potential limitations in data quality, such as missing values or inconsistencies in fare reporting, which may require common preprocessing steps like data cleaning and normalization. The DB1B dataset supports a variety of analytical approaches, including regression analysis, machine learning, and descriptive statistics, enabling researchers to derive insights into the factors influencing airline pricing and consumer choices. Overall, this dataset is a vital resource for anyone looking to conduct in-depth analysis in the field of transportation economics and technology."
  },
  {
    "name": "LastFM-1B",
    "description": "1 billion listening events with long-term user histories. Music recommendation and listening behavior research",
    "category": "Entertainment & Media",
    "url": "http://www.cp.jku.at/datasets/LFM-1b/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "music",
      "listening",
      "recommendations",
      "large-scale",
      "LastFM"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "music",
      "listening behavior",
      "recommendations"
    ],
    "summary": "The LastFM-1B dataset consists of 1 billion listening events that capture long-term user histories, making it a valuable resource for research in music recommendation systems and listening behavior analysis. Researchers can utilize this dataset to explore user preferences, develop recommendation algorithms, and study trends in music consumption.",
    "use_cases": [
      "Developing music recommendation algorithms based on user listening history",
      "Analyzing trends in music consumption over time",
      "Studying the impact of user demographics on music preferences",
      "Exploring the relationship between listening behavior and social interactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the LastFM-1B dataset?",
      "How can I access the LastFM-1B dataset for music recommendation research?",
      "What types of analyses can be performed using the LastFM-1B dataset?",
      "What are the key variables in the LastFM-1B dataset?",
      "How does the LastFM-1B dataset support music listening behavior studies?",
      "What data collection methods were used for the LastFM-1B dataset?",
      "What are the limitations of the LastFM-1B dataset?",
      "What preprocessing steps are needed for the LastFM-1B dataset?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/logos/jku.png",
    "embedding_text": "The LastFM-1B dataset is a comprehensive collection of 1 billion listening events that represent long-term user histories in the realm of music consumption. This dataset is structured in a tabular format, where each row corresponds to a unique listening event, and the columns capture various attributes such as user ID, track ID, artist ID, timestamp, and potentially other metadata related to the listening experience. The sheer volume of data allows researchers to delve into intricate patterns of user behavior, making it an invaluable resource for those studying music recommendation systems and listening behavior. The data was collected from the Last.fm platform, which aggregates user interactions with music tracks, providing a rich source of information on how users engage with music over time. The methodology likely involved tracking user interactions through the Last.fm API, which captures listening events as users play tracks, providing a longitudinal view of their music preferences. While the dataset is extensive, it is important to acknowledge potential limitations in data quality, such as missing values, biases in user engagement, and the representativeness of the sample population. Researchers may need to perform common preprocessing steps, including data cleaning, normalization, and transformation, to prepare the dataset for analysis. The LastFM-1B dataset supports a variety of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers can explore questions such as how user demographics influence listening behavior, the effectiveness of different recommendation algorithms, and the evolution of music trends over time. By leveraging this dataset, researchers can contribute to the development of more accurate and personalized music recommendation systems, ultimately enhancing the user experience in digital music platforms."
  },
  {
    "name": "YouTube Engagement Dataset",
    "description": "5M videos with watch percentage, engagement maps, Freebase topic labels. Video-level engagement metrics for content research",
    "category": "Entertainment & Media",
    "url": "https://github.com/avalanchesiqi/youtube-engagement",
    "docs_url": null,
    "github_url": "https://github.com/avalanchesiqi/youtube-engagement",
    "tags": [
      "YouTube",
      "engagement",
      "video",
      "watch time",
      "topics"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The YouTube Engagement Dataset contains data on 5 million videos, including metrics such as watch percentage and engagement maps, along with Freebase topic labels. This dataset is valuable for content research, enabling users to analyze video engagement and understand viewer behavior.",
    "use_cases": [
      "Analyzing viewer engagement trends over time",
      "Comparing engagement metrics across different content categories",
      "Identifying factors that influence watch time on YouTube videos"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the engagement metrics for YouTube videos?",
      "How can I analyze watch time for YouTube content?",
      "What topics are associated with high engagement on YouTube?",
      "What is the average watch percentage for videos in this dataset?",
      "How do engagement maps vary across different video categories?",
      "What insights can be gained from analyzing 5 million YouTube videos?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/datasets/youtube-engagement-dataset.png",
    "embedding_text": "The YouTube Engagement Dataset is an extensive collection of data encompassing 5 million videos, providing a rich resource for researchers and analysts interested in video content engagement. The dataset includes various metrics such as watch percentage, which indicates the proportion of a video that viewers watch, and engagement maps that visually represent how users interact with the content over time. Additionally, the dataset features Freebase topic labels, which categorize videos into specific topics, facilitating targeted analysis of content performance across different genres. The structure of the dataset is tabular, with rows representing individual videos and columns containing key variables such as video ID, watch percentage, engagement metrics, and topic labels. This schema allows for straightforward data manipulation and analysis using tools like pandas in Python. The collection methodology for this dataset likely involves scraping data from the YouTube platform or utilizing YouTube's API, ensuring a comprehensive capture of engagement metrics across a vast array of videos. However, specific details regarding the data collection process are not provided, which may limit the understanding of potential biases or gaps in the dataset. Coverage in terms of temporal or geographic dimensions is not explicitly mentioned, indicating that the dataset may encompass a wide range of videos without specific time frames or regional focus. The key variables in the dataset measure various aspects of viewer engagement, such as how long viewers stay engaged with a video and how interaction varies based on the topic of the video. Researchers may encounter limitations in data quality, particularly if the dataset includes videos with low view counts or engagement metrics that do not accurately reflect viewer behavior. Common preprocessing steps may include cleaning the data to remove duplicates, handling missing values, and normalizing engagement metrics for comparative analysis. The dataset supports a variety of research questions, such as identifying trends in viewer engagement, exploring the relationship between video topics and engagement levels, and assessing the impact of video length on watch percentage. Analysts can employ various types of analyses, including regression analysis to predict engagement based on video characteristics, machine learning techniques to classify videos by engagement potential, and descriptive statistics to summarize overall trends in the data. Researchers typically use this dataset to gain insights into content performance on YouTube, informing strategies for content creation and marketing in the digital media landscape."
  },
  {
    "name": "MobilityData GTFS Catalog",
    "description": "Curated directory of 1,327+ GTFS feeds from transit agencies globally with quality metrics, update frequency, and standardized metadata.",
    "category": "Transportation Economics & Technology",
    "url": "https://database.mobilitydata.org/",
    "docs_url": "https://mobilitydata.org/",
    "github_url": null,
    "tags": [
      "transit",
      "GTFS",
      "open-data",
      "schedules",
      "catalog"
    ],
    "best_for": "Finding and comparing transit data quality across cities and agencies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban-planning",
      "data-analysis"
    ],
    "summary": "The MobilityData GTFS Catalog is a comprehensive directory that aggregates over 1,327 General Transit Feed Specification (GTFS) feeds from transit agencies around the world. This dataset provides essential quality metrics, update frequency, and standardized metadata, enabling researchers and developers to analyze public transportation data effectively.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the MobilityData GTFS Catalog?",
      "How many GTFS feeds are included in the MobilityData catalog?",
      "What quality metrics are available in the MobilityData GTFS Catalog?",
      "How frequently is the MobilityData GTFS Catalog updated?",
      "What types of data are included in GTFS feeds?",
      "How can I access the MobilityData GTFS Catalog?",
      "What are the benefits of using GTFS data for transportation analysis?",
      "What is the significance of standardized metadata in transit data?"
    ],
    "use_cases": [
      "Analyzing public transportation accessibility in urban areas",
      "Evaluating the performance of transit agencies based on quality metrics",
      "Developing applications that utilize real-time transit data",
      "Conducting research on the impact of transit schedules on commuter behavior"
    ],
    "domain_tags": [
      "transportation",
      "urban-planning",
      "data-science"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/mobilitydata.png",
    "embedding_text": "The MobilityData GTFS Catalog serves as a vital resource for researchers, developers, and policymakers interested in public transportation systems. This dataset is structured in a tabular format, containing rows that represent individual GTFS feeds from various transit agencies and columns that detail key attributes such as agency name, feed URL, quality metrics, and update frequency. The catalog aggregates over 1,327 feeds, making it one of the largest collections of GTFS data available. Each feed typically includes data on transit schedules, routes, stops, and other operational details, which are standardized to facilitate analysis and comparison across different regions and agencies. The collection methodology involves curating feeds from transit agencies globally, ensuring that the data is up-to-date and relevant. Researchers can utilize this dataset to explore various research questions, such as assessing the reliability of public transportation, understanding the impact of transit availability on urban mobility, and evaluating the effectiveness of transit policies. The dataset supports a range of analyses, including descriptive statistics, regression modeling, and machine learning applications, enabling users to derive insights from the data. However, users should be aware of potential limitations, such as variations in data quality across different agencies and the need for preprocessing steps to clean and standardize the data before analysis. Overall, the MobilityData GTFS Catalog is an essential tool for anyone looking to leverage public transit data for research or application development."
  },
  {
    "name": "EM-DAT International Disaster Database",
    "description": "Global database of 26K+ natural and technological disasters since 1900 with human and economic impact data",
    "category": "Insurance & Actuarial",
    "url": "https://www.emdat.be/",
    "docs_url": "https://doc.emdat.be/",
    "github_url": null,
    "tags": [
      "disasters",
      "catastrophe",
      "global-data",
      "economic-losses",
      "humanitarian"
    ],
    "best_for": "Global catastrophe analysis, reinsurance pricing, and disaster trend research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The EM-DAT International Disaster Database is a comprehensive global repository that catalogs over 26,000 natural and technological disasters dating back to 1900. It provides valuable data on the human and economic impacts of these events, making it a crucial resource for researchers and policymakers interested in disaster management and economic analysis.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the EM-DAT International Disaster Database?",
      "How can I access the EM-DAT disaster data?",
      "What types of disasters are included in the EM-DAT database?",
      "What economic impacts are measured in the EM-DAT database?",
      "How has the frequency of disasters changed over time?",
      "What are the human impacts of disasters recorded in EM-DAT?"
    ],
    "use_cases": [
      "Analyzing trends in natural disasters over the last century.",
      "Assessing the economic impact of specific catastrophic events.",
      "Evaluating the effectiveness of disaster response strategies.",
      "Conducting risk assessments for insurance and actuarial purposes."
    ],
    "domain_tags": [
      "insurance",
      "humanitarian",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/emdat.png",
    "embedding_text": "The EM-DAT International Disaster Database is a vital resource for understanding the impacts of natural and technological disasters worldwide. It contains a structured dataset comprising over 26,000 records of disasters that have occurred since 1900, making it one of the most comprehensive databases available for this purpose. The data is organized in a tabular format, with each row representing a distinct disaster event and columns detailing various attributes such as disaster type, date, location, human casualties, and economic losses. This structure allows for easy manipulation and analysis using data analysis tools and programming languages such as Python and R. The collection methodology for the EM-DAT database involves gathering data from multiple reputable sources, including national governments, international organizations, and academic institutions. This multi-source approach enhances the reliability of the data, although researchers should be aware of potential limitations, such as variations in reporting standards across different countries and time periods. Key variables in the dataset include the type of disaster (e.g., earthquake, flood, technological accident), the date of occurrence, the geographic location, the number of fatalities, and the estimated economic losses. These variables are crucial for conducting analyses that seek to understand the frequency and severity of disasters, as well as their socio-economic impacts. Data quality is generally high, but users should be cautious of gaps in data for certain regions or periods, which may affect the comprehensiveness of analyses. Common preprocessing steps for utilizing the EM-DAT database may include cleaning the data to handle missing values, normalizing economic loss figures to account for inflation, and categorizing disasters into broader types for comparative analysis. Researchers typically use this dataset to address a range of questions, such as how disaster frequency correlates with climate change, the socio-economic factors that influence disaster vulnerability, and the effectiveness of disaster preparedness measures. The dataset supports various types of analyses, including descriptive statistics, regression modeling, and machine learning applications, making it a versatile tool for academics, policymakers, and practitioners in the fields of disaster management, economics, and humanitarian aid. By leveraging the insights derived from the EM-DAT database, stakeholders can make informed decisions aimed at mitigating the impacts of future disasters and improving resilience in affected communities."
  },
  {
    "name": "Indonesian Fashion",
    "description": "Fashion items for image classification tasks from Indonesia",
    "category": "Fashion & Apparel",
    "url": "https://www.kaggle.com/datasets/latifahhukma/fashion-campus",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fashion",
      "image classification",
      "Indonesia"
    ],
    "best_for": "Learning fashion & apparel analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Indonesian Fashion dataset contains a collection of fashion items specifically curated for image classification tasks. This dataset can be utilized to train machine learning models for recognizing and categorizing various fashion items from Indonesia, enabling applications in e-commerce and fashion analytics.",
    "use_cases": [
      "Training image classification models",
      "Analyzing fashion trends in Indonesia"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Indonesian fashion image dataset",
      "fashion items for image classification Indonesia",
      "fashion image classification dataset",
      "fashion dataset for machine learning",
      "Indonesia fashion items dataset",
      "image classification fashion Indonesia"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "image",
    "geographic_scope": "Indonesia",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/indonesian-fashion.png",
    "embedding_text": "The Indonesian Fashion dataset is a rich collection of images representing various fashion items sourced from Indonesia, specifically designed for image classification tasks. This dataset is structured in a way that allows for easy integration into machine learning workflows, with each image likely accompanied by metadata that describes the item, such as category, style, and possibly other attributes that are relevant for classification purposes. The data structure typically includes rows representing individual fashion items, with columns that may include variables such as image file paths, item descriptions, and labels for classification. The collection methodology for this dataset may involve curating images from online sources, fashion shows, or local retailers, ensuring a diverse representation of fashion styles prevalent in Indonesia. However, the exact methodology and data sources are not explicitly detailed, which may pose challenges in terms of reproducibility and validation of the dataset. The dataset's coverage is geographically focused on Indonesia, reflecting the local fashion scene, but it does not specify any temporal coverage, meaning it may not capture trends over time or seasonal variations in fashion. Key variables in this dataset include the images themselves, which serve as the primary data points for classification, along with any associated labels that indicate the category of each fashion item. The quality of the dataset can vary based on the sources from which the images were collected; thus, researchers should be aware of potential limitations such as image resolution, consistency in labeling, and the presence of noise in the data. Common preprocessing steps may include resizing images, normalizing pixel values, and augmenting the dataset to improve model robustness. Researchers can leverage this dataset to address various research questions related to fashion recognition, consumer preferences, and the impact of cultural influences on fashion choices. The types of analyses supported by this dataset range from supervised machine learning tasks, such as training convolutional neural networks for image classification, to exploratory data analysis aimed at understanding fashion trends and consumer behavior in the Indonesian market. Overall, the Indonesian Fashion dataset serves as a valuable resource for both academic and commercial applications in the fashion industry, providing insights into the vibrant and diverse fashion landscape of Indonesia."
  },
  {
    "name": "HateXplain",
    "description": "20K social media posts with human rationales across 10 hate speech target categories. Explainable AI for content moderation",
    "category": "Content Moderation",
    "url": "https://github.com/hate-alert/HateXplain",
    "docs_url": null,
    "github_url": "https://github.com/hate-alert/HateXplain",
    "tags": [
      "hate speech",
      "explainability",
      "NLP",
      "content moderation",
      "annotations"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "NLP",
      "content moderation",
      "hate speech"
    ],
    "summary": "HateXplain is a dataset consisting of 20,000 social media posts annotated with human rationales across 10 categories of hate speech. It serves as a resource for developing explainable AI models aimed at improving content moderation practices in online platforms.",
    "use_cases": [
      "Developing machine learning models for hate speech detection",
      "Analyzing the effectiveness of content moderation strategies",
      "Studying human rationales behind hate speech classification",
      "Exploring the relationship between hate speech and social media engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the HateXplain dataset?",
      "How can I use HateXplain for hate speech analysis?",
      "What are the categories of hate speech in HateXplain?",
      "What kind of AI models can be built using HateXplain?",
      "How does HateXplain support explainable AI?",
      "What are the key features of the HateXplain dataset?",
      "What research questions can be addressed using HateXplain?",
      "How can HateXplain improve content moderation?"
    ],
    "domain_tags": [
      "social media",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/hatexplain.png",
    "embedding_text": "The HateXplain dataset is a comprehensive collection of 20,000 social media posts that have been meticulously annotated to provide insights into hate speech across ten distinct target categories. This dataset is particularly valuable for researchers and practitioners in the fields of natural language processing (NLP) and content moderation, as it not only includes the raw text of the posts but also human rationales that explain the classification decisions made regarding each post. The data structure consists of rows representing individual posts, with columns detailing the text content, the assigned hate speech category, and the corresponding human rationale. The key variables in this dataset measure various aspects of hate speech, including the type of hate speech, the context in which it occurs, and the rationale behind its classification. These variables enable researchers to conduct detailed analyses of hate speech dynamics on social media platforms. The collection methodology involved sourcing posts from various social media platforms, ensuring a diverse representation of hate speech instances. However, researchers should be aware of potential limitations regarding data quality, such as the subjective nature of human annotations and the evolving definitions of hate speech over time. Common preprocessing steps for utilizing this dataset may include text normalization, tokenization, and the removal of irrelevant content. Researchers can leverage HateXplain to address a variety of research questions, such as examining the prevalence of different types of hate speech, understanding the motivations behind hate speech, and evaluating the effectiveness of AI models in detecting and moderating harmful content. The dataset supports various types of analyses, including regression analysis, machine learning model development, and descriptive statistics, making it a versatile tool for advancing the understanding of hate speech in digital communication. Overall, HateXplain serves as a crucial resource for enhancing the capabilities of explainable AI in content moderation, providing insights that can lead to more effective strategies for managing hate speech on social media."
  },
  {
    "name": "MEPS (Medical Expenditure Panel Survey)",
    "description": "Nationally representative survey of healthcare utilization, expenditures, insurance coverage, and health status for the US civilian population",
    "category": "Insurance & Actuarial",
    "url": "https://meps.ahrq.gov/mepsweb/",
    "docs_url": "https://meps.ahrq.gov/mepsweb/data_stats/download_data_files.jsp",
    "github_url": null,
    "tags": [
      "health-insurance",
      "healthcare-costs",
      "expenditure-data",
      "survey-data",
      "panel-data"
    ],
    "best_for": "Health insurance research, healthcare cost modeling, and demand analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Medical Expenditure Panel Survey (MEPS) is a comprehensive dataset that provides insights into healthcare utilization, expenditures, insurance coverage, and health status among the US civilian population. Researchers can leverage this data to analyze healthcare costs, assess insurance impacts, and evaluate health outcomes across various demographics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Medical Expenditure Panel Survey?",
      "How can I access MEPS data?",
      "What variables are included in the MEPS dataset?",
      "What insights can be derived from healthcare expenditure data?",
      "How does MEPS measure health insurance coverage?",
      "What demographic information is available in MEPS?",
      "How can MEPS data be used for healthcare cost analysis?"
    ],
    "use_cases": [
      "Analyzing trends in healthcare expenditures over time",
      "Evaluating the impact of insurance coverage on health outcomes",
      "Comparing healthcare utilization among different demographic groups"
    ],
    "domain_tags": [
      "healthcare",
      "insurance"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/ahrq.png",
    "embedding_text": "The Medical Expenditure Panel Survey (MEPS) is a nationally representative survey designed to collect detailed information on healthcare utilization, expenditures, insurance coverage, and health status for the civilian population of the United States. This dataset is structured in a tabular format, comprising various rows and columns that represent individual respondents and their respective healthcare-related data. Key variables within the dataset include healthcare expenditures, types of insurance coverage, frequency of healthcare utilization, and demographic characteristics such as age, sex, race, and income level. The MEPS data is collected through a combination of interviews and surveys, which ensure a comprehensive understanding of the healthcare landscape in the U.S. The survey methodology involves multiple rounds of data collection over a period of time, allowing researchers to analyze trends and changes in healthcare behavior and costs. While MEPS provides valuable insights, it is important to acknowledge certain limitations, such as potential biases in self-reported data and variations in response rates across different demographic groups. Common preprocessing steps for MEPS data may include cleaning the dataset to handle missing values, normalizing expenditures for inflation, and categorizing variables for easier analysis. Researchers typically use MEPS data to address a variety of research questions, such as examining the relationship between insurance coverage and healthcare access, assessing the financial burden of healthcare costs on households, and evaluating the effectiveness of health policies. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for scholars and practitioners in the fields of health economics and public health. Overall, MEPS serves as a critical tool for understanding the dynamics of healthcare in the United States, providing a foundation for evidence-based policy-making and research."
  },
  {
    "name": "Revelio Labs COSMOS",
    "description": "4.1B job postings from 6.6M companies. Deduplicated, parsed, enriched workforce data (commercial/academic partnerships)",
    "category": "Labor Markets",
    "url": "https://www.reveliolabs.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "job postings",
      "workforce",
      "companies",
      "labor",
      "commercial"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Revelio Labs COSMOS dataset comprises 4.1 billion job postings sourced from 6.6 million companies, providing a comprehensive view of the labor market. This enriched workforce data allows researchers and analysts to explore trends in employment, company hiring practices, and the dynamics of the labor market across various sectors.",
    "use_cases": [
      "Analyzing trends in job postings over time to identify labor market shifts.",
      "Comparing hiring practices across different industries to understand workforce dynamics.",
      "Investigating the relationship between job postings and economic indicators.",
      "Exploring skills demand in the labor market through job posting analysis."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest job postings from various companies?",
      "How does workforce data vary across different industries?",
      "What trends can be identified in job postings over time?",
      "Which companies are hiring the most in the current labor market?",
      "How do job postings relate to economic conditions?",
      "What skills are most frequently mentioned in job postings?",
      "How can I analyze labor market trends using job posting data?",
      "What demographic insights can be drawn from workforce data?"
    ],
    "domain_tags": [
      "labor",
      "workforce"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/revelio-labs-cosmos.png",
    "embedding_text": "The Revelio Labs COSMOS dataset is a rich repository of workforce data, encompassing an extensive collection of 4.1 billion job postings from 6.6 million companies. This dataset is structured in a tabular format, with rows representing individual job postings and columns containing various attributes such as job title, company name, location, posting date, and required skills. The data is deduplicated and parsed, ensuring that researchers can rely on its accuracy and relevance. The collection methodology involves aggregating job postings from multiple online platforms and company websites, which allows for a comprehensive view of the labor market landscape. However, users should be aware of potential limitations, such as variations in job posting formats and the possibility of missing data for certain companies or job types. Key variables in the dataset include job titles, which measure the demand for specific roles; company names, which provide insights into hiring trends; and posting dates, which allow for temporal analysis of job market fluctuations. Common preprocessing steps may involve cleaning the data to standardize job titles and company names, as well as filtering postings based on specific criteria such as location or industry. Researchers can leverage this dataset to address a variety of research questions, such as identifying the most in-demand skills in the labor market, analyzing hiring trends across different sectors, and exploring the impact of economic conditions on job postings. The dataset supports various types of analyses, including regression analysis to predict hiring trends, machine learning models to classify job postings, and descriptive statistics to summarize workforce characteristics. Overall, the Revelio Labs COSMOS dataset serves as a valuable resource for researchers, policymakers, and analysts seeking to understand the complexities of the labor market and make informed decisions based on empirical evidence."
  },
  {
    "name": "Social Blade",
    "description": "Public subscriber/follower counts and growth metrics across YouTube, Twitch, Instagram, Twitter, TikTok",
    "category": "Creator Economy",
    "url": "https://socialblade.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "YouTube",
      "Twitch",
      "Instagram",
      "followers",
      "growth metrics"
    ],
    "best_for": "Learning creator economy analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "creator-economy",
      "social-media",
      "analytics"
    ],
    "summary": "The Social Blade dataset provides public subscriber and follower counts along with growth metrics for various social media platforms such as YouTube, Twitch, Instagram, Twitter, and TikTok. Researchers and analysts can utilize this data to track the performance of content creators, analyze trends in follower growth, and understand audience engagement across different platforms.",
    "use_cases": [
      "Analyzing the growth trends of social media influencers over time.",
      "Comparing follower counts across different platforms to identify the most effective channels.",
      "Investigating the impact of content strategies on subscriber growth.",
      "Examining audience engagement metrics in relation to follower counts."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the subscriber counts for popular YouTube channels?",
      "How do follower growth metrics vary across different social media platforms?",
      "What trends can be observed in Twitch follower counts over the past year?",
      "How does Instagram engagement compare to TikTok follower growth?",
      "What are the average growth rates for social media influencers?",
      "How can I analyze the performance of a specific content creator on Twitter?",
      "What metrics are available for measuring growth on social media platforms?",
      "How does follower count impact engagement rates on YouTube?"
    ],
    "domain_tags": [
      "social-media",
      "digital-marketing",
      "analytics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Social Blade dataset is a comprehensive collection of public subscriber and follower counts along with growth metrics across various social media platforms, including YouTube, Twitch, Instagram, Twitter, and TikTok. This dataset is structured in a tabular format, where each row typically represents a unique content creator or channel, and the columns contain key variables such as subscriber counts, follower counts, growth rates, and engagement metrics. The data is collected from publicly available information on the respective platforms, ensuring that it reflects real-time statistics that can be accessed by anyone interested in analyzing social media trends. The coverage of this dataset spans multiple social media platforms, allowing for comparative analysis across different channels. However, it does not specify temporal or geographic coverage explicitly, which may limit certain types of longitudinal studies or regional analyses. Key variables within the dataset include subscriber counts, which measure the total number of subscribers for a YouTube channel or followers for other platforms, and growth metrics that indicate how quickly these numbers are changing over time. While the dataset provides valuable insights, researchers should be aware of potential limitations regarding data quality, as fluctuations in follower counts can occur due to various factors such as bot accounts or platform algorithm changes. Common preprocessing steps may include cleaning the data to remove any inconsistencies, normalizing growth metrics to allow for fair comparisons, and possibly aggregating data over specific time frames to analyze trends effectively. This dataset supports a variety of research questions, such as how different content strategies impact follower growth, the relationship between engagement rates and subscriber counts, and the overall performance of influencers across platforms. Analysts can employ various types of analyses, including regression analysis to identify predictors of growth, machine learning techniques to classify content strategies, and descriptive statistics to summarize engagement metrics. Researchers typically use this dataset in studies focused on the creator economy, social media marketing strategies, and audience behavior, making it a valuable resource for anyone looking to understand the dynamics of social media influence."
  },
  {
    "name": "NREL NSRDB (National Solar Radiation Database)",
    "description": "High-resolution solar irradiance data covering the Americas with 30-minute temporal resolution",
    "category": "Energy",
    "url": "https://nsrdb.nrel.gov/",
    "docs_url": "https://nsrdb.nrel.gov/data-sets/api-instructions.html",
    "github_url": null,
    "tags": [
      "solar",
      "irradiance",
      "renewable",
      "weather",
      "API"
    ],
    "best_for": "Solar energy potential assessment and renewable energy forecasting",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NREL NSRDB provides high-resolution solar irradiance data specifically designed for the Americas, featuring a temporal resolution of 30 minutes. This dataset allows users to analyze solar energy potential, assess renewable energy resources, and study weather patterns related to solar irradiance.",
    "use_cases": [
      "Analyzing solar energy potential for specific locations",
      "Assessing the impact of solar irradiance on renewable energy systems",
      "Studying weather patterns and their relationship with solar energy generation",
      "Conducting regression analysis to predict solar energy output"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NREL NSRDB?",
      "How can I access solar irradiance data for the Americas?",
      "What is the temporal resolution of the NREL NSRDB?",
      "What types of analyses can be performed with solar irradiance data?",
      "Where can I find renewable energy datasets?",
      "What are the key variables in the NREL NSRDB?",
      "How is solar irradiance measured in the NREL NSRDB?",
      "What are common use cases for the NREL NSRDB?"
    ],
    "domain_tags": [
      "energy",
      "renewable"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "1998-present",
    "geographic_scope": "Americas",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/nrel-nsrdb-national-solar-radiation-database.png",
    "embedding_text": "The NREL NSRDB (National Solar Radiation Database) is a comprehensive dataset that provides high-resolution solar irradiance data, specifically tailored for the Americas. This dataset features a temporal resolution of 30 minutes, allowing for detailed analysis of solar energy potential across various geographic locations. The data structure typically includes rows representing time intervals and columns that capture key variables such as solar irradiance measurements in watts per square meter, geographic coordinates, and timestamps. The collection methodology involves rigorous data gathering from a network of solar radiation monitoring stations, ensuring high accuracy and reliability. While the dataset covers a vast geographic scope, it is essential to note that it is primarily focused on the Americas, making it particularly useful for researchers and practitioners in this region. Key variables in the dataset measure solar irradiance, which is crucial for evaluating solar energy generation capabilities. However, users should be aware of potential limitations regarding data quality, such as gaps in data collection or variations in measurement techniques across different monitoring stations. Common preprocessing steps may include handling missing values, normalizing data, and aggregating measurements to align with specific research needs. Researchers can leverage the NREL NSRDB to address various research questions, such as assessing the feasibility of solar energy projects, understanding seasonal variations in solar irradiance, and modeling the impact of solar energy on grid systems. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for energy researchers and data scientists. Typically, researchers utilize this dataset in studies focused on renewable energy resource assessment, climate impact analysis, and the development of predictive models for solar energy generation."
  },
  {
    "name": "Spotify Million Playlist",
    "description": "1M playlists with 2M unique tracks from 300K artists. RecSys 2018 Challenge for playlist continuation research",
    "category": "Entertainment & Media",
    "url": "https://www.aicrowd.com/challenges/spotify-million-playlist-dataset-challenge",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Spotify",
      "playlists",
      "music",
      "recommendations",
      "RecSys"
    ],
    "best_for": "Learning entertainment & media analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "music",
      "recommendation-systems",
      "data-analysis"
    ],
    "summary": "The Spotify Million Playlist dataset consists of 1 million playlists featuring 2 million unique tracks from 300,000 artists. This dataset is primarily used for research in playlist continuation and recommendation systems, allowing researchers to explore music preferences and develop algorithms for personalized music recommendations.",
    "use_cases": [
      "Analyzing user preferences in music playlists",
      "Developing algorithms for music recommendation systems",
      "Studying the diversity of music across different playlists",
      "Investigating trends in music consumption over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Spotify Million Playlist dataset?",
      "How can I access the Spotify Million Playlist dataset?",
      "What types of analyses can be performed with the Spotify Million Playlist dataset?",
      "What are the unique tracks in the Spotify Million Playlist dataset?",
      "How many artists are represented in the Spotify Million Playlist dataset?",
      "What insights can be gained from analyzing playlists in the Spotify Million Playlist dataset?",
      "What research has been conducted using the Spotify Million Playlist dataset?",
      "How does the Spotify Million Playlist dataset contribute to recommendation systems?"
    ],
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/spotify-million-playlist.png",
    "embedding_text": "The Spotify Million Playlist dataset is a rich resource for researchers and practitioners in the fields of music information retrieval and recommendation systems. Comprising 1 million playlists, this dataset features a total of 2 million unique tracks sourced from a diverse array of 300,000 artists. The structure of the dataset is primarily tabular, with rows representing individual playlists and columns capturing various attributes such as playlist ID, track ID, artist ID, and potentially other metadata related to the playlists and tracks. This schema allows for straightforward data manipulation and analysis using common data processing tools such as pandas in Python. The collection methodology for this dataset involves aggregating playlists from the Spotify platform, which is known for its extensive music catalog and user-generated content. While the dataset does not explicitly mention temporal or geographic coverage, it provides a comprehensive snapshot of music playlists that can be analyzed for trends and patterns in music consumption. Key variables in the dataset include playlist identifiers, track identifiers, and artist identifiers, which facilitate the exploration of relationships between playlists and the music they contain. Researchers can leverage this dataset to address a variety of research questions, such as understanding user preferences, analyzing the diversity of music selections, and developing algorithms for personalized music recommendations. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for specific analytical needs. Despite its richness, researchers should be aware of potential limitations, such as biases in playlist selection or the representation of certain genres or artists. Overall, the Spotify Million Playlist dataset serves as a valuable tool for advancing research in music recommendations and understanding the dynamics of music consumption in the digital age."
  },
  {
    "name": "CFR Cyber Operations Tracker",
    "description": "Database of publicly known state-sponsored cyber incidents since 2005 with threat actor attribution",
    "category": "Cybersecurity",
    "url": "https://www.cfr.org/cyber-operations/",
    "docs_url": "https://www.cfr.org/cyber-operations/",
    "github_url": null,
    "tags": [
      "cyber attacks",
      "nation-state",
      "attribution",
      "incidents"
    ],
    "best_for": "Tracking state-sponsored cyber operations and attack patterns",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "cybersecurity",
      "cyber attacks",
      "nation-state"
    ],
    "summary": "The CFR Cyber Operations Tracker is a comprehensive database that catalogs publicly known state-sponsored cyber incidents since 2005, focusing on threat actor attribution. Researchers and analysts can utilize this dataset to study patterns in cyber attacks, understand the motivations behind state-sponsored cyber operations, and analyze the implications for national security.",
    "use_cases": [
      "Analyzing trends in state-sponsored cyber attacks over time.",
      "Examining the attribution of cyber incidents to specific nation-states.",
      "Studying the impact of cyber operations on international relations.",
      "Identifying common tactics and techniques used in cyber incidents."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the known state-sponsored cyber incidents since 2005?",
      "How can I analyze threat actor attribution in cyber attacks?",
      "What patterns exist in nation-state cyber operations?",
      "What types of cyber incidents are documented in the CFR Cyber Operations Tracker?",
      "How has the landscape of state-sponsored cyber attacks evolved over time?",
      "What data is available on attribution for cyber incidents?",
      "How can I access the CFR Cyber Operations Tracker dataset?",
      "What insights can be gained from analyzing state-sponsored cyber incidents?"
    ],
    "domain_tags": [
      "government",
      "national security",
      "cybersecurity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2005-present",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/cfr.png",
    "embedding_text": "The CFR Cyber Operations Tracker is an extensive database that provides a detailed account of publicly known state-sponsored cyber incidents from 2005 onwards. This dataset is structured in a tabular format, consisting of rows that represent individual cyber incidents and columns that capture various attributes of these incidents, including the date, type of attack, targeted entities, and the nation-state attributed as the perpetrator. The data is collected from a variety of reputable sources, including government reports, cybersecurity firms, and media outlets, ensuring a high level of reliability and accuracy. However, researchers should be aware of potential limitations in data quality, such as incomplete records or discrepancies in attribution due to the covert nature of cyber operations. Common preprocessing steps may include cleaning the data for consistency, handling missing values, and normalizing incident types for analysis. This dataset supports a range of research questions, such as identifying trends in cyber attacks, understanding the motivations behind these incidents, and analyzing the geopolitical implications of state-sponsored cyber operations. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics to extract insights from the data. Researchers typically use the CFR Cyber Operations Tracker to inform studies on cybersecurity policy, international relations, and the evolving landscape of cyber warfare, making it a valuable resource for both academic and practical applications in the field of cybersecurity."
  },
  {
    "name": "Instacart Market Basket Analysis",
    "description": "3 million grocery orders from 200,000 Instacart users with product details and order sequences. Released for a Kaggle competition to predict which products users will reorder.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kaggle.com/c/instacart-market-basket-analysis/data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "market-basket",
      "sequential",
      "grocery",
      "reorder-prediction"
    ],
    "best_for": "Learning market basket analysis, sequential recommendations, and next-basket prediction",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "basic-statistics"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "data-analysis"
    ],
    "summary": "The Instacart Market Basket Analysis dataset contains 3 million grocery orders from 200,000 users, detailing product information and order sequences. It is designed for predictive modeling, particularly to forecast which products users are likely to reorder based on their past purchasing behavior.",
    "use_cases": [
      "Predicting product reorders based on historical data",
      "Analyzing consumer purchasing patterns",
      "Segmenting users based on order behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Instacart Market Basket Analysis dataset?",
      "How can I predict grocery reorders using Instacart data?",
      "What insights can be gained from analyzing grocery order sequences?",
      "What are the key variables in the Instacart dataset?",
      "How many orders are included in the Instacart Market Basket Analysis?",
      "What types of analyses can be performed on grocery order data?",
      "What is the significance of market basket analysis in e-commerce?",
      "How do I access the Instacart dataset for analysis?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "embedding_text": "The Instacart Market Basket Analysis dataset is a comprehensive collection of grocery order data that encompasses 3 million transactions from 200,000 users. This dataset is structured in a tabular format, where each row represents an individual order, and the columns include variables such as user ID, product ID, order ID, and the sequence of products purchased. The primary objective of this dataset is to facilitate predictive modeling, particularly in understanding and forecasting consumer behavior regarding product reorders. Researchers and data scientists can leverage this dataset to explore various aspects of consumer purchasing patterns, enabling them to derive insights that can inform marketing strategies and inventory management. The collection methodology for this dataset involves aggregating real transaction data from Instacart's platform, which provides a rich source of information on consumer behavior in the grocery sector. However, it is important to note that while the dataset offers a vast amount of data, it may also come with limitations such as potential biases in user behavior or incomplete data for certain transactions. Common preprocessing steps that may be required include cleaning the data to handle missing values, transforming categorical variables into numerical formats, and normalizing the data for analysis. Researchers can utilize this dataset to address a variety of research questions, such as identifying which products are frequently purchased together, understanding seasonal trends in grocery shopping, and developing machine learning models to predict future purchases. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for those interested in the intersection of technology and consumer behavior in the retail space. Overall, the Instacart Market Basket Analysis dataset serves as a valuable resource for researchers aiming to gain deeper insights into the dynamics of grocery shopping and consumer preferences."
  },
  {
    "name": "HCUP (Healthcare Cost and Utilization Project)",
    "description": "Largest collection of longitudinal hospital care data in the US with 100+ million records per year covering inpatient and emergency visits",
    "category": "Insurance & Actuarial",
    "url": "https://hcup-us.ahrq.gov/",
    "docs_url": "https://hcup-us.ahrq.gov/databases.jsp",
    "github_url": null,
    "tags": [
      "hospital-data",
      "inpatient",
      "emergency",
      "healthcare-utilization",
      "cost-data"
    ],
    "best_for": "Hospital cost analysis, readmission prediction, and healthcare ML applications",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "cost-analysis",
      "utilization"
    ],
    "summary": "The HCUP dataset is the largest collection of longitudinal hospital care data in the United States, encompassing over 100 million records annually that detail inpatient and emergency visits. Researchers and analysts can utilize this comprehensive dataset to explore healthcare utilization patterns, analyze cost trends, and assess the quality of care across various demographics and regions.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the HCUP dataset?",
      "How can I access hospital care data in the US?",
      "What types of records are included in HCUP?",
      "How does HCUP data help in healthcare cost analysis?",
      "What are the key variables in the HCUP dataset?",
      "What research questions can be answered using HCUP data?",
      "What is the significance of the HCUP dataset in healthcare research?",
      "How can HCUP data be used to analyze emergency visits?"
    ],
    "use_cases": [
      "Analyzing trends in hospital admissions over time",
      "Comparing costs associated with inpatient and emergency visits",
      "Investigating demographic differences in healthcare utilization",
      "Assessing the impact of policy changes on hospital care"
    ],
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0002,
    "image_url": "/images/logos/ahrq.png",
    "embedding_text": "The Healthcare Cost and Utilization Project (HCUP) is a comprehensive dataset that provides an extensive collection of longitudinal hospital care data in the United States, featuring over 100 million records per year. This dataset encompasses a wide array of information related to inpatient and emergency visits, making it a valuable resource for researchers, policymakers, and healthcare analysts. The data structure of HCUP is organized in a tabular format, where each row represents a unique hospital visit, and the columns contain various variables that capture essential details about the visit. Key variables include patient demographics, diagnosis codes, procedure codes, length of stay, and total charges, among others. These variables allow for a detailed analysis of healthcare utilization patterns and cost trends across different populations and settings. HCUP data is collected from a variety of sources, including state hospital discharge databases, which are compiled and standardized to ensure consistency and reliability. The dataset is known for its robust coverage of hospital care in the US, although researchers should be aware of potential limitations regarding data completeness and accuracy, particularly in terms of coding practices and reporting standards across different states. Common preprocessing steps may involve cleaning the data to handle missing values, standardizing variable formats, and creating derived variables for analysis. Researchers typically use HCUP data to address a range of research questions, such as examining trends in hospital admissions, analyzing the cost implications of various treatments, and investigating disparities in healthcare access and outcomes. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for understanding the complexities of healthcare delivery in the US. Overall, HCUP serves as a critical resource for advancing knowledge in the field of healthcare economics and utilization, providing insights that can inform policy decisions and improve patient care.",
    "benchmark_usage": [
      "Healthcare cost analysis",
      "Utilization studies",
      "Policy impact assessments"
    ]
  },
  {
    "name": "DoD National Defense Budget Estimates (Green Book)",
    "description": "Detailed U.S. defense spending by program element, military department, and appropriation from FY1945 to present",
    "category": "Defense Economics",
    "url": "https://comptroller.defense.gov/Budget-Materials/",
    "docs_url": "https://comptroller.defense.gov/Portals/45/Documents/defbudget/fy2024/FY2024_Green_Book.pdf",
    "github_url": null,
    "tags": [
      "US defense",
      "budget",
      "Pentagon",
      "appropriations"
    ],
    "best_for": "Detailed analysis of U.S. military spending by program and service branch",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The DoD National Defense Budget Estimates (Green Book) provides detailed insights into U.S. defense spending across various program elements, military departments, and appropriations from FY1945 to the present. Researchers and analysts can utilize this dataset to explore trends in defense spending, analyze budget allocations, and assess the impact of fiscal policies on national defense.",
    "use_cases": [
      "Analyzing the impact of budget changes on military readiness",
      "Comparing defense spending across different administrations",
      "Evaluating the effectiveness of appropriations in achieving defense objectives"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical trends in U.S. defense spending?",
      "How is the U.S. defense budget allocated across different military departments?",
      "What are the key program elements in the DoD National Defense Budget Estimates?",
      "How have appropriations for defense changed over the years?",
      "What factors influence U.S. defense budget decisions?",
      "How does the Pentagon's budget compare to previous years?",
      "What is the significance of the Green Book in defense economics?"
    ],
    "domain_tags": [
      "defense",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "FY1945 to present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/defense.png",
    "embedding_text": "The DoD National Defense Budget Estimates, commonly referred to as the Green Book, is a comprehensive dataset that details U.S. defense spending from fiscal year 1945 to the present. This dataset is structured in a tabular format, consisting of rows that represent individual budget entries and columns that capture various attributes such as program elements, military departments, appropriations, and fiscal years. Each entry provides insights into how funds are allocated within the defense sector, allowing for a granular analysis of spending patterns over time. The data is collected through official Department of Defense reports and budget documents, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, such as changes in accounting practices over the decades, which may affect the comparability of data across different time periods. Key variables in the dataset include program elements that measure specific defense initiatives, military departments that categorize spending by branch, and appropriations that indicate the authorized budget for various defense activities. Common preprocessing steps may involve cleaning the data to handle missing values, standardizing formats for fiscal years, and aggregating data for higher-level analyses. This dataset supports a variety of research questions, including the evaluation of historical trends in defense spending, the analysis of budget allocations relative to national security objectives, and the assessment of the impact of economic conditions on defense appropriations. Researchers typically employ statistical analyses such as regression modeling, descriptive statistics, and machine learning techniques to derive insights from the data. The Green Book serves as a vital resource for policymakers, economists, and defense analysts, providing a foundation for informed decision-making in the realm of national defense economics."
  },
  {
    "name": "Telco Customer Churn (IBM)",
    "description": "7,043 customers from a telecommunications company with 21 features including demographics, services, account information, and churn status. Industry-standard dataset for churn prediction benchmarking.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kaggle.com/datasets/blastchar/telco-customer-churn",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "churn",
      "classification",
      "telecom",
      "customer-analytics"
    ],
    "best_for": "Learning churn prediction, classification algorithms, and retention analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "customer-churn",
      "data-analysis",
      "predictive-modeling"
    ],
    "summary": "The Telco Customer Churn dataset consists of 7,043 customers from a telecommunications company, featuring 21 attributes that encompass demographics, services, account information, and churn status. This dataset serves as an industry-standard benchmark for churn prediction, enabling analysts and data scientists to develop and evaluate classification models aimed at predicting customer retention.",
    "use_cases": [
      "Building predictive models to identify customers likely to churn.",
      "Analyzing the impact of various features on customer retention.",
      "Segmenting customers based on their likelihood to churn.",
      "Evaluating the effectiveness of marketing strategies aimed at reducing churn."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Telco Customer Churn dataset?",
      "How can I use the Telco Customer Churn dataset for classification?",
      "What features are included in the Telco Customer Churn dataset?",
      "Where can I find the Telco Customer Churn dataset?",
      "What are the common use cases for the Telco Customer Churn dataset?",
      "How does the Telco Customer Churn dataset help in customer analytics?",
      "What analysis can be performed on the Telco Customer Churn dataset?",
      "What industry standards does the Telco Customer Churn dataset follow?"
    ],
    "domain_tags": [
      "telecommunications"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/telco-customer-churn-ibm.jpg",
    "embedding_text": "The Telco Customer Churn dataset is a comprehensive collection of data gathered from 7,043 customers of a telecommunications company, specifically designed to facilitate the analysis and prediction of customer churn. This dataset comprises 21 distinct features, which include demographic information, services utilized by customers, account details, and the crucial churn status indicating whether a customer has left the service. The structured nature of this dataset, organized in a tabular format, allows for straightforward application of various data analysis techniques, including regression analysis and machine learning algorithms. Researchers and data scientists often leverage this dataset to build predictive models that can accurately forecast customer retention and identify factors contributing to churn. The dataset's schema includes variables such as customer age, gender, account length, service usage, and payment methods, each providing insights into customer behavior and preferences. The collection methodology for this dataset typically involves surveys and transactional data from the telecommunications provider, ensuring a rich source of information for analysis. However, like any dataset, it may have limitations regarding data quality, such as missing values or biases in customer representation. Common preprocessing steps include handling missing data, encoding categorical variables, and normalizing numerical features to prepare the dataset for analysis. The Telco Customer Churn dataset is instrumental in addressing several research questions, such as identifying key predictors of churn, understanding customer segments at risk of leaving, and evaluating the effectiveness of retention strategies. Analysts can perform a variety of analyses, ranging from descriptive statistics to advanced machine learning techniques, making this dataset a valuable resource for anyone interested in customer analytics within the telecommunications sector. By utilizing the Telco Customer Churn dataset, researchers can gain critical insights into customer behavior, ultimately aiding in the development of strategies to enhance customer satisfaction and loyalty.",
    "benchmark_usage": [
      "Churn prediction benchmarking"
    ]
  },
  {
    "name": "FHWA Highway Statistics",
    "description": "Annual data on US highway system including vehicle miles traveled, fuel consumption, road infrastructure, and highway financing since 1945.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.fhwa.dot.gov/policyinformation/statistics.cfm",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "highways",
      "VMT",
      "infrastructure",
      "federal",
      "fuel"
    ],
    "best_for": "Aggregate transportation trends, infrastructure analysis, and policy research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "infrastructure",
      "economics"
    ],
    "summary": "The FHWA Highway Statistics dataset provides annual data on the US highway system, including key metrics such as vehicle miles traveled (VMT), fuel consumption, road infrastructure, and highway financing. Researchers and analysts can use this dataset to study trends in transportation economics, assess infrastructure needs, and evaluate the impact of highway financing on road usage.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the annual vehicle miles traveled in the US?",
      "How has fuel consumption changed over the years in the US highway system?",
      "What is the state of road infrastructure in the US according to FHWA statistics?",
      "How does highway financing correlate with vehicle miles traveled?",
      "What trends can be observed in US highway statistics since 1945?",
      "How can I access the FHWA Highway Statistics dataset?",
      "What key variables are included in the FHWA Highway Statistics dataset?",
      "What methodologies are used to collect data for the FHWA Highway Statistics?"
    ],
    "use_cases": [
      "Analyzing trends in vehicle miles traveled over decades.",
      "Evaluating the relationship between fuel consumption and highway financing.",
      "Assessing the condition and investment needs of road infrastructure.",
      "Studying the economic impact of transportation policies on highway usage."
    ],
    "domain_tags": [
      "transportation",
      "infrastructure",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1945-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/logos/dot.png",
    "embedding_text": "The FHWA Highway Statistics dataset is a comprehensive collection of annual data pertaining to the United States highway system, encompassing a wide range of variables crucial for understanding transportation dynamics. This dataset includes key metrics such as vehicle miles traveled (VMT), fuel consumption, road infrastructure conditions, and highway financing, which have been meticulously compiled since 1945. The data is structured in a tabular format, with rows representing different years and columns capturing various metrics related to highway usage and infrastructure. Key variables include total vehicle miles traveled, types of fuel consumed, expenditures on highway maintenance and construction, and the overall condition of road infrastructure. These variables are essential for researchers and policymakers aiming to analyze trends in transportation economics and infrastructure development.\n\nThe collection methodology for the FHWA Highway Statistics dataset involves gathering data from multiple sources, including state transportation agencies, federal reports, and surveys conducted by the Federal Highway Administration. This multi-source approach enhances the dataset's reliability, although it may also introduce variability in data quality depending on the reporting practices of different states and agencies. Researchers should be aware of potential limitations, such as inconsistencies in historical data reporting and variations in definitions used across states, which may affect longitudinal analyses.\n\nCommon preprocessing steps for utilizing this dataset include cleaning and standardizing variable formats, handling missing values, and potentially aggregating data to align with specific research questions. Researchers typically employ this dataset to address a variety of research questions, such as examining the impact of economic factors on highway usage, assessing the effectiveness of transportation policies, and exploring correlations between fuel consumption and infrastructure investment.\n\nAnalyses supported by the FHWA Highway Statistics dataset range from descriptive statistics to more complex regression analyses and machine learning applications. For instance, researchers might use regression models to predict future trends in vehicle miles traveled based on historical data, or they may apply machine learning techniques to identify patterns in fuel consumption relative to economic indicators. Overall, the FHWA Highway Statistics dataset serves as a vital resource for understanding the intricacies of the US highway system and its implications for transportation economics and policy."
  },
  {
    "name": "gridstatus",
    "description": "Unified Python API for accessing real-time and historical data from all major U.S. ISOs",
    "category": "Energy",
    "url": "https://www.gridstatus.io/",
    "docs_url": "https://docs.gridstatus.io/",
    "github_url": "https://github.com/gridstatus/gridstatus",
    "tags": [
      "ISO",
      "API",
      "real-time",
      "Python",
      "unified"
    ],
    "best_for": "Accessing standardized data across multiple U.S. electricity markets",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "energy",
      "data-analysis",
      "real-time-data"
    ],
    "summary": "The gridstatus dataset provides a unified Python API that allows users to access both real-time and historical data from all major U.S. Independent System Operators (ISOs). This dataset can be utilized for various analyses related to energy consumption, grid stability, and operational efficiency within the energy sector.",
    "use_cases": [
      "Analyzing trends in energy consumption across different U.S. ISOs.",
      "Evaluating the impact of renewable energy sources on grid stability.",
      "Conducting regression analysis to forecast energy demand.",
      "Comparing historical and real-time data to assess operational efficiency."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the gridstatus dataset?",
      "How can I access real-time data from U.S. ISOs?",
      "What historical data is available through the gridstatus API?",
      "What Python libraries are used with the gridstatus dataset?",
      "How do I analyze energy data using gridstatus?",
      "What are the key features of the gridstatus API?",
      "Can I use gridstatus for machine learning applications?",
      "What types of analyses can be performed with gridstatus data?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "varies by ISO",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0002,
    "image_url": "/images/datasets/gridstatus.png",
    "embedding_text": "The gridstatus dataset is a comprehensive resource designed for accessing real-time and historical data from all major U.S. Independent System Operators (ISOs) through a unified Python API. This dataset is structured primarily as a time-series, capturing various metrics related to energy consumption, grid performance, and operational statistics. Each entry in the dataset typically consists of rows representing time-stamped observations, with columns that may include variables such as energy demand, generation capacity, and grid frequency. The data is collected from multiple sources, including ISO reports, operational logs, and real-time monitoring systems, ensuring a robust and accurate representation of the energy landscape in the United States. Researchers and analysts can leverage this dataset to address a variety of research questions, such as understanding the dynamics of energy supply and demand, evaluating the integration of renewable energy sources, and assessing the overall reliability of the grid. Common preprocessing steps may include cleaning the data for missing values, normalizing time stamps, and aggregating data to specific intervals for analysis. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for energy researchers and data scientists. However, users should be aware of potential limitations in data quality, such as discrepancies in reporting between different ISOs or variations in data collection methodologies. Overall, the gridstatus dataset serves as a vital resource for those looking to explore the complexities of the U.S. energy sector, providing insights that can inform policy decisions, operational strategies, and academic research."
  },
  {
    "name": "Yelp Dataset",
    "description": "Business attributes, reviews, user data, and check-ins",
    "category": "Social & Web",
    "url": "https://www.yelp.com/dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "businesses",
      "local",
      "NLP"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Yelp Dataset contains a wealth of information about businesses, user reviews, and check-in data. This dataset can be utilized for various analyses, including sentiment analysis, local business performance evaluation, and user behavior studies.",
    "use_cases": [
      "Sentiment analysis of user reviews",
      "Performance evaluation of local businesses",
      "User behavior analysis based on check-in data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the business attributes in the Yelp Dataset?",
      "How can I analyze user reviews from the Yelp Dataset?",
      "What insights can be gained from check-in data in the Yelp Dataset?",
      "What local businesses are represented in the Yelp Dataset?",
      "How does the Yelp Dataset support NLP applications?",
      "What are the common tags associated with businesses in the Yelp Dataset?",
      "How can I visualize data from the Yelp Dataset?",
      "What trends can be identified in user reviews over time?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/yelp-dataset.jpg",
    "embedding_text": "The Yelp Dataset is a comprehensive collection of data that encompasses various aspects of local businesses, including their attributes, user-generated reviews, and check-in information. The dataset is structured in a tabular format, consisting of rows and columns that represent different variables related to businesses and user interactions. Key variables include business ID, user ID, review text, star ratings, and check-in timestamps, among others. Each row typically corresponds to a unique review or check-in event, while the columns capture essential details about the business and the user experience. The data is collected from the Yelp platform, which aggregates user reviews and business information from various locations, providing a rich source for analysis. However, researchers should be aware of potential limitations in data quality, such as the presence of biased reviews or incomplete business profiles, which can affect the reliability of insights drawn from the dataset. Common preprocessing steps may include text normalization for review analysis, handling missing values, and encoding categorical variables for machine learning applications. The dataset supports a wide range of research questions, such as examining the impact of customer reviews on business performance, understanding consumer behavior patterns, and exploring the dynamics of local markets. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically utilize the Yelp Dataset in studies focused on consumer behavior, local business dynamics, and the effectiveness of online reviews in influencing purchasing decisions."
  },
  {
    "name": "DataCo Supply Chain",
    "description": "Synthetic supply chain dataset covering sales and returns",
    "category": "Logistics & Supply Chain",
    "url": "https://tianchi.aliyun.com/dataset/89959",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "supply chain",
      "synthetic",
      "returns"
    ],
    "best_for": "Learning logistics & supply chain analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "supply chain",
      "logistics",
      "data analysis"
    ],
    "summary": "The DataCo Supply Chain dataset is a synthetic dataset designed to simulate various aspects of supply chain operations, including sales and returns. It can be utilized for analyzing supply chain efficiency, understanding consumer return behavior, and testing predictive models in logistics.",
    "use_cases": [
      "Analyzing sales trends in supply chain",
      "Modeling return rates and their impact on inventory",
      "Testing machine learning algorithms for supply chain optimization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "synthetic supply chain dataset",
      "supply chain sales data",
      "returns dataset for analysis",
      "logistics dataset",
      "data for supply chain modeling",
      "sales and returns simulation dataset"
    ],
    "domain_tags": [
      "logistics",
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The DataCo Supply Chain dataset is a synthetic dataset that provides a comprehensive view of supply chain dynamics, particularly focusing on sales and returns. This dataset is structured in a tabular format, consisting of rows and columns that represent various variables pertinent to supply chain management. Each row corresponds to a transaction or event within the supply chain, while the columns capture essential attributes such as product identifiers, transaction dates, quantities sold, return reasons, and other relevant metrics. The dataset is designed to facilitate a range of analyses, making it a valuable resource for researchers and practitioners in the field of logistics and supply chain management. The collection methodology for this dataset involves the generation of synthetic data that mimics real-world supply chain scenarios, ensuring that the data is both realistic and useful for analytical purposes. As a synthetic dataset, it does not rely on actual transactional data, thus eliminating concerns related to data privacy and confidentiality. However, it is important to note that while synthetic data can be highly useful for modeling and simulation, it may not capture all the nuances of real-world supply chain operations, which can be a limitation in certain analytical contexts. The key variables within the dataset include sales figures, return rates, product categories, and timestamps, among others. These variables allow researchers to measure and analyze various aspects of supply chain performance, such as sales trends, return patterns, and overall efficiency. Common preprocessing steps that may be required include data cleaning to address any inconsistencies, normalization of numerical values, and encoding of categorical variables for use in machine learning models. The dataset supports a variety of analyses, including regression analysis to predict sales based on historical data, machine learning applications for demand forecasting, and descriptive statistics to summarize key performance indicators. Researchers typically use this dataset to explore questions related to supply chain optimization, consumer behavior regarding returns, and the impact of sales strategies on overall supply chain performance. By leveraging the insights gained from this dataset, organizations can make informed decisions that enhance their supply chain operations and improve customer satisfaction."
  },
  {
    "name": "UK Land Registry Price Paid",
    "description": "4.3GB of UK property sales transactions going back decades, messy real-world government data",
    "category": "Real Estate",
    "url": "https://www.gov.uk/government/collections/price-paid-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "UK",
      "government data",
      "large-scale",
      "transactions",
      "messy data"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "real estate",
      "government data",
      "data analysis"
    ],
    "summary": "The UK Land Registry Price Paid dataset contains historical property sales transactions across the UK, providing a rich source of messy real-world government data. Researchers and analysts can utilize this dataset to explore property market trends, analyze pricing patterns, and study the impact of various factors on real estate transactions.",
    "use_cases": [
      "Analyzing trends in property prices over time",
      "Studying the impact of economic factors on real estate transactions",
      "Identifying patterns in property sales across different regions",
      "Evaluating the effectiveness of government policies on housing markets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the UK Land Registry Price Paid dataset?",
      "How can I access UK property sales data?",
      "What are the common uses of the UK Land Registry dataset?",
      "What types of analyses can be performed on UK property transactions?",
      "Where can I find messy real-world government data?",
      "What insights can be gained from UK property sales transactions?",
      "How has the UK property market changed over the decades?",
      "What variables are included in the UK Land Registry dataset?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "geographic_scope": "UK",
    "size_category": "large",
    "model_score": 0.0001,
    "image_url": "/images/datasets/uk-land-registry-price-paid.png",
    "embedding_text": "The UK Land Registry Price Paid dataset is a comprehensive collection of property sales transactions that spans several decades, offering a unique glimpse into the dynamics of the UK real estate market. This dataset is structured in a tabular format, containing numerous rows and columns that represent individual property transactions. Key variables within the dataset include transaction dates, property prices, property types, and geographical identifiers, among others. These variables are crucial for understanding the various factors that influence property sales and pricing in the UK. The dataset is collected from the UK Land Registry, which is the official government body responsible for recording property ownership and transactions. As such, the data is considered authoritative, although it may contain inconsistencies and inaccuracies typical of large-scale government datasets. Researchers often encounter challenges related to data quality, including missing values, duplicate entries, and variations in data formatting. Common preprocessing steps include data cleaning, normalization, and transformation to ensure that the dataset is suitable for analysis. Analysts can leverage this dataset to address a variety of research questions, such as examining how property prices have evolved over time, identifying regional disparities in property values, and assessing the impact of economic conditions on housing markets. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the real estate sector. Researchers typically use this dataset to conduct studies that inform policy decisions, guide investment strategies, and enhance the understanding of market dynamics within the UK property landscape."
  },
  {
    "name": "Merative MarketScan",
    "description": "De-identified commercial claims from 273+ million unique patients since 1995. Includes Commercial Claims, Medicare Supplemental, and Multi-State Medicaid databases. Cited in 2,650+ peer-reviewed studies.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.merative.com/real-world-evidence",
    "source": "Merative (formerly IBM Watson Health)",
    "type": "Claims Database",
    "access": "Institutional license required",
    "format": "SAS",
    "tags": [
      "Healthcare",
      "Claims",
      "Commercial",
      "Longitudinal"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Merative MarketScan dataset comprises de-identified commercial claims from over 273 million unique patients since 1995, providing a comprehensive view of healthcare utilization and costs. Researchers can leverage this dataset to analyze trends in healthcare claims, evaluate the effectiveness of treatments, and inform policy decisions in healthcare economics.",
    "use_cases": [
      "Analyzing healthcare costs over time",
      "Evaluating the impact of policy changes on healthcare utilization",
      "Studying the effectiveness of specific treatments across different demographics"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Merative MarketScan dataset?",
      "How can I access the Merative MarketScan claims data?",
      "What types of analyses can be performed using the Merative MarketScan dataset?",
      "What demographic information is available in the MarketScan dataset?",
      "How has the Merative MarketScan dataset been used in peer-reviewed studies?",
      "What are the key variables in the Merative MarketScan dataset?",
      "What is the temporal coverage of the MarketScan dataset?",
      "What are the limitations of using the Merative MarketScan dataset?"
    ],
    "update_frequency": "Quarterly",
    "geographic_coverage": "United States (employer-insured)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1995-present",
    "size_category": "massive",
    "benchmark_usage": [
      "Cited in 2,650+ peer-reviewed studies"
    ],
    "model_score": 0.0001,
    "image_url": "/images/logos/merative.png",
    "embedding_text": "The Merative MarketScan dataset is a rich source of de-identified commercial claims data, encompassing over 273 million unique patients since 1995. This extensive dataset includes various types of claims, such as Commercial Claims, Medicare Supplemental, and Multi-State Medicaid databases, making it a valuable resource for researchers in healthcare economics and health technology. The data structure typically consists of rows representing individual claims and columns detailing various attributes such as patient demographics, diagnosis codes, procedure codes, and costs associated with healthcare services. Researchers often utilize this dataset to explore a wide range of research questions, including the analysis of healthcare costs, the effectiveness of treatments, and the impact of healthcare policies on patient outcomes. The collection methodology involves aggregating claims data from multiple sources while ensuring that patient identities are protected through de-identification processes. While the dataset provides comprehensive coverage of healthcare claims, researchers should be aware of potential limitations, such as biases in the data due to the exclusion of uninsured populations or those not seeking care. Common preprocessing steps include cleaning the data to handle missing values, standardizing codes, and aggregating claims to analyze trends over time. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, enabling researchers to derive insights that can inform healthcare policy and practice. Overall, the Merative MarketScan dataset serves as a foundational tool for understanding healthcare utilization and costs, making it indispensable for researchers aiming to address critical questions in the field."
  },
  {
    "name": "Chicago Property Data",
    "description": "Property assessment values and sales data from Cook County",
    "category": "Real Estate",
    "url": "https://datacatalog.cookcountyil.gov/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "property",
      "Chicago",
      "assessments",
      "government"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Chicago Property Data dataset contains property assessment values and sales data sourced from Cook County, providing insights into real estate trends in Chicago. Researchers and analysts can utilize this data to explore property valuation, market trends, and the impact of government assessments on property sales.",
    "use_cases": [
      "Analyzing the relationship between property assessments and sales prices.",
      "Investigating trends in property values over time in Chicago.",
      "Examining the impact of government policies on real estate markets."
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Chicago Property Data?",
      "How can I access property assessment values in Chicago?",
      "What sales data is available for Cook County properties?",
      "What are the trends in Chicago real estate assessments?",
      "How do government assessments affect property sales in Chicago?",
      "Where can I find datasets on Chicago property values?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Chicago",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/cookcountyil.png",
    "embedding_text": "The Chicago Property Data dataset is a comprehensive collection of property assessment values and sales data specifically from Cook County, Illinois. This dataset is structured in a tabular format, where each row represents a unique property and its associated assessment and sales information. The columns typically include variables such as property ID, address, assessment value, sale price, sale date, and property type, among others. These variables are crucial for understanding the dynamics of the real estate market in Chicago, as they provide insights into how properties are valued and sold over time. The data is collected from official government sources, ensuring a level of reliability and accuracy, although users should be aware of potential limitations such as missing values or discrepancies in property classifications. The dataset's coverage is geographically limited to the city of Chicago, allowing for focused analysis within this urban environment. While temporal coverage is not explicitly mentioned, the dataset likely includes recent years of data, given the nature of property transactions and assessments. Researchers and analysts can utilize this dataset to address various research questions, such as the correlation between property assessments and actual sale prices, the impact of economic factors on property values, and the identification of trends in the real estate market over time. Common preprocessing steps may include handling missing data, normalizing property types, and converting sale dates into a consistent format for time series analysis. The dataset supports a range of analytical techniques, including regression analysis to predict property values, machine learning models for classification of property types, and descriptive statistics to summarize the data. Overall, the Chicago Property Data serves as a valuable resource for those interested in real estate economics, urban studies, and public policy analysis, providing a foundation for empirical research and data-driven decision-making."
  },
  {
    "name": "SNAP Facebook Ego Networks",
    "description": "4K users with social circles and anonymized node features. Stanford Network Analysis Project dataset",
    "category": "Social & Web",
    "url": "https://snap.stanford.edu/data/ego-Facebook.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Facebook",
      "social network",
      "ego networks",
      "SNAP",
      "Stanford"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The SNAP Facebook Ego Networks dataset consists of social circles and anonymized node features for 4,000 users. This dataset allows researchers to analyze social network structures and user interactions within Facebook, providing insights into ego networks and social dynamics.",
    "use_cases": [
      "Analyzing the structure of social networks",
      "Studying user interactions within social circles",
      "Investigating the dynamics of ego networks",
      "Exploring the impact of social connections on behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the SNAP Facebook Ego Networks dataset?",
      "How can I access the SNAP Facebook Ego Networks dataset?",
      "What features are included in the SNAP Facebook Ego Networks dataset?",
      "What research can be conducted using the SNAP Facebook Ego Networks dataset?",
      "How does the SNAP Facebook Ego Networks dataset measure social interactions?",
      "What are the anonymized node features in the SNAP Facebook Ego Networks dataset?",
      "What insights can be gained from analyzing ego networks in Facebook?"
    ],
    "domain_tags": [
      "social network",
      "technology"
    ],
    "data_modality": "graph",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The SNAP Facebook Ego Networks dataset is a comprehensive collection of social network data that focuses on 4,000 users and their associated social circles. This dataset is part of the Stanford Network Analysis Project (SNAP), which aims to provide researchers with high-quality data for the analysis of complex networks. The dataset contains anonymized node features, which are essential for understanding the characteristics of individual users within their social contexts. The data structure typically includes rows representing individual users and columns that capture various attributes and relationships, such as user IDs, connections to other users, and specific features that describe their social behavior. Researchers can leverage this dataset to explore a multitude of research questions related to social interactions, network dynamics, and the influence of social structures on individual behavior. The collection methodology for this dataset involves gathering data from Facebook's social networking platform, ensuring that user privacy is maintained through anonymization. This means that while the dataset provides rich insights into user interactions, it does not disclose personally identifiable information. The coverage of the dataset is primarily focused on social interactions within the Facebook platform, and it does not specify temporal or geographic dimensions, which may limit certain types of longitudinal analyses. Key variables in the dataset include user connections, which measure the strength and nature of relationships between users, as well as various anonymized features that can indicate user demographics and engagement levels. However, researchers should be aware of potential limitations regarding data quality, such as the representativeness of the sample and the accuracy of the anonymized features. Common preprocessing steps may include data cleaning to remove any inconsistencies, normalization of features for comparative analysis, and the transformation of network data into a suitable format for analysis. The SNAP Facebook Ego Networks dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics. Researchers typically use this dataset to conduct studies that aim to understand the implications of social networks on behavior, the formation of social ties, and the overall dynamics of online interactions. By analyzing ego networks, researchers can uncover patterns that may inform theories of social influence, community detection, and the spread of information within social networks. Overall, the SNAP Facebook Ego Networks dataset serves as a valuable resource for those interested in the intersection of technology and social science, providing a foundation for exploring the complexities of human interaction in digital spaces."
  },
  {
    "name": "World Inequality Database",
    "description": "Income and wealth inequality data for 100+ countries by Piketty, Saez, and Zucman",
    "category": "Dataset Aggregators",
    "url": "https://wid.world",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "inequality",
      "wealth",
      "income",
      "Piketty"
    ],
    "best_for": "Income and wealth distribution research with free visualization tools",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "inequality",
      "economics",
      "sociology"
    ],
    "summary": "The World Inequality Database provides comprehensive data on income and wealth inequality across more than 100 countries, compiled by leading economists Piketty, Saez, and Zucman. This dataset allows researchers and policymakers to analyze trends in inequality, understand the distribution of wealth and income, and inform discussions on economic policy and social justice.",
    "use_cases": [
      "Analyzing trends in income inequality over time",
      "Comparing wealth distribution across different countries",
      "Evaluating the impact of economic policies on inequality",
      "Conducting regression analysis to understand factors influencing income disparity"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the World Inequality Database?",
      "How can I access income inequality data?",
      "What countries are included in the World Inequality Database?",
      "What variables are measured in the World Inequality Database?",
      "How is wealth inequality calculated?",
      "What are the trends in income inequality according to Piketty, Saez, and Zucman?",
      "What research can be conducted using the World Inequality Database?",
      "What methodologies are used to collect data in the World Inequality Database?"
    ],
    "domain_tags": [
      "economics",
      "social sciences"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/wid.png",
    "embedding_text": "The World Inequality Database is a vital resource for researchers and policymakers interested in the dynamics of income and wealth inequality on a global scale. This dataset is structured in a tabular format, comprising rows that represent different countries and columns that include various metrics related to income and wealth distribution. Key variables in the dataset include measures of income shares, wealth shares, and other indicators that reflect the economic disparities within and between nations. The data is meticulously compiled by renowned economists Thomas Piketty, Emmanuel Saez, and Gabriel Zucman, who have utilized a combination of national accounts, tax records, and survey data to ensure comprehensive coverage and accuracy. While the dataset spans over 100 countries, it is essential to note that the temporal coverage and specific geographic details are not explicitly mentioned, which may limit certain longitudinal analyses. Researchers typically engage with this dataset to address critical research questions surrounding economic inequality, such as the effects of taxation on wealth distribution, the relationship between economic growth and income inequality, and the socio-economic factors contributing to disparities in wealth. Common preprocessing steps may include normalization of income data, handling missing values, and transforming variables for regression analysis. The dataset supports a variety of analytical approaches, including descriptive statistics, regression modeling, and machine learning techniques, making it a versatile tool for exploring the multifaceted nature of inequality. Researchers often leverage this dataset to inform policy discussions, advocate for social justice, and contribute to the broader discourse on economic equity. Overall, the World Inequality Database serves as an indispensable foundation for empirical research in the fields of economics and social sciences, providing insights that are crucial for understanding and addressing the challenges posed by inequality in contemporary society."
  },
  {
    "name": "OpenStreetMap Planet",
    "description": "84GB PBF (2TB+ uncompressed) complete world map database with full edit history, weekly updates",
    "category": "Social & Web",
    "url": "https://planet.openstreetmap.org/",
    "docs_url": "https://wiki.openstreetmap.org/wiki/Planet.osm",
    "github_url": null,
    "tags": [
      "database dump",
      "geospatial",
      "large-scale",
      "real-world",
      "PostgreSQL",
      "messy data"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The OpenStreetMap Planet dataset is a comprehensive, large-scale world map database that includes full edit history and is updated weekly. It provides a rich source of geospatial data suitable for various analyses, including urban planning, transportation modeling, and geographic information systems (GIS).",
    "use_cases": [
      "Urban planning and infrastructure development analysis",
      "Transportation network optimization",
      "Geospatial data visualization",
      "Environmental impact assessments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the OpenStreetMap Planet dataset?",
      "How can I access the OpenStreetMap Planet data?",
      "What types of analyses can be performed with OpenStreetMap data?",
      "What are the updates frequency for the OpenStreetMap Planet dataset?",
      "What is the size of the OpenStreetMap Planet database?",
      "How is the OpenStreetMap Planet dataset structured?",
      "What are the main features of the OpenStreetMap Planet dataset?",
      "What tools can be used to analyze OpenStreetMap data?"
    ],
    "domain_tags": [
      "geospatial",
      "urban planning",
      "transportation"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/openstreetmap.png",
    "embedding_text": "The OpenStreetMap Planet dataset is a massive, comprehensive database that contains a complete world map with full edit history, totaling approximately 84GB in PBF format and over 2TB when uncompressed. This dataset is structured to include various geospatial features such as points of interest, road networks, and land use classifications, making it a valuable resource for researchers and practitioners in fields like urban planning, transportation, and geographic information systems (GIS). The data is collected through a collaborative mapping process where contributors from around the globe add and edit geographic information, ensuring a high level of detail and accuracy. The dataset is updated weekly, providing users with the most current information available. Key variables in the dataset include geographic coordinates, feature types, and metadata about edits, which measure aspects like location, type of infrastructure, and the history of changes made to the data. However, users should be aware of potential data quality issues, such as inconsistencies in mapping standards or incomplete coverage in certain regions. Common preprocessing steps may include filtering out irrelevant data, normalizing feature types, and converting data formats for compatibility with analysis tools. Researchers can utilize the OpenStreetMap Planet dataset to address a variety of research questions, such as analyzing urban growth patterns, assessing transportation accessibility, or evaluating the impact of infrastructure changes on local communities. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing for in-depth exploration of spatial relationships and trends. Overall, the OpenStreetMap Planet dataset serves as a foundational resource for a wide range of studies, enabling researchers to leverage rich geospatial data for informed decision-making and innovative solutions in urban and environmental contexts."
  },
  {
    "name": "Meta Content Library",
    "description": "Full Facebook/Instagram public archive via ICPSR application. Posts, Pages, groups, events for academic research",
    "category": "Social & Web",
    "url": "https://transparency.meta.com/researchtools/meta-content-library",
    "docs_url": "https://developers.facebook.com/docs/content-library-and-api",
    "github_url": null,
    "tags": [
      "Facebook",
      "Instagram",
      "Meta",
      "social media",
      "posts"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Meta Content Library is a comprehensive archive of public posts, pages, groups, and events from Facebook and Instagram, made available through the ICPSR application. This dataset serves as a valuable resource for academic research, enabling users to analyze social media interactions and trends.",
    "use_cases": [
      "Analyzing trends in social media engagement over time",
      "Studying the impact of social media posts on public opinion",
      "Investigating the dynamics of online communities and groups",
      "Exploring event participation and its correlation with social media activity"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Meta Content Library dataset?",
      "How can I access the Facebook and Instagram public archive?",
      "What types of social media data are included in the Meta Content Library?",
      "How can researchers use the Meta Content Library for academic studies?",
      "What are the key features of the Meta Content Library dataset?",
      "What kind of analysis can be performed on the Meta Content Library data?",
      "What is the significance of the Meta Content Library for social media research?"
    ],
    "domain_tags": [
      "social media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/meta.png",
    "embedding_text": "The Meta Content Library provides a rich dataset that encompasses a wide array of public posts, pages, groups, and events from Facebook and Instagram. This dataset is particularly valuable for researchers interested in social media dynamics, as it allows for an in-depth exploration of user interactions and content dissemination across two of the largest social media platforms. The data structure typically includes rows representing individual posts or events, with columns detailing various attributes such as post content, timestamps, user engagement metrics (likes, shares, comments), and metadata related to the originating pages or groups. Researchers can leverage this dataset to address a multitude of research questions, such as understanding the factors that drive engagement on social media, analyzing the evolution of public discourse over time, or examining the role of social media in shaping community interactions. The collection methodology involves aggregating publicly available data through the ICPSR application, ensuring that the dataset reflects real-world interactions while adhering to privacy standards. While the dataset is extensive, researchers should be aware of potential limitations, including data quality issues stemming from incomplete posts or variations in user engagement metrics. Common preprocessing steps may include cleaning the data to remove duplicates, standardizing formats for timestamps, and categorizing posts based on content themes. The versatility of the Meta Content Library supports various types of analyses, including regression analysis to identify trends, machine learning techniques for predictive modeling, and descriptive statistics to summarize user engagement patterns. Overall, the Meta Content Library serves as a foundational resource for academic research, enabling scholars to derive insights into social media behavior and its implications for society."
  },
  {
    "name": "Zillow Research Data",
    "description": "Home values (ZHVI), rents (ZORI), inventory, and market heat indices across US metros and zip codes",
    "category": "Real Estate",
    "url": "https://www.zillow.com/research/data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "housing prices",
      "rents",
      "large-scale",
      "time series"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "real estate",
      "housing market",
      "data analysis"
    ],
    "summary": "The Zillow Research Data provides comprehensive insights into home values, rents, inventory levels, and market heat indices across various US metropolitan areas and zip codes. This dataset can be utilized for analyzing trends in the housing market, understanding rental dynamics, and assessing real estate investment opportunities.",
    "use_cases": [
      "Analyzing trends in home values over time to forecast future prices.",
      "Comparing rental prices across different metropolitan areas to identify investment opportunities.",
      "Assessing the impact of inventory levels on housing prices in specific regions.",
      "Investigating the relationship between market heat indices and housing demand."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest home values in my zip code?",
      "How have rents changed over the past year in major US metros?",
      "What is the current inventory level for homes in San Francisco?",
      "How does the market heat index vary across different regions?",
      "What trends can be observed in housing prices over time?",
      "How do rental prices compare between urban and suburban areas?",
      "What factors influence home values in different zip codes?",
      "How can I analyze the real estate market using Zillow Research Data?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "time-series",
    "geographic_scope": "US metros and zip codes",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/zillow-research-data.jpg",
    "embedding_text": "The Zillow Research Data is a rich dataset that encompasses various aspects of the housing market in the United States. It includes key variables such as the Zillow Home Value Index (ZHVI), which measures the median home values across different regions, and the Zillow Rent Index (ZORI), which tracks rental prices. Additionally, the dataset provides insights into housing inventory levels and market heat indices, which reflect the competitive nature of the housing market in specific areas. The data is structured in a tabular format, with rows representing different geographical units (metros and zip codes) and columns capturing the various metrics over time. This allows for detailed analysis of trends and patterns in the real estate market. The data is collected from a variety of sources, including public records, user-generated content, and proprietary algorithms developed by Zillow. While the dataset is extensive, researchers should be aware of potential limitations, such as data quality issues stemming from variations in reporting standards across different regions. Common preprocessing steps may include cleaning the data to handle missing values, normalizing the data for comparative analysis, and transforming variables for specific analytical needs. Researchers can leverage this dataset to address a range of research questions, such as understanding the factors that drive changes in home values, exploring the dynamics of rental markets, and evaluating the impact of economic conditions on real estate trends. The dataset supports various types of analyses, including regression analysis to model relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize key findings. Overall, the Zillow Research Data serves as a valuable resource for researchers, analysts, and decision-makers interested in the real estate sector, providing the necessary data to inform strategic decisions and enhance understanding of market dynamics."
  },
  {
    "name": "NHTSA FARS (Fatality Analysis Reporting System)",
    "description": "Complete census of fatal traffic crashes in the United States since 1975 with vehicle, person, and crash-level details",
    "category": "Insurance & Actuarial",
    "url": "https://www.nhtsa.gov/research-data/fatality-analysis-reporting-system-fars",
    "docs_url": "https://crashstats.nhtsa.dot.gov/",
    "github_url": null,
    "tags": [
      "traffic-safety",
      "auto-insurance",
      "crash-data",
      "fatality-data",
      "vehicle-safety"
    ],
    "best_for": "Auto insurance risk modeling, safety analysis, and claims severity research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NHTSA FARS dataset provides a comprehensive census of fatal traffic crashes in the United States, offering detailed insights into vehicle, person, and crash-level data since 1975. Researchers and analysts can utilize this dataset to explore trends in traffic safety, assess the impact of various factors on fatal accidents, and inform policy decisions related to road safety and vehicle regulations.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NHTSA FARS dataset?",
      "How can I access fatal traffic crash data in the US?",
      "What variables are included in the NHTSA FARS dataset?",
      "What trends can be analyzed using the NHTSA FARS data?",
      "How has traffic safety changed in the US since 1975?",
      "What are the key factors contributing to fatal traffic crashes?",
      "How can I use FARS data for vehicle safety analysis?",
      "What methodologies are used in the NHTSA FARS dataset collection?"
    ],
    "use_cases": [
      "Analyzing trends in fatal traffic accidents over the decades.",
      "Assessing the impact of vehicle safety features on crash outcomes.",
      "Investigating demographic factors associated with traffic fatalities.",
      "Evaluating the effectiveness of traffic safety policies and regulations."
    ],
    "domain_tags": [
      "transportation",
      "insurance",
      "public-safety"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1975-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nhtsa-fars-fatality-analysis-reporting-system.png",
    "embedding_text": "The NHTSA Fatality Analysis Reporting System (FARS) dataset is a vital resource for understanding fatal traffic crashes in the United States. This dataset encompasses a complete census of fatal traffic incidents, providing a wealth of information that spans from 1975 to the present. The data structure is primarily tabular, consisting of rows that represent individual fatal crashes and columns that detail various attributes of each incident, including vehicle characteristics, person demographics, and crash circumstances. Key variables within the dataset include the type of vehicle involved, the age and gender of the individuals, the time and location of the crash, and contributing factors such as weather conditions and road types. These variables allow for a multifaceted analysis of traffic fatalities, making it possible to identify patterns and correlations that can inform public policy and safety measures. The data collection methodology employed by the NHTSA involves gathering information from multiple sources, including police reports, state databases, and other relevant agencies, ensuring a comprehensive and accurate representation of fatal crashes across the country. However, researchers should be aware of potential limitations in data quality, such as underreporting or inconsistencies in how crashes are documented across different jurisdictions. Common preprocessing steps may include cleaning the data for missing values, standardizing variable formats, and aggregating data for specific analyses. The FARS dataset supports a variety of analytical approaches, including regression analysis, machine learning techniques, and descriptive statistics. Researchers typically utilize this dataset to address critical questions surrounding traffic safety, such as identifying high-risk demographics, evaluating the effectiveness of safety interventions, and exploring the relationship between environmental factors and crash outcomes. Overall, the NHTSA FARS dataset serves as an essential tool for researchers, policymakers, and safety advocates aiming to enhance road safety and reduce traffic-related fatalities."
  },
  {
    "name": "UK Biobank",
    "description": "Prospective cohort of 500,000 UK participants aged 40-69 with genetic data, imaging, and longitudinal health records. Extensive phenotyping including MRI, accelerometry, and linked hospital records.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.ukbiobank.ac.uk/",
    "source": "UK Biobank",
    "type": "Cohort + Biobank",
    "access": "Application required (fees apply)",
    "format": "Various",
    "tags": [
      "Healthcare",
      "Genomics",
      "Imaging",
      "Longitudinal"
    ],
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The UK Biobank is a comprehensive prospective cohort study involving 500,000 participants from the UK, aged between 40 and 69. It provides a wealth of genetic data, imaging results, and longitudinal health records, enabling researchers to explore various health-related questions and the interplay between genetics and health outcomes.",
    "use_cases": [
      "Analyzing the relationship between genetics and chronic diseases.",
      "Investigating the impact of lifestyle factors on health outcomes.",
      "Studying the effects of imaging results on health trajectories.",
      "Exploring longitudinal health trends in a large population."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the UK Biobank dataset?",
      "How can I access the UK Biobank data?",
      "What types of health records are included in the UK Biobank?",
      "What genetic data is available in the UK Biobank?",
      "How does the UK Biobank support health research?",
      "What imaging data does the UK Biobank provide?",
      "What are the demographics of participants in the UK Biobank?",
      "What phenotyping methods are used in the UK Biobank?"
    ],
    "update_frequency": "Ongoing linkage",
    "geographic_coverage": "United Kingdom",
    "domain_tags": [
      "healthcare",
      "genomics"
    ],
    "data_modality": "mixed",
    "geographic_scope": "UK",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/datasets/uk-biobank.jpg",
    "embedding_text": "The UK Biobank is a large-scale biomedical database and research resource that contains extensive information on the health and genetic data of 500,000 participants from the United Kingdom, aged 40 to 69 years. This dataset is structured to include a variety of data types, including genetic information, imaging data, and longitudinal health records. The data is organized into rows representing individual participants and columns that capture a wide array of variables such as demographic information, health status, lifestyle factors, and results from various health assessments. The collection methodology for the UK Biobank involved recruiting participants through a combination of community outreach and health service engagement, ensuring a diverse representation of the UK population. Participants provided informed consent and underwent a series of assessments, including physical measurements, questionnaires, and biological sample collection, which contribute to the richness of the dataset. Key variables within the UK Biobank include genetic markers, health outcomes (such as incidence of diseases), imaging results (such as MRI scans), and lifestyle factors (such as physical activity levels and dietary habits). These variables are crucial for understanding the complex interactions between genetics, environment, and health. The data quality is generally high due to rigorous collection protocols, although researchers must be aware of potential limitations, such as self-reported data biases and the need for careful handling of missing data. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers typically utilize the UK Biobank in various studies, addressing research questions related to the etiology of diseases, the effectiveness of interventions, and the role of genetics in health outcomes. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile resource for health-related research. Overall, the UK Biobank serves as a vital tool for advancing our understanding of health and disease, providing insights that can inform public health strategies and clinical practices."
  },
  {
    "name": "ETHOS Hate Speech",
    "description": "998 online comments labeled for hate speech detection in English. Binary and multi-label annotations",
    "category": "Content Moderation",
    "url": "https://zenodo.org/records/4459923",
    "docs_url": null,
    "github_url": "https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset",
    "tags": [
      "hate speech",
      "NLP",
      "annotations",
      "English"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ETHOS Hate Speech dataset consists of 998 online comments that have been labeled for hate speech detection in English. This dataset can be utilized for training and evaluating machine learning models aimed at identifying and categorizing hate speech in textual data.",
    "use_cases": [
      "Training hate speech detection models",
      "Evaluating NLP algorithms for text classification",
      "Conducting sentiment analysis on online comments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the ETHOS Hate Speech dataset?",
      "How can I use the ETHOS Hate Speech dataset for NLP?",
      "What types of annotations are present in the ETHOS Hate Speech dataset?",
      "Where can I find datasets for hate speech detection?",
      "What are the characteristics of the ETHOS Hate Speech dataset?",
      "How many comments are in the ETHOS Hate Speech dataset?",
      "What languages are represented in the ETHOS Hate Speech dataset?",
      "What is the purpose of the ETHOS Hate Speech dataset?"
    ],
    "domain_tags": [
      "content moderation"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/ethos-hate-speech.png",
    "embedding_text": "The ETHOS Hate Speech dataset is a collection of 998 online comments that have been meticulously labeled for the purpose of hate speech detection in the English language. This dataset is structured in a tabular format, where each row corresponds to a unique online comment, and the columns contain various attributes related to the comments, including the text of the comment itself and its associated labels indicating whether it contains hate speech. The dataset features both binary and multi-label annotations, allowing for nuanced classification tasks that can differentiate between various types of hate speech. The collection methodology for this dataset involves sourcing comments from online platforms, where users engage in discussions that may contain harmful or offensive language. The comments are then labeled by annotators who assess the content based on predefined criteria for hate speech. This rigorous labeling process ensures a level of quality and consistency in the annotations, although it is important to note that the subjective nature of hate speech can introduce variability in the labeling process. The dataset does not explicitly mention temporal or geographic coverage, which suggests that it is primarily focused on the content of the comments rather than the context in which they were made. Key variables in the dataset include the comment text and the associated labels, which measure the presence and type of hate speech. Researchers utilizing this dataset can address a variety of research questions related to hate speech detection, such as identifying linguistic patterns associated with hate speech, evaluating the effectiveness of different machine learning models in classifying hate speech, and exploring the impact of hate speech on online discourse. Common preprocessing steps for this dataset may include text normalization, tokenization, and the removal of stop words, which are essential for preparing the text data for analysis. The dataset supports various types of analyses, including regression, machine learning classification tasks, and descriptive statistics, making it a versatile resource for researchers in the fields of natural language processing and social media analysis. Overall, the ETHOS Hate Speech dataset serves as a valuable tool for advancing research in hate speech detection and content moderation, providing insights into the complexities of online communication."
  },
  {
    "name": "MovieLens 25M",
    "description": "25 million ratings and 1 million tag applications across 62,000 movies by 162,000 users. The gold-standard benchmark for collaborative filtering research with rich metadata including genres, tags, and timestamps.",
    "category": "MarTech & Customer Analytics",
    "url": "https://grouplens.org/datasets/movielens/25m/",
    "docs_url": "https://files.grouplens.org/datasets/movielens/ml-25m-README.html",
    "github_url": null,
    "tags": [
      "recommendations",
      "collaborative-filtering",
      "movies",
      "ratings"
    ],
    "best_for": "Learning recommendation systems, matrix factorization, and collaborative filtering",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "recommendations",
      "collaborative-filtering",
      "movies",
      "ratings"
    ],
    "summary": "The MovieLens 25M dataset comprises 25 million ratings and 1 million tag applications across 62,000 movies by 162,000 users. It serves as a gold-standard benchmark for collaborative filtering research, providing rich metadata including genres, tags, and timestamps, which can be utilized for various recommendation system analyses.",
    "use_cases": [
      "Building a movie recommendation system",
      "Analyzing user preferences in film ratings",
      "Studying the impact of tags on movie ratings",
      "Evaluating collaborative filtering algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the MovieLens 25M dataset?",
      "How can I access the MovieLens 25M ratings?",
      "What types of analyses can be performed with MovieLens 25M?",
      "What are the key features of the MovieLens 25M dataset?",
      "How does collaborative filtering work with MovieLens 25M?",
      "What are the limitations of the MovieLens 25M dataset?",
      "What research questions can be explored using MovieLens 25M?",
      "How many users are involved in the MovieLens 25M dataset?"
    ],
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "benchmark_usage": [
      "Collaborative filtering research",
      "Recommendation system development"
    ],
    "model_score": 0.0001,
    "image_url": "/images/datasets/movielens-25m.jpg",
    "embedding_text": "The MovieLens 25M dataset is a comprehensive collection of movie ratings and tags that serves as a crucial resource for researchers and practitioners in the field of recommendation systems and collaborative filtering. This dataset contains 25 million ratings, which are the result of user interactions with 62,000 different movies, and it includes 1 million tag applications made by 162,000 users. The data is structured in a tabular format, where each row represents a unique rating or tag application, and the columns include variables such as user ID, movie ID, rating value, timestamp, and tags associated with the movies. This rich schema allows for a variety of analyses and research questions to be explored. The data collection methodology for MovieLens involved users voluntarily rating movies and applying tags, which ensures a diverse range of opinions and preferences. However, this also introduces potential biases, as the dataset may not fully represent the general population's movie preferences. The temporal coverage of the dataset is not explicitly mentioned, but it is known that the ratings span several years, providing a longitudinal view of user preferences over time. The geographic scope is also not specified, but the dataset is primarily based on users from the MovieLens platform, which is accessible globally. Key variables in the dataset include user ID, movie ID, rating (typically on a scale from 1 to 5), and tags, which measure user preferences and sentiments towards specific films. Researchers often face data quality issues, such as sparsity in user ratings for less popular movies, which can affect the performance of recommendation algorithms. Common preprocessing steps include normalizing ratings, handling missing values, and transforming tags into a usable format for analysis. The dataset supports various types of analyses, including regression analysis, machine learning model development, and descriptive statistics, making it suitable for a wide range of research questions. Typical research questions that can be addressed using the MovieLens 25M dataset include examining the effectiveness of different collaborative filtering algorithms, understanding user behavior and preferences in movie ratings, and analyzing the influence of metadata such as genres and tags on user ratings. Overall, the MovieLens 25M dataset is a valuable asset for those interested in exploring the dynamics of user preferences in the entertainment industry and developing advanced recommendation systems."
  },
  {
    "name": "HateDay",
    "description": "Global representative sample of real-world hate speech across languages. 2024 benchmark for content moderation",
    "category": "Content Moderation",
    "url": "https://arxiv.org/abs/2404.06465",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hate speech",
      "multilingual",
      "benchmark",
      "content moderation"
    ],
    "best_for": "Learning content moderation analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "HateDay is a global representative sample dataset that captures real-world hate speech across various languages. It serves as a benchmark for content moderation, allowing researchers and practitioners to analyze and develop strategies for identifying and mitigating hate speech online.",
    "use_cases": [
      "Analyzing hate speech trends across different languages",
      "Developing machine learning models for hate speech detection",
      "Evaluating content moderation strategies",
      "Benchmarking hate speech detection algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the HateDay dataset?",
      "How can I access the HateDay dataset?",
      "What types of hate speech are included in HateDay?",
      "What languages are represented in the HateDay dataset?",
      "How is HateDay used for content moderation?",
      "What are the benchmarks provided by HateDay?",
      "What research can be conducted using the HateDay dataset?",
      "What methodologies can be applied to analyze HateDay data?"
    ],
    "domain_tags": [
      "social media",
      "technology"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/hateday.png",
    "embedding_text": "The HateDay dataset is a comprehensive collection of real-world hate speech instances gathered from various online platforms, representing a global sample across multiple languages. This dataset is structured in a tabular format, where each row corresponds to a unique instance of hate speech, and the columns capture various attributes such as the text of the speech, the language in which it is expressed, and potentially the context or platform from which it was sourced. The primary aim of HateDay is to provide a benchmark for content moderation, making it an invaluable resource for researchers and practitioners in the field of online safety and community management. The collection methodology involves scraping publicly available data from social media platforms and forums, ensuring a diverse representation of hate speech occurrences. However, it is important to note that the dataset may have limitations in terms of data quality, including potential biases in the sources and the subjective nature of what constitutes hate speech. Researchers utilizing the HateDay dataset can address a variety of research questions, such as the prevalence of hate speech in different languages, the effectiveness of existing content moderation tools, and the identification of patterns or trends in hate speech usage over time. Common preprocessing steps may include text normalization, language detection, and the removal of irrelevant metadata. The dataset supports various types of analyses, including regression analysis, machine learning model training, and descriptive statistics, allowing for a robust exploration of hate speech dynamics. Overall, HateDay serves as a critical resource for advancing the understanding of hate speech in digital environments and improving content moderation practices.",
    "benchmark_usage": [
      "Content moderation research",
      "Hate speech detection algorithm evaluation"
    ]
  },
  {
    "name": "CelesTrak",
    "description": "Well-organized TLE and OMM orbital data by satellite category including SOCRATES collision assessment tool",
    "category": "Space",
    "url": "https://celestrak.org/",
    "docs_url": "https://celestrak.org/NORAD/documentation/",
    "github_url": null,
    "tags": [
      "satellites",
      "orbital elements",
      "collision",
      "debris"
    ],
    "best_for": "Accessible satellite data organized by mission type and constellation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "CelesTrak provides well-organized Two-Line Element (TLE) and Orbital Maneuver Model (OMM) data categorized by satellite type. This dataset can be utilized for collision assessments and tracking space debris, making it valuable for researchers and engineers in the aerospace sector.",
    "use_cases": [
      "Assessing potential satellite collisions using TLE data.",
      "Analyzing the distribution of space debris over time.",
      "Tracking satellite positions for mission planning.",
      "Evaluating the effectiveness of collision avoidance maneuvers."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is CelesTrak orbital data?",
      "How to access TLE data for satellites?",
      "What are the categories of satellites in CelesTrak?",
      "How can I use CelesTrak for collision assessment?",
      "What is the SOCRATES collision assessment tool?",
      "Where can I find orbital elements for space debris?",
      "How does CelesTrak organize satellite data?",
      "What tools can analyze TLE data from CelesTrak?"
    ],
    "domain_tags": [
      "space"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "current",
    "geographic_scope": "Global (orbital)",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/celestrak.png",
    "embedding_text": "CelesTrak is a comprehensive resource for Two-Line Element (TLE) and Orbital Maneuver Model (OMM) data, meticulously organized by satellite category. The dataset is particularly useful for researchers and professionals in the aerospace industry, providing essential information for tracking and analyzing satellites and space debris. The structure of the dataset typically includes rows representing individual satellites, with columns detailing various orbital parameters such as inclination, eccentricity, and period, as well as identifiers for each satellite. These variables are crucial for understanding the dynamics of satellite orbits and their potential interactions with other objects in space.\n\nThe data is collected from various authoritative sources, including satellite operators and space agencies, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as the frequency of updates and the completeness of the data, which may vary depending on the satellite's operational status and the reporting practices of different organizations. Preprocessing steps may include filtering out outdated or irrelevant entries, normalizing data formats, and integrating additional datasets for enhanced analysis.\n\nCelesTrak supports a wide range of research questions, including those related to collision risk assessment, orbital decay analysis, and the tracking of space debris. Researchers can apply various analytical methods, from descriptive statistics to more complex regression models and machine learning techniques, to derive insights from the data. Typical studies might involve evaluating the probability of collision between satellites, analyzing trends in space debris accumulation, or assessing the effectiveness of collision avoidance strategies.\n\nOverall, CelesTrak serves as a vital tool for anyone involved in space research and satellite operations, offering a wealth of information that can inform decision-making and enhance our understanding of the orbital environment."
  },
  {
    "name": "Wikipedia Pageviews",
    "description": "296B views/year since 2007. Hourly pageview data for all Wikimedia projects. attention metrics at scale",
    "category": "Social & Web",
    "url": "https://dumps.wikimedia.org/other/pageviews/",
    "docs_url": "https://dumps.wikimedia.org/other/pageviews/readme.html",
    "github_url": null,
    "tags": [
      "Wikipedia",
      "pageviews",
      "attention",
      "time-series",
      "large-scale"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "social media",
      "data analysis",
      "web analytics"
    ],
    "summary": "The Wikipedia Pageviews dataset provides hourly pageview data for all Wikimedia projects, capturing a total of 296 billion views per year since 2007. This dataset allows researchers and analysts to explore trends in user engagement and attention metrics at scale.",
    "use_cases": [
      "Analyzing trends in user engagement over time",
      "Comparing pageviews across different Wikimedia projects",
      "Investigating the impact of external events on Wikipedia traffic",
      "Studying the relationship between pageviews and article edits"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the hourly pageviews for Wikipedia?",
      "How can I analyze trends in Wikipedia pageviews?",
      "What metrics can be derived from Wikimedia project pageviews?",
      "How has Wikipedia traffic changed over the years?",
      "What are the attention metrics for various Wikipedia articles?",
      "How do pageviews correlate with current events?",
      "What is the total number of pageviews for Wikimedia projects in 2023?",
      "How can I visualize Wikipedia pageview data?"
    ],
    "domain_tags": [
      "media",
      "education"
    ],
    "data_modality": "time-series",
    "temporal_coverage": "2007-2023",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/wikimedia.png",
    "embedding_text": "The Wikipedia Pageviews dataset is a comprehensive collection of hourly pageview data across all Wikimedia projects, capturing a staggering total of 296 billion views annually since its inception in 2007. This dataset is structured in a time-series format, with rows representing individual time intervals (hourly) and columns detailing various metrics related to pageviews. Key variables include the timestamp, project name (e.g., Wikipedia, Wikimedia Commons), and the number of pageviews, which collectively provide insights into user engagement patterns over time. The collection methodology involves aggregating pageview data from Wikimedia's servers, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as variations in data collection methods across different Wikimedia projects and the absence of demographic information about the users accessing the pages. Common preprocessing steps may include time zone adjustments, handling missing data, and normalizing pageview counts for comparative analyses. Researchers often utilize this dataset to address a variety of research questions, such as examining the impact of significant global events on Wikipedia traffic, analyzing seasonal trends in user engagement, or correlating pageviews with other metrics like article edits or external media coverage. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a valuable resource for both academic research and practical applications in media studies and web analytics. Overall, the Wikipedia Pageviews dataset serves as a vital tool for understanding digital engagement and the dynamics of information consumption in the modern era."
  },
  {
    "name": "Creator Economy Reports",
    "description": "Survey-based earnings breakdowns by platform (YouTube, TikTok, Instagram, Twitch). Influencer Marketing Factory research",
    "category": "Creator Economy",
    "url": "https://theinfluencermarketingfactory.com/creator-economy/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "creator economy",
      "earnings",
      "influencers",
      "surveys"
    ],
    "best_for": "Learning creator economy analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "creator economy",
      "influencer marketing"
    ],
    "summary": "The Creator Economy Reports provide survey-based earnings breakdowns across various platforms such as YouTube, TikTok, Instagram, and Twitch. This dataset allows researchers and marketers to analyze influencer earnings and understand market trends in the creator economy.",
    "use_cases": [
      "Analyzing earnings trends across social media platforms",
      "Comparing influencer earnings to assess platform effectiveness",
      "Understanding demographic influences on influencer earnings",
      "Evaluating the impact of influencer marketing on brand performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the earnings of influencers on YouTube?",
      "How do TikTok earnings compare to Instagram?",
      "What platforms generate the highest revenue for creators?",
      "What insights can be drawn from influencer marketing surveys?",
      "How does the creator economy impact social media platforms?",
      "What trends are observed in influencer earnings across different platforms?",
      "What demographic factors influence earnings in the creator economy?",
      "How can brands leverage influencer earnings data for marketing strategies?"
    ],
    "domain_tags": [
      "marketing",
      "media",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/theinfluencermarketingfactory.png",
    "embedding_text": "The Creator Economy Reports dataset is a comprehensive collection of survey-based earnings breakdowns that focuses on various social media platforms, including YouTube, TikTok, Instagram, and Twitch. This dataset is structured in a tabular format, consisting of rows representing individual survey responses from influencers and columns detailing key variables such as platform name, earnings figures, audience engagement metrics, and demographic information of the influencers. The data is collected through systematic surveys conducted by Influencer Marketing Factory, which aims to capture a wide array of insights into the earnings landscape of the creator economy. The methodology involves reaching out to influencers across different platforms to gather self-reported earnings data, ensuring a diverse representation of the creator community. Coverage of the dataset spans various demographic segments, although specific temporal and geographic coverage details are not explicitly mentioned. Key variables within the dataset measure essential aspects such as earnings by platform, engagement rates, and influencer demographics, providing valuable insights into the dynamics of influencer marketing. However, users should be aware of potential limitations, such as self-reporting biases and the variability in earnings based on factors like niche and audience size. Common preprocessing steps may include cleaning the data for inconsistencies, normalizing earnings figures for comparative analysis, and segmenting the data by platform or demographic characteristics. Researchers can leverage this dataset to address various research questions, such as identifying which platforms yield the highest earnings for influencers, understanding the factors that influence earnings variability, and evaluating the effectiveness of influencer marketing strategies. The dataset supports a range of analyses, including regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize the earnings landscape. Typically, researchers utilize this dataset in studies focused on the economics of social media, the effectiveness of influencer marketing campaigns, and the evolving landscape of digital content creation."
  },
  {
    "name": "Pushshift Reddit Archive",
    "description": "5.6B comments, 651M posts since 2005. Full Reddit history for social/economic research. 100+ papers published",
    "category": "Social & Web",
    "url": "https://arxiv.org/abs/2001.08435",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Reddit",
      "social media",
      "comments",
      "NLP",
      "large-scale"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "social media",
      "NLP",
      "large-scale analysis"
    ],
    "summary": "The Pushshift Reddit Archive is a comprehensive dataset containing 5.6 billion comments and 651 million posts from Reddit since 2005. It serves as a valuable resource for social and economic research, enabling researchers to analyze trends, sentiments, and behaviors within online communities.",
    "use_cases": [
      "Analyzing sentiment trends in Reddit comments over time",
      "Studying the impact of social media discourse on economic events",
      "Exploring community dynamics and user engagement on Reddit",
      "Conducting NLP tasks such as topic modeling and text classification"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Pushshift Reddit Archive?",
      "How can I access the Pushshift Reddit Archive?",
      "What types of analyses can be performed on Reddit comments?",
      "What is the size of the Pushshift Reddit Archive dataset?",
      "How has Reddit content evolved over time?",
      "What research has been conducted using the Pushshift Reddit Archive?",
      "What variables are included in the Pushshift Reddit Archive?",
      "How can the Pushshift Reddit Archive be used for NLP tasks?"
    ],
    "domain_tags": [
      "social media",
      "research",
      "economics"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "benchmark_usage": [
      "100+ papers published"
    ],
    "model_score": 0.0001,
    "image_url": "/images/datasets/pushshift-reddit-archive.png",
    "embedding_text": "The Pushshift Reddit Archive is a vast dataset that encompasses an extensive collection of Reddit comments and posts, totaling 5.6 billion comments and 651 million posts since its inception in 2005. This dataset is structured primarily as text data, with each entry representing a unique comment or post made by users on the Reddit platform. The data schema includes various variables such as user ID, post ID, subreddit, timestamp, and the content of the comment or post itself. The collection methodology involves scraping Reddit's public API and archiving the data systematically to ensure comprehensive coverage of the platform's history. Researchers can utilize this dataset to explore a wide range of social and economic research questions, including the evolution of online discourse, community engagement patterns, and sentiment analysis across different subreddits. The dataset's temporal coverage spans nearly two decades, providing insights into how discussions and user interactions have changed over time. However, researchers should be aware of potential limitations, such as the presence of noise in user-generated content and the challenges of interpreting context in online discussions. Common preprocessing steps may include text normalization, removal of stop words, and tokenization to prepare the data for analysis. The dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for both qualitative and quantitative research. By leveraging the Pushshift Reddit Archive, researchers can address critical questions related to social behavior, economic trends, and the impact of digital communication on society.",
    "temporal_coverage": "2005-2023"
  },
  {
    "name": "USAspending.gov",
    "description": "User-friendly interface to federal spending data with bulk downloads in CSV/JSON and visualization tools",
    "category": "Defense Economics",
    "url": "https://www.usaspending.gov/",
    "docs_url": "https://api.usaspending.gov/",
    "github_url": "https://github.com/fedspendingtransparency/usaspending-api",
    "tags": [
      "spending",
      "contracts",
      "grants",
      "government"
    ],
    "best_for": "Accessible exploration of federal defense spending without FPDS complexity",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "USAspending.gov provides a user-friendly interface to access federal spending data, allowing users to explore spending patterns through bulk downloads in CSV and JSON formats. The platform includes visualization tools that enable users to analyze government contracts and grants effectively.",
    "use_cases": [
      "Analyzing trends in federal spending over time",
      "Comparing government contracts across different agencies",
      "Evaluating the distribution of grants in various sectors",
      "Visualizing the impact of federal spending on local economies"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is USAspending.gov?",
      "How can I access federal spending data?",
      "What types of data are available on USAspending.gov?",
      "How do I download data from USAspending.gov?",
      "What visualization tools are offered by USAspending.gov?",
      "What are the main categories of federal spending data?",
      "How can I analyze government contracts using USAspending.gov?",
      "What is the significance of federal spending data?"
    ],
    "domain_tags": [
      "government",
      "defense"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2000-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/usaspendinggov.png",
    "embedding_text": "USAspending.gov serves as a comprehensive platform for accessing federal spending data, structured in a user-friendly manner that facilitates both casual browsing and in-depth analysis. The dataset primarily comprises tabular data, organized into rows and columns that represent various spending categories, including contracts and grants awarded by the federal government. Each entry in the dataset typically includes key variables such as the agency responsible for the spending, the amount allocated, the type of spending (contract or grant), and the recipient of the funds. This structure allows for straightforward analysis and visualization of spending patterns across different sectors and time periods. The collection methodology for the data involves aggregating information from various federal agencies, ensuring a robust dataset that reflects the government's financial activities. However, users should be aware of potential limitations in data quality, such as inconsistencies in reporting or delays in data updates, which may affect the accuracy of analyses. Common preprocessing steps may include cleaning the data to remove duplicates, handling missing values, and transforming variables for better analysis. Researchers can leverage this dataset to address a variety of research questions, such as examining the impact of federal spending on economic growth, analyzing the distribution of funds across different regions, or investigating the relationship between government contracts and industry performance. The dataset supports various types of analyses, including descriptive statistics to summarize spending trends, regression analyses to identify factors influencing spending patterns, and machine learning techniques for predictive modeling. Overall, USAspending.gov is an invaluable resource for researchers, policymakers, and anyone interested in understanding the intricacies of federal spending and its implications for the economy."
  },
  {
    "name": "NHL API",
    "description": "Official NHL statistics and play-by-play data from 2010-present including shot locations, player stats, and game events",
    "category": "Sports & Athletics",
    "url": "https://api-web.nhle.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "hockey",
      "NHL",
      "play-by-play",
      "shot-locations",
      "player-stats"
    ],
    "best_for": "Hockey analytics, expected goals modeling, and player evaluation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NHL API provides access to official NHL statistics and play-by-play data from the 2010 season to the present. Users can analyze player statistics, shot locations, and game events, enabling in-depth exploration of hockey performance and strategy.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the official NHL statistics available through the NHL API?",
      "How can I access play-by-play data for NHL games from 2010 to present?",
      "What player stats are included in the NHL API?",
      "Where can I find shot location data for NHL games?",
      "What game events are tracked in the NHL API?",
      "How do I use the NHL API to analyze hockey performance?"
    ],
    "use_cases": [
      "Analyzing player performance trends over multiple seasons.",
      "Examining shot location data to assess team strategies.",
      "Investigating the correlation between game events and game outcomes."
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2010-present",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/nhle.png",
    "embedding_text": "The NHL API serves as a comprehensive resource for accessing official NHL statistics and play-by-play data, covering the seasons from 2010 to the present. This dataset is structured in a tabular format, where each row represents a unique event or statistic related to NHL games, players, and teams. Key columns in the dataset include player identifiers, game identifiers, event types (such as goals, assists, and penalties), shot locations, and various player statistics that measure performance metrics like goals scored, assists, and time on ice. The data is collected through official NHL sources, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as missing data for certain games or players, which may necessitate preprocessing steps like data cleaning and imputation. Researchers can utilize this dataset to address a variety of research questions, such as analyzing the impact of player performance on game outcomes, exploring trends in scoring over time, or assessing the effectiveness of different strategies based on shot location data. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both novice and experienced data scientists interested in sports analytics. Typically, researchers leverage the NHL API to conduct performance evaluations, develop predictive models, and enhance their understanding of game dynamics, ultimately contributing to the broader field of sports analytics."
  },
  {
    "name": "Retrosheet",
    "description": "Play-by-play data for MLB games from 1911-2024 including detailed event files, game logs, and transaction records",
    "category": "Sports & Athletics",
    "url": "https://www.retrosheet.org/",
    "docs_url": "https://www.retrosheet.org/datause.htm",
    "github_url": null,
    "tags": [
      "baseball",
      "play-by-play",
      "MLB",
      "game-logs",
      "historical"
    ],
    "best_for": "Granular game analysis, situational statistics, and historical baseball research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Retrosheet dataset provides comprehensive play-by-play data for Major League Baseball (MLB) games spanning from 1911 to 2024. This dataset allows users to analyze game events, player performances, and historical trends in baseball, making it a valuable resource for sports analysts and enthusiasts.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Retrosheet dataset?",
      "How can I access MLB play-by-play data?",
      "What historical baseball data is available from 1911 to 2024?",
      "What types of analyses can be performed with MLB game logs?",
      "Where can I find detailed event files for MLB games?",
      "What are the key variables in the Retrosheet dataset?",
      "How is transaction record data structured in Retrosheet?",
      "What insights can be gained from analyzing historical MLB data?"
    ],
    "use_cases": [
      "Analyzing player performance trends over decades.",
      "Studying the impact of specific game events on outcomes.",
      "Comparing historical game strategies and their evolution.",
      "Investigating the relationship between player transactions and team performance."
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1911-2024",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/retrosheet.png",
    "embedding_text": "The Retrosheet dataset is a rich repository of play-by-play data for Major League Baseball (MLB) games, covering an extensive temporal range from 1911 to 2024. This dataset is structured in a tabular format, with rows representing individual game events and columns detailing various attributes such as game date, teams involved, player actions, and outcomes. Key variables in the dataset include player IDs, event types (e.g., hits, strikes, errors), and game-specific information like inning and score, which collectively enable a granular analysis of baseball games. The data is collected from official MLB sources, ensuring a high level of accuracy and reliability, although users should be aware of potential limitations such as missing data for certain games or events due to historical record-keeping practices. Researchers and analysts typically preprocess the data by cleaning and normalizing variables, handling missing values, and possibly aggregating data for specific analyses. The dataset supports a variety of analytical approaches, including regression analysis, machine learning, and descriptive statistics, allowing users to explore research questions related to player performance, game strategies, and historical trends in baseball. Common research inquiries might involve examining how specific events influence game outcomes or analyzing the evolution of player performance metrics over time. Overall, the Retrosheet dataset serves as a foundational tool for anyone interested in the statistical analysis of baseball, providing insights that can enhance our understanding of the sport's history and its players."
  },
  {
    "name": "Online Retail II (UCI)",
    "description": "1M+ transactions from a UK-based online retailer (2009-2011). Contains invoice number, stock code, description, quantity, invoice date, unit price, customer ID, and country. Standard benchmark for RFM analysis and CLV modeling.",
    "category": "MarTech & Customer Analytics",
    "url": "https://archive.ics.uci.edu/ml/datasets/Online+Retail+II",
    "docs_url": "https://archive.ics.uci.edu/ml/datasets/Online+Retail+II",
    "github_url": null,
    "tags": [
      "CLV",
      "RFM",
      "e-commerce",
      "transaction-data"
    ],
    "best_for": "Learning CLV modeling with BG/NBD, RFM segmentation, and cohort analysis",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "basic-statistics"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "transaction-analysis"
    ],
    "summary": "The Online Retail II dataset consists of over one million transactions from a UK-based online retailer spanning the years 2009 to 2011. It includes detailed information such as invoice numbers, stock codes, descriptions, quantities, invoice dates, unit prices, customer IDs, and countries, making it a valuable resource for conducting RFM analysis and customer lifetime value (CLV) modeling.",
    "use_cases": [
      "Analyzing customer purchase behavior over time",
      "Segmenting customers based on RFM metrics",
      "Modeling customer lifetime value for marketing strategies",
      "Identifying trends in product sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Online Retail II dataset?",
      "How can I analyze customer lifetime value using transaction data?",
      "What variables are included in the Online Retail II dataset?",
      "Where can I find e-commerce transaction datasets?",
      "How to perform RFM analysis on retail data?",
      "What insights can be derived from online retail transaction data?",
      "What is the significance of invoice dates in transaction analysis?",
      "How to preprocess transaction data for analysis?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "temporal_coverage": "2009-2011",
    "geographic_scope": "UK",
    "benchmark_usage": [
      "Standard benchmark for RFM analysis",
      "Commonly used for CLV modeling"
    ],
    "model_score": 0.0001,
    "embedding_text": "The Online Retail II dataset is a comprehensive collection of transaction data from a UK-based online retailer, encompassing over one million individual transactions recorded between 2009 and 2011. This dataset is structured in a tabular format, featuring rows that represent individual transactions and columns that capture key variables such as invoice number, stock code, product description, quantity purchased, invoice date, unit price, customer ID, and the country of the customer. The richness of this dataset makes it an essential resource for researchers and data scientists interested in e-commerce analytics, particularly in the realms of customer behavior and marketing effectiveness. The collection methodology for this dataset involved systematic recording of transactions as they occurred in the online retail environment, ensuring that the data reflects real-world purchasing behavior. This dataset's temporal coverage spans two years, providing a longitudinal view of consumer purchasing patterns, while its geographic scope is limited to the UK market, making it particularly relevant for studies focused on British consumer behavior in the online retail sector. Key variables within the dataset include invoice number, which serves as a unique identifier for each transaction; stock code, which identifies the specific product sold; description, providing details about the product; quantity, indicating how many units were purchased; invoice date, which is crucial for time-series analysis; unit price, which allows for revenue calculations; customer ID, enabling customer segmentation; and country, which can be used to analyze geographic trends. The quality of the data is generally high, as it is sourced directly from transactional records; however, researchers should be aware of potential limitations such as missing values or anomalies in transaction data that may arise from returns or cancellations. Common preprocessing steps include handling missing data, converting invoice dates into a suitable format for analysis, and aggregating data to derive metrics such as total spend per customer or frequency of purchases. The dataset supports a variety of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it versatile for addressing a range of research questions. Researchers typically use this dataset to explore customer lifetime value, segment customers based on recency, frequency, and monetary value (RFM) metrics, and identify trends in product sales over time. The insights gained from analyzing this dataset can inform marketing strategies, inventory management, and overall business decision-making in the e-commerce sector."
  },
  {
    "name": "Glassdoor Reviews",
    "description": "Company ratings, salary reports, interview experiences. Employer review platform data for labor analytics",
    "category": "Labor Markets",
    "url": "https://www.glassdoor.com/research/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "salaries",
      "company reviews",
      "interviews",
      "employer ratings"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Glassdoor Reviews dataset contains valuable insights into company ratings, salary reports, and interview experiences, sourced from an employer review platform. Researchers and analysts can utilize this data to perform labor analytics, examining trends in employee satisfaction, compensation structures, and recruitment processes.",
    "use_cases": [
      "Analyzing trends in employee satisfaction across various companies.",
      "Comparing salary reports to industry standards.",
      "Examining the relationship between company ratings and employee retention.",
      "Investigating common themes in interview experiences to improve hiring processes."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average salaries reported on Glassdoor?",
      "How do company ratings vary across different industries?",
      "What are common interview experiences shared by candidates?",
      "How do employee reviews correlate with company performance?",
      "What trends can be identified in employer ratings over time?",
      "How do salary reports differ by geographic location?",
      "What factors influence employee satisfaction according to Glassdoor reviews?",
      "How do Glassdoor ratings impact job seekers' decisions?"
    ],
    "domain_tags": [
      "labor markets"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The Glassdoor Reviews dataset is structured in a tabular format, consisting of rows and columns that capture various aspects of employee experiences and employer evaluations. Each row typically represents an individual review or report, while the columns include variables such as company name, job title, salary, rating, interview experience, and review text. This dataset is collected from the Glassdoor platform, where employees voluntarily share their insights about their employers, including ratings on company culture, work-life balance, compensation, and career opportunities. The collection methodology relies on user-generated content, which means that the data reflects personal experiences and opinions, potentially introducing bias. However, the large volume of reviews can help mitigate individual biases and provide a more comprehensive view of employer performance. Key variables in the dataset include overall company ratings, which measure employee satisfaction, salary figures that provide insights into compensation trends, and qualitative feedback that highlights specific strengths and weaknesses of employers. Researchers should be aware of data quality issues, such as the potential for fraudulent reviews or the overrepresentation of certain companies due to higher employee engagement on the platform. Common preprocessing steps may involve cleaning the text data, normalizing salary figures, and categorizing reviews based on sentiment analysis. This dataset supports various types of analyses, including regression analysis to explore relationships between employee satisfaction and company performance metrics, machine learning techniques for predictive modeling, and descriptive statistics to summarize key trends. Researchers typically use this dataset to address questions related to labor market dynamics, such as the impact of company culture on employee retention, the effectiveness of recruitment strategies, and the correlation between salary levels and job satisfaction. Overall, the Glassdoor Reviews dataset serves as a rich resource for labor analytics, enabling researchers and analysts to derive meaningful insights into the employment landscape."
  },
  {
    "name": "NOAA Storm Events Database",
    "description": "Detailed records of significant weather events including property and crop damage estimates from 1950-present",
    "category": "Insurance & Actuarial",
    "url": "https://www.ncdc.noaa.gov/stormevents/",
    "docs_url": "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/",
    "github_url": null,
    "tags": [
      "weather-data",
      "catastrophe",
      "natural-disasters",
      "property-damage",
      "climate"
    ],
    "best_for": "Catastrophe modeling, climate risk assessment, and property insurance pricing",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NOAA Storm Events Database contains detailed records of significant weather events, including estimates of property and crop damage from 1950 to the present. Researchers and analysts can utilize this dataset to study the impact of natural disasters on various sectors, assess risk for insurance purposes, and inform climate-related policy decisions.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NOAA Storm Events Database?",
      "How can I access the NOAA Storm Events Database?",
      "What types of data are included in the NOAA Storm Events Database?",
      "What years does the NOAA Storm Events Database cover?",
      "How does the NOAA Storm Events Database measure property damage?",
      "What are the key variables in the NOAA Storm Events Database?",
      "How can the NOAA Storm Events Database be used for climate research?",
      "What are the limitations of the NOAA Storm Events Database?"
    ],
    "use_cases": [
      "Analyzing trends in property damage due to natural disasters over time.",
      "Assessing the economic impact of severe weather events on agriculture.",
      "Evaluating the effectiveness of disaster preparedness measures.",
      "Modeling the risk of future weather-related events for insurance underwriting."
    ],
    "domain_tags": [
      "insurance",
      "agriculture",
      "climate"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1950-present",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/noaa.png",
    "embedding_text": "The NOAA Storm Events Database is a comprehensive collection of data detailing significant weather events across the United States from 1950 to the present. This dataset is structured in a tabular format, consisting of rows that represent individual weather events and columns that capture various attributes of these events. Key variables include the type of event (e.g., tornado, flood, hurricane), date and time of occurrence, geographic location, and estimates of property and crop damage. The data is collected from multiple sources, including local and national meteorological agencies, and is regularly updated to ensure accuracy and relevance. Researchers often utilize this dataset to explore a range of research questions, such as the correlation between climate change and the frequency of severe weather events, the economic impact of these events on different sectors, and the effectiveness of disaster response strategies. Common preprocessing steps may include cleaning the data to handle missing values, standardizing formats for dates and locations, and aggregating data for specific analyses. The dataset supports various types of analyses, including regression analysis to identify trends over time, machine learning models to predict future events, and descriptive statistics to summarize the impact of weather events on communities. However, researchers should be aware of potential limitations, such as variations in reporting standards across different regions and the challenges of estimating damage accurately. Overall, the NOAA Storm Events Database serves as a vital resource for understanding the implications of natural disasters and informing policy decisions related to disaster preparedness and climate resilience."
  },
  {
    "name": "Medical Expenditure Panel Survey (MEPS)",
    "description": "Definitive U.S. data on healthcare expenditures, utilization, and insurance coverage. Surveys ~15,000 households annually with detailed spending by payer and service type. Free public use files.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://meps.ahrq.gov/mepsweb/",
    "source": "Agency for Healthcare Research and Quality (AHRQ)",
    "type": "Survey",
    "access": "Free public use files",
    "format": "SAS/Stata/CSV",
    "tags": [
      "Healthcare",
      "Expenditures",
      "Survey",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "economics",
      "data-analysis"
    ],
    "summary": "The Medical Expenditure Panel Survey (MEPS) provides comprehensive data on healthcare expenditures, utilization, and insurance coverage in the U.S. Researchers can analyze spending patterns by payer and service type, making it a valuable resource for understanding healthcare economics.",
    "use_cases": [
      "Analyzing trends in healthcare spending over time.",
      "Examining the impact of insurance coverage on healthcare utilization.",
      "Comparing expenditures across different demographic groups.",
      "Investigating the relationship between service type and payer."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Medical Expenditure Panel Survey?",
      "How can I access the MEPS data?",
      "What types of healthcare expenditures are covered in MEPS?",
      "What demographic information is included in the MEPS dataset?",
      "How does MEPS data help in understanding healthcare utilization?",
      "What are the key variables in the MEPS dataset?",
      "Can I use MEPS data for regression analysis?",
      "What are the limitations of the MEPS dataset?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare",
      "economics"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The Medical Expenditure Panel Survey (MEPS) is a vital dataset that provides definitive data on healthcare expenditures, utilization, and insurance coverage in the United States. Conducted annually, MEPS surveys approximately 15,000 households, collecting detailed information on spending categorized by payer and service type. The dataset is structured in a tabular format, containing multiple rows representing individual households and columns that capture various variables related to healthcare expenditures, such as total spending, types of services utilized, and insurance coverage details. Key variables include total expenditures, out-of-pocket costs, and the type of insurance coverage, which are essential for analyzing healthcare economics. MEPS employs a rigorous data collection methodology that includes interviews and surveys, ensuring a comprehensive representation of the U.S. population. However, researchers should be aware of potential limitations, such as response bias and the accuracy of self-reported data. Common preprocessing steps may involve cleaning the data, handling missing values, and transforming variables for analysis. The MEPS dataset supports a wide range of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically utilize MEPS data to address critical research questions related to healthcare spending trends, the impact of insurance on healthcare access, and disparities in healthcare utilization across different demographic groups. Overall, MEPS serves as a crucial resource for policymakers, economists, and healthcare researchers aiming to understand and improve the healthcare system in the U.S."
  },
  {
    "name": "nflverse",
    "description": "Comprehensive NFL play-by-play data from 1999-present with EPA, win probability, and player participation data",
    "category": "Sports & Athletics",
    "url": "https://github.com/nflverse/nflverse-data",
    "docs_url": "https://nflreadr.nflverse.com/",
    "github_url": "https://github.com/nflverse/nflverse-data",
    "tags": [
      "football",
      "NFL",
      "play-by-play",
      "EPA",
      "win-probability"
    ],
    "best_for": "NFL analytics, expected points analysis, and game strategy optimization",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The nflverse dataset provides comprehensive play-by-play data for NFL games from 1999 to the present. It includes advanced metrics such as Expected Points Added (EPA) and win probability, along with detailed player participation data, making it a valuable resource for analyzing game performance and strategy.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the nflverse dataset?",
      "How can I access NFL play-by-play data?",
      "What metrics are included in the nflverse dataset?",
      "How does EPA relate to NFL game outcomes?",
      "What player participation data is available in nflverse?",
      "Where can I find comprehensive NFL data from 1999 to present?",
      "What analyses can be performed using nflverse data?"
    ],
    "use_cases": [
      "Analyzing the impact of specific plays on game outcomes using EPA.",
      "Examining trends in win probability throughout NFL seasons.",
      "Evaluating player performance based on participation and play data.",
      "Conducting regression analyses to predict game outcomes."
    ],
    "domain_tags": [
      "sports",
      "analytics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1999-present",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nflverse.png",
    "embedding_text": "The nflverse dataset is a rich repository of comprehensive NFL play-by-play data, spanning from 1999 to the present. This dataset is structured in a tabular format, with rows representing individual plays and columns detailing various attributes of each play, such as the type of play, player involvement, yardage gained, and critical metrics like Expected Points Added (EPA) and win probability. The inclusion of player participation data allows for in-depth analysis of individual contributions to team performance. The data is collected from official NFL sources, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as missing data for certain plays or discrepancies in player statistics due to recording errors. Common preprocessing steps may include cleaning the data for missing values, normalizing metrics for comparative analysis, and transforming categorical variables into numerical formats suitable for machine learning algorithms. Researchers can utilize the nflverse dataset to address a variety of research questions, such as the effectiveness of different offensive strategies, the correlation between player performance metrics and game outcomes, and the evolution of play-calling trends over time. The dataset supports a range of analytical techniques, including regression analysis, machine learning models, and descriptive statistics, making it a versatile tool for sports analysts and data scientists alike. By leveraging the insights derived from this dataset, researchers can contribute to the understanding of game dynamics and enhance predictive modeling in sports analytics."
  },
  {
    "name": "National Transit Database (NTD)",
    "description": "Definitive source for US transit statistics since 2002. Ridership, operating expenses, capital expenses, safety incidents for all federally-funded transit agencies.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.transit.dot.gov/ntd",
    "docs_url": "https://www.transit.dot.gov/ntd/ntd-data",
    "github_url": null,
    "tags": [
      "transit",
      "ridership",
      "financials",
      "safety",
      "US"
    ],
    "best_for": "Transit system performance analysis, cost benchmarking, and ridership trends",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "economics",
      "public policy"
    ],
    "summary": "The National Transit Database (NTD) serves as the definitive source for US transit statistics, providing comprehensive data on ridership, operating expenses, capital expenses, and safety incidents for federally-funded transit agencies. Researchers and analysts can utilize this dataset to assess transit agency performance, analyze trends in public transportation usage, and evaluate the financial health of transit systems across the United States.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest statistics on US transit ridership?",
      "How do operating expenses vary among different transit agencies?",
      "What safety incidents have been reported in US transit systems?",
      "How has capital expenditure changed in the transit sector over the years?",
      "What trends can be observed in public transportation usage since 2002?",
      "How do financial metrics of transit agencies correlate with ridership data?",
      "What are the key performance indicators for federally-funded transit agencies?",
      "How does the NTD dataset support research in transportation economics?"
    ],
    "use_cases": [
      "Analyzing trends in ridership over time to inform policy decisions.",
      "Evaluating the financial sustainability of different transit agencies.",
      "Investigating the relationship between operating expenses and ridership levels.",
      "Assessing safety incidents to improve transit agency operations."
    ],
    "domain_tags": [
      "transportation",
      "public policy",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2002-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/national-transit-database-ntd.png",
    "embedding_text": "The National Transit Database (NTD) is a comprehensive and authoritative source of transit statistics in the United States, established in 2002. It encompasses a wide array of data related to public transportation, including ridership figures, operating expenses, capital expenditures, and safety incidents reported by federally-funded transit agencies. The dataset is structured in a tabular format, with rows representing individual transit agencies and columns capturing various metrics such as total ridership, fare revenues, operating costs, and incident reports. This structured approach allows for easy manipulation and analysis using data analysis tools and programming languages such as Python and R. The collection methodology involves gathering data directly from transit agencies that receive federal funding, ensuring a standardized reporting process across the nation. This dataset covers a significant temporal range, from 2002 to the present, providing a longitudinal view of trends and changes in the public transportation landscape. Geographically, the NTD encompasses all federally-funded transit agencies in the United States, making it a valuable resource for researchers interested in regional and national transportation issues. Key variables within the dataset include total ridership, which measures the number of passengers using transit services, operating expenses that reflect the costs incurred by agencies to provide services, and safety incident counts that track occurrences of accidents or safety violations. These variables are critical for understanding the performance and safety of transit systems. However, users should be aware of potential limitations, such as variations in reporting practices among agencies and the impact of external factors like economic downturns or public health crises on ridership data. Common preprocessing steps may involve cleaning the data to address missing values, normalizing financial figures for inflation, and aggregating data to analyze trends over time. Researchers typically leverage the NTD dataset to address a variety of research questions, such as examining the impact of funding changes on transit agency performance, analyzing the effectiveness of safety measures, and exploring demographic trends in public transportation usage. The dataset supports various types of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize key findings. Overall, the NTD serves as a vital tool for policymakers, researchers, and transit advocates seeking to enhance the understanding and effectiveness of public transportation systems in the United States.",
    "benchmark_usage": [
      "Performance analysis of transit agencies",
      "Financial health assessment of public transportation systems"
    ]
  },
  {
    "name": "KDD Cup 2009 Customer Relationship Prediction",
    "description": "Orange Telecom CRM dataset with 50,000 customers and 230 anonymized features. Predict churn, appetency (propensity to buy), and up-selling. Classic benchmark for CRM analytics.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data",
    "docs_url": "https://www.kdd.org/kdd-cup/view/kdd-cup-2009",
    "github_url": null,
    "tags": [
      "CRM",
      "churn",
      "uplift",
      "classification"
    ],
    "best_for": "Learning CRM analytics, multi-task learning, and handling messy real-world data",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis",
      "classification-techniques"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "customer-retention"
    ],
    "summary": "The KDD Cup 2009 Customer Relationship Prediction dataset consists of 50,000 customer records from Orange Telecom, featuring 230 anonymized attributes. It is primarily used to predict customer churn, appetency, and up-selling opportunities, making it a classic benchmark for CRM analytics.",
    "use_cases": [
      "Predicting customer churn for telecom services",
      "Analyzing customer appetency for targeted marketing campaigns",
      "Developing up-selling strategies based on customer behavior",
      "Benchmarking CRM analytics methodologies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the KDD Cup 2009 Customer Relationship Prediction dataset?",
      "How can I use the Orange Telecom CRM dataset for churn prediction?",
      "What features are included in the KDD Cup 2009 dataset?",
      "Where can I find the KDD Cup 2009 Customer Relationship Prediction dataset?",
      "What analyses can be performed with the Orange Telecom CRM dataset?",
      "How to predict customer appetency using the KDD Cup 2009 dataset?",
      "What machine learning techniques are suitable for analyzing the KDD Cup 2009 dataset?",
      "What are the challenges in using the KDD Cup 2009 dataset for customer analytics?"
    ],
    "domain_tags": [
      "telecommunications",
      "marketing"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Common benchmark for CRM analytics and customer behavior prediction"
    ],
    "model_score": 0.0001,
    "image_url": "/images/logos/kdd.png",
    "embedding_text": "The KDD Cup 2009 Customer Relationship Prediction dataset is a comprehensive collection of customer data from Orange Telecom, encompassing 50,000 customer records and 230 anonymized features. This dataset is structured in a tabular format, where each row represents a unique customer, and each column corresponds to a specific attribute or feature related to the customer's behavior and demographics. The features include various metrics that are crucial for understanding customer relationships, such as usage patterns, service subscriptions, and demographic information. The dataset is designed to facilitate the prediction of customer churn, appetency, and up-selling opportunities, making it a valuable resource for researchers and practitioners in the fields of marketing technology and customer analytics. The collection methodology for this dataset involved gathering data from Orange Telecom's customer interactions and transactions, ensuring that the data reflects real-world customer behavior in a telecommunications context. However, it is important to note that the dataset is anonymized to protect customer privacy, which may limit the ability to conduct certain types of demographic analyses. The key variables in this dataset measure various aspects of customer behavior, including their likelihood to discontinue service (churn), their propensity to purchase additional services (appetency), and their potential for up-selling based on past interactions. Researchers often encounter data quality challenges such as missing values or inconsistencies in the dataset, which necessitate common preprocessing steps like data cleaning, normalization, and feature selection before conducting analyses. The KDD Cup 2009 dataset supports a wide range of analytical techniques, including regression analysis, machine learning algorithms, and descriptive statistics. Researchers typically utilize this dataset to address critical research questions related to customer retention strategies, marketing effectiveness, and the overall understanding of customer behavior in the telecommunications industry. By leveraging the insights gained from this dataset, businesses can develop targeted marketing strategies, enhance customer engagement, and ultimately improve customer retention rates."
  },
  {
    "name": "Lahman Baseball Database",
    "description": "Complete historical baseball statistics from 1871-2024 including batting, pitching, fielding, and salaries for every MLB player and team",
    "category": "Sports & Athletics",
    "url": "https://www.seanlahman.com/baseball-archive/statistics/",
    "docs_url": "https://www.seanlahman.com/baseball-archive/statistics/",
    "github_url": "https://github.com/chadwickbureau/baseballdatabank",
    "tags": [
      "baseball",
      "historical",
      "MLB",
      "player-statistics",
      "sabermetrics"
    ],
    "best_for": "Learning baseball analytics, historical trend analysis, and player valuation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Lahman Baseball Database is a comprehensive collection of historical baseball statistics that spans from 1871 to 2024. It includes detailed data on batting, pitching, fielding, and salaries for every Major League Baseball (MLB) player and team, making it a valuable resource for sports analysts and enthusiasts alike.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical batting statistics in the Lahman Baseball Database?",
      "How can I access MLB player salary data from 1871 to 2024?",
      "What pitching statistics are available in the Lahman Baseball Database?",
      "Where can I find comprehensive baseball statistics for every MLB player?",
      "What is the coverage of the Lahman Baseball Database?",
      "How does the Lahman Baseball Database support sabermetrics analysis?",
      "What types of analyses can be performed using the Lahman Baseball Database?",
      "What historical baseball data is included in the Lahman Baseball Database?"
    ],
    "use_cases": [
      "Analyzing player performance trends over time",
      "Comparing team statistics across different seasons",
      "Conducting sabermetric analyses to evaluate player value",
      "Exploring the impact of player salaries on team performance"
    ],
    "domain_tags": [
      "sports",
      "analytics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1871-2024",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/lahman-baseball-database.png",
    "embedding_text": "The Lahman Baseball Database is an extensive repository of historical baseball statistics that provides a wealth of information for researchers, analysts, and baseball enthusiasts. This dataset encompasses a wide array of data points, including batting, pitching, fielding, and salary statistics for every Major League Baseball (MLB) player and team from 1871 to 2024. The data is structured in a tabular format, with rows representing individual players and teams, and columns detailing various statistics such as batting averages, earned run averages, and salary figures. The database is meticulously compiled from a variety of sources, including official MLB records, historical publications, and other reputable baseball statistics resources, ensuring a high level of accuracy and reliability. Coverage of the dataset is extensive, spanning over a century of baseball history, which allows for longitudinal studies and trend analysis in the sport. Key variables within the dataset include player identifiers, team identifiers, game dates, and various performance metrics that measure player contributions in different aspects of the game. For instance, batting statistics may include metrics such as home runs, runs batted in, and on-base percentages, while pitching statistics may cover earned run averages, strikeouts, and walks. Despite its comprehensive nature, users should be aware of potential limitations in data quality, such as discrepancies in historical records or changes in statistical definitions over time. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing statistics to account for changes in the game, and aggregating data for specific analyses. Researchers often utilize the Lahman Baseball Database to address a variety of research questions, such as evaluating the impact of player performance on team success, analyzing trends in player salaries relative to performance metrics, and conducting sabermetric analyses to uncover hidden insights into player value. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in sports analytics. Overall, the Lahman Baseball Database serves as a foundational resource for anyone interested in exploring the rich history of baseball through data-driven insights."
  },
  {
    "name": "Transitland GTFS Feeds",
    "description": "Aggregated GTFS data from 2,500+ transit agencies across 55+ countries. The largest open transit data aggregator with REST and GraphQL APIs.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.transit.land/",
    "docs_url": "https://www.transit.land/documentation",
    "github_url": "https://github.com/transitland",
    "tags": [
      "transit",
      "GTFS",
      "schedules",
      "public-transportation",
      "global"
    ],
    "best_for": "Transit network analysis, accessibility research, and cross-city comparisons",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Transitland GTFS Feeds dataset provides aggregated General Transit Feed Specification (GTFS) data from over 2,500 transit agencies across more than 55 countries. This dataset serves as the largest open transit data aggregator, offering REST and GraphQL APIs for easy access and integration into various applications and analyses.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Transitland GTFS Feeds dataset?",
      "How can I access GTFS data from multiple transit agencies?",
      "What are the benefits of using aggregated GTFS data?",
      "In which countries is the Transitland GTFS Feeds data available?",
      "How does the Transitland GTFS Feeds support public transportation analysis?",
      "What APIs are available for accessing the Transitland GTFS Feeds?",
      "What types of transit data can I find in the Transitland GTFS Feeds?",
      "How can I use GTFS data for transportation economics research?"
    ],
    "use_cases": [
      "Analyzing public transportation accessibility in urban areas",
      "Evaluating the efficiency of transit schedules across different regions",
      "Studying the impact of transit availability on economic activity",
      "Comparing transit systems in various countries for best practices"
    ],
    "domain_tags": [
      "transportation",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/transitland-gtfs-feeds.png",
    "embedding_text": "The Transitland GTFS Feeds dataset is a comprehensive collection of General Transit Feed Specification (GTFS) data that aggregates information from over 2,500 transit agencies spanning more than 55 countries. This dataset is pivotal for researchers and developers interested in transportation economics and technology, providing a wealth of information about public transportation systems worldwide. The data is structured in a tabular format, typically consisting of rows representing individual transit trips or schedules, and columns detailing various attributes such as trip ID, route ID, stop times, and geographic coordinates. Each variable in the dataset captures essential aspects of public transportation, enabling detailed analysis and insights into transit operations. The collection methodology for the Transitland GTFS Feeds involves aggregating publicly available GTFS data from transit agencies, ensuring a broad and representative dataset that reflects the global landscape of public transportation. While the dataset excels in its breadth and accessibility, researchers should be aware of potential limitations related to data quality, such as inconsistencies in agency reporting or variations in data completeness. Common preprocessing steps may include cleaning the data to handle missing values, standardizing formats, and merging datasets from different agencies to create a unified view of transit operations. This dataset supports a variety of research questions, including those related to transit accessibility, schedule efficiency, and the economic impacts of public transportation systems. Analysts can employ various methodologies, including regression analysis, machine learning techniques, and descriptive statistics, to derive meaningful insights from the data. Researchers typically leverage the Transitland GTFS Feeds in studies that aim to enhance public transportation systems, improve urban mobility, and inform policy decisions related to transportation infrastructure. By utilizing the REST and GraphQL APIs provided by Transitland, users can seamlessly integrate this rich dataset into their applications, facilitating innovative solutions and analyses in the realm of transportation economics and technology."
  },
  {
    "name": "NHANES (National Health and Nutrition Examination Survey)",
    "description": "Unique combination of interviews and physical examinations including blood/urine samples. Covers nutrition, chronic diseases, and environmental exposures. ~5,000 participants annually.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.cdc.gov/nchs/nhanes/",
    "source": "CDC National Center for Health Statistics",
    "type": "Survey + Biomarkers",
    "access": "Free public use files",
    "format": "SAS/XPT",
    "tags": [
      "Healthcare",
      "Survey",
      "Biomarkers",
      "Nutrition",
      "Free"
    ],
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NHANES dataset provides a comprehensive collection of health and nutrition data from a diverse group of participants through interviews and physical examinations, including blood and urine samples. Researchers can utilize this dataset to analyze the relationships between nutrition, chronic diseases, and environmental exposures, contributing valuable insights into public health.",
    "use_cases": [
      "Analyzing the impact of dietary habits on chronic disease prevalence.",
      "Investigating environmental exposures and their correlation with health outcomes.",
      "Studying the nutritional status of different demographic groups.",
      "Evaluating the effectiveness of public health interventions."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the NHANES dataset?",
      "How can I access the NHANES health survey data?",
      "What types of analyses can be performed using NHANES data?",
      "What variables are included in the NHANES dataset?",
      "How does NHANES measure chronic diseases?",
      "What are the key findings from NHANES studies?",
      "How is the NHANES data collected?",
      "What are the limitations of the NHANES dataset?"
    ],
    "update_frequency": "Biennial cycles",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare",
      "health-tech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/nhanes-national-health-and-nutrition-examination-survey.png",
    "embedding_text": "The National Health and Nutrition Examination Survey (NHANES) is a vital dataset that combines interviews and physical examinations to gather comprehensive health and nutrition information from the U.S. population. This dataset is unique in its approach, as it includes both self-reported data through interviews and objective measurements obtained from physical examinations, including blood and urine samples. The NHANES dataset typically encompasses around 5,000 participants annually, providing a rich source of data for researchers interested in health trends, nutrition, and chronic disease epidemiology. The data structure consists of rows representing individual participants and columns that capture a wide array of variables related to health status, dietary intake, and environmental exposures. Key variables include demographic information, health indicators such as body mass index (BMI), blood pressure, cholesterol levels, and various biomarkers that reflect nutritional status and exposure to environmental toxins. The collection methodology for NHANES is rigorous, employing a complex sampling design that ensures a representative sample of the U.S. population. Data is collected through interviews conducted by trained professionals, followed by physical examinations performed in mobile examination centers. This dual approach enhances the reliability of the data, although researchers should be aware of potential limitations, such as self-reporting bias and the challenges of generalizing findings beyond the surveyed population. Common preprocessing steps for NHANES data may include handling missing values, normalizing data, and transforming variables for analysis. Researchers often use NHANES data to address a variety of research questions, such as examining the relationship between dietary patterns and the prevalence of obesity or chronic diseases like diabetes and cardiovascular conditions. The dataset supports various types of analyses, including regression analyses to identify predictors of health outcomes, machine learning techniques for predictive modeling, and descriptive statistics to summarize health trends over time. By leveraging the NHANES dataset, researchers can contribute to a deeper understanding of public health issues, inform policy decisions, and develop targeted interventions aimed at improving health outcomes across diverse populations."
  },
  {
    "name": "JobHop (Flanders)",
    "description": "2.3M occupations and 391K resumes with real career trajectories mapped to ESCO codes. Labor mobility research",
    "category": "Labor Markets",
    "url": "https://huggingface.co/datasets/VDAB/jobhop",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "careers",
      "resumes",
      "occupations",
      "labor mobility",
      "ESCO"
    ],
    "best_for": "Learning labor markets analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The JobHop dataset contains 2.3 million occupations and 391,000 resumes that map real career trajectories to ESCO codes, facilitating labor mobility research. Researchers can analyze labor market trends, career progression, and the relationship between occupations and skills.",
    "use_cases": [
      "Analyzing labor mobility trends",
      "Mapping career trajectories to skills",
      "Investigating the relationship between occupations and labor market demands"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the career trajectories mapped to ESCO codes?",
      "How many resumes are included in the JobHop dataset?",
      "What occupations are represented in the JobHop dataset?",
      "What insights can be gained from analyzing labor mobility?",
      "How does the JobHop dataset support labor market research?",
      "What is the significance of ESCO codes in career analysis?",
      "What trends can be identified in the 2.3M occupations?",
      "How can the JobHop dataset be used for skills analysis?"
    ],
    "domain_tags": [
      "labor markets"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Flanders",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/huggingface.png",
    "embedding_text": "The JobHop dataset is a comprehensive resource for understanding labor mobility and career trajectories within the Flanders region. It comprises a substantial collection of 2.3 million occupations and 391,000 resumes, each meticulously mapped to ESCO (European Skills, Competences, Qualifications and Occupations) codes. This mapping is crucial as it allows researchers and analysts to explore the intricate relationships between various occupations and the skills required for each, thereby providing insights into labor market dynamics. The dataset is structured in a tabular format, with rows representing individual resumes and occupations, and columns detailing key variables such as job titles, skills, career paths, and associated ESCO codes. This structure facilitates a range of analyses, from descriptive statistics to more complex regression and machine learning models. The collection methodology for the JobHop dataset involves aggregating real-world data from various sources, including job postings, resume databases, and labor market reports, ensuring a rich and diverse representation of the labor market in Flanders. However, like any dataset, it comes with certain limitations. The accuracy of the data is contingent upon the quality of the resumes and the completeness of the information provided by individuals. Common preprocessing steps may include data cleaning to remove duplicates, standardizing job titles, and ensuring consistency in skill representations. Researchers can leverage the JobHop dataset to address a myriad of research questions, such as identifying trends in labor mobility, understanding how specific skills influence career advancement, and analyzing the impact of economic changes on job availability. The dataset supports various types of analyses, including descriptive analyses to summarize the data, regression analyses to explore relationships between variables, and machine learning techniques to predict future labor market trends. In summary, the JobHop dataset serves as a vital tool for researchers, policymakers, and educators interested in labor market studies, providing a wealth of information that can inform decisions and strategies aimed at enhancing workforce development and mobility."
  },
  {
    "name": "Amazon Fine Foods Reviews",
    "description": "500,000+ food product reviews from Amazon spanning 1999-2012. Includes user/product IDs, ratings, helpfulness votes, and full review text. Popular for sentiment analysis and review-based recommendations.",
    "category": "MarTech & Customer Analytics",
    "url": "https://snap.stanford.edu/data/web-FineFoods.html",
    "docs_url": "https://snap.stanford.edu/data/web-FineFoods.html",
    "github_url": null,
    "tags": [
      "reviews",
      "sentiment",
      "food",
      "NLP",
      "recommendations"
    ],
    "best_for": "Learning sentiment analysis, review mining, and hybrid recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "sentiment-analysis"
    ],
    "summary": "The Amazon Fine Foods Reviews dataset contains over 500,000 food product reviews collected from Amazon between 1999 and 2012. This dataset is ideal for conducting sentiment analysis and building recommendation systems based on user feedback and product ratings.",
    "use_cases": [
      "Analyzing customer sentiment towards various food products.",
      "Building a recommendation system based on user reviews and ratings.",
      "Identifying trends in food product reviews over time.",
      "Evaluating the relationship between review helpfulness and ratings."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most helpful food product reviews on Amazon?",
      "How can I analyze sentiment in Amazon food reviews?",
      "What trends can be identified in Amazon food product ratings over time?",
      "How do user ratings correlate with helpfulness votes in Amazon reviews?",
      "What insights can be drawn from Amazon Fine Foods Reviews for product recommendations?",
      "How can I use Amazon food reviews for natural language processing tasks?",
      "What are the common themes in customer feedback for food products on Amazon?",
      "How can I visualize the distribution of ratings in Amazon food reviews?"
    ],
    "domain_tags": [
      "retail",
      "food"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "temporal_coverage": "1999-2012",
    "model_score": 0.0001,
    "embedding_text": "The Amazon Fine Foods Reviews dataset is a comprehensive collection of over 500,000 reviews of food products available on Amazon, spanning from 1999 to 2012. This dataset is structured in a tabular format, containing various columns that represent key variables such as user IDs, product IDs, ratings, helpfulness votes, and the full text of the reviews. Each row corresponds to an individual review, allowing for detailed analysis of consumer feedback and product performance. The data collection methodology involved scraping reviews from the Amazon platform, ensuring a rich source of user-generated content that reflects consumer opinions and experiences with food products. The dataset's temporal coverage is explicitly defined, providing a historical perspective on consumer preferences and trends in the food industry over a significant period. However, it does not specify a geographic scope, which means the reviews may originate from various locations without a clear delineation of regional differences. Key variables within the dataset include the rating given by the user, which typically ranges from 1 to 5 stars, and the helpfulness votes, which indicate how many users found the review useful. These variables are crucial for measuring customer satisfaction and the perceived value of the reviews. Data quality is generally high, given the large volume of reviews; however, limitations may include potential biases in user feedback, as reviews can be influenced by factors such as promotional activities or individual user experiences. Common preprocessing steps for this dataset may involve cleaning the review text, handling missing values, and converting ratings into numerical formats suitable for analysis. Researchers can leverage this dataset to address various research questions, such as understanding the factors that influence consumer sentiment, exploring the relationship between product ratings and review helpfulness, and identifying trends in consumer preferences over time. The dataset supports a wide range of analyses, including regression analysis to predict ratings based on review text, machine learning models for sentiment classification, and descriptive statistics to summarize review characteristics. Typically, researchers utilize the Amazon Fine Foods Reviews dataset in studies focused on consumer behavior, sentiment analysis, and the development of recommendation systems, making it a valuable resource for both academic and commercial applications in the fields of marketing and customer analytics.",
    "benchmark_usage": [
      "Sentiment analysis",
      "Review-based recommendations"
    ]
  },
  {
    "name": "National Household Travel Survey (NHTS)",
    "description": "Comprehensive US travel behavior data since 1969 capturing daily non-commercial travel by all modes. The authoritative source on American travel patterns.",
    "category": "Transportation Economics & Technology",
    "url": "https://nhts.ornl.gov/",
    "docs_url": "https://nhts.ornl.gov/documentation",
    "github_url": null,
    "tags": [
      "travel-behavior",
      "survey",
      "mode-choice",
      "demographics",
      "commuting"
    ],
    "best_for": "Understanding travel demand patterns, mode choice analysis, and transportation planning",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "travel-behavior",
      "demographics"
    ],
    "summary": "The National Household Travel Survey (NHTS) provides comprehensive data on daily non-commercial travel behavior across the United States since 1969. Researchers can utilize this dataset to analyze travel patterns, mode choices, and demographic influences on commuting behaviors.",
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the National Household Travel Survey?",
      "How can I access the NHTS dataset?",
      "What types of travel behaviors are captured in the NHTS?",
      "What demographic information is included in the NHTS?",
      "How has travel behavior changed in the US since 1969?",
      "What modes of transportation are analyzed in the NHTS?",
      "How can the NHTS data inform transportation policy?",
      "What are the key findings from the National Household Travel Survey?"
    ],
    "use_cases": [
      "Analyzing trends in commuting patterns over the decades.",
      "Investigating the impact of demographics on mode choice for travel.",
      "Examining the relationship between travel behavior and urban development.",
      "Assessing the effects of policy changes on travel behavior."
    ],
    "domain_tags": [
      "transportation",
      "urban planning",
      "sociology"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1969-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/ornl.png",
    "embedding_text": "The National Household Travel Survey (NHTS) is a pivotal dataset that captures the intricacies of daily non-commercial travel behavior in the United States, providing insights into how individuals and households travel across various modes of transportation. Since its inception in 1969, the NHTS has evolved to include a wide array of variables that reflect the changing landscape of travel patterns in America. This dataset is structured in a tabular format, consisting of numerous rows representing individual travel records and columns that detail various attributes such as trip purpose, mode of transportation, distance traveled, and demographic information of the travelers. Key variables within the dataset include trip duration, distance, mode choice (e.g., car, public transit, walking), and demographic factors such as age, income, and household size. These variables allow researchers to conduct detailed analyses of travel behavior and identify trends over time.\n\nThe methodology for data collection in the NHTS involves a combination of surveys and interviews conducted with households across the nation. Participants are asked to report their travel behavior over a specified period, providing a comprehensive view of daily travel activities. This approach ensures a rich dataset that captures not only the frequency of travel but also the context in which travel occurs. The NHTS is particularly valuable for its demographic coverage, as it includes diverse populations from urban, suburban, and rural areas, enabling researchers to analyze travel behavior across different segments of society.\n\nDespite its strengths, the NHTS does have limitations. Data quality can be affected by recall bias, as respondents may not accurately remember all trips taken during the survey period. Additionally, changes in survey methodology over the years may introduce inconsistencies in longitudinal analyses. Common preprocessing steps for researchers using the NHTS data include cleaning the dataset to handle missing values, standardizing variable formats, and aggregating data for specific analyses.\n\nThe NHTS can address a variety of research questions, such as how demographic factors influence mode choice, the impact of urban design on travel behavior, and trends in commuting patterns over time. It supports various types of analyses, including regression analysis to identify relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize travel behaviors. Researchers typically leverage the NHTS in studies related to transportation policy, urban planning, and social behavior, making it an essential resource for understanding the dynamics of travel in the United States."
  },
  {
    "name": "FDA Adverse Event Reporting System (FAERS)",
    "description": "Database of 21+ million adverse event reports for drugs and therapeutic biologics. Free API access through openFDA. Supports pharmacovigilance and drug safety research.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://open.fda.gov/data/faers/",
    "source": "FDA",
    "type": "Adverse Event Database",
    "access": "Free (API available)",
    "format": "JSON API/Downloads",
    "tags": [
      "Healthcare",
      "Drug Safety",
      "Pharmacovigilance",
      "Free",
      "API"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "drug-safety",
      "pharmacovigilance"
    ],
    "summary": "The FDA Adverse Event Reporting System (FAERS) is a comprehensive database containing over 21 million reports of adverse events related to drugs and therapeutic biologics. Researchers can utilize this dataset to conduct pharmacovigilance studies, analyze drug safety, and explore the relationships between medications and adverse outcomes.",
    "use_cases": [
      "Analyzing trends in adverse drug reactions over time.",
      "Identifying potential safety signals for newly approved drugs.",
      "Evaluating the effectiveness of risk management strategies.",
      "Conducting comparative studies on the safety profiles of different therapeutic biologics."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the FDA Adverse Event Reporting System?",
      "How can I access the FAERS database?",
      "What types of data are included in the FAERS dataset?",
      "What are the common uses of the FAERS data?",
      "How does the FAERS support drug safety research?",
      "What is the significance of pharmacovigilance in healthcare?",
      "How many adverse event reports are in the FAERS database?",
      "What methodologies can be applied to analyze FAERS data?"
    ],
    "update_frequency": "Quarterly",
    "geographic_coverage": "United States (primarily)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "embedding_text": "The FDA Adverse Event Reporting System (FAERS) is a vital resource for researchers and healthcare professionals interested in drug safety and pharmacovigilance. This extensive database contains over 21 million adverse event reports related to drugs and therapeutic biologics, making it one of the largest repositories of such information. The data is structured in a tabular format, with rows representing individual adverse event reports and columns capturing various attributes such as the drug involved, the nature of the adverse event, patient demographics, and outcomes. Key variables include drug names, adverse event terms, patient age, gender, and the seriousness of the event, which are essential for understanding the context and implications of reported incidents. The collection methodology for FAERS involves the voluntary reporting of adverse events by healthcare professionals, patients, and manufacturers, which can lead to variability in data completeness and quality. While the dataset provides a wealth of information, researchers should be aware of its limitations, including potential underreporting and reporting biases. Common preprocessing steps for FAERS data may include data cleaning to handle missing values, normalization of drug names, and categorization of adverse events. Researchers can leverage this dataset to address critical research questions related to drug safety, such as identifying trends in adverse reactions, assessing the risk associated with specific medications, and evaluating the effectiveness of regulatory interventions. The types of analyses supported by FAERS data range from descriptive statistics to more complex methodologies, including regression analysis and machine learning techniques, allowing for comprehensive exploration of drug safety issues. Overall, FAERS serves as a foundational tool for advancing knowledge in pharmacovigilance and improving patient safety in healthcare."
  },
  {
    "name": "NIST National Vulnerability Database (NVD)",
    "description": "Comprehensive database of 500,000+ CVE vulnerability entries with CVSS severity scores and free API access",
    "category": "Cybersecurity",
    "url": "https://nvd.nist.gov/",
    "docs_url": "https://nvd.nist.gov/developers",
    "github_url": null,
    "tags": [
      "vulnerabilities",
      "CVE",
      "CVSS",
      "security"
    ],
    "best_for": "Vulnerability research and security investment prioritization",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NIST National Vulnerability Database (NVD) is a comprehensive repository of over 500,000 CVE vulnerability entries, each accompanied by CVSS severity scores. This dataset allows users to analyze vulnerabilities, assess security risks, and develop mitigation strategies through its free API access.",
    "use_cases": [
      "Analyzing trends in cybersecurity vulnerabilities over time.",
      "Assessing the impact of specific vulnerabilities on software security.",
      "Developing security tools that utilize vulnerability data.",
      "Creating educational resources on cybersecurity best practices."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NIST National Vulnerability Database?",
      "How can I access CVE vulnerability entries?",
      "What are CVSS severity scores?",
      "What types of vulnerabilities are recorded in the NVD?",
      "How can I use the NVD for cybersecurity research?",
      "What API access does the NVD provide?",
      "How many CVE entries are in the NVD?",
      "What is the significance of the NIST National Vulnerability Database?"
    ],
    "domain_tags": [
      "cybersecurity"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1999-present",
    "geographic_scope": "Global",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/logos/nist.png",
    "embedding_text": "The NIST National Vulnerability Database (NVD) serves as a critical resource for cybersecurity professionals, researchers, and organizations seeking to understand and mitigate vulnerabilities in software systems. This extensive database contains over 500,000 entries of Common Vulnerabilities and Exposures (CVE), each meticulously cataloged with associated Common Vulnerability Scoring System (CVSS) severity scores. The data structure of the NVD is primarily tabular, consisting of rows representing individual CVE entries and columns detailing various attributes such as the CVE identifier, description, published date, and CVSS score. The schema is designed to facilitate easy querying and analysis, making it accessible for users with varying levels of technical expertise. The collection methodology of the NVD involves gathering data from multiple sources, including security researchers, vendors, and public reports, ensuring a comprehensive and up-to-date repository of vulnerabilities. While the NVD does not explicitly mention temporal or geographic coverage, it is widely recognized for its global relevance, as vulnerabilities can affect software used across different regions and time periods. Key variables within the dataset include the CVE ID, which uniquely identifies each vulnerability, the description that outlines the nature of the vulnerability, and the CVSS score, which quantifies the severity of the vulnerability based on factors such as exploitability and impact. The quality of the data is generally high, though users should be aware of potential limitations, such as the lag in reporting newly discovered vulnerabilities or the varying levels of detail provided for different entries. Common preprocessing steps for utilizing the NVD data may include filtering for specific CVE types, normalizing CVSS scores for comparative analysis, and merging with other datasets for enriched insights. Researchers can leverage the NVD to address a variety of research questions, such as identifying the most prevalent vulnerabilities in specific software categories, analyzing the effectiveness of security patches over time, or understanding the relationship between vulnerability severity and exploitability. The dataset supports various types of analyses, including descriptive statistics to summarize vulnerability trends, regression analyses to explore factors influencing vulnerability severity, and machine learning techniques to predict future vulnerabilities based on historical data. In practice, cybersecurity researchers and analysts typically use the NVD to inform their security strategies, develop tools for vulnerability management, and contribute to the broader understanding of cybersecurity threats in an increasingly digital world."
  },
  {
    "name": "Retail Rocket Recommender System Dataset",
    "description": "4.5 months of behavior data from a real e-commerce site: 2.7M sessions, item properties, and purchase events. Designed for session-based recommendation research.",
    "category": "MarTech & Customer Analytics",
    "url": "https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "session-based",
      "e-commerce",
      "recommendations",
      "behavior"
    ],
    "best_for": "Learning session-based recommendations and real-time personalization",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The Retail Rocket Recommender System Dataset contains 4.5 months of behavior data from a real e-commerce site, featuring 2.7 million sessions, item properties, and purchase events. This dataset is designed for researchers focusing on session-based recommendation systems, allowing for the exploration of consumer behavior and the effectiveness of recommendation algorithms.",
    "use_cases": [
      "Analyzing consumer behavior patterns",
      "Developing session-based recommendation algorithms",
      "Evaluating the effectiveness of marketing strategies",
      "Understanding item purchase trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Retail Rocket Recommender System Dataset?",
      "How can I analyze session-based recommendations using this dataset?",
      "What types of consumer behavior insights can be derived from 2.7M sessions?",
      "What item properties are included in the Retail Rocket dataset?",
      "How does this dataset support e-commerce research?",
      "What are the key variables in the Retail Rocket dataset?",
      "What research questions can be addressed with this dataset?",
      "How can I use this dataset for machine learning applications?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/retail-rocket-recommender-system-dataset.png",
    "embedding_text": "The Retail Rocket Recommender System Dataset is a rich source of behavioral data collected over a period of 4.5 months from a real e-commerce platform, encompassing 2.7 million sessions. This dataset is particularly valuable for researchers and practitioners interested in session-based recommendation systems, as it provides a comprehensive view of user interactions with items on the site. The data structure is primarily tabular, consisting of rows representing individual sessions and columns detailing various attributes such as session IDs, item IDs, timestamps, and user actions (e.g., views, clicks, purchases). The dataset includes key variables that measure user engagement and item properties, which are crucial for understanding consumer behavior and developing effective recommendation algorithms. The collection methodology involves tracking user interactions on the e-commerce site, ensuring that the dataset captures a wide range of behaviors and preferences. However, researchers should be aware of potential limitations regarding data quality, such as missing values or biases in user behavior that may affect the generalizability of findings. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. This dataset supports various types of analyses, including regression, machine learning, and descriptive statistics, enabling researchers to explore questions related to consumer preferences, item popularity, and the impact of recommendations on purchasing decisions. Overall, the Retail Rocket Recommender System Dataset serves as a foundational resource for advancing knowledge in the fields of e-commerce and recommendation systems, providing insights that can inform both academic research and practical applications in the retail industry."
  },
  {
    "name": "Ember Global Electricity Data",
    "description": "Monthly electricity generation, capacity, and emissions data for 200+ countries",
    "category": "Energy",
    "url": "https://ember-climate.org/data/",
    "docs_url": "https://ember-climate.org/data-catalogue/",
    "github_url": "https://github.com/ember-climate/data-guidelines",
    "tags": [
      "global",
      "electricity",
      "emissions",
      "monthly",
      "countries"
    ],
    "best_for": "Cross-country energy transition analysis and global electricity trends",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Ember Global Electricity Data provides comprehensive monthly data on electricity generation, capacity, and emissions across over 200 countries. This dataset allows researchers and analysts to explore trends in energy production and its environmental impact, facilitating informed discussions on energy policy and sustainability.",
    "use_cases": [
      "Analyzing the impact of electricity generation on carbon emissions.",
      "Comparing electricity generation capacities across different countries.",
      "Investigating trends in renewable versus non-renewable energy sources.",
      "Evaluating the effectiveness of energy policies in reducing emissions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Ember Global Electricity Data?",
      "How can I access monthly electricity generation data for different countries?",
      "What are the emissions associated with electricity generation in various countries?",
      "Where can I find capacity data for global electricity production?",
      "What trends can be analyzed using the Ember Global Electricity Data?",
      "How does electricity generation vary across 200+ countries?",
      "What are the key variables in the Ember Global Electricity Data?",
      "What methodologies are used to collect electricity generation data?"
    ],
    "domain_tags": [
      "energy"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2000-present",
    "geographic_scope": "global",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/datasets/ember-global-electricity-data.png",
    "embedding_text": "The Ember Global Electricity Data is a robust dataset that encompasses monthly electricity generation, capacity, and emissions data for over 200 countries, making it a critical resource for researchers, policymakers, and analysts interested in energy systems and environmental impacts. The dataset is structured in a tabular format, with rows representing individual data entries for each country and month, and columns detailing key variables such as total electricity generation, capacity, and associated emissions. This structured approach allows for easy manipulation and analysis using data analysis tools like pandas. The collection methodology for this dataset involves aggregating data from various reputable sources, including national energy agencies, international organizations, and energy market reports, ensuring a high level of reliability and accuracy. However, users should be aware of potential limitations, such as discrepancies in reporting standards across different countries and the availability of data, which may affect the completeness of the dataset. Key variables in the dataset include total electricity generation measured in megawatt-hours (MWh), installed capacity in megawatts (MW), and emissions data typically expressed in carbon dioxide equivalents (CO2e). These variables are essential for understanding the relationship between energy production and environmental impact, enabling researchers to address critical questions related to energy sustainability and climate change. Common preprocessing steps may include handling missing values, normalizing data for comparative analysis, and transforming variables for specific analytical methods. The dataset supports a variety of analyses, including regression analysis to explore relationships between variables, machine learning models for predictive analytics, and descriptive statistics to summarize trends over time. Researchers typically utilize this dataset to conduct studies on the effectiveness of energy policies, assess the transition to renewable energy sources, and evaluate the implications of electricity generation on global emissions. Overall, the Ember Global Electricity Data serves as a vital tool for advancing our understanding of global electricity systems and their environmental consequences."
  },
  {
    "name": "Amazon Reviews 2023",
    "description": "233 million product reviews across all Amazon categories with user IDs, timestamps, ratings, and review text. The largest public e-commerce review dataset for recommendation research.",
    "category": "MarTech & Customer Analytics",
    "url": "https://amazon-reviews-2023.github.io/",
    "docs_url": "https://amazon-reviews-2023.github.io/",
    "github_url": "https://github.com/hyp1231/AmazonReviews2023",
    "tags": [
      "recommendations",
      "e-commerce",
      "reviews",
      "NLP",
      "sentiment"
    ],
    "best_for": "Large-scale recommendation systems, sentiment analysis, and cross-domain recommendations",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The Amazon Reviews 2023 dataset contains 233 million product reviews from various Amazon categories, including user IDs, timestamps, ratings, and review text. This extensive dataset is ideal for conducting recommendation research and analyzing consumer sentiment in e-commerce.",
    "use_cases": [
      "Sentiment analysis of product reviews to understand consumer opinions.",
      "Building recommendation systems based on user ratings and review text.",
      "Analyzing trends in product reviews over time to identify popular products.",
      "Investigating the relationship between review length and rating scores."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common product categories in the Amazon Reviews 2023 dataset?",
      "How can I analyze sentiment in Amazon product reviews?",
      "What insights can be derived from user ratings in the Amazon Reviews dataset?",
      "How does the Amazon Reviews 2023 dataset support recommendation system development?",
      "What is the structure of the Amazon Reviews 2023 dataset?",
      "What types of analyses can be performed on Amazon product reviews?",
      "How can I access the Amazon Reviews 2023 dataset for research?",
      "What are the limitations of using the Amazon Reviews dataset for analysis?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "size_category": "massive",
    "model_score": 0.0001,
    "image_url": "/images/datasets/amazon-reviews-2023.png",
    "embedding_text": "The Amazon Reviews 2023 dataset is a comprehensive collection of 233 million product reviews sourced from various categories on Amazon, making it one of the largest public e-commerce review datasets available for research purposes. This dataset includes key variables such as user IDs, timestamps, ratings, and review text, providing a rich structure for analysis. Researchers can leverage this dataset to explore various dimensions of consumer behavior, including sentiment analysis, trends in product reviews, and the effectiveness of recommendation systems. The data is structured in a tabular format, with each row representing an individual review and columns containing the associated metadata. The primary variables include user IDs, which allow for tracking individual user behavior; timestamps, which provide temporal context for the reviews; ratings, which quantify user satisfaction; and review text, which offers qualitative insights into consumer opinions. The collection methodology for this dataset involves aggregating reviews from Amazon's platform, ensuring a diverse representation of products and categories. However, researchers should be aware of potential limitations, such as biases in user-generated content and the challenges of preprocessing text data for analysis. Common preprocessing steps may include text normalization, tokenization, and sentiment scoring. This dataset supports a variety of analyses, including regression analysis, machine learning applications, and descriptive statistics. Researchers typically use the Amazon Reviews dataset to address research questions related to consumer sentiment, product performance, and the effectiveness of marketing strategies. By analyzing the reviews, researchers can gain insights into consumer preferences and behaviors, which can inform product development and marketing efforts in the e-commerce sector."
  },
  {
    "name": "World Bank Open Data",
    "description": "1,400+ development indicators for 217 economies spanning 50+ years with free API",
    "category": "Dataset Aggregators",
    "url": "https://data.worldbank.org",
    "docs_url": "https://datahelpdesk.worldbank.org",
    "github_url": null,
    "tags": [
      "development",
      "international",
      "poverty",
      "GDP",
      "API"
    ],
    "best_for": "Cross-country comparisons in development economics and poverty research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The World Bank Open Data provides over 1,400 development indicators for 217 economies, covering more than 50 years of data. This dataset allows users to analyze various socio-economic factors such as poverty levels, GDP, and international development trends through a free API.",
    "use_cases": [
      "Analyzing trends in GDP across different economies over time.",
      "Comparing poverty rates between countries to identify patterns.",
      "Evaluating the impact of international aid on development indicators."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the development indicators available in the World Bank Open Data?",
      "How can I access the World Bank Open Data API?",
      "What economies are covered in the World Bank Open Data?",
      "What time span does the World Bank Open Data cover?",
      "How does the World Bank measure poverty in its datasets?",
      "What types of analyses can be performed using World Bank Open Data?",
      "Where can I find the World Bank Open Data documentation?",
      "What are some examples of development indicators provided by the World Bank?"
    ],
    "domain_tags": [
      "international",
      "development",
      "economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "temporal_coverage": "50+ years",
    "geographic_scope": "217 economies",
    "model_score": 0.0001,
    "image_url": "/images/datasets/world-bank-open-data.svg",
    "embedding_text": "The World Bank Open Data is a comprehensive repository of over 1,400 development indicators that span more than 50 years and cover 217 economies. This dataset is structured in a tabular format, with rows representing individual data points and columns corresponding to various indicators, such as GDP, poverty rates, and other socio-economic variables. Each indicator is designed to measure specific aspects of development, providing valuable insights into the economic and social conditions of different countries. The collection methodology for this dataset involves rigorous data gathering from various sources, including national statistical offices, international organizations, and other reputable entities, ensuring a high level of data quality. However, users should be aware of potential limitations, such as data gaps for certain countries or time periods, which may affect the comprehensiveness of analyses. Researchers often employ common preprocessing steps like data cleaning, normalization, and transformation to prepare the data for analysis. The World Bank Open Data supports a wide range of analyses, including regression, machine learning, and descriptive statistics, making it a versatile tool for researchers and analysts. Typical research questions addressed using this dataset include examining the relationship between economic growth and poverty reduction, assessing the effectiveness of international development programs, and exploring trends in health and education indicators across different regions. By leveraging the World Bank Open Data, researchers can gain a deeper understanding of global development challenges and inform policy decisions aimed at improving economic and social outcomes."
  },
  {
    "name": "Space-Track.org",
    "description": "Official U.S. Space Force data on tracked orbital objects with 138+ million historical element sets",
    "category": "Space",
    "url": "https://www.space-track.org/",
    "docs_url": "https://www.space-track.org/documentation",
    "github_url": null,
    "tags": [
      "satellites",
      "orbital debris",
      "TLE",
      "space situational awareness"
    ],
    "best_for": "Tracking satellites and debris for space economics research",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Space-Track.org provides official data from the U.S. Space Force on tracked orbital objects, featuring over 138 million historical element sets. This dataset can be utilized for analyzing satellite trajectories, understanding orbital debris, and enhancing space situational awareness.",
    "use_cases": [
      "Analyzing satellite trajectories",
      "Studying orbital debris patterns",
      "Enhancing space situational awareness",
      "Researching the history of tracked orbital objects"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Space-Track.org?",
      "How can I access U.S. Space Force orbital data?",
      "What are historical element sets in satellite tracking?",
      "What data is available on orbital debris?",
      "How does Space-Track.org support space situational awareness?",
      "What types of satellites are tracked by Space-Track.org?",
      "How many historical element sets does Space-Track.org have?",
      "What is the significance of TLE data in space tracking?"
    ],
    "domain_tags": [
      "space"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1957-present",
    "geographic_scope": "Global (orbital)",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/space-track.png",
    "embedding_text": "Space-Track.org is a comprehensive dataset that provides official data from the U.S. Space Force concerning tracked orbital objects, encompassing over 138 million historical element sets. The dataset is structured in a tabular format, where each row represents a unique orbital object, and the columns include various attributes such as the object's identification number, its two-line element (TLE) data, and other relevant metadata. The TLE data is crucial for understanding the position and trajectory of satellites and other objects in orbit. The collection methodology involves systematic tracking and cataloging of objects by the U.S. Space Force, ensuring that the data is accurate and up-to-date. However, users should be aware of potential limitations in data quality, such as gaps in historical records or inaccuracies in tracking due to orbital changes. Common preprocessing steps may include cleaning the data to remove duplicates, normalizing TLE formats, and filtering out irrelevant objects. Researchers can leverage this dataset to address various research questions, such as analyzing the movement patterns of satellites, studying the implications of orbital debris on space missions, and enhancing overall space situational awareness. The types of analyses supported by this dataset include regression analysis to predict satellite trajectories, machine learning applications for classifying objects, and descriptive statistics to summarize orbital data. Typically, researchers utilize Space-Track.org in studies related to aerospace engineering, astrophysics, and space policy, making it a valuable resource for anyone interested in the dynamics of space and satellite operations."
  },
  {
    "name": "Data.gov",
    "description": "370,000+ datasets from US federal, state, and local agencies",
    "category": "Dataset Aggregators",
    "url": "https://data.gov",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "government",
      "US",
      "open data",
      "CKAN"
    ],
    "best_for": "Central clearinghouse for BLS, Census, BEA, and trade data",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Data.gov is a comprehensive repository containing over 370,000 datasets sourced from various US federal, state, and local agencies. This extensive collection enables users to access a wide range of open data, facilitating research, analysis, and policy-making across multiple domains.",
    "use_cases": [
      "Analyzing trends in public health data from various agencies.",
      "Conducting research on environmental data collected by state agencies.",
      "Evaluating economic indicators from federal datasets.",
      "Exploring demographic information available through local government datasets."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on Data.gov?",
      "How can I access government datasets from Data.gov?",
      "What types of open data are provided by Data.gov?",
      "Where can I find datasets related to US federal agencies?",
      "What is the size of the dataset collection on Data.gov?",
      "How do I search for specific datasets on Data.gov?",
      "What categories of datasets are available on Data.gov?",
      "What tags are associated with datasets on Data.gov?"
    ],
    "domain_tags": [
      "government",
      "open data"
    ],
    "data_modality": "mixed",
    "size_category": "massive",
    "geographic_scope": "US",
    "model_score": 0.0001,
    "embedding_text": "Data.gov serves as a pivotal resource for individuals and organizations seeking access to a vast array of datasets compiled from US federal, state, and local agencies. With over 370,000 datasets available, this platform is designed to promote transparency and facilitate the use of government data for research, analysis, and public policy development. The datasets cover a wide range of topics, including health, education, transportation, and environmental issues, making it a valuable tool for researchers, policymakers, and the general public alike. The data structure typically consists of tabular formats, with rows representing individual records and columns denoting various attributes or variables pertinent to each dataset. Common variables may include demographic information, geographic identifiers, and specific metrics related to the dataset's focus area. The collection methodology involves aggregating data from multiple government sources, ensuring that the datasets are comprehensive and representative of the respective domains. However, users should be aware of potential limitations in data quality, such as inconsistencies in reporting standards across different agencies or gaps in data coverage. Preprocessing steps may be necessary to clean and standardize the data, particularly when merging datasets from disparate sources. Researchers can leverage Data.gov to address a multitude of research questions, ranging from analyzing public health trends to evaluating the impact of economic policies. The datasets support various types of analyses, including descriptive statistics, regression modeling, and machine learning applications. By utilizing the wealth of information available on Data.gov, researchers can gain insights that inform decision-making and contribute to a deeper understanding of societal issues."
  },
  {
    "name": "Papers With Code Datasets",
    "description": "Datasets linked to research papers, code implementations, and SOTA leaderboards",
    "category": "Dataset Aggregators",
    "url": "https://paperswithcode.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reproducibility",
      "SOTA",
      "benchmarks",
      "papers"
    ],
    "best_for": "Tracking which datasets power cutting-edge ML research with code links",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Papers With Code Datasets provide a comprehensive collection of datasets that are directly linked to research papers, code implementations, and state-of-the-art (SOTA) leaderboards. Researchers and practitioners can utilize these datasets to benchmark their models, reproduce results from academic literature, and explore various machine learning and data science methodologies.",
    "use_cases": [
      "Benchmarking machine learning models against established SOTA results.",
      "Reproducing results from academic papers to validate findings.",
      "Exploring the relationship between datasets and their corresponding code implementations.",
      "Analyzing trends in dataset usage across different research domains."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are linked to research papers in machine learning?",
      "How can I find datasets for benchmarking models?",
      "Where can I access datasets associated with SOTA leaderboards?",
      "What are the available datasets for reproducibility in research?",
      "Which datasets are used in conjunction with code implementations?",
      "How do I find datasets related to specific research papers?"
    ],
    "domain_tags": [
      "Research"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "benchmark_usage": [
      "Datasets are commonly used for benchmarking machine learning algorithms and models."
    ],
    "model_score": 0.0001,
    "image_url": "/images/datasets/papers-with-code-datasets.png",
    "embedding_text": "The Papers With Code Datasets represent a significant resource for researchers and practitioners in the fields of machine learning and data science. This collection encompasses a wide variety of datasets that are systematically linked to corresponding research papers, code implementations, and state-of-the-art (SOTA) leaderboards. The data structure typically consists of rows representing individual datasets, with columns detailing various attributes such as dataset names, associated papers, links to code implementations, and performance metrics on SOTA benchmarks. Each dataset entry may include variables that measure aspects like dataset size, number of instances, features, and the specific tasks for which the dataset is designed. The collection methodology involves aggregating datasets from diverse sources, including academic publications, online repositories, and contributions from the research community. This ensures a broad coverage of topics and methodologies, facilitating a rich environment for exploration and analysis. While the datasets cover a range of domains, the specific geographic and temporal coverage is not explicitly mentioned, leaving it open to interpretation based on the individual datasets included. Key variables within the datasets often include performance metrics, dataset characteristics, and links to relevant research papers, which provide context and allow for deeper analysis. However, users should be aware of potential limitations in data quality, such as inconsistencies in dataset documentation or variations in how datasets are reported across different studies. Common preprocessing steps may involve cleaning the data, standardizing formats, and ensuring compatibility with various machine learning frameworks. Researchers can leverage these datasets to address a multitude of research questions, such as evaluating the effectiveness of different algorithms, understanding the evolution of methodologies over time, and identifying gaps in existing research. The types of analyses supported by these datasets range from regression analysis to machine learning model training and validation, as well as descriptive statistics to summarize dataset characteristics. In practice, researchers typically use the Papers With Code Datasets to benchmark their own models against established results, reproduce findings from academic literature, and explore the interplay between datasets and their corresponding code implementations, thereby contributing to the ongoing discourse in the field."
  },
  {
    "name": "All of Us Research Program",
    "description": "NIH precision medicine initiative enrolling 1+ million diverse U.S. participants. Includes EHR, surveys, wearables, and genomics. Cloud-based Researcher Workbench provides secure access.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://allofus.nih.gov/",
    "source": "NIH",
    "type": "Cohort + Multi-modal",
    "access": "Free (registration required)",
    "format": "Cloud-based (BigQuery)",
    "tags": [
      "Healthcare",
      "Precision Medicine",
      "Diverse",
      "Wearables",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The All of Us Research Program is a comprehensive NIH initiative aimed at advancing precision medicine by enrolling over one million diverse participants across the United States. This dataset includes a variety of data types such as electronic health records (EHR), survey responses, wearable device data, and genomic information, all accessible through a secure cloud-based Researcher Workbench. Researchers can utilize this rich dataset to explore health trends, conduct analyses on diverse populations, and develop personalized health interventions.",
    "use_cases": [
      "Analyzing health outcomes across diverse populations",
      "Studying the impact of wearable technology on health metrics",
      "Exploring genomic data in relation to electronic health records",
      "Conducting surveys to assess health behaviors and attitudes"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the All of Us Research Program dataset?",
      "How can I access the All of Us Research Program data?",
      "What types of data are included in the All of Us Research Program?",
      "What research can be conducted using the All of Us Research Program dataset?",
      "What is the significance of diversity in the All of Us Research Program?",
      "How does the All of Us Research Program support precision medicine?",
      "What are the data collection methods used in the All of Us Research Program?",
      "What types of analyses can be performed with the All of Us Research Program data?"
    ],
    "update_frequency": "Ongoing enrollment",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "mixed",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0001,
    "image_url": "/images/logos/nih.png",
    "embedding_text": "The All of Us Research Program is a landmark initiative by the National Institutes of Health (NIH) aimed at transforming the landscape of precision medicine. This program seeks to enroll over one million participants from diverse backgrounds across the United States, thereby ensuring that research findings are applicable to a wide range of populations. The dataset generated from this program is multifaceted, comprising electronic health records (EHR), survey responses, data from wearable devices, and genomic information. This rich array of data is structured to facilitate comprehensive analysis and research in healthcare economics and health technology. The data schema includes various rows and columns representing different variables such as demographic information, health metrics, survey responses, and genomic data. Each participant's data is meticulously collected through a combination of self-reported surveys, clinical assessments, and continuous monitoring via wearable technology. This methodology not only enhances the depth of the dataset but also ensures that it captures a holistic view of health and wellness across diverse populations. The All of Us Research Program emphasizes the importance of diversity, aiming to include participants from various racial, ethnic, and socioeconomic backgrounds. This focus on inclusivity allows researchers to address critical questions about health disparities and the effectiveness of medical treatments across different demographic groups. Key variables within the dataset measure a range of health indicators, including chronic disease prevalence, lifestyle factors, and genetic predispositions. However, researchers should be aware of potential limitations in data quality, such as self-reporting biases in survey responses and variations in data collection methods across different sites. Common preprocessing steps may include data cleaning, normalization, and integration of various data types to prepare for analysis. The All of Us dataset supports a wide array of research questions, from understanding the social determinants of health to evaluating the efficacy of personalized treatment plans. Researchers can employ various analytical techniques, including regression analysis, machine learning, and descriptive statistics, to derive insights from the data. Typically, studies utilizing the All of Us Research Program data focus on identifying trends in health outcomes, assessing the impact of lifestyle changes, and exploring the relationship between genetic factors and health conditions. Overall, the All of Us Research Program represents a significant advancement in the field of precision medicine, providing researchers with invaluable resources to improve health outcomes for all individuals."
  },
  {
    "name": "World Bank Light Every Night",
    "description": "30 years of nighttime satellite imagery (250 terabytes) from DMSP and VIIRS sensors. Foundational dataset for using lights as GDP proxy.",
    "category": "Geospatial",
    "url": "https://registry.opendata.aws/wb-light-every-night/",
    "docs_url": "https://worldbank.github.io/OpenNightLights/welcome.html",
    "github_url": "https://github.com/worldbank/OpenNightLights",
    "tags": [
      "satellite",
      "nighttime-lights",
      "GDP",
      "development",
      "World Bank"
    ],
    "best_for": "Economic measurement using satellite imagery, development economics research",
    "model_score": 0.0001,
    "image_url": "/images/datasets/world-bank-light-every-night.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The World Bank Light Every Night dataset comprises 30 years of nighttime satellite imagery collected from DMSP and VIIRS sensors, totaling 250 terabytes of data. This foundational dataset is instrumental in utilizing nighttime lights as a proxy for GDP, enabling researchers and analysts to explore economic development trends over time.",
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the World Bank Light Every Night dataset?",
      "How can nighttime satellite imagery be used as a GDP proxy?",
      "What sensors are used in the Light Every Night dataset?",
      "What types of analyses can be performed with nighttime lights data?",
      "How does the World Bank utilize satellite imagery for development?",
      "What are the key variables in the Light Every Night dataset?",
      "What is the significance of nighttime lights in economic research?",
      "How can I access the World Bank Light Every Night dataset?"
    ],
    "use_cases": [
      "Analyzing economic growth trends in developing countries using nighttime lights data.",
      "Comparing GDP estimates derived from satellite imagery with official statistics.",
      "Studying the impact of urbanization on nighttime light intensity.",
      "Investigating the relationship between energy consumption and economic activity."
    ],
    "embedding_text": "The World Bank Light Every Night dataset is a comprehensive collection of nighttime satellite imagery that spans three decades, specifically from 1992 to 2022. This dataset is derived from two prominent satellite systems: the Defense Meteorological Satellite Program (DMSP) and the Visible Infrared Imaging Radiometer Suite (VIIRS). With a staggering size of 250 terabytes, it provides a wealth of information that can be harnessed to analyze economic development through the lens of nighttime illumination. The primary structure of the dataset consists of images captured at various times, which can be processed to extract quantitative data on light intensity across different geographic regions. Each image serves as a snapshot of light emissions, which researchers can correlate with economic indicators such as GDP. The collection methodology involves advanced satellite imaging techniques that capture light emissions from the Earth's surface, allowing for the monitoring of changes in urbanization, industrial activity, and population growth over time. The key variables within this dataset include light intensity measurements, which reflect the level of economic activity in a given area, and can be used to infer patterns of development, energy consumption, and social behavior. However, researchers must be aware of certain limitations, such as the potential for data quality issues due to atmospheric conditions, cloud cover, and the varying sensitivity of the sensors used. Common preprocessing steps may include calibrating the images for consistent light intensity readings, filtering out noise, and aligning the data with geographic information systems (GIS) for spatial analysis. This dataset supports a variety of analyses, including regression modeling to establish relationships between nighttime light intensity and economic performance, machine learning applications for predictive analytics, and descriptive statistics to summarize trends over time. Researchers typically utilize the Light Every Night dataset to address critical questions regarding economic growth, urbanization patterns, and the effectiveness of development policies, making it a vital resource for economists, urban planners, and policymakers alike. By leveraging the insights gained from this dataset, stakeholders can better understand the dynamics of economic development and make informed decisions that promote sustainable growth.",
    "domain_tags": [
      "development",
      "economics",
      "geospatial"
    ],
    "data_modality": "image",
    "temporal_coverage": "1992-2022",
    "size_category": "massive",
    "benchmark_usage": [
      "Using nighttime lights as a proxy for GDP estimation"
    ]
  },
  {
    "name": "Predicting Poverty Replication Data",
    "description": "Satellite imagery and survey data from Jean et al. (Science 2016) for predicting poverty in African countries using deep learning.",
    "category": "Geospatial",
    "url": "https://github.com/nealjean/predicting-poverty",
    "docs_url": null,
    "github_url": "https://github.com/nealjean/predicting-poverty",
    "tags": [
      "satellite",
      "poverty",
      "deep-learning",
      "Africa",
      "development"
    ],
    "best_for": "Replicating and extending satellite-based poverty prediction research",
    "model_score": 0.0001,
    "image_url": "/images/datasets/predicting-poverty-replication-data.png",
    "difficulty": "intermediate",
    "prerequisites": [
      "deep-learning",
      "geospatial-analysis"
    ],
    "topic_tags": [],
    "summary": "The Predicting Poverty Replication Data consists of satellite imagery and survey data aimed at predicting poverty levels in African countries through deep learning techniques. Researchers can utilize this dataset to explore the relationship between geospatial features and socioeconomic indicators, potentially leading to innovative solutions for poverty alleviation.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to use satellite imagery to predict poverty?",
      "What deep learning techniques are effective for poverty prediction?",
      "Where can I find datasets for predicting poverty in Africa?",
      "What variables are included in the Predicting Poverty Replication Data?",
      "How does satellite data correlate with survey data in poverty studies?",
      "What are the challenges in using deep learning for poverty prediction?",
      "How can I access the Predicting Poverty Replication Data?",
      "What methodologies are used in analyzing poverty with satellite imagery?"
    ],
    "use_cases": [
      "Analyzing the impact of environmental factors on poverty",
      "Developing predictive models for poverty alleviation",
      "Evaluating the effectiveness of development programs using geospatial data"
    ],
    "embedding_text": "The Predicting Poverty Replication Data is a comprehensive dataset that combines satellite imagery with survey data, specifically designed for the purpose of predicting poverty levels in various African countries. This dataset is rooted in the research conducted by Jean et al. (Science 2016), which emphasizes the potential of using advanced deep learning techniques to analyze and interpret complex geospatial data alongside traditional socioeconomic indicators. The structure of the dataset typically includes rows representing individual data points, such as specific geographic locations, and columns that encompass a variety of variables, including satellite-derived features (e.g., vegetation indices, land use classifications) and survey-based socioeconomic indicators (e.g., income levels, education status). The collection methodology involves the integration of high-resolution satellite imagery, which captures detailed information about the physical environment, and comprehensive survey data that reflects the lived experiences of individuals within those environments. This dual approach allows for a more nuanced understanding of the factors contributing to poverty. While the dataset provides valuable insights, it is important to acknowledge potential limitations, such as data quality issues related to the accuracy of survey responses or the resolution of satellite images. Researchers utilizing this dataset may need to perform common preprocessing steps, including data cleaning, normalization, and feature extraction, to prepare the data for analysis. The dataset supports a range of research questions, such as examining the correlation between environmental conditions and poverty levels, identifying spatial patterns of poverty distribution, and assessing the effectiveness of interventions aimed at poverty reduction. Analysts can apply various types of analyses, including regression modeling, machine learning algorithms, and descriptive statistics, to derive meaningful conclusions from the data. In practice, researchers often leverage this dataset to develop predictive models that inform policy decisions and contribute to the broader goal of sustainable development in Africa. By harnessing the power of deep learning and geospatial analysis, this dataset serves as a critical resource for understanding and addressing the complex issue of poverty in the region.",
    "domain_tags": [
      "development",
      "geospatial"
    ],
    "data_modality": "mixed",
    "geographic_scope": "Africa",
    "size_category": "medium"
  },
  {
    "name": "US 2020 Election Study",
    "description": "Facebook/Instagram impact on political attitudes. Published in Science/Nature 2023. SOMAR Michigan access",
    "category": "Social & Web",
    "url": "https://www.icpsr.umich.edu/web/ICPSR/series/2045",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Facebook",
      "Instagram",
      "elections",
      "politics",
      "social media"
    ],
    "best_for": "Learning social & web analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "political-science",
      "social-media",
      "public-opinion"
    ],
    "summary": "The US 2020 Election Study examines the impact of Facebook and Instagram on political attitudes during the 2020 election cycle. Researchers can utilize this dataset to analyze how social media influences voter behavior and political engagement.",
    "use_cases": [
      "Analyzing the correlation between social media usage and voter turnout.",
      "Studying the effects of targeted political ads on social media platforms.",
      "Investigating demographic differences in social media influence on political attitudes."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the impact of Facebook on political attitudes in the 2020 election?",
      "How did Instagram influence voter behavior during the US 2020 election?",
      "What are the effects of social media on elections?",
      "Can social media predict political engagement?",
      "What demographic factors affect social media's influence on politics?",
      "How do social media platforms differ in their impact on elections?"
    ],
    "domain_tags": [
      "politics",
      "social-media"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/umich.png",
    "embedding_text": "The US 2020 Election Study is a comprehensive dataset that investigates the influence of social media platforms, specifically Facebook and Instagram, on political attitudes during the 2020 United States presidential election. This dataset is structured in a tabular format, containing various rows and columns that capture a wide range of variables related to social media usage, political engagement, and demographic information of respondents. The data collection methodology involved surveys administered to a diverse group of participants, ensuring a representative sample of the electorate. The key variables in this dataset include measures of social media engagement, political affiliation, voter turnout, and attitudes towards political issues. Each variable is designed to provide insights into how social media may shape public opinion and influence electoral outcomes. Researchers utilizing this dataset can explore a variety of research questions, such as the extent to which social media affects voter turnout and how different demographic groups respond to political messaging on these platforms. The dataset supports various types of analyses, including regression analysis, machine learning techniques, and descriptive statistics, allowing for a multifaceted exploration of the data. However, researchers should be aware of potential limitations in data quality, such as self-reported biases and the dynamic nature of social media interactions. Common preprocessing steps may include cleaning the data for inconsistencies, normalizing responses, and segmenting the dataset by demographic characteristics. Overall, the US 2020 Election Study serves as a vital resource for scholars and practitioners interested in understanding the intersection of social media and political behavior, providing a foundation for further research in this rapidly evolving field."
  },
  {
    "name": "Human Mortality Database",
    "description": "Detailed mortality and population data for 40+ countries with life tables and exposure-to-risk calculations",
    "category": "Insurance & Actuarial",
    "url": "https://www.mortality.org/",
    "docs_url": "https://www.mortality.org/Data/DataAvailability",
    "github_url": null,
    "tags": [
      "mortality",
      "life-insurance",
      "demographics",
      "life-tables",
      "longevity"
    ],
    "best_for": "Life insurance pricing, longevity research, and demographic modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Human Mortality Database provides detailed mortality and population data for over 40 countries, including life tables and exposure-to-risk calculations. This dataset is essential for researchers and analysts in the fields of insurance, actuarial science, and demographics, allowing for in-depth analysis of mortality trends and life expectancy.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Human Mortality Database?",
      "How can I access mortality data for different countries?",
      "What are life tables and how are they used in mortality analysis?",
      "What demographic information is included in the Human Mortality Database?",
      "How does the Human Mortality Database support actuarial studies?",
      "What are exposure-to-risk calculations?",
      "What countries are covered in the Human Mortality Database?",
      "How can mortality data inform life insurance policies?"
    ],
    "use_cases": [
      "Analyzing trends in life expectancy across different countries.",
      "Conducting actuarial assessments for life insurance products.",
      "Studying demographic shifts and their impact on mortality rates.",
      "Evaluating the effectiveness of public health interventions on mortality."
    ],
    "domain_tags": [
      "insurance",
      "healthcare",
      "demographics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/mortality.png",
    "embedding_text": "The Human Mortality Database is a comprehensive resource that offers detailed mortality and population data for over 40 countries. This dataset is structured in a tabular format, consisting of rows that represent different demographic groups and columns that include various variables such as age, sex, mortality rates, and life expectancy. The data is meticulously collected from national statistical offices and vital registration systems, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, such as variations in data collection methodologies across countries and the availability of data for specific years. Key variables in the dataset include age-specific mortality rates, which measure the likelihood of death within specific age groups, and life tables that provide insights into the expected lifespan of individuals at different ages. Common preprocessing steps may involve cleaning the data to address missing values or inconsistencies, as well as transforming variables for specific analyses. The dataset supports a wide range of research questions, including those related to the impact of socio-economic factors on mortality, the effectiveness of healthcare policies, and demographic changes over time. Analysts can employ various types of analyses, such as regression modeling, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically utilize the Human Mortality Database in studies aimed at understanding mortality trends, evaluating public health initiatives, and informing actuarial practices in the insurance industry. Overall, this dataset serves as a vital tool for anyone interested in the fields of mortality research, public health, and demographic analysis."
  },
  {
    "name": "Medicare Claims (ResDAC)",
    "description": "Comprehensive claims data covering 98%+ of adults 65+ in the United States. Includes inpatient, outpatient, physician, and prescription drug claims. Research Identifiable Files require 6-12 month DUA approval.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://resdac.org/",
    "source": "CMS via Research Data Assistance Center",
    "type": "Claims Database",
    "access": "Fee-based (DUA required)",
    "format": "SAS/CSV",
    "tags": [
      "Healthcare",
      "Claims",
      "Medicare",
      "Administrative"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Medicare Claims dataset provides comprehensive claims data for over 98% of adults aged 65 and older in the United States. This dataset includes various types of claims, such as inpatient, outpatient, physician, and prescription drug claims, making it a valuable resource for healthcare economics research.",
    "use_cases": [
      "Analyzing healthcare utilization patterns among seniors",
      "Evaluating the impact of policy changes on Medicare claims",
      "Studying prescription drug usage trends in older adults",
      "Investigating the relationship between healthcare costs and patient outcomes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the Medicare Claims dataset?",
      "How can I access Medicare claims data?",
      "What types of claims are included in the Medicare dataset?",
      "What demographic does the Medicare Claims dataset cover?",
      "What research can be conducted using Medicare claims data?",
      "How is Medicare claims data structured?",
      "What are the limitations of the Medicare Claims dataset?",
      "What approval is needed to use Medicare research identifiable files?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare",
      "administrative"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/resdac.png",
    "embedding_text": "The Medicare Claims dataset, provided by ResDAC, is an extensive repository of claims data that encompasses over 98% of adults aged 65 and older in the United States. This dataset is structured in a tabular format, containing rows that represent individual claims and columns that capture various attributes of these claims, such as claim type, date of service, provider information, and patient demographics. The dataset includes a diverse array of claims, including inpatient, outpatient, physician, and prescription drug claims, making it a rich resource for researchers interested in healthcare economics and health technology. The collection methodology for this dataset involves aggregating claims data from multiple sources, primarily from Medicare administrative records, ensuring a comprehensive coverage of the senior population's healthcare interactions. While the dataset provides a wealth of information, researchers should be aware of potential limitations, such as the absence of certain demographic variables or the need for data use agreements (DUAs) for accessing research identifiable files, which may require a waiting period of 6-12 months for approval. Common preprocessing steps for utilizing this dataset may include data cleaning to handle missing values, normalization of variables, and transformation of categorical data into numerical formats suitable for analysis. Researchers typically leverage this dataset to address a variety of research questions, such as examining healthcare utilization patterns, evaluating the effects of policy changes on Medicare claims, and studying trends in prescription drug usage among older adults. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, thereby enabling researchers to derive insights into the healthcare landscape for the elderly population. Overall, the Medicare Claims dataset serves as a crucial tool for understanding the complexities of healthcare delivery and costs for seniors, facilitating informed decision-making and policy development in the healthcare sector."
  },
  {
    "name": "NYC Property Sales",
    "description": "NYC property sales transactions across all boroughs",
    "category": "Real Estate",
    "url": "https://data.cityofnewyork.us/Housing-Development/NYC-Calendar-Sales-Archive-/uzf5-f8n2/about_data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "real estate",
      "NYC",
      "transactions",
      "government"
    ],
    "best_for": "Learning real estate analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "real estate",
      "urban studies",
      "data analysis"
    ],
    "summary": "The NYC Property Sales dataset contains detailed records of property sales transactions across all boroughs of New York City. Researchers and analysts can utilize this dataset to explore trends in real estate prices, identify patterns in property ownership, and assess the impact of government policies on the housing market.",
    "use_cases": [
      "Analyzing trends in property prices over time",
      "Identifying the impact of zoning laws on property sales",
      "Comparing property sales across different neighborhoods",
      "Assessing the relationship between property sales and economic indicators"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest property sales in NYC?",
      "How do property prices vary across different boroughs in NYC?",
      "What trends can be observed in NYC property transactions over the years?",
      "What factors influence property sales in New York City?",
      "How does government policy affect real estate transactions in NYC?",
      "What is the average property sale price in NYC?",
      "How many transactions occurred in NYC last year?",
      "What types of properties are most commonly sold in NYC?"
    ],
    "domain_tags": [
      "real estate"
    ],
    "data_modality": "tabular",
    "geographic_scope": "New York City",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The NYC Property Sales dataset is a comprehensive collection of property transaction records from New York City, encompassing all five boroughs: Manhattan, Brooklyn, Queens, The Bronx, and Staten Island. This dataset is structured in a tabular format, with rows representing individual property sales and columns containing various attributes related to each transaction. Key variables typically include the sale price, property type, address, sale date, and buyer/seller information. The dataset is invaluable for researchers and analysts interested in urban studies, real estate economics, and public policy analysis. The collection methodology involves aggregating data from public records maintained by city agencies, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as incomplete records for certain transactions or variations in data reporting standards across different boroughs. Common preprocessing steps may include data cleaning to handle missing values, normalization of property types, and temporal alignment of sale dates for trend analysis. This dataset supports a variety of analytical approaches, including regression analysis to model price trends, machine learning techniques for predictive modeling, and descriptive statistics to summarize key characteristics of the property market. Researchers often utilize this dataset to address questions regarding the dynamics of the real estate market, the influence of economic conditions on property sales, and the effects of legislative changes on housing availability and affordability. By leveraging the rich information contained within the NYC Property Sales dataset, analysts can gain insights that inform urban planning, investment strategies, and policy development aimed at improving housing conditions in one of the most dynamic real estate markets in the world."
  },
  {
    "name": "Jeff Sackmann Tennis Data",
    "description": "Comprehensive tennis match results and point-by-point data from 1973-present for ATP, WTA, and Grand Slam tournaments",
    "category": "Sports & Athletics",
    "url": "https://github.com/JeffSackmann/tennis_atp",
    "docs_url": null,
    "github_url": "https://github.com/JeffSackmann/tennis_atp",
    "tags": [
      "tennis",
      "ATP",
      "WTA",
      "match-results",
      "point-by-point"
    ],
    "best_for": "Tennis analytics, player performance analysis, and tournament prediction",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Jeff Sackmann Tennis Data provides a comprehensive collection of tennis match results and point-by-point data spanning from 1973 to the present. This dataset can be utilized for various analyses, including performance evaluation, match prediction, and historical trend analysis in tennis.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the historical tennis match results from 1973 to present?",
      "How can I analyze point-by-point data for ATP and WTA matches?",
      "What datasets are available for tennis statistics?",
      "Where can I find comprehensive tennis data for Grand Slam tournaments?",
      "What insights can be derived from tennis match results?",
      "How to access Jeff Sackmann's tennis data for analysis?",
      "What are the key variables in tennis match data?",
      "How has tennis performance evolved over the years?"
    ],
    "use_cases": [
      "Analyzing player performance trends over time",
      "Predicting match outcomes based on historical data",
      "Studying the impact of surface type on match results",
      "Evaluating the effectiveness of different playing strategies"
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1973-present",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/jeff-sackmann-tennis-data.png",
    "embedding_text": "The Jeff Sackmann Tennis Data is a rich and extensive dataset that encompasses a wide array of tennis match results and point-by-point data from 1973 to the present. This dataset is structured in a tabular format, with rows representing individual matches and columns detailing various attributes such as player names, match scores, tournament types, and point-by-point breakdowns. The collection methodology involves aggregating data from official ATP and WTA sources, as well as Grand Slam tournament records, ensuring a high level of accuracy and reliability. The dataset covers a significant temporal range, allowing researchers to analyze trends and patterns in tennis performance over several decades. Key variables within the dataset include player statistics, match outcomes, and detailed point-by-point scoring, which provide insights into player strategies and match dynamics. However, users should be aware of potential limitations, such as discrepancies in data reporting or variations in match formats over the years. Common preprocessing steps may include cleaning the data for missing values, normalizing player statistics, and transforming point-by-point data into a more analyzable format. Researchers can leverage this dataset to address a variety of research questions, such as examining the correlation between player rankings and match outcomes, analyzing the effects of different playing surfaces on player performance, or exploring historical trends in match statistics. The dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and practical applications in the field of sports analytics. Overall, the Jeff Sackmann Tennis Data serves as an invaluable tool for anyone interested in exploring the intricacies of tennis performance and match dynamics."
  },
  {
    "name": "Citi Bike System Data",
    "description": "Trip-level records for NYC's bike-share system since 2013. ~2 million trips monthly in peak season with station-level origin-destination and duration data.",
    "category": "Transportation Economics & Technology",
    "url": "https://citibikenyc.com/system-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "bike-share",
      "NYC",
      "micromobility",
      "trip-data",
      "cycling"
    ],
    "best_for": "Bike-share demand analysis, first/last mile research, and micromobility studies",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban planning",
      "data analysis"
    ],
    "summary": "The Citi Bike System Data provides trip-level records for New York City's bike-share system, offering insights into cycling patterns and micromobility trends. Researchers can analyze station-level origin-destination data and trip durations to understand usage patterns and improve urban transportation planning.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trip patterns in NYC's Citi Bike system?",
      "How does bike usage vary by time of day in NYC?",
      "What is the average duration of trips in the Citi Bike system?",
      "How many trips are taken monthly in peak season?",
      "What are the most popular bike stations in NYC?",
      "How does weather affect bike-share usage in NYC?",
      "What demographic factors influence bike-share usage?",
      "How does the Citi Bike system impact urban mobility in NYC?"
    ],
    "use_cases": [
      "Analyzing the impact of bike-share programs on urban congestion.",
      "Studying the correlation between weather conditions and bike usage.",
      "Evaluating the effectiveness of bike-share systems in promoting sustainable transportation.",
      "Identifying peak usage times to optimize bike station placements."
    ],
    "domain_tags": [
      "transportation",
      "urban planning",
      "sustainability"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2013-present",
    "geographic_scope": "New York City",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/citibikenyc.png",
    "embedding_text": "The Citi Bike System Data is a comprehensive dataset that captures trip-level records from New York City's bike-share system, which has been operational since 2013. This dataset includes approximately 2 million trips per month during peak seasons, making it a rich source for analyzing urban transportation dynamics and micromobility trends. The data is structured in a tabular format, with rows representing individual trips and columns detailing various attributes such as trip duration, start and end station IDs, timestamps, and user types. Key variables include trip duration, which measures how long each trip takes, and station IDs, which indicate the origin and destination of each trip, allowing for detailed analysis of travel patterns within the city. The data is collected through the Citi Bike system's operational infrastructure, which records each trip as it occurs, ensuring a high level of accuracy and timeliness. However, researchers should be aware of potential limitations, such as missing data for certain trips or variations in user demographics that may not be fully captured. Common preprocessing steps may include cleaning the data to handle missing values, aggregating trips by time intervals for analysis, and merging with additional datasets to enrich the context, such as demographic or geographic information. This dataset supports a variety of analyses, including regression analysis to understand factors influencing bike usage, machine learning models to predict future usage patterns, and descriptive statistics to summarize overall trends. Researchers typically use this dataset to address questions related to urban mobility, the effectiveness of bike-share programs, and the impact of external factors such as weather on cycling behavior. By leveraging this dataset, urban planners and transportation researchers can gain valuable insights that inform policy decisions and enhance the sustainability of urban transport systems."
  },
  {
    "name": "NBA Stats API",
    "description": "Official NBA statistics including shot charts, play-by-play, player tracking data, and historical records",
    "category": "Sports & Athletics",
    "url": "https://www.nba.com/stats/",
    "docs_url": "https://github.com/swar/nba_api/blob/master/docs/table_of_contents.md",
    "github_url": null,
    "tags": [
      "basketball",
      "NBA",
      "shot-charts",
      "tracking-data",
      "play-by-play"
    ],
    "best_for": "Basketball analytics, shot analysis, and player performance evaluation",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NBA Stats API provides comprehensive official statistics for the National Basketball Association, including detailed shot charts, play-by-play data, player tracking information, and historical records. This dataset allows users to analyze player performance, game strategies, and historical trends in basketball.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the official NBA statistics available through the NBA Stats API?",
      "How can I access shot charts and player tracking data for NBA games?",
      "What historical records does the NBA Stats API provide?",
      "How can I analyze play-by-play data from NBA games?",
      "What types of analyses can I perform using the NBA Stats API?",
      "Where can I find detailed statistics for NBA players and teams?"
    ],
    "use_cases": [
      "Analyzing player shooting efficiency using shot charts",
      "Evaluating team performance through play-by-play data",
      "Tracking player movements and strategies with tracking data",
      "Conducting historical performance comparisons across seasons"
    ],
    "domain_tags": [
      "sports",
      "athletics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/nba-stats-api.png",
    "embedding_text": "The NBA Stats API is a rich repository of official statistics for the National Basketball Association, encompassing a wide array of data types that are crucial for both casual fans and serious analysts. The dataset includes shot charts, which visually represent player shooting patterns on the court, allowing for a detailed analysis of shooting efficiency and decision-making. Additionally, the API provides play-by-play data, which captures every action during a game, from rebounds to assists, offering a granular view of game dynamics. Player tracking data is also available, which tracks player movements and positions on the court, providing insights into player behavior and team strategies. Historical records are included, enabling users to explore trends over time and compare current player performances with those from past seasons. The data is structured in a tabular format, with rows representing individual game events or player statistics and columns containing variables such as player names, shot locations, time stamps, and game identifiers. This structure facilitates easy manipulation and analysis using data analysis tools like Python's pandas library. The collection methodology for this dataset involves official NBA data collection processes, ensuring high accuracy and reliability. However, users should be aware of potential limitations such as missing data for certain games or players, which can affect analyses. Common preprocessing steps may include cleaning the data to handle missing values, normalizing shot locations for comparative analysis, and aggregating statistics for team-level insights. Researchers and analysts can utilize this dataset to address a variety of research questions, such as evaluating the impact of player movements on game outcomes, analyzing shooting trends across different game situations, or comparing the effectiveness of different offensive strategies. The dataset supports various types of analyses, including regression analysis to identify factors influencing player performance, machine learning models for predictive analytics, and descriptive statistics to summarize player and team performance metrics. Overall, the NBA Stats API serves as a valuable resource for sports analysts, researchers, and enthusiasts looking to deepen their understanding of basketball dynamics and player performance."
  },
  {
    "name": "StatsBomb Open Data",
    "description": "Free soccer event data with 3,400+ events per match including xG, freeze-frame data, and 360 player positioning. Includes Messi's complete La Liga career and multiple World Cups",
    "category": "Sports & Athletics",
    "url": "https://github.com/statsbomb/open-data",
    "docs_url": "https://github.com/statsbomb/open-data/tree/master/doc",
    "github_url": "https://github.com/statsbomb/open-data",
    "tags": [
      "soccer",
      "football",
      "event-data",
      "xG",
      "tracking-data"
    ],
    "best_for": "Soccer analytics, expected goals modeling, and tactical analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "StatsBomb Open Data provides a comprehensive dataset of soccer event data, featuring over 3,400 events per match. This dataset includes advanced metrics such as expected goals (xG), freeze-frame data, and detailed player positioning, making it a valuable resource for analyzing soccer performance and strategies.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is StatsBomb Open Data?",
      "How can I access soccer event data?",
      "What metrics are included in StatsBomb Open Data?",
      "Where can I find detailed player positioning data for soccer?",
      "What is xG in soccer analytics?",
      "How many events are recorded per match in StatsBomb Open Data?",
      "Does StatsBomb Open Data include data from World Cups?",
      "What are the applications of soccer event data in research?"
    ],
    "use_cases": [
      "Analyzing player performance over a season",
      "Comparing team strategies in different matches",
      "Studying the impact of player positioning on match outcomes"
    ],
    "domain_tags": [
      "sports",
      "analytics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/statsbomb-open-data.png",
    "embedding_text": "StatsBomb Open Data is a rich dataset that offers extensive soccer event data, capturing over 3,400 events per match. This dataset is structured in a tabular format, where each row represents a distinct event occurring during a match, and columns include various metrics such as player identifiers, event types, timestamps, xG values, and spatial coordinates for player positioning. The data is meticulously collected from live matches, ensuring high accuracy and detail, which makes it suitable for both academic research and practical applications in sports analytics. Key variables in the dataset include expected goals (xG), which quantifies the likelihood of a goal being scored from a particular position on the field, and freeze-frame data that provides a snapshot of player positions at critical moments. While the dataset is robust, it may have limitations regarding the completeness of events in certain matches or variations in data collection methods across different leagues. Common preprocessing steps may involve cleaning the data for missing values, normalizing player identifiers, and transforming spatial data into usable formats for analysis. Researchers can leverage this dataset to address a variety of questions, such as evaluating player effectiveness, understanding team dynamics, and exploring the relationship between tactical decisions and match outcomes. The dataset supports various types of analyses, including regression analysis to predict match results, machine learning models for player performance forecasting, and descriptive statistics to summarize event occurrences. Overall, StatsBomb Open Data is an invaluable resource for anyone interested in the quantitative analysis of soccer, providing insights that can enhance the understanding of the game and inform coaching strategies."
  },
  {
    "name": "Amazon ShopBench (KDD Cup 2024)",
    "description": "57 tasks, 20K questions derived from real Amazon shopping data for LLM shopping assistants",
    "category": "Data Portals",
    "url": "https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms",
    "docs_url": null,
    "github_url": "https://github.com/fzp0424/ec-guide-kddup-2024",
    "tags": [
      "KDD",
      "Amazon",
      "e-commerce",
      "LLM",
      "2024",
      "multi-task"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Amazon ShopBench dataset consists of 57 tasks and 20,000 questions derived from real Amazon shopping data, specifically designed for training large language model (LLM) shopping assistants. Researchers and developers can utilize this dataset to enhance the performance of LLMs in understanding consumer behavior and improving e-commerce interactions.",
    "use_cases": [
      "Training LLMs for e-commerce applications",
      "Analyzing consumer behavior patterns",
      "Developing multi-task learning models",
      "Improving recommendation systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Amazon ShopBench dataset?",
      "How can I use Amazon shopping data for LLM training?",
      "What tasks are included in the KDD Cup 2024 dataset?",
      "What types of questions are derived from Amazon data?",
      "How does the Amazon ShopBench support multi-task learning?",
      "What insights can be gained from analyzing e-commerce data?",
      "How can LLMs improve shopping assistant experiences?",
      "What are the challenges in using Amazon shopping data for research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/amazon-shopbench-kdd-cup-2024.jpg",
    "embedding_text": "The Amazon ShopBench dataset is a comprehensive collection designed for the KDD Cup 2024, featuring a total of 57 tasks and 20,000 questions that are derived from authentic Amazon shopping data. This dataset is specifically tailored for the development and training of large language models (LLMs) aimed at enhancing shopping assistant capabilities. The data structure consists of a variety of rows and columns that encapsulate different aspects of the shopping experience, including product details, user interactions, and question formats. Each task within the dataset is structured to challenge the LLMs in various ways, allowing for a multi-faceted approach to understanding consumer behavior and improving e-commerce functionalities.\n\nThe collection methodology for the Amazon ShopBench dataset involves aggregating real-world shopping data from Amazon, ensuring that the questions and tasks reflect genuine user inquiries and interactions. This authentic data source provides a rich foundation for training models that can accurately interpret and respond to user needs in a shopping context. While the dataset does not explicitly mention temporal or geographic coverage, it is assumed to represent a diverse range of shopping scenarios typical of a global e-commerce platform.\n\nKey variables within the dataset include product categories, user demographics, interaction types, and question formats. These variables are essential for measuring aspects such as consumer preferences, purchasing patterns, and the effectiveness of various LLM responses. However, researchers should be aware of potential limitations in data quality, such as biases inherent in user-generated content and the dynamic nature of e-commerce trends, which may affect the dataset's applicability over time.\n\nCommon preprocessing steps needed for utilizing the Amazon ShopBench dataset may include data cleaning to remove duplicates or irrelevant entries, normalization of question formats, and categorization of product types to facilitate analysis. Researchers can leverage this dataset to address a variety of research questions, such as understanding how different product attributes influence purchasing decisions or how LLMs can be optimized to provide better customer service.\n\nThe types of analyses supported by this dataset range from regression analysis to machine learning applications and descriptive statistics. Researchers typically use the Amazon ShopBench dataset to develop and evaluate models that can predict consumer behavior, enhance recommendation systems, and improve the overall shopping experience. By employing this dataset, researchers can contribute valuable insights into the intersection of technology and consumer behavior, ultimately leading to more effective e-commerce solutions."
  },
  {
    "name": "Alibaba User Behavior 2018",
    "description": "649M user interactions (clicks, carts, buys) on 25M items",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/649",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "user behavior",
      "large-scale",
      "interactions"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba User Behavior 2018 dataset contains 649 million user interactions, including clicks, carts, and purchases, across 25 million items. This extensive dataset allows researchers and analysts to explore user behavior patterns, optimize marketing strategies, and enhance product recommendations.",
    "use_cases": [
      "Analyzing user engagement metrics to improve marketing strategies.",
      "Developing recommendation systems based on user interaction data.",
      "Studying consumer behavior trends in e-commerce.",
      "Identifying factors that influence purchase decisions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the user interactions in the Alibaba User Behavior 2018 dataset?",
      "How can I analyze user behavior on e-commerce platforms?",
      "What insights can be gained from 649M user interactions?",
      "What types of items are included in the Alibaba User Behavior dataset?",
      "How does user behavior vary across different product categories?",
      "What are common patterns in clicks, carts, and purchases?",
      "How can this dataset help improve e-commerce strategies?",
      "What machine learning techniques can be applied to this user behavior data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018",
    "size_category": "massive",
    "model_score": 0.0,
    "embedding_text": "The Alibaba User Behavior 2018 dataset is a comprehensive collection of user interactions on the Alibaba platform, encompassing 649 million interactions such as clicks, items added to carts, and completed purchases across 25 million distinct items. This dataset is structured in a tabular format, where each row represents a unique user interaction and the columns capture various attributes associated with these interactions. Key variables include user IDs, item IDs, interaction types (click, cart, buy), timestamps, and potentially other demographic or contextual information that may be relevant to understanding user behavior. The data collection methodology likely involved tracking user actions on the Alibaba platform, ensuring a rich dataset that reflects real-world user engagement patterns. However, researchers should be aware of potential limitations, such as biases in user demographics or the influence of external factors on user behavior during the data collection period. Common preprocessing steps may include cleaning the data to handle missing values, normalizing interaction types, and aggregating data for analysis at different levels (e.g., user level, item level). This dataset supports various types of analyses, including regression analysis, machine learning applications for predictive modeling, and descriptive statistics to summarize user behavior trends. Researchers typically leverage this dataset to address research questions related to consumer behavior, such as identifying factors that drive user engagement, understanding the relationship between clicks and purchases, and developing effective recommendation systems. Overall, the Alibaba User Behavior 2018 dataset serves as a valuable resource for those interested in exploring the dynamics of e-commerce user interactions and enhancing the understanding of consumer behavior in digital marketplaces."
  },
  {
    "name": "Open e-commerce 1.0",
    "description": "1.85M Amazon purchases from 5,027 US consumers (2018-2022) linked to demographics (age, gender, income, education)",
    "category": "E-Commerce",
    "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/YGLYDY",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Amazon",
      "demographics",
      "purchases",
      "linked data"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "demographics"
    ],
    "summary": "The Open e-commerce 1.0 dataset contains 1.85 million Amazon purchases made by 5,027 consumers in the US between 2018 and 2022. It includes demographic information such as age, gender, income, and education, allowing for various analyses related to consumer behavior and purchasing patterns.",
    "use_cases": [
      "Analyzing consumer spending habits based on demographic factors.",
      "Investigating the impact of income on purchasing decisions.",
      "Studying trends in e-commerce over a four-year period.",
      "Exploring the relationship between education and product categories purchased."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the demographics of Amazon consumers in the US?",
      "How do income levels affect purchasing behavior on Amazon?",
      "What trends can be identified in Amazon purchases from 2018 to 2022?",
      "How does age influence the types of products purchased on Amazon?",
      "What is the relationship between education level and spending on Amazon?",
      "Can demographic factors predict purchasing patterns on Amazon?",
      "What insights can be gained from analyzing 1.85M Amazon purchases?",
      "How to access the Open e-commerce 1.0 dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018-2022",
    "geographic_scope": "US",
    "size_category": "massive",
    "model_score": 0.0,
    "embedding_text": "The Open e-commerce 1.0 dataset is a comprehensive collection of 1.85 million Amazon purchases made by 5,027 consumers in the United States from 2018 to 2022. This dataset is structured in a tabular format, consisting of rows representing individual purchases and columns that capture various attributes of those purchases, including demographic variables such as age, gender, income, and education level of the consumers. The collection methodology involved aggregating purchase data from Amazon, ensuring a rich dataset that reflects real consumer behavior over a significant period. The temporal coverage of this dataset spans four years, providing insights into evolving consumer trends and preferences in the e-commerce landscape. The geographic scope is limited to the United States, making it particularly relevant for studies focused on American consumer behavior. Key variables in the dataset include demographic information that allows researchers to measure and analyze the influence of various factors on purchasing decisions. For instance, age and income can be correlated with the types of products purchased, while education level may provide insights into spending patterns. Data quality is a crucial aspect of this dataset, as it is derived from actual transactions; however, researchers should be aware of potential limitations such as biases in consumer representation and the dynamic nature of e-commerce, which may affect the generalizability of findings. Common preprocessing steps may include cleaning the data to handle missing values, normalizing income levels, and categorizing products for analysis. This dataset supports a variety of research questions, such as how demographic factors influence purchasing behavior, what trends can be identified in consumer spending over time, and how different income levels affect the types of products purchased. Analysts can employ various methods, including regression analysis, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically use this dataset to inform studies on consumer behavior, market trends, and the impact of demographic variables on purchasing decisions, making it a valuable resource for anyone interested in the intersection of technology and economics in the retail sector."
  },
  {
    "name": "ASOS Digital Experiments",
    "description": "99 real A/B experiments with 24,153 time-granular snapshots for adaptive stopping research",
    "category": "E-Commerce",
    "url": "https://osf.io/64jsb/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "A/B testing",
      "experiments",
      "adaptive",
      "fashion"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The ASOS Digital Experiments dataset contains 99 real A/B experiments with 24,153 time-granular snapshots, specifically designed for adaptive stopping research. Researchers can utilize this dataset to analyze consumer behavior in the fashion industry, evaluate the effectiveness of different marketing strategies, and optimize pricing models.",
    "use_cases": [
      "Analyzing the impact of different marketing strategies on consumer behavior",
      "Evaluating the effectiveness of pricing models",
      "Optimizing adaptive stopping criteria in experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "ASOS A/B testing dataset",
      "fashion e-commerce experiments data",
      "adaptive stopping research dataset",
      "A/B experiments in retail",
      "consumer behavior analysis ASOS",
      "digital marketing experiments data",
      "time-granular snapshots in A/B testing"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/osf.png",
    "embedding_text": "The ASOS Digital Experiments dataset is a comprehensive collection of 99 real A/B experiments, providing a rich resource for researchers and practitioners interested in the dynamics of consumer behavior within the fashion e-commerce sector. This dataset comprises 24,153 time-granular snapshots, which are crucial for conducting adaptive stopping research. The structure of the dataset is primarily tabular, with rows representing individual experiments and columns capturing various attributes related to each experiment, such as experiment ID, treatment groups, metrics measured (e.g., conversion rates, average order value), and timestamps. The key variables in this dataset measure the effectiveness of different marketing strategies and consumer responses to various pricing models, allowing for a nuanced analysis of consumer behavior in a competitive retail environment. Researchers can leverage this dataset to address critical research questions, such as how different factors influence consumer purchasing decisions and the optimal conditions for implementing adaptive stopping in A/B testing frameworks. The data collection methodology involves real-time tracking of user interactions and responses during the experiments, ensuring a high level of data quality. However, known limitations may include potential biases in user selection and external factors affecting consumer behavior that are not captured in the dataset. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing metrics for comparative analysis, and segmenting the data based on demographic or behavioral characteristics. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic research and practical applications in the field of digital marketing. Researchers typically utilize this dataset to derive insights that inform strategic decisions, enhance customer engagement, and optimize overall marketing effectiveness in the fashion e-commerce landscape."
  },
  {
    "name": "Alibaba Mobile 2021",
    "description": "Mobile user behavior data from Alibaba (2021)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/109858",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "mobile",
      "user behavior",
      "Alibaba"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Alibaba Mobile 2021 dataset contains mobile user behavior data collected from Alibaba's platform in 2021. This dataset can be utilized to analyze user engagement, purchasing patterns, and mobile usage trends, providing valuable insights for e-commerce strategies.",
    "use_cases": [
      "Analyzing mobile user engagement on Alibaba",
      "Understanding purchasing patterns of mobile users",
      "Identifying trends in mobile e-commerce behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Alibaba Mobile 2021 dataset?",
      "How to analyze mobile user behavior data?",
      "What insights can be gained from Alibaba's mobile data?",
      "What are the trends in e-commerce mobile usage in 2021?",
      "How does user behavior on Alibaba differ from other platforms?",
      "What variables are included in the Alibaba Mobile 2021 dataset?",
      "How can I access Alibaba Mobile 2021 data?",
      "What analysis can be performed on Alibaba's mobile user data?"
    ],
    "domain_tags": [
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2021",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Alibaba Mobile 2021 dataset is a comprehensive collection of mobile user behavior data sourced from Alibaba's e-commerce platform during the year 2021. This dataset is structured in a tabular format, consisting of various rows and columns that capture key variables related to user interactions, transactions, and engagement metrics on mobile devices. The collection methodology involves aggregating data from user activities on the Alibaba platform, ensuring a rich representation of mobile user behavior. Key variables in the dataset may include user IDs, session durations, page views, purchase amounts, and product categories, each measuring different aspects of user engagement and purchasing behavior. The dataset is particularly valuable for researchers and analysts interested in e-commerce trends, consumer behavior, and mobile usage patterns. However, it is important to note that data quality may vary, and limitations such as potential biases in user representation or missing values could impact analyses. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Researchers can leverage this dataset to address various research questions, such as identifying factors influencing mobile purchasing decisions, analyzing the impact of mobile user experience on sales, and exploring demographic differences in mobile shopping behavior. The types of analyses supported by this dataset include regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for understanding the dynamics of mobile e-commerce. Typically, researchers use this dataset to inform marketing strategies, enhance user experience, and optimize product offerings on mobile platforms."
  },
  {
    "name": "PaySim Synthetic Transactions",
    "description": "6M+ mobile money transactions simulating real fraud patterns. Agent-based model calibrated on real African mobile money logs",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/datasets/ealaxi/paysim1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "fraud detection",
      "mobile payments",
      "transactions",
      "large-scale",
      "synthetic"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "financial services",
      "fraud detection",
      "mobile payments"
    ],
    "summary": "The PaySim Synthetic Transactions dataset consists of over 6 million mobile money transactions that simulate real-world fraud patterns. This dataset is particularly useful for researchers and practitioners in the financial services sector looking to analyze and develop fraud detection algorithms in mobile payment systems.",
    "use_cases": [
      "Developing fraud detection algorithms",
      "Analyzing transaction patterns in mobile payments",
      "Testing machine learning models for fraud detection"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "PaySim dataset for fraud detection",
      "synthetic mobile money transactions",
      "analyze mobile payment fraud patterns",
      "6M mobile transactions dataset",
      "agent-based model for fraud detection",
      "financial services synthetic data",
      "mobile payments transaction analysis",
      "fraud detection in mobile money"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Africa",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/paysim-synthetic-transactions.jpg",
    "embedding_text": "The PaySim Synthetic Transactions dataset is a comprehensive collection of over 6 million mobile money transactions, designed to simulate real-world fraud patterns. This dataset is structured in a tabular format, with each row representing an individual transaction. The columns include various attributes that capture essential information about the transactions, such as transaction amount, transaction type, and user characteristics. The data is generated through an agent-based model that has been calibrated using real transaction logs from mobile money services in Africa, ensuring that the simulated transactions reflect authentic user behavior and fraud patterns observed in real-world scenarios. Researchers and data scientists can leverage this dataset to explore various aspects of mobile payment systems, particularly in the context of fraud detection. The key variables within the dataset measure critical aspects of transactions, including the frequency of transactions, amounts involved, and the types of users engaging in these transactions. This rich dataset allows for a wide range of analyses, including regression analysis, machine learning model training, and descriptive statistics, enabling researchers to uncover insights into transaction behaviors and the effectiveness of different fraud detection strategies. However, it is important to note that, while the dataset is extensive, it is synthetic in nature, which may introduce certain limitations in terms of generalizability to real-world scenarios. Common preprocessing steps may include normalization of transaction amounts, encoding categorical variables, and handling any missing values to prepare the data for analysis. Researchers typically use this dataset to address critical research questions related to fraud detection in mobile payments, such as identifying key indicators of fraudulent transactions, evaluating the performance of various machine learning algorithms in detecting fraud, and understanding the dynamics of user behavior in mobile money transactions. Overall, the PaySim Synthetic Transactions dataset serves as a valuable resource for advancing the field of fraud detection in financial services, particularly within the rapidly evolving domain of mobile payments."
  },
  {
    "name": "European Car Market",
    "description": "Car information including prices and attributes (1970-1999)",
    "category": "Automotive",
    "url": "https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cars",
      "Europe",
      "prices",
      "IO"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "automotive",
      "pricing",
      "market-analysis"
    ],
    "summary": "The European Car Market dataset contains detailed information about cars sold in Europe between 1970 and 1999, including various attributes and prices. Researchers and analysts can utilize this dataset to study trends in car pricing, consumer preferences, and the evolution of automotive features over time.",
    "use_cases": [
      "Analyzing price trends of European cars over the decades.",
      "Studying the impact of economic factors on car pricing.",
      "Exploring consumer preferences in car attributes during the specified period."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the price trends of cars in Europe from 1970 to 1999?",
      "How have car attributes changed in the European market over the decades?",
      "What factors influence car prices in Europe during the 1970-1999 period?",
      "Which car brands were most popular in Europe between 1970 and 1999?",
      "How do car prices in Europe compare to other regions during the same time frame?",
      "What demographic factors can be analyzed using the European Car Market dataset?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1970-1999",
    "geographic_scope": "Europe",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/google.png",
    "embedding_text": "The European Car Market dataset is a comprehensive collection of car-related data that spans from 1970 to 1999, focusing on various attributes and pricing information relevant to the automotive industry in Europe. This dataset is structured in a tabular format, consisting of rows representing individual car entries and columns that detail specific variables such as make, model, year, price, engine size, fuel type, and other relevant attributes. The collection methodology likely involved aggregating data from various sources, including automotive sales records, manufacturer databases, and possibly consumer reports, ensuring a rich dataset for analysis. The temporal coverage of this dataset is explicitly defined, covering nearly three decades of automotive history, which allows researchers to observe trends and shifts in consumer behavior and market dynamics over time. The geographic scope is limited to Europe, providing insights into the unique characteristics of the European car market during this period. Key variables within the dataset measure essential aspects of car performance and consumer preferences, such as price fluctuations, engine specifications, and fuel efficiency, which can be critical for understanding market trends. However, potential limitations include the availability of data for certain models or years, which may affect the comprehensiveness of analyses. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing price data for inflation, and encoding categorical variables for analysis. Researchers can leverage this dataset to address various research questions, such as identifying the factors that influence car prices, analyzing the popularity of different car brands, and exploring how economic conditions impacted consumer choices in the automotive sector. The dataset supports a range of analytical techniques, including regression analysis to model price determinants, machine learning for predictive analytics, and descriptive statistics to summarize key trends. Overall, the European Car Market dataset serves as a valuable resource for scholars and industry professionals interested in the automotive market, enabling them to conduct in-depth studies and derive meaningful insights from historical data."
  },
  {
    "name": "Mendeley Food Delivery Reviews",
    "description": "1.69M reviews from DoorDash, Grubhub, Uber Eats. Ratings, text reviews, restaurant metadata. Gig economy platform research",
    "category": "Food & Delivery",
    "url": "https://data.mendeley.com/datasets/m5jk7wzyg7/1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "food delivery",
      "reviews",
      "DoorDash",
      "Grubhub",
      "Uber Eats",
      "gig economy"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "gig-economy"
    ],
    "summary": "The Mendeley Food Delivery Reviews dataset contains 1.69 million reviews from popular food delivery services such as DoorDash, Grubhub, and Uber Eats. This dataset provides insights into consumer preferences, service quality, and restaurant performance, making it valuable for research in the gig economy and food delivery sectors.",
    "use_cases": [
      "Analyzing customer satisfaction trends across different food delivery platforms.",
      "Examining the impact of restaurant characteristics on review ratings.",
      "Investigating the sentiment of text reviews to understand consumer preferences.",
      "Comparing service quality between different gig economy platforms."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common ratings for DoorDash reviews?",
      "How do Grubhub reviews compare to Uber Eats in terms of customer satisfaction?",
      "What trends can be observed in food delivery reviews over time?",
      "Which restaurants receive the highest ratings on food delivery platforms?",
      "What factors influence customer ratings in food delivery services?",
      "How does the gig economy impact consumer behavior in food delivery?",
      "What are the common themes in text reviews for food delivery services?",
      "How can machine learning be applied to analyze food delivery reviews?"
    ],
    "domain_tags": [
      "retail",
      "food-service",
      "gig-economy"
    ],
    "data_modality": "text",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/mendeley.png",
    "embedding_text": "The Mendeley Food Delivery Reviews dataset is a comprehensive collection of 1.69 million reviews sourced from leading food delivery platforms, including DoorDash, Grubhub, and Uber Eats. This dataset is structured in a tabular format, with rows representing individual reviews and columns containing various attributes such as ratings, text reviews, and restaurant metadata. Key variables include numerical ratings that measure customer satisfaction, qualitative text reviews that provide insights into the dining experience, and metadata related to the restaurants, such as cuisine type and location. The collection methodology involves aggregating user-generated content from these platforms, ensuring a diverse representation of consumer opinions across different demographics and geographic locations. However, researchers should be aware of potential limitations, including biases in user reviews, as they may not represent the entire customer base of these services. Common preprocessing steps may include text normalization, sentiment analysis, and filtering out irrelevant or duplicate reviews to enhance data quality. This dataset supports various types of analyses, including regression analysis to identify factors influencing ratings, machine learning techniques for sentiment classification, and descriptive statistics to summarize review trends. Researchers typically utilize this dataset to address questions related to consumer behavior in the gig economy, evaluate service quality across platforms, and explore the relationship between restaurant characteristics and customer satisfaction. Overall, the Mendeley Food Delivery Reviews dataset serves as a valuable resource for understanding the dynamics of the food delivery industry and the evolving landscape of consumer preferences."
  },
  {
    "name": "Alibaba Brick and Mortar (IJCAI-16)",
    "description": "Online and offline check-ins/purchases from 1,000+ stores",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/53",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "O2O",
      "offline",
      "retail",
      "IJCAI"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "retail"
    ],
    "summary": "The Alibaba Brick and Mortar dataset comprises online and offline check-ins and purchases from over 1,000 stores. This dataset allows researchers and analysts to explore consumer behavior in both digital and physical retail environments, offering insights into purchasing patterns and preferences.",
    "use_cases": [
      "Analyzing consumer purchasing behavior across online and offline channels",
      "Identifying trends in retail shopping patterns",
      "Exploring the impact of digital marketing on physical store sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Brick and Mortar dataset?",
      "How can I analyze offline purchases from the Alibaba dataset?",
      "What insights can be gained from the Alibaba check-in data?",
      "What are the key variables in the Alibaba Brick and Mortar dataset?",
      "How does online behavior correlate with offline purchases?",
      "What trends can be identified in retail using the Alibaba dataset?",
      "How can this dataset help in understanding consumer behavior?",
      "What are the applications of the Alibaba Brick and Mortar data in e-commerce research?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Alibaba Brick and Mortar dataset is a comprehensive collection of online and offline check-ins and purchases recorded from over 1,000 retail stores. This dataset is structured in a tabular format, consisting of rows representing individual transactions or check-ins, with columns capturing various attributes such as transaction ID, store ID, timestamp, purchase amount, and customer demographics. The data is collected through a combination of direct reporting from retail partners and user-generated check-ins via mobile applications, ensuring a robust representation of consumer interactions in both digital and physical environments. \n\nKey variables in this dataset include transaction identifiers, timestamps that denote when purchases were made, and amounts spent, which provide insights into consumer spending behavior. Additionally, demographic information about customers, when available, can help in segmenting the data for more detailed analysis. The dataset is particularly valuable for its ability to bridge the gap between online and offline shopping behaviors, allowing researchers to explore how digital interactions influence physical purchases. \n\nHowever, like any dataset, the Alibaba Brick and Mortar dataset has its limitations. Data quality can vary depending on the accuracy of user check-ins and the completeness of transaction records provided by retail partners. Researchers may need to undertake common preprocessing steps such as cleaning the data to handle missing values, normalizing transaction amounts, and filtering out outlier transactions that may skew results. \n\nThis dataset supports a variety of analyses, including regression analysis to understand factors influencing purchase decisions, machine learning techniques for predictive modeling of consumer behavior, and descriptive statistics to summarize shopping trends. Researchers typically utilize this dataset to address questions related to consumer behavior, such as how online marketing efforts affect in-store purchases, what demographic factors influence shopping habits, and how seasonal trends impact retail sales. By leveraging the rich data available in the Alibaba Brick and Mortar dataset, analysts can generate actionable insights that inform marketing strategies and improve retail operations."
  },
  {
    "name": "ICPSR Auction Studies",
    "description": "Search results for auction studies from ICPSR",
    "category": "Advertising",
    "url": "https://www.openicpsr.org/openicpsr/search/studies",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "auctions",
      "research",
      "social science"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The ICPSR Auction Studies dataset provides a comprehensive collection of auction-related research data, allowing users to explore various aspects of auction mechanisms and outcomes. Researchers can utilize this dataset to analyze bidding behaviors, auction design, and market dynamics.",
    "use_cases": [
      "Analyzing bidding strategies in auctions",
      "Examining the impact of auction design on outcomes",
      "Investigating consumer behavior in auction settings"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key findings from the ICPSR Auction Studies?",
      "How can I access auction research data from ICPSR?",
      "What variables are included in the ICPSR Auction Studies dataset?",
      "What methodologies are used in auction studies?",
      "How do auction outcomes vary across different studies?",
      "What insights can be gained from analyzing auction data?",
      "What are the common themes in auction research?",
      "How do bidding strategies affect auction results?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The ICPSR Auction Studies dataset is a valuable resource for researchers interested in the field of auction studies, encompassing a variety of auction formats and participant behaviors. The dataset is structured in a tabular format, typically containing rows representing individual auction instances and columns detailing various attributes such as auction type, participant demographics, bid amounts, and outcomes. Researchers can expect to find key variables that measure aspects like winning bids, bidder strategies, and auction duration, which are crucial for understanding the dynamics of auction environments. The collection methodology involves aggregating data from multiple auction studies, ensuring a diverse representation of auction scenarios and participant behaviors. While the dataset provides a wealth of information, researchers should be aware of potential limitations regarding data quality, including variations in study design and participant engagement levels, which may affect the generalizability of findings. Common preprocessing steps may include cleaning data to address missing values, normalizing bid amounts for comparative analysis, and categorizing auction types for more nuanced insights. The dataset supports various types of analyses, including regression analysis to identify factors influencing auction outcomes, machine learning techniques to predict bidding behaviors, and descriptive statistics to summarize participant trends. Researchers typically utilize this dataset to address questions related to auction efficiency, bidder psychology, and the effectiveness of different auction formats in eliciting true valuations from participants. Overall, the ICPSR Auction Studies dataset serves as a foundational tool for advancing knowledge in auction theory and practice, providing insights that can inform both academic research and practical applications in market design."
  },
  {
    "name": "Online Shopping Intention",
    "description": "12,330 user sessions with numerical and categorical features for purchase prediction",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/henrysue/online-shoppers-intention",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "purchase prediction",
      "sessions",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Online Shopping Intention dataset consists of 12,330 user sessions, featuring both numerical and categorical data aimed at predicting purchase behavior. This dataset can be utilized for various analyses, including regression modeling and machine learning applications to understand consumer purchasing patterns.",
    "use_cases": [
      "Analyzing factors influencing online purchase decisions",
      "Building predictive models for e-commerce sales",
      "Segmenting user sessions based on purchasing behavior"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Online Shopping Intention dataset?",
      "How can I use the Online Shopping Intention dataset for purchase prediction?",
      "What features are included in the Online Shopping Intention dataset?",
      "Where can I find the Online Shopping Intention dataset?",
      "What analyses can be performed with the Online Shopping Intention dataset?",
      "Is the Online Shopping Intention dataset suitable for machine learning?",
      "What is the size of the Online Shopping Intention dataset?",
      "What are the key variables in the Online Shopping Intention dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/online-shopping-intention.jpg",
    "embedding_text": "The Online Shopping Intention dataset is a comprehensive collection of 12,330 user sessions, structured in a tabular format, which includes both numerical and categorical features essential for predicting purchase behavior in an e-commerce context. Each session is represented as a row in the dataset, while the columns encompass various variables that capture user interactions and characteristics during their online shopping experience. The data is particularly valuable for researchers and data scientists interested in understanding consumer behavior, as it provides insights into the factors that influence purchasing decisions. The dataset's collection methodology is rooted in real user interactions, likely sourced from an e-commerce platform, although specific details regarding the data collection process are not provided. This dataset does not specify temporal or geographic coverage, making it versatile for various analytical applications without constraints related to time or location. Key variables within the dataset may include user demographics, session duration, product categories viewed, and actions taken during the session, all of which are critical for measuring user intent and predicting purchase likelihood. However, researchers should be aware of potential limitations in data quality, such as missing values or biases inherent in user behavior, which may affect the robustness of analyses conducted. Common preprocessing steps may involve cleaning the data, handling missing values, and encoding categorical variables to prepare for modeling. The dataset supports a range of analyses, including regression techniques to identify relationships between variables and machine learning approaches to build predictive models. Researchers typically leverage this dataset to address questions related to consumer purchasing patterns, evaluate the effectiveness of marketing strategies, and develop insights that can inform e-commerce practices. Overall, the Online Shopping Intention dataset serves as a valuable resource for those looking to explore the dynamics of online shopping behavior and enhance their understanding of the factors that drive consumer purchases in the digital marketplace."
  },
  {
    "name": "Criteo Attribution Dataset",
    "description": "30 days of advertising traffic with conversion attribution data for multi-touch attribution research",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/criteo-attribution-modeling-bidding-dataset/",
    "docs_url": "https://ailab.criteo.com/criteo-attribution-modeling-bidding-dataset/",
    "github_url": null,
    "tags": [
      "attribution",
      "conversions",
      "touchpoints",
      "Criteo"
    ],
    "best_for": "Multi-touch attribution modeling and conversion path analysis",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Criteo Attribution Dataset provides 30 days of advertising traffic data, including conversion attribution for multi-touch attribution research. Researchers can utilize this dataset to analyze the effectiveness of various advertising touchpoints and their influence on consumer conversions.",
    "use_cases": [
      "Analyzing the impact of different advertising channels on conversions",
      "Evaluating the effectiveness of multi-touch attribution models",
      "Understanding consumer behavior in response to advertising touchpoints"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Criteo Attribution Dataset?",
      "How can I analyze multi-touch attribution with Criteo data?",
      "What variables are included in the Criteo Attribution Dataset?",
      "What insights can be gained from advertising traffic data?",
      "How does conversion attribution work in advertising?",
      "What are the key metrics in the Criteo Attribution Dataset?",
      "How can I preprocess Criteo Attribution data for analysis?",
      "What types of analyses can be performed on the Criteo Attribution Dataset?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "30 days",
    "geographic_scope": "Global",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/criteo-attribution-dataset.jpg",
    "embedding_text": "The Criteo Attribution Dataset is a comprehensive collection of advertising traffic data designed for the purpose of multi-touch attribution research. This dataset spans a period of 30 days and contains detailed information regarding conversion attribution, which is crucial for understanding how different advertising touchpoints contribute to consumer conversions. The data structure is organized in a tabular format, consisting of rows representing individual advertising interactions and columns that capture various attributes such as timestamps, advertising channel identifiers, user identifiers, and conversion outcomes. Key variables within the dataset include the type of advertisement, the sequence of touchpoints leading to a conversion, and the time taken for a user to convert after interacting with an ad. These variables are essential for measuring the effectiveness of advertising strategies and understanding consumer behavior patterns. The methodology for data collection involves tracking user interactions with advertisements across multiple platforms, which allows for a rich dataset that reflects real-world advertising scenarios. However, researchers should be aware of potential limitations in data quality, including issues related to user privacy, data completeness, and the representativeness of the sample. Common preprocessing steps may include data cleaning to handle missing values, normalization of variables for comparative analysis, and encoding categorical variables for use in machine learning models. This dataset supports a variety of analyses, including regression analysis to determine the impact of different advertising strategies on conversion rates, machine learning techniques for predictive modeling, and descriptive statistics to summarize user engagement patterns. Researchers typically leverage the Criteo Attribution Dataset to address critical research questions related to the effectiveness of advertising campaigns, the role of different touchpoints in the conversion process, and the overall impact of digital marketing strategies on consumer behavior. By utilizing this dataset, analysts can derive actionable insights that inform future advertising decisions and optimize marketing efforts."
  },
  {
    "name": "BTS Airline On-Time Performance",
    "description": "All US flights since 1987. Delays, cancellations, fares, capacity. Revenue management research goldmine",
    "category": "Transportation & Mobility",
    "url": "https://www.transtats.bts.gov/ontime/",
    "docs_url": "https://www.bts.gov/topics/airline-time-tables",
    "github_url": null,
    "tags": [
      "airline",
      "flights",
      "delays",
      "pricing",
      "large-scale"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "data-analysis",
      "economics"
    ],
    "summary": "The BTS Airline On-Time Performance dataset contains comprehensive records of all US flights since 1987, detailing delays, cancellations, fares, and capacity. This dataset serves as a valuable resource for revenue management research, enabling users to analyze trends in airline performance and pricing strategies.",
    "use_cases": [
      "Analyzing trends in flight delays over time",
      "Evaluating the impact of pricing strategies on flight cancellations",
      "Studying the relationship between flight capacity and on-time performance",
      "Investigating seasonal variations in airline delays and cancellations"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the BTS Airline On-Time Performance dataset?",
      "How can I analyze US flight delays using the BTS dataset?",
      "What insights can be gained from the BTS Airline dataset?",
      "Where can I find data on airline cancellations in the US?",
      "What variables are included in the BTS Airline On-Time Performance dataset?",
      "How has airline pricing changed over the years?",
      "What are common trends in US flight delays since 1987?",
      "How can I use the BTS dataset for revenue management research?"
    ],
    "domain_tags": [
      "transportation",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1987-present",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The BTS Airline On-Time Performance dataset is a rich and extensive collection of data that encompasses all US flights since 1987, providing a detailed view of various performance metrics including delays, cancellations, fares, and capacity. The dataset is structured in a tabular format, with rows representing individual flights and columns capturing key variables such as flight dates, departure and arrival times, delays, cancellations, and fare information. This comprehensive schema allows researchers to conduct in-depth analyses of airline performance over time. The data is collected by the Bureau of Transportation Statistics (BTS) from various sources, including airlines and airport authorities, ensuring a high level of accuracy and reliability. However, researchers should be aware of potential limitations, such as missing data for certain flights or variations in reporting practices among airlines. Common preprocessing steps may include handling missing values, normalizing fare data, and aggregating flight information by month or year to facilitate trend analysis. The dataset supports a wide range of research questions, such as examining the factors contributing to flight delays, analyzing the impact of economic conditions on airline pricing, and exploring the relationship between flight capacity and on-time performance. Researchers typically employ various analytical techniques, including regression analysis, machine learning models, and descriptive statistics, to derive insights from the data. Overall, the BTS Airline On-Time Performance dataset serves as a valuable resource for anyone interested in the transportation sector, providing the necessary data to inform studies on airline efficiency, pricing strategies, and consumer behavior in the aviation industry."
  },
  {
    "name": "ORBITAAL Bitcoin Transactions",
    "description": "13 years of Bitcoin transaction graphs (2009-2022). Complete blockchain with labeled entities. network analysis at scale",
    "category": "Financial Services",
    "url": "https://zenodo.org/records/7958648",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Bitcoin",
      "blockchain",
      "transactions",
      "network analysis",
      "cryptocurrency"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "cryptocurrency",
      "financial analysis",
      "network analysis"
    ],
    "summary": "The ORBITAAL Bitcoin Transactions dataset encompasses 13 years of Bitcoin transaction data from 2009 to 2022, providing a comprehensive view of the blockchain with labeled entities. Researchers and analysts can utilize this dataset for extensive network analysis, exploring transaction patterns, and understanding the dynamics of cryptocurrency markets.",
    "use_cases": [
      "Analyzing the growth of Bitcoin transactions over time.",
      "Identifying key entities and their roles in the Bitcoin network.",
      "Exploring transaction patterns and their implications for market behavior.",
      "Conducting network analysis to understand the relationships between different Bitcoin addresses."
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the transaction patterns in the ORBITAAL Bitcoin dataset?",
      "How can I analyze Bitcoin transactions over the years?",
      "What entities are labeled in the ORBITAAL Bitcoin Transactions dataset?",
      "What insights can be gained from Bitcoin network analysis?",
      "How does the ORBITAAL dataset support blockchain research?",
      "What tools can be used to visualize Bitcoin transaction graphs?",
      "How can I access the ORBITAAL Bitcoin Transactions dataset?",
      "What are the key variables in the ORBITAAL Bitcoin dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "graph",
    "temporal_coverage": "2009-2022",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/zenodo.png",
    "embedding_text": "The ORBITAAL Bitcoin Transactions dataset is a rich resource for researchers and analysts interested in the dynamics of Bitcoin and the broader cryptocurrency market. This dataset spans 13 years, from 2009 to 2022, and includes a complete blockchain with labeled entities, allowing for detailed network analysis at scale. The data structure consists of transaction graphs where each row represents a transaction, and columns include variables such as transaction ID, timestamp, sender and receiver addresses, transaction amounts, and labels for entities involved in the transactions. This comprehensive schema enables users to perform various analyses, from basic descriptive statistics to complex machine learning models. The collection methodology involves aggregating data from the Bitcoin blockchain, ensuring that it captures the full spectrum of transactions over the specified temporal coverage. However, users should be aware of potential data quality issues, such as incomplete records or inaccuracies in entity labeling, which may arise from the decentralized nature of blockchain technology. Common preprocessing steps may include cleaning the data to handle missing values, normalizing transaction amounts, and transforming the data into a suitable format for analysis. Researchers can leverage this dataset to address a variety of research questions, such as identifying trends in Bitcoin usage, understanding the impact of market events on transaction behavior, and exploring the relationships between different entities within the Bitcoin network. The dataset supports various types of analyses, including regression analysis to predict future transaction trends, machine learning techniques for classification of transaction types, and descriptive analyses to summarize transaction characteristics. Overall, the ORBITAAL Bitcoin Transactions dataset serves as a foundational tool for studies focused on cryptocurrency, offering insights into transaction dynamics and network behavior that are crucial for understanding the evolving landscape of digital currencies."
  },
  {
    "name": "Criteo 1TB Click Logs",
    "description": "World's largest public ML advertising dataset with 4+ billion events, 13 integer and 26 categorical features across 24 days",
    "category": "Advertising",
    "url": "https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/",
    "docs_url": "https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/",
    "github_url": null,
    "tags": [
      "CTR prediction",
      "click logs",
      "large-scale",
      "Criteo"
    ],
    "best_for": "Training and benchmarking large-scale CTR prediction models",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "machine-learning",
      "data-analysis"
    ],
    "summary": "The Criteo 1TB Click Logs dataset is the world's largest public machine learning advertising dataset, containing over 4 billion click events. It features 13 integer and 26 categorical variables collected over a span of 24 days, making it ideal for training models in click-through rate (CTR) prediction and understanding user interactions with advertisements.",
    "use_cases": [
      "Building predictive models for click-through rates in online advertising.",
      "Analyzing user behavior patterns based on click logs.",
      "Evaluating the effectiveness of different advertising strategies.",
      "Conducting large-scale experiments in machine learning."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Criteo 1TB Click Logs dataset?",
      "How can I use the Criteo dataset for CTR prediction?",
      "What features are included in the Criteo click logs?",
      "Where can I find large-scale advertising datasets?",
      "What types of analyses can be performed with the Criteo dataset?",
      "What are the applications of click logs in advertising?",
      "How does the Criteo dataset support machine learning research?",
      "What are the limitations of the Criteo 1TB Click Logs dataset?"
    ],
    "domain_tags": [
      "advertising",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "24 days",
    "geographic_scope": "Global",
    "size_category": "massive",
    "model_score": 0.0,
    "embedding_text": "The Criteo 1TB Click Logs dataset represents a significant resource for researchers and practitioners in the field of advertising and machine learning. Comprising over 4 billion individual click events, this dataset is structured in a tabular format with 39 features, including 13 integer variables and 26 categorical variables. The data is collected over a period of 24 days, capturing a wide range of user interactions with online advertisements. Each row in the dataset corresponds to a unique click event, providing detailed insights into user behavior and advertisement performance. The dataset is particularly valuable for training machine learning models aimed at predicting click-through rates (CTR), a critical metric in online advertising that measures the effectiveness of ad placements. The key variables in the dataset include user identifiers, ad identifiers, and various features that describe the context of each click event, such as time of day, device type, and ad characteristics. Researchers can leverage this dataset to explore a variety of research questions, including how different factors influence user engagement with ads, the effectiveness of targeting strategies, and the overall performance of advertising campaigns. However, it is important to note that the dataset may have limitations related to data quality, such as missing values or biases inherent in user behavior. Common preprocessing steps may include handling missing data, encoding categorical variables, and normalizing numerical features to prepare the dataset for analysis. The Criteo dataset supports various types of analyses, including regression modeling, machine learning classification tasks, and descriptive statistics. Researchers typically use this dataset to conduct large-scale experiments, validate algorithms, and develop new methodologies in the realm of online advertising and consumer behavior analysis. Overall, the Criteo 1TB Click Logs dataset serves as a foundational tool for advancing research and applications in the advertising domain, enabling deeper insights into the dynamics of digital marketing and user interaction.",
    "benchmark_usage": [
      "CTR prediction",
      "large-scale machine learning experiments"
    ]
  },
  {
    "name": "JD.com Open Datasets",
    "description": "Open dataset portal for e-commerce and logistics from JD.com",
    "category": "Data Portals",
    "url": "https://datascience.jd.com/page/opendataset.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "JD.com",
      "e-commerce",
      "logistics",
      "China"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "logistics"
    ],
    "summary": "The JD.com Open Datasets provide a comprehensive collection of data related to e-commerce and logistics operations from JD.com, one of China's largest online retailers. Researchers and analysts can utilize this dataset to explore consumer behavior, optimize logistics processes, and conduct market analysis in the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Optimizing supply chain logistics",
      "Conducting market trend analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from JD.com for e-commerce analysis?",
      "How can I access JD.com Open Datasets for logistics research?",
      "What types of data does JD.com provide for consumer behavior studies?",
      "Are there any datasets from JD.com related to online shopping trends?",
      "What variables are included in JD.com Open Datasets?",
      "How can JD.com datasets be used in academic research?",
      "What insights can be gained from JD.com logistics data?",
      "Where can I find open datasets for e-commerce in China?"
    ],
    "domain_tags": [
      "retail",
      "e-commerce"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/jd.png",
    "embedding_text": "The JD.com Open Datasets serve as a vital resource for researchers and practitioners interested in the fields of e-commerce and logistics. This dataset is structured in a tabular format, containing various rows and columns that represent different data points collected from JD.com's extensive operations. Each row typically corresponds to a unique transaction or logistical event, while the columns include key variables such as transaction ID, product categories, customer demographics, order timestamps, and logistics performance metrics. The collection methodology involves aggregating data from JD.com's operational databases, ensuring that the dataset reflects real-world scenarios and trends in the Chinese e-commerce market. However, users should be aware of potential limitations in data quality, such as missing values or inconsistencies arising from the vast scale of data collection. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the dataset for analysis. Researchers can leverage this dataset to address various research questions, such as understanding consumer behavior trends, identifying factors influencing purchase decisions, and evaluating the efficiency of logistics operations. The types of analyses supported by this dataset range from descriptive statistics to more complex regression models and machine learning applications, enabling users to derive actionable insights. Typically, researchers utilize JD.com Open Datasets in studies focused on market analysis, consumer behavior research, and logistics optimization, making it a valuable asset for anyone looking to explore the dynamics of e-commerce in China."
  },
  {
    "name": "RecSys Challenge 2025 (Synerise)",
    "description": "1M users, 6 months of real e-commerce behavior logs with 5 event types for universal behavioral modeling",
    "category": "Data Portals",
    "url": "https://recsys.synerise.com/data-set",
    "docs_url": "https://recsys.synerise.com/",
    "github_url": "https://github.com/Synerise/recsys2025",
    "tags": [
      "RecSys",
      "e-commerce",
      "large-scale",
      "real-world",
      "2025",
      "user behavior"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The RecSys Challenge 2025 dataset contains 1 million users' real e-commerce behavior logs over a period of six months, encompassing five different event types. This dataset is ideal for researchers and practitioners interested in universal behavioral modeling, enabling them to analyze user interactions and derive insights into consumer behavior in a large-scale e-commerce context.",
    "use_cases": [
      "Analyzing user engagement over time",
      "Modeling consumer purchasing behavior",
      "Identifying trends in e-commerce interactions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the user behavior patterns in the RecSys Challenge 2025 dataset?",
      "How can I analyze e-commerce behavior using the RecSys dataset?",
      "What insights can be derived from 1M users' behavior logs?",
      "What event types are included in the RecSys Challenge 2025 dataset?",
      "How does user behavior change over six months in e-commerce?",
      "What are the implications of large-scale user data for behavioral modeling?",
      "How can regression analysis be applied to this dataset?",
      "What are the challenges in analyzing real-world e-commerce logs?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "large",
    "model_score": 0.0,
    "image_url": "/images/datasets/recsys-challenge-2025-synerise.png",
    "embedding_text": "The RecSys Challenge 2025 dataset is a comprehensive collection of e-commerce behavior logs from 1 million users over a six-month period, featuring five distinct event types. This dataset is structured in a tabular format, with rows representing individual user interactions and columns capturing various attributes such as user ID, event type, timestamp, product ID, and other relevant metrics that characterize user behavior. The collection methodology involved aggregating real-world interaction data from an e-commerce platform, ensuring a rich and diverse representation of consumer behavior in a digital shopping environment. The temporal coverage of this dataset spans six months, providing a longitudinal view of user engagement and behavior patterns, while the geographic scope is not explicitly defined, indicating that the data may represent a broad range of users without specific regional limitations. Key variables within the dataset include event types, which measure different user actions such as product views, purchases, and cart additions, as well as timestamps that allow for the analysis of temporal trends in user behavior. Researchers should be aware of potential data quality issues, such as missing values or inconsistencies in event logging, which may necessitate common preprocessing steps like data cleaning, normalization, and transformation to prepare the dataset for analysis. The dataset supports a variety of research questions, including inquiries into user engagement metrics, the effectiveness of marketing strategies, and the impact of seasonal trends on purchasing behavior. Analysts can employ various techniques, including regression analysis, machine learning models, and descriptive statistics, to extract meaningful insights from the data. Researchers typically utilize this dataset to explore user behavior dynamics, develop predictive models for consumer actions, and assess the effectiveness of different e-commerce strategies, making it a valuable resource for both academic and practical applications in the field of e-commerce and behavioral modeling.",
    "temporal_coverage": "2025"
  },
  {
    "name": "IJCAI Competitions",
    "description": "International AI conference with competitions",
    "category": "Data Portals",
    "url": "https://www.ijcai.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "IJCAI",
      "AI",
      "competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "AI",
      "competitions"
    ],
    "summary": "The IJCAI Competitions dataset is associated with the International Joint Conference on Artificial Intelligence, which hosts various competitions in the field of AI. Researchers and practitioners can utilize this dataset to analyze competition results, benchmark AI algorithms, and explore advancements in AI methodologies.",
    "use_cases": [
      "Analyzing trends in AI competition results",
      "Benchmarking AI algorithms against competition winners",
      "Exploring advancements in AI methodologies through competition data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the IJCAI competitions?",
      "How can I access the IJCAI Competitions dataset?",
      "What AI competitions are held at IJCAI?",
      "What are the results of past IJCAI competitions?",
      "How do IJCAI competitions contribute to AI research?",
      "What methodologies are used in IJCAI competitions?",
      "Where can I find data on IJCAI competition winners?",
      "What is the significance of IJCAI in the AI community?"
    ],
    "domain_tags": [
      "technology",
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/ijcai.png",
    "embedding_text": "The IJCAI Competitions dataset is a valuable resource for researchers and practitioners in the field of artificial intelligence. This dataset is derived from the International Joint Conference on Artificial Intelligence (IJCAI), which is a premier venue for AI research and innovation. The competitions held at IJCAI provide a platform for testing and benchmarking various AI algorithms and methodologies. The data structure typically consists of rows representing individual competition entries or submissions, while columns may include variables such as participant identifiers, competition categories, scores, and rankings. The collection methodology involves gathering results from the competitions, which are often publicly available after the events conclude. This dataset covers a range of AI challenges, allowing for diverse analyses and insights into the performance of different AI approaches. Key variables in the dataset measure aspects such as algorithm performance, submission times, and participant demographics, providing a comprehensive view of the competitive landscape in AI. However, researchers should be aware of potential limitations in data quality, including incomplete submissions or variations in competition rules across years. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing scores for comparison, and categorizing submissions based on competition types. The dataset can address various research questions, such as identifying trends in AI performance over time, understanding the impact of different methodologies on competition outcomes, and exploring the relationship between participant experience and success rates. Types of analyses supported by this dataset include regression analysis to predict outcomes based on past performance, machine learning techniques to identify patterns in submissions, and descriptive analyses to summarize competition results. Researchers typically use this dataset to benchmark their own AI systems against established competitors, analyze the evolution of AI techniques, and contribute to the broader discourse on AI advancements."
  },
  {
    "name": "OTTO Session Data",
    "description": "12M German e-commerce sessions with click \u2192 cart \u2192 order sequences. RecSys 2022 competition",
    "category": "E-Commerce",
    "url": "https://github.com/otto-de/recsys-dataset",
    "docs_url": null,
    "github_url": "https://github.com/otto-de/recsys-dataset",
    "tags": [
      "sessions",
      "recommendations",
      "Germany",
      "RecSys"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendation-systems"
    ],
    "summary": "The OTTO Session Data consists of 12 million e-commerce sessions from Germany, capturing the click, cart, and order sequences of users. This dataset is particularly useful for developing and testing recommendation systems, allowing researchers to analyze consumer behavior and improve e-commerce strategies.",
    "use_cases": [
      "Analyzing user behavior patterns in e-commerce.",
      "Developing recommendation algorithms based on session data.",
      "Evaluating the effectiveness of marketing strategies.",
      "Studying the impact of user interactions on purchase decisions."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the OTTO Session Data?",
      "Where can I find e-commerce session data for Germany?",
      "How can I analyze click to cart sequences in e-commerce?",
      "What datasets are available for recommendation systems?",
      "Where can I access data for the RecSys 2022 competition?",
      "What are the characteristics of the OTTO Session Data?",
      "How to use session data for consumer behavior analysis?",
      "What are the key variables in the OTTO Session Data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/otto-session-data.png",
    "embedding_text": "The OTTO Session Data is a comprehensive dataset comprising 12 million e-commerce sessions, specifically designed for the RecSys 2022 competition. It captures detailed sequences of user interactions, including clicks, items added to the cart, and completed orders. This dataset is structured in a tabular format, with rows representing individual sessions and columns detailing various attributes such as session ID, user ID, timestamps, product IDs, and action types (click, add to cart, purchase). The data collection methodology involves tracking user interactions on the OTTO e-commerce platform, ensuring a rich source of behavioral data that reflects real-world shopping experiences in Germany. The temporal coverage of the dataset is not explicitly mentioned, but it encompasses a substantial volume of interactions, making it suitable for time-series analyses and trend evaluations in e-commerce. The geographic scope is limited to Germany, providing insights into consumer behavior specific to this market. Key variables in the dataset include session ID, which uniquely identifies each session; user ID, indicating the individual user; product ID, representing the items interacted with; and action type, which categorizes the nature of the interaction (e.g., click, cart, order). These variables enable researchers to measure user engagement, conversion rates, and the effectiveness of various marketing strategies. Data quality is generally high, given that it originates from a reputable e-commerce platform; however, known limitations may include potential biases in user behavior, such as seasonal variations in shopping patterns or the influence of promotional events. Common preprocessing steps involve cleaning the data to handle missing values, normalizing timestamps for analysis, and encoding categorical variables for machine learning applications. Researchers can utilize this dataset to address various research questions, such as identifying factors that influence conversion rates, understanding the dynamics of user engagement, and developing predictive models for recommendation systems. The dataset supports a range of analyses, including regression analysis to explore relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize user behavior. Typically, researchers leverage the OTTO Session Data to enhance recommendation algorithms, evaluate user experience improvements, and conduct market research to inform business decisions in the e-commerce sector."
  },
  {
    "name": "ORBITAAL Bitcoin Graph",
    "description": "13 years (2009-2021) of entity-level Bitcoin transaction networks with BTC/USD values",
    "category": "Financial Services",
    "url": "https://www.nature.com/articles/s41597-023-02416-6",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Bitcoin",
      "crypto",
      "graph",
      "transactions"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "graph-theory",
      "network-analysis"
    ],
    "topic_tags": [
      "financial-services",
      "cryptocurrency",
      "data-analysis"
    ],
    "summary": "The ORBITAAL Bitcoin Graph dataset provides a comprehensive view of Bitcoin transaction networks over a 13-year period, from 2009 to 2021. Researchers can utilize this dataset to analyze transaction patterns, study the dynamics of Bitcoin's value against USD, and explore the relationships between different entities in the Bitcoin ecosystem.",
    "use_cases": [
      "Analyzing transaction networks to identify key players in Bitcoin",
      "Studying the impact of market events on Bitcoin transaction behavior",
      "Exploring the relationships between different entities in the Bitcoin ecosystem",
      "Conducting regression analysis to predict Bitcoin value trends"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the transaction patterns in the ORBITAAL Bitcoin Graph?",
      "How does the BTC/USD value fluctuate over time?",
      "What insights can be drawn from entity-level Bitcoin transactions?",
      "How can network analysis be applied to Bitcoin transactions?",
      "What are the key entities in the Bitcoin ecosystem?",
      "How do transaction networks evolve in the cryptocurrency space?",
      "What are the implications of Bitcoin transaction data for financial services?",
      "How can this dataset be used to study cryptocurrency trends?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "graph",
    "temporal_coverage": "2009-2021",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/nature.png",
    "embedding_text": "The ORBITAAL Bitcoin Graph dataset is a rich resource for researchers and analysts interested in the dynamics of Bitcoin transactions over a significant temporal span, specifically from 2009 to 2021. This dataset comprises entity-level transaction networks, which means it captures detailed interactions between various entities involved in Bitcoin transactions. The data structure typically includes rows representing individual transactions and columns that detail key variables such as transaction ID, timestamp, sender and receiver addresses, transaction amounts, and the corresponding BTC/USD values at the time of each transaction. This comprehensive schema allows for in-depth analysis of transaction flows and the relationships between different entities within the Bitcoin network.\n\nThe collection methodology for this dataset involves aggregating transaction data from the Bitcoin blockchain, which is publicly accessible. This ensures that the data is both extensive and representative of actual transaction behaviors in the cryptocurrency market. However, researchers should be aware of potential limitations in data quality, such as the presence of noise due to fraudulent transactions or the challenges in accurately identifying entities behind Bitcoin addresses. Preprocessing steps may include cleaning the data to remove outliers, normalizing transaction values, and transforming the data into a suitable format for network analysis.\n\nKey variables within the dataset measure various aspects of Bitcoin transactions, including transaction volume, frequency of transactions between entities, and the temporal dynamics of Bitcoin's value against USD. These variables enable researchers to explore a wide range of research questions, such as how transaction patterns change in response to market trends, the identification of influential entities within the Bitcoin ecosystem, and the overall impact of external factors on Bitcoin's market behavior.\n\nThe dataset supports various types of analyses, including regression modeling to predict future Bitcoin values based on historical transaction data, machine learning techniques to classify transaction types or identify anomalies, and descriptive statistics to summarize transaction behaviors. Researchers typically use this dataset to conduct studies that aim to understand the underlying mechanisms of cryptocurrency markets, assess the impact of regulatory changes on transaction patterns, and evaluate the effectiveness of different trading strategies. Overall, the ORBITAAL Bitcoin Graph dataset serves as a valuable tool for advancing knowledge in the field of financial services, particularly in the context of cryptocurrency and blockchain technology."
  },
  {
    "name": "Adform Display",
    "description": "Display advertising dataset with impressions and clicks",
    "category": "Advertising",
    "url": "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TADBY7",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "display ads",
      "impressions",
      "Harvard Dataverse"
    ],
    "best_for": "Learning advertising analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "advertising",
      "digital marketing"
    ],
    "summary": "The Adform Display dataset contains information on display advertising, specifically focusing on impressions and clicks. This dataset can be utilized to analyze the effectiveness of display ads, understand consumer engagement, and optimize advertising strategies.",
    "use_cases": [
      "Analyzing the performance of display ads",
      "Understanding consumer interaction with ads",
      "Optimizing advertising strategies based on click-through rates"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Adform Display dataset?",
      "How can I analyze display ad impressions and clicks?",
      "What insights can be gained from display advertising data?",
      "Where can I find datasets on display advertising?",
      "How to optimize display ads using data?",
      "What are the key metrics in display advertising?"
    ],
    "domain_tags": [
      "advertising"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Adform Display dataset is a comprehensive collection of data related to display advertising, specifically focusing on key metrics such as impressions and clicks. This dataset is structured in a tabular format, typically consisting of rows representing individual ad impressions and columns detailing various attributes such as timestamp, ad ID, user demographics, and engagement metrics. The collection methodology for this dataset involves aggregating data from digital advertising platforms, where impressions are recorded each time an ad is displayed to a user, and clicks are logged when users interact with these ads. This dataset is particularly valuable for researchers and analysts interested in the digital marketing landscape, as it provides insights into consumer behavior and advertising effectiveness. Key variables within the dataset include the number of impressions, which measures how often an ad is displayed, and the number of clicks, which indicates user engagement. These variables are crucial for calculating metrics such as click-through rates (CTR), which help assess the performance of advertising campaigns. However, users should be aware of potential limitations in data quality, such as incomplete records or discrepancies in user tracking, which can affect the reliability of analyses. Common preprocessing steps may include data cleaning to remove duplicates, handling missing values, and normalizing metrics for comparative analysis. Researchers can leverage this dataset to address various research questions, such as the impact of ad placement on user engagement, the effectiveness of different ad formats, and trends in consumer interaction over time. The dataset supports a range of analytical approaches, including regression analysis to identify factors influencing ad performance, machine learning techniques for predictive modeling, and descriptive statistics to summarize user engagement patterns. In studies, researchers typically utilize this dataset to inform advertising strategies, optimize campaign performance, and enhance understanding of consumer behavior in the digital advertising space."
  },
  {
    "name": "Tmall Reviews",
    "description": "Product reviews from Tmall (Alibaba's B2C platform)",
    "category": "E-Commerce",
    "url": "https://tianchi.aliyun.com/dataset/140281",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "Tmall",
      "China",
      "B2C"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Tmall Reviews dataset consists of product reviews collected from Tmall, Alibaba's B2C platform in China. This dataset can be utilized to analyze consumer sentiment, identify trends in product preferences, and evaluate the impact of reviews on purchasing decisions.",
    "use_cases": [
      "Sentiment analysis of product reviews",
      "Trend analysis in consumer preferences",
      "Impact assessment of reviews on sales",
      "Comparative analysis of product categories based on reviews"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the most common sentiments expressed in Tmall reviews?",
      "How do product ratings correlate with review length on Tmall?",
      "What trends can be observed in consumer preferences on Tmall over time?",
      "How do Tmall reviews influence purchasing decisions?",
      "What are the key themes in customer feedback for Tmall products?",
      "How does the sentiment of Tmall reviews vary across different product categories?",
      "What demographic factors can be inferred from Tmall reviews?",
      "How can Tmall reviews be used to improve product offerings?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "text",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Tmall Reviews dataset is a rich collection of product reviews sourced from Tmall, which is Alibaba's prominent B2C e-commerce platform in China. This dataset is structured in a tabular format, where each row represents an individual review, and the columns typically include variables such as review ID, product ID, user ID, review text, rating, timestamp, and possibly additional metadata related to the review or the reviewer. The primary data collection methodology involves scraping publicly available reviews from the Tmall website, ensuring that the dataset reflects a wide range of consumer opinions and experiences with various products available on the platform. Given the nature of the data, it is expected to cover a diverse array of product categories, reflecting the extensive offerings on Tmall, although specific demographic details of the reviewers may not be explicitly captured in the dataset. Key variables in this dataset include the review text, which provides qualitative insights into consumer sentiment, and the rating, which quantifies the reviewer's satisfaction with the product. These variables can be analyzed to measure aspects such as overall customer satisfaction, common themes in feedback, and correlations between review characteristics and product performance. However, researchers should be aware of potential limitations in data quality, including biases in user-generated content, variations in review authenticity, and the temporal relevance of the reviews, as they may not always reflect current consumer sentiment. Common preprocessing steps for this dataset may involve cleaning the review text to remove noise, normalizing ratings, and possibly performing sentiment analysis to categorize reviews into positive, negative, or neutral sentiments. Researchers can address various research questions using this dataset, such as understanding the factors that influence consumer purchasing decisions, identifying trends in product preferences over time, and evaluating the effectiveness of marketing strategies based on consumer feedback. The dataset supports a range of analytical approaches, including regression analysis to explore relationships between review characteristics and sales performance, machine learning techniques for sentiment classification, and descriptive statistics to summarize consumer behavior patterns. Overall, the Tmall Reviews dataset serves as a valuable resource for researchers and practitioners interested in e-commerce, consumer behavior, and market analysis, providing insights that can inform product development, marketing strategies, and customer engagement initiatives."
  },
  {
    "name": "Airline Delay",
    "description": "Airline flight delays and carrier information",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/sriharshaeedala/airline-delay",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "airlines",
      "delays",
      "transportation"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Airline Delay dataset provides comprehensive information on airline flight delays along with carrier details. Researchers and analysts can utilize this dataset to explore patterns in flight delays, assess the performance of different airlines, and identify factors contributing to delays.",
    "use_cases": [
      "Analyzing the impact of weather on flight delays",
      "Comparing on-time performance across different airlines",
      "Identifying peak times for flight delays",
      "Evaluating the effectiveness of airline policies on reducing delays"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average delays for different airlines?",
      "How do weather conditions affect flight delays?",
      "What time of year sees the most flight delays?",
      "Which airlines have the best on-time performance?",
      "How do delays vary by airport?",
      "What are the trends in airline delays over the years?"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/airline-delay.jpg",
    "embedding_text": "The Airline Delay dataset is structured in a tabular format, consisting of rows representing individual flights and columns capturing various attributes related to those flights. Key variables typically include flight identification numbers, departure and arrival times, delay durations, carrier information, and possibly additional contextual variables such as weather conditions or airport details. The data is likely collected from airline operational records, government aviation databases, or flight tracking services, ensuring a robust source of information for analysis. Coverage may span multiple years, although specific temporal details are not provided. Geographic scope is also unspecified, but it may encompass a range of airports and airlines operating within a particular region or globally.\n\nKey variables in the dataset measure critical aspects of flight performance, such as the length of delays, which can be categorized into departure delays, arrival delays, and cancellations. Understanding these variables is essential for researchers aiming to analyze trends in airline performance or to assess the impact of external factors on flight punctuality. Data quality is a crucial consideration; while operational data is generally reliable, it may contain inaccuracies due to reporting discrepancies or data entry errors. Common preprocessing steps may include handling missing values, normalizing time formats, and categorizing delays into meaningful bins for analysis.\n\nThis dataset supports a variety of research questions, such as examining the relationship between flight delays and factors like time of day, day of the week, or seasonal variations. Analysts can employ various analytical techniques, including regression analysis to identify predictors of delays, machine learning models to classify flights based on their likelihood of delay, or descriptive statistics to summarize overall performance metrics. Researchers typically use this dataset to inform studies on airline efficiency, customer satisfaction, and operational improvements, making it a valuable resource for both academic and industry-focused investigations. By leveraging the insights gained from this dataset, stakeholders can make data-driven decisions aimed at enhancing the overall travel experience for passengers."
  },
  {
    "name": "Flipkart Products",
    "description": "Product information scraped from Flipkart e-commerce platform",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/PromptCloudHQ/flipkart-products",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "products",
      "India",
      "e-commerce"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Flipkart Products dataset contains product information scraped from the Flipkart e-commerce platform, primarily focusing on the Indian market. This dataset can be utilized for various analyses related to consumer behavior, pricing strategies, and market trends in the e-commerce sector.",
    "use_cases": [
      "Analyzing pricing trends of products over time",
      "Studying consumer preferences based on product ratings",
      "Comparing product availability across different categories",
      "Identifying market trends in the Indian e-commerce sector"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the available products on Flipkart?",
      "How do prices vary across different categories on Flipkart?",
      "What consumer behavior trends can be observed from Flipkart product data?",
      "What are the most popular products in India on Flipkart?",
      "How does product availability change over time on Flipkart?",
      "What are the average ratings for products in specific categories on Flipkart?",
      "What pricing strategies are evident in the Flipkart product listings?",
      "How does the product assortment on Flipkart compare to other e-commerce platforms?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "India",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/flipkart-products.png",
    "embedding_text": "The Flipkart Products dataset is a comprehensive collection of product information sourced from the Flipkart e-commerce platform, which is one of the largest online retailers in India. The dataset is structured in a tabular format, consisting of various rows and columns that represent different products and their attributes. Key variables typically included in this dataset are product names, categories, prices, ratings, and availability status. Each row corresponds to a unique product, while the columns provide detailed information about the product's characteristics. The collection methodology involves web scraping techniques that extract data from the Flipkart website, ensuring that the dataset reflects the current offerings available to consumers. However, it is important to note that web scraping can lead to data quality issues, such as missing values or inaccuracies due to changes in the website's structure. Researchers using this dataset may need to perform common preprocessing steps, including data cleaning, normalization, and handling of missing values, to prepare the data for analysis. The dataset supports various types of analyses, including regression analysis to explore pricing trends, machine learning models to predict consumer preferences, and descriptive statistics to summarize product characteristics. Researchers can leverage this dataset to address a range of research questions, such as understanding consumer behavior patterns, evaluating the effectiveness of pricing strategies, and analyzing market dynamics within the Indian e-commerce landscape. Overall, the Flipkart Products dataset serves as a valuable resource for data scientists and researchers interested in the retail sector, providing insights that can inform business strategies and enhance understanding of consumer interactions in the digital marketplace."
  },
  {
    "name": "OTTO Session-based Recommendations",
    "description": "12M+ e-commerce sessions with click \u2192 cart \u2192 order sequences. Real multi-stage conversion funnel data from German retailer",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/otto/recsys-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "sessions",
      "conversion funnel",
      "Kaggle",
      "recommendations",
      "e-commerce"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The OTTO Session-based Recommendations dataset contains over 12 million e-commerce sessions, detailing the click, cart, and order sequences of users. This dataset provides valuable insights into the multi-stage conversion funnel, allowing researchers and practitioners to analyze consumer behavior and improve recommendation systems.",
    "use_cases": [
      "Analyzing user click patterns to optimize product recommendations.",
      "Studying the conversion funnel to identify drop-off points in the purchasing process.",
      "Developing machine learning models to predict future purchases based on session data.",
      "Evaluating the effectiveness of marketing strategies on user behavior."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the click-to-cart conversion rates in the OTTO dataset?",
      "How can I analyze user behavior with the OTTO Session-based Recommendations dataset?",
      "What insights can be derived from the e-commerce session data provided by OTTO?",
      "How does the OTTO dataset help in building recommendation systems?",
      "What are the common patterns in the conversion funnel from the OTTO dataset?",
      "How can I visualize the order sequences in the OTTO dataset?",
      "What machine learning techniques can be applied to the OTTO Session-based Recommendations dataset?",
      "What preprocessing steps are needed for analyzing the OTTO dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/otto-session-based-recommendations.png",
    "embedding_text": "The OTTO Session-based Recommendations dataset is a comprehensive collection of over 12 million e-commerce sessions from a prominent German retailer, OTTO. This dataset captures the intricate details of user interactions as they navigate through the online shopping experience, specifically focusing on the sequential steps of clicking on products, adding items to the cart, and ultimately placing orders. The data structure is organized in a tabular format, comprising rows that represent individual sessions and columns that detail various attributes such as session ID, user ID, timestamps, product IDs, and actions taken (click, cart, order). Each session encapsulates the entire journey of a user, providing a rich tapestry of interactions that can be analyzed to glean insights into consumer behavior and preferences. The collection methodology involves tracking user interactions on the OTTO platform, ensuring that the dataset reflects real-world shopping behaviors and patterns. This makes it a valuable resource for researchers and practitioners interested in understanding the dynamics of online retail. The dataset's coverage is primarily focused on the German market, allowing for region-specific analyses and insights into local consumer behaviors. Key variables within the dataset include session ID, user ID, product ID, action type (click, cart, order), and timestamps, which collectively measure user engagement and conversion rates throughout the shopping process. While the dataset is extensive, researchers should be aware of potential limitations, such as the lack of demographic information about users, which could restrict analyses aimed at understanding different consumer segments. Common preprocessing steps may include data cleaning to handle missing values, normalization of timestamps, and encoding of categorical variables for machine learning applications. The OTTO dataset supports a variety of research questions, such as identifying factors that influence conversion rates, understanding the impact of product placement on user behavior, and developing predictive models for future purchases. Analysts can employ various types of analyses, including regression analysis to explore relationships between variables, machine learning techniques for classification and prediction, and descriptive statistics to summarize user behavior trends. Researchers typically utilize this dataset to enhance recommendation systems, optimize marketing strategies, and improve overall user experience on e-commerce platforms. By leveraging the insights derived from the OTTO Session-based Recommendations dataset, businesses can make informed decisions that drive sales and enhance customer satisfaction."
  },
  {
    "name": "Inside Airbnb Raw Data",
    "description": "Raw data files from Inside Airbnb project",
    "category": "Data Portals",
    "url": "http://insideairbnb.com/get-the-data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Airbnb",
      "raw data",
      "rentals"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Inside Airbnb Raw Data consists of comprehensive datasets that provide insights into the rental market on Airbnb. Researchers and analysts can utilize this data to explore various aspects of the rental market, including pricing trends, occupancy rates, and consumer behavior in the context of short-term rentals.",
    "use_cases": [
      "Analyzing pricing trends in Airbnb rentals",
      "Studying consumer behavior in short-term rental markets",
      "Evaluating the impact of regulations on Airbnb listings"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Inside Airbnb raw data?",
      "How can I analyze Airbnb rental trends?",
      "What insights can be gained from Airbnb data?",
      "Where can I find raw data on Airbnb rentals?",
      "What variables are included in the Inside Airbnb dataset?",
      "How does Airbnb pricing vary by location?",
      "What are common use cases for Airbnb data analysis?",
      "What data is available from the Inside Airbnb project?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/insideairbnb.png",
    "embedding_text": "The Inside Airbnb Raw Data provides a rich and detailed dataset that captures various dimensions of the short-term rental market facilitated by Airbnb. This dataset is structured in a tabular format, consisting of multiple rows and columns that represent different listings, their attributes, and performance metrics. Each row typically corresponds to a unique Airbnb listing, while the columns include variables such as listing ID, host ID, neighborhood, price, availability, number of reviews, and overall rating. These variables are crucial for understanding the dynamics of the rental market, as they allow researchers to analyze trends and patterns in pricing, occupancy rates, and consumer preferences.\n\nThe data is collected through web scraping techniques from the Airbnb website, ensuring that it reflects real-time information about listings across various cities. This methodology allows for a comprehensive view of the rental landscape, although it may also introduce limitations in data quality, such as missing values or inconsistencies in how hosts report information. Researchers should be aware of these potential issues and may need to perform common preprocessing steps, such as data cleaning, normalization, and handling of missing data, before conducting analyses.\n\nKey variables in the dataset include price, which measures the cost per night for each listing, and availability, which indicates how many days a listing is available for booking. Other important variables include the number of reviews, which can serve as a proxy for popularity and consumer trust, and overall rating, which reflects guest satisfaction. These variables enable researchers to address a variety of research questions, such as how pricing strategies affect occupancy rates or how consumer behavior varies across different neighborhoods.\n\nThe types of analyses supported by this dataset are diverse, ranging from descriptive statistics to more complex regression analyses and machine learning models. Researchers can use the data to perform exploratory data analysis, visualize trends, and build predictive models to forecast rental prices or occupancy rates based on various factors. This versatility makes the Inside Airbnb Raw Data a valuable resource for academics, policymakers, and industry practitioners interested in understanding the implications of short-term rentals on local housing markets and economies.\n\nIn summary, the Inside Airbnb Raw Data serves as a foundational dataset for exploring the multifaceted nature of the short-term rental market. Its rich structure and comprehensive coverage allow for a wide range of analyses, making it an essential tool for anyone looking to gain insights into the evolving landscape of Airbnb rentals."
  },
  {
    "name": "Microsoft Research",
    "description": "Research tools and datasets across multiple domains",
    "category": "Data Portals",
    "url": "https://www.microsoft.com/en-us/research/tools/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Microsoft",
      "research",
      "various domains"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Microsoft Research provides a comprehensive collection of research tools and datasets spanning various domains, enabling users to explore and analyze data effectively. This platform is ideal for researchers and practitioners looking to leverage diverse datasets for their analytical needs.",
    "use_cases": [
      "Exploring datasets for academic research",
      "Utilizing research tools for data analysis",
      "Conducting comparative studies across various domains"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on Microsoft Research?",
      "How can I access research tools from Microsoft?",
      "What domains does Microsoft Research cover?",
      "Are there any datasets for machine learning on Microsoft Research?",
      "What types of research can I conduct using Microsoft Research datasets?",
      "Where can I find Microsoft Research datasets for academic purposes?"
    ],
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/microsoft-research.jpg",
    "embedding_text": "Microsoft Research is a leading platform that aggregates a wide array of research tools and datasets across multiple domains. The datasets available on this platform are structured in various formats, including tabular data, text, and other mixed modalities, catering to the diverse needs of researchers and data scientists. Each dataset typically comprises rows and columns that represent different variables pertinent to the research questions being addressed. The collection methodology employed by Microsoft Research involves gathering data from various credible sources, ensuring that the datasets are both relevant and reliable for academic and practical applications. While specific temporal and geographic coverage details are not explicitly mentioned, the datasets are designed to support a broad range of research inquiries, making them suitable for various analytical tasks. Key variables within these datasets measure critical aspects of the domains they cover, although specific variable names and definitions are not provided in the description. Users should be aware of potential data quality issues and limitations that may arise from the datasets, which can include missing values or inconsistencies that may require preprocessing prior to analysis. Common preprocessing steps might involve cleaning the data, handling missing values, and transforming variables to suit specific analytical needs. Researchers typically utilize Microsoft Research datasets to address a variety of research questions, ranging from exploratory data analysis to more complex modeling tasks such as regression analysis and machine learning. The platform serves as a valuable resource for those looking to conduct studies that require robust datasets and analytical tools, facilitating the advancement of knowledge across various fields of study.",
    "data_modality": "mixed"
  },
  {
    "name": "Hugging Face Datasets",
    "description": "ML/NLP datasets hub with 100K+ datasets. Easy loading via Python library. Community-driven repository",
    "category": "Data Portals",
    "url": "https://huggingface.co/datasets",
    "docs_url": "https://huggingface.co/docs/datasets",
    "github_url": null,
    "tags": [
      "ML",
      "NLP",
      "datasets",
      "Hugging Face",
      "community"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "ML",
      "NLP",
      "datasets"
    ],
    "summary": "The Hugging Face Datasets is a comprehensive hub for machine learning and natural language processing datasets, boasting over 100,000 datasets. It allows users to easily load datasets via a Python library, making it a valuable resource for both researchers and practitioners in the field.",
    "use_cases": [
      "Training machine learning models for NLP tasks",
      "Benchmarking NLP algorithms on diverse datasets",
      "Exploring community-contributed datasets for research",
      "Conducting comparative analysis of different datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the available datasets in Hugging Face Datasets?",
      "How can I load datasets from Hugging Face using Python?",
      "What types of ML/NLP datasets are available on Hugging Face?",
      "Is there a community-driven aspect to Hugging Face Datasets?",
      "What is the size of the dataset collection on Hugging Face?",
      "How can I contribute to Hugging Face Datasets?"
    ],
    "domain_tags": [
      "technology"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/hugging-face-datasets.png",
    "embedding_text": "The Hugging Face Datasets is an extensive repository that serves as a central hub for machine learning (ML) and natural language processing (NLP) datasets, featuring over 100,000 datasets contributed by a vibrant community of researchers and practitioners. This platform is designed to facilitate easy access and loading of datasets through its user-friendly Python library, which significantly streamlines the workflow for data scientists and machine learning engineers. The datasets available cover a wide range of topics and applications, making it an invaluable resource for anyone looking to explore the capabilities of machine learning in the context of natural language processing. The data structure within the Hugging Face Datasets varies depending on the specific dataset, but generally, datasets are organized into rows and columns, where each row represents an individual data point and columns represent different features or variables relevant to that data point. This schema allows for a diverse range of analyses, from simple descriptive statistics to complex machine learning model training. The collection methodology is community-driven, meaning that datasets are often contributed by users who share their own data collections or research outputs, thus ensuring a rich variety of data sources and types. However, this also implies that the quality and reliability of the datasets can vary, and users are encouraged to assess the data quality and potential limitations before use. Common preprocessing steps may include data cleaning, normalization, and transformation to ensure that the datasets are suitable for analysis or model training. Researchers typically use Hugging Face Datasets to address a variety of research questions, such as evaluating the performance of different NLP models, exploring the nuances of language in various contexts, or conducting comparative studies across different datasets. The platform supports various types of analyses, including regression analysis, machine learning model training, and descriptive analytics, providing a flexible environment for both novice and experienced data scientists to engage with cutting-edge research in the field of natural language processing. Overall, Hugging Face Datasets stands out as a key resource for advancing research and practical applications in machine learning and NLP, fostering collaboration and innovation within the community."
  },
  {
    "name": "Hugging Face Datasets",
    "description": "659,000+ datasets across text, image, audio, and tabular with one-line data loaders",
    "category": "Dataset Aggregators",
    "url": "https://huggingface.co/datasets",
    "docs_url": "https://huggingface.co/docs/datasets",
    "github_url": "https://github.com/huggingface/datasets",
    "tags": [
      "NLP",
      "AI",
      "ML",
      "transformers",
      "streaming"
    ],
    "best_for": "Modern AI/NLP research with seamless ML framework integration",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "NLP",
      "AI",
      "ML"
    ],
    "summary": "The Hugging Face Datasets collection provides access to over 659,000 datasets across various modalities including text, image, audio, and tabular formats. It enables users to easily load and utilize these datasets for a wide range of applications in natural language processing, machine learning, and artificial intelligence.",
    "use_cases": [
      "Training machine learning models",
      "Conducting natural language processing tasks",
      "Analyzing image datasets",
      "Exploring audio data for research"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on Hugging Face?",
      "How can I access datasets for NLP tasks?",
      "Where can I find datasets for machine learning?",
      "What types of data does Hugging Face provide?",
      "How to load datasets from Hugging Face?",
      "What are the most popular datasets on Hugging Face?",
      "Can I find audio datasets on Hugging Face?",
      "What are the features of Hugging Face Datasets?"
    ],
    "domain_tags": [
      "AI",
      "NLP"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/hugging-face-datasets.png",
    "embedding_text": "The Hugging Face Datasets collection is a comprehensive repository that hosts over 659,000 datasets, catering to a diverse range of data modalities including text, image, audio, and tabular formats. This extensive collection is designed to facilitate researchers, data scientists, and developers in accessing high-quality datasets for various applications in natural language processing (NLP), machine learning (ML), and artificial intelligence (AI). The datasets are structured in a way that allows for easy loading and manipulation, providing one-line data loaders that streamline the process of integrating these datasets into machine learning workflows. The data structure typically includes rows representing individual data points, with columns corresponding to various features or attributes relevant to the specific dataset. For instance, text datasets may include columns for the text itself, labels, and metadata, while image datasets may include image file paths and associated labels. The collection methodology employed by Hugging Face involves aggregating datasets from a multitude of sources, ensuring a rich diversity in the types of data available. Researchers can find datasets that cover a wide array of topics and domains, making it an invaluable resource for exploratory data analysis and model training. While the collection is vast, users should be aware of potential limitations regarding data quality, as datasets may vary in terms of completeness, accuracy, and relevance. Common preprocessing steps may include cleaning the data, handling missing values, and transforming data into suitable formats for analysis. The datasets support a variety of research questions, ranging from sentiment analysis in text data to image classification tasks. They are suitable for various types of analyses, including regression, machine learning, and descriptive statistics. Researchers typically leverage the Hugging Face Datasets collection in their studies to enhance their models' performance, validate hypotheses, and explore new avenues of inquiry in the rapidly evolving fields of AI and ML."
  },
  {
    "name": "Alibaba Personalized Re-Ranking",
    "description": "Mobile shopping user click data on recommended items",
    "category": "E-Commerce",
    "url": "http://yongfeng.me/dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "re-ranking",
      "personalization",
      "recommendations"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "recommendations"
    ],
    "summary": "The Alibaba Personalized Re-Ranking dataset contains mobile shopping user click data on recommended items, providing insights into user preferences and behavior. Researchers can utilize this dataset to analyze the effectiveness of recommendation algorithms and improve personalization strategies in e-commerce.",
    "use_cases": [
      "Analyzing user engagement with recommended items",
      "Improving recommendation algorithms",
      "Studying the impact of personalization on sales",
      "Evaluating user behavior patterns in mobile shopping"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Alibaba Personalized Re-Ranking dataset?",
      "How can I analyze mobile shopping user click data?",
      "What insights can be derived from e-commerce recommendation systems?",
      "How does personalization affect user engagement?",
      "What variables are included in the Alibaba dataset?",
      "How can I use this dataset for machine learning?",
      "What are common preprocessing steps for click data analysis?",
      "What research questions can be explored with this dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Alibaba Personalized Re-Ranking dataset is a valuable resource for researchers and practitioners in the field of e-commerce, particularly those focused on recommendation systems and user behavior analysis. This dataset comprises mobile shopping user click data on recommended items, structured in a tabular format. The data typically includes various columns representing user interactions, such as user IDs, item IDs, timestamps, and click-through rates, among others. Each row corresponds to a unique interaction between a user and a recommended item, allowing for detailed analysis of user preferences and engagement patterns.\n\nThe collection methodology for this dataset involves tracking user interactions on the Alibaba mobile shopping platform, capturing data as users engage with personalized recommendations. This data is sourced from real-time user activity, providing a rich context for understanding how users respond to different recommendation strategies. The dataset is particularly useful for analyzing the effectiveness of various algorithms in re-ranking items based on user preferences, thereby enhancing the personalization of the shopping experience.\n\nWhile the dataset does not explicitly mention temporal or geographic coverage, it is inferred that the data reflects user interactions over a specific period and is likely representative of a diverse demographic, given Alibaba's extensive user base. Key variables in the dataset measure important aspects of user behavior, such as the frequency of clicks on recommended items, the timing of these interactions, and the overall engagement levels with the platform. Understanding these variables is crucial for developing effective recommendation systems that cater to user preferences.\n\nData quality is an essential consideration, as the dataset may contain noise due to user behavior variability, such as accidental clicks or non-representative sampling of user interactions. Researchers should be aware of these limitations and consider common preprocessing steps, such as filtering out irrelevant interactions, normalizing click rates, and handling missing data, to ensure robust analysis.\n\nThe Alibaba Personalized Re-Ranking dataset supports a variety of research questions and analytical approaches. Researchers can explore questions related to the impact of personalized recommendations on user engagement, the effectiveness of different ranking algorithms, and the overall influence of personalization on sales and conversion rates. The dataset is suitable for various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, enabling researchers to derive actionable insights from user behavior data.\n\nIn studies, researchers typically use this dataset to evaluate the performance of recommendation algorithms, identify trends in user engagement, and develop strategies for enhancing the personalization of e-commerce experiences. By leveraging the insights gained from this dataset, businesses can refine their recommendation systems, ultimately leading to improved user satisfaction and increased sales."
  },
  {
    "name": "German Used Cars",
    "description": "Used car listings or sales in Germany",
    "category": "Automotive",
    "url": "https://www.kaggle.com/datasets/gogotchuri/myautogecardetails",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "used cars",
      "Germany",
      "listings"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "automotive",
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The German Used Cars dataset consists of listings or sales data for used cars in Germany. This dataset can be utilized to analyze market trends, consumer preferences, and pricing strategies within the automotive sector.",
    "use_cases": [
      "Analyzing price trends of used cars over time",
      "Examining consumer preferences for different car brands",
      "Evaluating the impact of car age on pricing",
      "Identifying regional differences in used car sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the current used car listings in Germany?",
      "How do prices of used cars vary by make and model?",
      "What are the most popular used car brands in Germany?",
      "How does the age of a car affect its resale value?",
      "What regions in Germany have the highest used car sales?",
      "What features are most commonly sought after in used cars?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Germany",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/german-used-cars.jpg",
    "embedding_text": "The German Used Cars dataset provides a comprehensive collection of used car listings or sales data specifically from Germany, making it an invaluable resource for researchers and analysts interested in the automotive market. The dataset is structured in a tabular format, with rows representing individual car listings and columns detailing various attributes of each vehicle. Key variables typically include car make, model, year of manufacture, mileage, price, and location of sale. These variables are crucial for understanding market dynamics and consumer behavior in the used car sector.\n\nThe collection methodology for this dataset often involves scraping data from online marketplaces, dealerships, and classified ads, ensuring a wide range of listings are captured. This approach allows for a diverse representation of the used car market, encompassing different brands, models, and price ranges. However, users should be aware of potential limitations in data quality, such as inconsistencies in listing formats, missing values, or outdated information, which may require preprocessing steps like data cleaning and normalization.\n\nResearchers can utilize this dataset to address various research questions, such as how the age of a vehicle impacts its resale value, or what features are most sought after by consumers in the used car market. The dataset supports a variety of analyses, including regression analysis to predict pricing trends, machine learning models to classify consumer preferences, and descriptive statistics to summarize the characteristics of the used car market.\n\nIn studies, analysts typically leverage this dataset to conduct market trend analyses, evaluate pricing strategies, and understand consumer behavior. By examining the relationships between key variables, researchers can gain insights into factors that influence car sales and pricing, ultimately aiding stakeholders in making informed decisions in the automotive industry. Overall, the German Used Cars dataset serves as a foundational tool for anyone looking to explore the intricacies of the used car market in Germany."
  },
  {
    "name": "Gamified Learning",
    "description": "Experiments on gamification in learning environments",
    "category": "Education",
    "url": "https://data.mendeley.com/datasets/7kgpn39m8w/1",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "gamification",
      "education",
      "experiments"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "education",
      "gamification",
      "learning"
    ],
    "summary": "The Gamified Learning dataset consists of experiments conducted to analyze the effects of gamification in educational settings. Researchers can use this dataset to explore how gamified elements impact learning outcomes and student engagement.",
    "use_cases": [
      "Analyzing the impact of gamification on student performance",
      "Evaluating engagement levels in gamified learning environments",
      "Comparing traditional learning methods with gamified approaches"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the effects of gamification on learning?",
      "How can gamified learning environments improve student engagement?",
      "What experiments have been conducted on gamification in education?",
      "What data is available on gamification in learning?",
      "How does gamification influence educational outcomes?",
      "What methodologies are used in gamified learning experiments?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Gamified Learning dataset is a collection of experiments designed to investigate the role of gamification in educational contexts. This dataset is structured in a mixed format, encompassing both qualitative and quantitative data derived from various experiments. The schema typically includes rows representing individual experiments or participants, with columns detailing variables such as engagement metrics, learning outcomes, and demographic information of participants. The key variables in this dataset measure aspects like student motivation, retention rates, and overall performance in gamified versus traditional learning environments. The collection methodology involves controlled experiments conducted in classroom settings, where different gamified elements are introduced and their effects on learning are observed. Data sources may include surveys, assessments, and direct observations during the learning process. While the dataset provides valuable insights into the effectiveness of gamification, it is essential to note potential limitations such as sample size, participant diversity, and the specific contexts in which the experiments were conducted. Common preprocessing steps might include cleaning the data for missing values, normalizing engagement scores, and categorizing qualitative feedback for analysis. Researchers can utilize this dataset to address various questions, such as how gamification influences student motivation and whether it leads to improved academic performance. The types of analyses supported by this dataset range from descriptive statistics to regression analyses and machine learning models aimed at predicting outcomes based on gamified interventions. Typically, researchers employ this dataset to compare the efficacy of gamified learning strategies against traditional educational methods, thereby contributing to the broader field of educational research and pedagogical strategies."
  },
  {
    "name": "KDD Cup",
    "description": "ACM SIGKDD annual data mining competition",
    "category": "Data Portals",
    "url": "https://kdd.org/kdd-cup",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "KDD",
      "data mining",
      "ACM"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The KDD Cup is an annual data mining competition organized by ACM SIGKDD, providing datasets for participants to develop and test their data mining skills. It offers a platform for researchers and practitioners to explore innovative solutions to real-world problems using the provided datasets.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the KDD Cup dataset?",
      "How can I participate in the KDD Cup competition?",
      "What type of data is available in the KDD Cup?",
      "What are the challenges in the KDD Cup?",
      "Where can I find KDD Cup datasets?",
      "What skills are needed for KDD Cup participants?",
      "What research questions can be addressed using KDD Cup data?",
      "How is the KDD Cup dataset structured?"
    ],
    "domain_tags": [
      "data mining"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/kdd.png",
    "embedding_text": "The KDD Cup, organized annually by ACM SIGKDD, serves as a premier competition in the field of data mining, attracting participants from various backgrounds including academia and industry. The datasets provided in the KDD Cup are typically structured in a tabular format, consisting of rows and columns that represent different observations and variables, respectively. Each dataset may vary in its schema, but common elements include features that capture various aspects of the data, such as categorical variables, numerical values, and sometimes text fields. The collection methodology for these datasets often involves real-world data gathering processes, which may include scraping, surveys, or collaborations with organizations that provide access to their data. As a result, the datasets can cover a wide range of topics and domains, reflecting current trends and challenges in data science. However, it is important to note that the quality of the data may vary, and researchers should be aware of potential limitations such as missing values, biases in data collection, or inconsistencies in data formatting. Common preprocessing steps that participants might undertake include data cleaning, normalization, and feature engineering to prepare the data for analysis. The KDD Cup datasets are designed to address a variety of research questions, allowing participants to explore predictive modeling, classification tasks, clustering, and other data mining techniques. Researchers typically utilize these datasets to develop and test machine learning algorithms, conduct exploratory data analysis, and validate their findings against benchmarks established in previous competitions. The KDD Cup not only fosters innovation in data mining but also serves as a valuable resource for the community, enabling knowledge sharing and collaboration among data scientists."
  },
  {
    "name": "RecSys Challenge 2024 (EB-NeRD)",
    "description": "2.3M users, 380M+ news impressions from Ekstra Bladet for news recommendation research",
    "category": "Data Portals",
    "url": "https://www.recsyschallenge.com/2024/",
    "docs_url": null,
    "github_url": "https://github.com/recsyspolimi/recsys-challenge-2024-ekstrabladet",
    "tags": [
      "RecSys",
      "news",
      "large-scale",
      "real-world",
      "2024",
      "impressions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "news recommendation",
      "user behavior",
      "machine learning"
    ],
    "summary": "The RecSys Challenge 2024 dataset, known as EB-NeRD, comprises 2.3 million users and over 380 million news impressions from Ekstra Bladet, aimed at advancing research in news recommendation systems. Researchers can utilize this dataset to explore user preferences, improve recommendation algorithms, and analyze large-scale user interaction with news content.",
    "use_cases": [
      "Analyzing user engagement with news articles",
      "Improving recommendation algorithms for personalized news feeds",
      "Studying trends in news consumption over time",
      "Evaluating the effectiveness of different recommendation strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the RecSys Challenge 2024 dataset?",
      "How can I access the EB-NeRD dataset for news recommendation research?",
      "What are the key features of the RecSys Challenge 2024 dataset?",
      "What types of analyses can be performed with the EB-NeRD dataset?",
      "How many users are included in the RecSys Challenge 2024 dataset?",
      "What is the size of the news impressions in the EB-NeRD dataset?",
      "What research questions can be addressed using the RecSys Challenge 2024 dataset?",
      "What are the common use cases for the EB-NeRD dataset?"
    ],
    "domain_tags": [
      "media",
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/logos/recsyschallenge.png",
    "embedding_text": "The RecSys Challenge 2024 dataset, referred to as EB-NeRD, is a comprehensive collection of user interaction data specifically designed for research in news recommendation systems. This dataset contains a staggering 2.3 million users and over 380 million news impressions sourced from Ekstra Bladet, a prominent news outlet. The primary goal of this dataset is to facilitate advancements in the field of news recommendation by providing researchers with a rich set of data that reflects real-world user behavior and preferences in news consumption. The dataset is structured in a tabular format, where each row represents a unique user interaction with a news article. Key variables in the dataset include user identifiers, article identifiers, timestamps of impressions, and possibly metadata about the articles such as categories or tags. This structure allows for a variety of analyses, including user behavior modeling and the development of machine learning algorithms aimed at enhancing recommendation systems. The collection methodology for the EB-NeRD dataset involves tracking user interactions with news articles on the Ekstra Bladet platform, capturing data on how users engage with content over time. This data can provide insights into user preferences, trends in news consumption, and the effectiveness of different recommendation strategies. However, researchers should be aware of potential limitations in data quality, such as biases in user engagement or the representativeness of the sample. Common preprocessing steps may include data cleaning to handle missing values, normalization of user interactions, and feature engineering to create meaningful variables for analysis. The dataset supports a wide range of research questions, including how user demographics influence news consumption patterns, the impact of article features on user engagement, and the effectiveness of various recommendation algorithms. Researchers typically use this dataset to conduct regression analyses, machine learning experiments, and descriptive studies to better understand user behavior and improve news recommendation systems. Overall, the EB-NeRD dataset is a valuable resource for anyone looking to explore the intersection of technology and media, particularly in the context of personalized news delivery."
  },
  {
    "name": "NGSIM Vehicle Trajectories",
    "description": "Vehicle trajectory data for traffic flow modeling",
    "category": "Transportation & Mobility",
    "url": "https://data.transportation.gov/Automobiles/Next-Generation-Simulation-NGSIM-Vehicle-Trajector/8ect-6jqj/about_data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "trajectories",
      "traffic",
      "simulation"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The NGSIM Vehicle Trajectories dataset contains detailed vehicle trajectory data that can be utilized for traffic flow modeling. Researchers and practitioners can analyze this data to understand traffic patterns, develop simulation models, and improve transportation systems.",
    "use_cases": [
      "Traffic flow analysis",
      "Simulation model development",
      "Traffic pattern recognition"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the NGSIM Vehicle Trajectories dataset?",
      "How can I use vehicle trajectory data for traffic flow modeling?",
      "What are the key variables in the NGSIM Vehicle Trajectories dataset?",
      "Where can I find vehicle trajectory data for transportation research?",
      "What analyses can be performed with the NGSIM Vehicle Trajectories dataset?",
      "What are the limitations of the NGSIM Vehicle Trajectories dataset?"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/transportation.png",
    "embedding_text": "The NGSIM Vehicle Trajectories dataset is a comprehensive collection of vehicle trajectory data specifically designed for traffic flow modeling and analysis. This dataset provides researchers and practitioners with a rich source of information on vehicle movements, enabling them to explore various aspects of traffic dynamics. The data structure typically consists of rows representing individual vehicle trajectories, with columns capturing key variables such as time, position, speed, and acceleration. Each trajectory is recorded over a defined period, allowing for detailed analysis of vehicle behavior in different traffic conditions. The collection methodology for this dataset involves advanced data acquisition techniques, often utilizing high-resolution video recordings and sophisticated tracking algorithms to ensure accuracy and reliability. The dataset may cover various traffic scenarios, although specific temporal and geographic coverage details are not explicitly mentioned. Key variables in the dataset include vehicle ID, timestamp, position coordinates (x, y), speed, and acceleration, which collectively measure the movement and behavior of vehicles over time. Researchers should be aware of potential data quality issues, such as missing values or inaccuracies in vehicle tracking, which may arise from environmental factors or limitations in the data collection process. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the data for analysis. This dataset supports a wide range of research questions related to traffic flow, including the impact of vehicle interactions, the effectiveness of traffic management strategies, and the development of predictive models for traffic forecasting. Analysts can employ various techniques, including regression analysis, machine learning algorithms, and descriptive statistics, to extract insights from the data. Researchers typically use the NGSIM Vehicle Trajectories dataset in studies aimed at improving transportation systems, enhancing traffic safety, and optimizing traffic flow, making it a valuable resource for those interested in transportation engineering and mobility research."
  },
  {
    "name": "ASSISTments Dataset",
    "description": "Data from online tutoring platform for educational data mining",
    "category": "Education",
    "url": "https://sites.google.com/site/las2016data/home",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "education",
      "tutoring",
      "learning analytics"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ASSISTments Dataset contains data from an online tutoring platform designed for educational data mining. Researchers can use this dataset to analyze student learning behaviors, evaluate tutoring effectiveness, and develop educational interventions.",
    "use_cases": [
      "Analyzing student performance trends",
      "Evaluating the effectiveness of tutoring methods",
      "Developing predictive models for student success",
      "Investigating learning behaviors in online environments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the ASSISTments Dataset?",
      "How can I access the ASSISTments Dataset?",
      "What types of analyses can be performed with the ASSISTments Dataset?",
      "What variables are included in the ASSISTments Dataset?",
      "What research questions can be addressed using the ASSISTments Dataset?",
      "What is the structure of the ASSISTments Dataset?",
      "What are the limitations of the ASSISTments Dataset?",
      "How is the ASSISTments Dataset collected?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The ASSISTments Dataset is a rich source of educational data derived from an online tutoring platform that facilitates the collection of student interactions and learning outcomes. This dataset is structured in a tabular format, comprising multiple rows and columns that represent various variables related to student performance, engagement, and tutoring effectiveness. Each row typically corresponds to a unique student interaction, while the columns capture key attributes such as student ID, time spent on tasks, correctness of answers, and feedback provided by the tutoring system. The collection methodology involves logging data from real-time interactions on the platform, ensuring that the dataset reflects authentic learning experiences. However, researchers should be aware of potential limitations, such as missing data points or variations in student engagement levels, which can affect the overall quality of the dataset. Common preprocessing steps may include handling missing values, normalizing data, and transforming categorical variables into numerical formats for analysis. The ASSISTments Dataset supports a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers can leverage this dataset to explore a multitude of research questions, such as identifying factors that contribute to successful learning outcomes, assessing the impact of different tutoring strategies, and examining the relationship between time spent on tasks and student performance. Overall, the ASSISTments Dataset serves as a valuable resource for educational researchers and data scientists aiming to enhance our understanding of online learning environments and improve educational practices."
  },
  {
    "name": "Amazon Fraud Detection Benchmark",
    "description": "9 consolidated fraud datasets with unified format. Includes IEEE-CIS, credit card, e-commerce fraud. Benchmark for fraud ML research",
    "category": "Financial Services",
    "url": "https://github.com/amazon-science/fraud-dataset-benchmark",
    "docs_url": null,
    "github_url": "https://github.com/amazon-science/fraud-dataset-benchmark",
    "tags": [
      "fraud detection",
      "benchmark",
      "Amazon",
      "ML",
      "fintech"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "fraud detection",
      "machine learning"
    ],
    "summary": "The Amazon Fraud Detection Benchmark is a collection of nine consolidated fraud datasets formatted uniformly to facilitate research in fraud detection using machine learning techniques. It includes datasets from various domains such as IEEE-CIS, credit card transactions, and e-commerce fraud, making it a comprehensive resource for benchmarking and developing fraud detection models.",
    "use_cases": [
      "Developing machine learning models for fraud detection",
      "Benchmarking different fraud detection algorithms",
      "Analyzing patterns in fraudulent transactions",
      "Evaluating the effectiveness of fraud detection techniques"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Amazon Fraud Detection Benchmark?",
      "How can I use the Amazon Fraud Detection datasets for machine learning?",
      "What types of fraud are included in the Amazon Fraud Detection Benchmark?",
      "Where can I find datasets for fraud detection research?",
      "What are the characteristics of the datasets in the Amazon Fraud Detection Benchmark?",
      "How do I benchmark fraud detection algorithms using the Amazon datasets?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Benchmark for fraud ML research"
    ],
    "model_score": 0.0,
    "image_url": "/images/datasets/amazon-fraud-detection-benchmark.png",
    "embedding_text": "The Amazon Fraud Detection Benchmark comprises nine consolidated datasets that are formatted uniformly to support research in fraud detection, particularly in the context of machine learning (ML). These datasets include notable sources such as IEEE-CIS, credit card transaction data, and e-commerce fraud cases, providing a rich resource for researchers and practitioners in the field of financial services and fraud detection. The data structure typically consists of rows representing individual transactions or instances of fraud, with columns containing various features that describe the transactions, such as transaction amount, time, user ID, and other relevant attributes. The datasets are designed to facilitate a variety of analyses, including regression, machine learning, and descriptive statistics. Researchers can leverage these datasets to develop and benchmark fraud detection algorithms, analyze patterns in fraudulent behavior, and evaluate the effectiveness of different detection techniques. Common preprocessing steps may include data cleaning, normalization, and feature engineering to enhance the quality and usability of the data. However, it is important to note that the datasets may have limitations in terms of data quality, such as missing values or potential biases in the data collection process. Overall, the Amazon Fraud Detection Benchmark serves as a vital tool for advancing research in fraud detection and developing robust machine learning models that can effectively identify and mitigate fraudulent activities."
  },
  {
    "name": "Grab Driving GPS Traces",
    "description": "GPS trace data from Grab ride-hailing platform",
    "category": "Transportation & Mobility",
    "url": "https://engineering.grab.com/grab-posisi",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "GPS",
      "ride-hailing",
      "Southeast Asia"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "mobility",
      "data-analysis"
    ],
    "summary": "The Grab Driving GPS Traces dataset contains GPS trace data collected from the Grab ride-hailing platform, primarily in Southeast Asia. This dataset can be used to analyze transportation patterns, assess ride-hailing efficiency, and study urban mobility trends.",
    "use_cases": [
      "Analyzing ride patterns in urban areas",
      "Assessing the impact of ride-hailing on traffic congestion",
      "Studying user behavior in ride-hailing applications",
      "Evaluating the efficiency of transportation services"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Grab Driving GPS Traces dataset?",
      "How can I access GPS trace data from Grab?",
      "What insights can be gained from Grab ride-hailing GPS data?",
      "Where can I find datasets related to ride-hailing services?",
      "What are the applications of GPS data in transportation analysis?",
      "How does Grab's GPS data reflect urban mobility trends?",
      "What variables are included in the Grab Driving GPS Traces dataset?",
      "What research can be conducted using ride-hailing GPS data?"
    ],
    "domain_tags": [
      "transportation",
      "mobility"
    ],
    "data_modality": "time-series",
    "geographic_scope": "Southeast Asia",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/grab-driving-gps-traces.png",
    "embedding_text": "The Grab Driving GPS Traces dataset is a valuable resource for researchers and analysts interested in the dynamics of urban transportation and mobility. This dataset comprises GPS trace data collected from the Grab ride-hailing platform, which operates extensively in Southeast Asia. The data structure typically includes rows representing individual trips, with columns capturing various attributes such as trip start and end times, geographic coordinates (latitude and longitude), trip duration, and possibly other relevant metrics such as fare estimates or passenger counts. The collection methodology involves the aggregation of GPS data from Grab's operational fleet, ensuring a comprehensive representation of ride-hailing activities across different urban environments. Researchers can leverage this dataset to explore a wide range of questions related to transportation efficiency, user behavior, and the overall impact of ride-hailing services on urban mobility. Key variables within the dataset may include trip distance, duration, and geographic patterns of demand, all of which can provide insights into how ride-hailing services are utilized in real-time. However, it is essential to consider potential limitations in data quality, such as inaccuracies in GPS tracking, variations in data collection methods, and the influence of external factors like traffic conditions. Common preprocessing steps might involve cleaning the data to address missing values, filtering out outlier trips, and transforming the data into a suitable format for analysis. The dataset supports various types of analyses, including descriptive statistics to summarize ride patterns, regression analyses to identify factors influencing ride demand, and machine learning models to predict future ride patterns based on historical data. Researchers typically use this dataset to address questions related to urban planning, transportation policy, and the optimization of ride-hailing services, making it a critical tool for understanding the evolving landscape of urban mobility."
  },
  {
    "name": "NYC TLC Trip Records",
    "description": "3B+ taxi and rideshare trips since 2009. Fares, tips, surge pricing, driver pay. The gold standard for marketplace analytics",
    "category": "Transportation & Mobility",
    "url": "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page",
    "docs_url": "https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf",
    "github_url": "https://github.com/toddwschneider/nyc-taxi-data",
    "tags": [
      "taxi",
      "Uber",
      "Lyft",
      "surge pricing",
      "NYC",
      "large-scale"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "mobility",
      "urban studies"
    ],
    "summary": "The NYC TLC Trip Records dataset contains over 3 billion records of taxi and rideshare trips in New York City since 2009. It provides detailed information on fares, tips, surge pricing, and driver pay, making it an invaluable resource for analyzing marketplace dynamics in urban transportation.",
    "use_cases": [
      "Analyzing the impact of surge pricing on rider behavior",
      "Studying trends in taxi fares over time",
      "Examining the relationship between trip distance and driver pay",
      "Investigating the distribution of taxi trips across different NYC neighborhoods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the NYC TLC Trip Records?",
      "How can I analyze taxi fares in NYC?",
      "What data is available on rideshare trips in New York City?",
      "What insights can be gained from NYC taxi trip data?",
      "How does surge pricing affect taxi fares in NYC?",
      "What are the trends in driver pay for NYC taxis?",
      "How many trips were recorded in NYC since 2009?",
      "What variables are included in the NYC TLC Trip Records dataset?"
    ],
    "domain_tags": [
      "transportation",
      "urban studies",
      "data analytics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2009-present",
    "geographic_scope": "New York City",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/nyc-tlc-trip-records.png",
    "embedding_text": "The NYC TLC Trip Records dataset is a comprehensive collection of over 3 billion records detailing taxi and rideshare trips in New York City, spanning from 2009 to the present. This dataset is structured in a tabular format, with each row representing an individual trip and various columns capturing key variables such as trip distance, fare amount, tip amount, surge pricing indicators, and driver pay. The data is collected by the New York City Taxi and Limousine Commission (TLC), which mandates that all taxi and rideshare operators submit trip records, ensuring a robust and extensive dataset that reflects the dynamics of urban transportation. The temporal coverage of this dataset allows researchers to analyze trends over more than a decade, providing insights into how factors such as pricing strategies and consumer behavior have evolved in response to changing market conditions. The geographic scope is limited to New York City, making it particularly relevant for studies focused on urban mobility and transportation economics in metropolitan areas. Key variables in the dataset include fare amounts, which measure the cost of trips; tip amounts, which reflect consumer satisfaction and driver earnings; and surge pricing indicators, which highlight periods of increased demand and pricing strategies employed by rideshare companies. Data quality is generally high, given the regulatory oversight by the TLC, but researchers should be aware of potential limitations such as missing data for certain trips or discrepancies in reporting practices among different operators. Common preprocessing steps may include data cleaning to handle missing values, normalization of fare amounts, and transformation of categorical variables for analysis. This dataset supports a wide range of research questions, including the impact of surge pricing on rider behavior, the correlation between trip distance and fare amounts, and the analysis of driver earnings over time. Researchers typically utilize this dataset for various types of analyses, including regression modeling to predict fare amounts based on trip characteristics, machine learning techniques to identify patterns in rider behavior, and descriptive statistics to summarize trip trends. Overall, the NYC TLC Trip Records dataset serves as a gold standard for marketplace analytics in the transportation sector, offering valuable insights for academics, policymakers, and industry professionals alike.",
    "benchmark_usage": [
      "Marketplace analytics",
      "Urban transportation studies"
    ]
  },
  {
    "name": "IBM Developer Data",
    "description": "AI, data science, healthcare, and weather datasets from IBM",
    "category": "Data Portals",
    "url": "https://developer.ibm.com/technologies/artificial-intelligence/data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "IBM",
      "AI",
      "healthcare",
      "weather"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "AI",
      "data science",
      "healthcare",
      "weather"
    ],
    "summary": "The IBM Developer Data consists of various datasets related to artificial intelligence, data science, healthcare, and weather. Researchers and developers can leverage this data for building AI models, conducting data analyses, and enhancing healthcare solutions.",
    "use_cases": [
      "Building predictive models in healthcare",
      "Analyzing weather patterns using AI",
      "Conducting data science projects with IBM datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets does IBM provide for AI?",
      "How can I access IBM's healthcare datasets?",
      "Are there weather datasets available from IBM?",
      "What data science resources does IBM offer?",
      "Where can I find AI datasets from IBM?",
      "What types of data does IBM Developer Data include?"
    ],
    "domain_tags": [
      "healthcare",
      "weather"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/ibm.png",
    "embedding_text": "The IBM Developer Data is a comprehensive collection of datasets that spans various domains, including artificial intelligence, data science, healthcare, and weather. This dataset is structured in a mixed modality, incorporating both tabular and unstructured data formats, which allows for a wide range of analytical applications. The data schema typically includes multiple rows and columns, with variables that capture essential metrics relevant to each domain. For instance, in the healthcare datasets, key variables may include patient demographics, treatment outcomes, and clinical measurements, while weather datasets may feature variables such as temperature, humidity, and precipitation levels. The collection methodology for these datasets involves rigorous data sourcing from reputable institutions and organizations, ensuring a high level of data quality and reliability. However, users should be aware of potential limitations, such as missing values or biases inherent in the data collection process. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for analysis. Researchers can address a variety of research questions using this data, such as exploring the impact of environmental factors on health outcomes or developing machine learning models to predict weather events. The datasets support various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making them versatile tools for both academic and practical applications. Typically, researchers utilize the IBM Developer Data in studies aimed at advancing knowledge in AI, improving healthcare delivery, and enhancing predictive capabilities in weather forecasting. Overall, this dataset serves as a valuable resource for those looking to harness the power of data in their respective fields."
  },
  {
    "name": "NYC TLC Trip Records",
    "description": "Complete trip-level data for all NYC taxi and for-hire vehicle trips including Uber and Lyft. Billions of records since 2009 with pickups, dropoffs, fares, and tips.",
    "category": "Transportation Economics & Technology",
    "url": "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page",
    "docs_url": "https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records.pdf",
    "github_url": null,
    "tags": [
      "taxi",
      "rideshare",
      "NYC",
      "trip-data",
      "surge-pricing"
    ],
    "best_for": "Ridesharing analysis, surge pricing research, and urban mobility patterns",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "economics",
      "data-analysis"
    ],
    "summary": "The NYC TLC Trip Records dataset contains comprehensive trip-level data for all taxi and for-hire vehicle trips in New York City, including those from services like Uber and Lyft. Researchers and analysts can utilize this dataset to explore patterns in transportation usage, fare structures, and the impact of surge pricing on consumer behavior.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trip-level details for NYC taxi rides?",
      "How does surge pricing affect taxi fares in NYC?",
      "What trends can be observed in NYC taxi trips since 2009?",
      "How many rides were completed by Uber and Lyft in NYC?",
      "What is the average fare for a taxi ride in NYC?",
      "How do pickup and dropoff locations vary across NYC?",
      "What are the common factors influencing taxi trip durations?",
      "How has the introduction of rideshare services changed taxi usage in NYC?"
    ],
    "use_cases": [
      "Analyzing the impact of surge pricing on ride frequency and fare amounts.",
      "Examining the distribution of pickups and dropoffs across different NYC neighborhoods.",
      "Studying the trends in taxi usage over time to identify peak travel periods.",
      "Investigating the relationship between trip distance and fare amounts."
    ],
    "domain_tags": [
      "transportation",
      "technology",
      "economics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2009-present",
    "geographic_scope": "New York City",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/nyc-tlc-trip-records.png",
    "embedding_text": "The NYC TLC Trip Records dataset is a rich repository of trip-level data encompassing all taxi and for-hire vehicle trips within New York City, including those provided by popular rideshare services such as Uber and Lyft. This dataset has been collected since 2009, resulting in billions of records that detail various aspects of each trip, including pickup and dropoff locations, fare amounts, tips, and trip durations. The structure of the dataset is primarily tabular, consisting of rows representing individual trips and columns that capture key variables such as trip ID, pickup and dropoff timestamps, geographic coordinates, fare amounts, and additional charges like tolls and tips. The data is sourced from the NYC Taxi and Limousine Commission (TLC), which maintains rigorous standards for data collection and reporting, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as missing data for certain trips or variations in reporting standards across different service providers. Researchers typically preprocess this dataset to handle missing values, normalize fare amounts, and convert timestamps into a more usable format for analysis. Common analyses performed on this dataset include regression modeling to understand the factors influencing fare amounts, machine learning techniques to predict trip durations, and descriptive statistics to summarize trip patterns over time. The dataset supports a wide range of research questions, such as examining the effects of surge pricing on consumer behavior, analyzing the spatial distribution of rides across the city, and investigating trends in transportation usage over time. Overall, the NYC TLC Trip Records dataset serves as a valuable resource for researchers and analysts interested in transportation economics and technology, providing insights into the dynamics of urban mobility in one of the world's largest cities.",
    "benchmark_usage": [
      "Commonly used for transportation demand analysis",
      "Used in studies regarding urban mobility and economic impact of ride-sharing"
    ]
  },
  {
    "name": "Ele.me Search",
    "description": "Search log dataset from Ele.me (Chinese food delivery)",
    "category": "Food & Delivery",
    "url": "https://tianchi.aliyun.com/dataset/120281",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "food delivery",
      "search logs",
      "China"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "food delivery"
    ],
    "summary": "The Ele.me Search dataset consists of search logs from Ele.me, a prominent food delivery service in China. This dataset can be utilized to analyze consumer behavior, search patterns, and preferences in the food delivery sector.",
    "use_cases": [
      "Analyzing consumer preferences in food delivery",
      "Identifying trends in search behavior over time",
      "Examining the impact of promotions on search queries",
      "Understanding regional differences in food preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the common search terms in the Ele.me Search dataset?",
      "How can I analyze consumer preferences using Ele.me search logs?",
      "What trends can be identified from the Ele.me search logs?",
      "How does search behavior vary across different food categories?",
      "What insights can be drawn from the search patterns in Ele.me?",
      "How can I visualize the search data from Ele.me?",
      "What are the peak search times for food delivery in China?",
      "How can I use the Ele.me Search dataset for market analysis?"
    ],
    "domain_tags": [
      "retail",
      "food delivery"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Ele.me Search dataset is a comprehensive collection of search logs generated by users of Ele.me, a leading food delivery platform in China. This dataset is structured in a tabular format, consisting of rows that represent individual search queries and columns that capture various attributes related to each search. Key variables in this dataset may include search terms, timestamps, user IDs, and possibly location data, which collectively provide insights into consumer behavior and preferences in the food delivery market. The collection methodology for this dataset involves capturing user interactions with the Ele.me platform, specifically focusing on the search functionality, which is a critical aspect of the user experience in food delivery services. While the exact collection process is not detailed, it is likely that the data is aggregated from user activity logs maintained by Ele.me, ensuring a rich source of information for analysis. Coverage of the dataset is primarily geographic, focusing on the Chinese market, which is significant given the rapid growth of the food delivery industry in the region. The dataset does not explicitly mention temporal coverage, but it is expected to reflect recent trends in consumer behavior as it pertains to food delivery. Researchers utilizing the Ele.me Search dataset can address a variety of research questions, such as identifying popular food items, understanding peak search times, and analyzing the effects of marketing campaigns on consumer search behavior. The dataset supports various types of analyses, including descriptive statistics to summarize search trends, regression analysis to explore relationships between search behaviors and external factors, and machine learning techniques to predict future consumer preferences based on historical data. Common preprocessing steps may include cleaning the data to remove duplicates, normalizing search terms for consistency, and potentially enriching the dataset with additional demographic or geographic information. Overall, the Ele.me Search dataset serves as a valuable resource for researchers and analysts interested in the dynamics of the food delivery market in China, providing a foundation for understanding consumer behavior and informing business strategies in this rapidly evolving sector."
  },
  {
    "name": "LOBSTER Order Book",
    "description": "NASDAQ limit order book data at millisecond precision. Level 1-10 depth, message-by-message reconstruction. Market microstructure research",
    "category": "Financial Services",
    "url": "https://lobsterdata.com/",
    "docs_url": "https://lobsterdata.com/info/DataStructure.php",
    "github_url": null,
    "tags": [
      "order book",
      "NASDAQ",
      "high-frequency",
      "market microstructure",
      "trading"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The LOBSTER Order Book dataset provides detailed NASDAQ limit order book data with millisecond precision, allowing for in-depth analysis of market microstructure. Researchers can utilize this dataset to study trading behaviors, order dynamics, and the impact of market events on liquidity.",
    "use_cases": [
      "Analyzing the impact of market events on order book dynamics",
      "Studying high-frequency trading strategies",
      "Investigating liquidity and price formation",
      "Exploring the effects of order types on market behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the LOBSTER Order Book dataset?",
      "How can I analyze NASDAQ limit order book data?",
      "What insights can be gained from high-frequency trading data?",
      "What are the applications of market microstructure research?",
      "How does order book depth affect trading strategies?",
      "What variables are included in the LOBSTER dataset?",
      "How is the LOBSTER Order Book data collected?",
      "What preprocessing steps are needed for analyzing order book data?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The LOBSTER Order Book dataset is a comprehensive collection of NASDAQ limit order book data, capturing market activity at millisecond precision. This dataset is structured in a tabular format, typically comprising rows that represent individual messages or events in the order book, with columns detailing various attributes such as price levels, order sizes, timestamps, and order types. The data is essential for researchers interested in market microstructure, as it allows for the reconstruction of the order book at different depths (Level 1-10), providing insights into the behavior of market participants. The collection methodology involves aggregating real-time data from NASDAQ, ensuring that the dataset reflects actual market conditions and trading activities. While the dataset does not explicitly mention temporal or geographic coverage, it is understood that it pertains to the NASDAQ exchange and captures data relevant to the U.S. financial markets. Key variables in the dataset include bid and ask prices, order sizes at various levels, and timestamps, which collectively measure market liquidity and price dynamics. Researchers should be aware of potential data quality issues, such as missing values or discrepancies in order execution times, which may arise from the high-frequency nature of the data. Common preprocessing steps include filtering out irrelevant messages, normalizing timestamps, and aggregating data to desired time intervals for analysis. The LOBSTER Order Book dataset supports a range of research questions, including the examination of trading strategies, the analysis of order flow, and the investigation of liquidity impacts during market events. It is particularly well-suited for regression analyses, machine learning applications, and descriptive studies that aim to uncover patterns in trading behavior. Researchers typically leverage this dataset to enhance their understanding of market mechanisms, develop predictive models for price movements, and evaluate the effectiveness of different trading strategies in high-frequency environments."
  },
  {
    "name": "Cainiao Last-Mile (MSOM18)",
    "description": "Cainiao Last-Mile Delivery dataset from MSOM 2018",
    "category": "Food & Delivery",
    "url": "https://tianchi.aliyun.com/competition/entrance/231623/information",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "last-mile",
      "delivery",
      "Cainiao",
      "MSOM"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "delivery"
    ],
    "summary": "The Cainiao Last-Mile Delivery dataset from MSOM 2018 provides insights into the logistics of last-mile delivery services. It can be utilized to analyze delivery efficiency, consumer preferences, and operational challenges in the food and delivery sector.",
    "use_cases": [
      "Analyzing delivery times and efficiency",
      "Studying consumer preferences in delivery services",
      "Evaluating operational challenges in last-mile logistics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Cainiao Last-Mile Delivery dataset?",
      "How can I analyze last-mile delivery efficiency?",
      "What insights can be gained from the MSOM 2018 Cainiao dataset?",
      "What variables are included in the Cainiao Last-Mile dataset?",
      "How does last-mile delivery impact consumer behavior?",
      "What are the challenges in last-mile delivery logistics?",
      "How can this dataset be used for regression analysis?",
      "What are the key metrics in last-mile delivery?"
    ],
    "domain_tags": [
      "retail",
      "logistics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/cainiao-last-mile-msom18.png",
    "embedding_text": "The Cainiao Last-Mile Delivery dataset from MSOM 2018 is a comprehensive resource designed to provide insights into the logistics and operational aspects of last-mile delivery services. This dataset is structured in a tabular format, consisting of multiple rows and columns that capture various variables related to delivery operations. Each row typically represents a unique delivery instance, while the columns encompass key variables such as delivery time, distance, consumer preferences, and operational metrics. The dataset is particularly valuable for researchers and analysts interested in the dynamics of e-commerce logistics, consumer behavior, and the efficiency of delivery systems. The collection methodology for this dataset involves gathering data from real-world last-mile delivery operations, likely through partnerships with delivery service providers or direct observation of delivery processes. This approach ensures that the dataset reflects actual delivery scenarios, making it a reliable source for analysis. While the specific temporal and geographic coverage of the dataset is not explicitly mentioned, it is assumed to encompass a range of delivery instances that could provide insights into trends and patterns over time. Key variables in the dataset measure aspects such as delivery speed, customer satisfaction, and logistical challenges faced during the delivery process. Understanding these variables is crucial for addressing research questions related to the effectiveness of last-mile delivery strategies and their impact on consumer satisfaction. However, like any dataset, the Cainiao Last-Mile Delivery dataset may have limitations regarding data quality, including potential biases in the data collection process or missing values. Common preprocessing steps that researchers may need to undertake include cleaning the data to handle missing values, normalizing variables for analysis, and potentially transforming variables to suit specific analytical methods. The dataset supports various types of analyses, including regression analysis to identify factors influencing delivery efficiency, machine learning models to predict delivery outcomes, and descriptive statistics to summarize delivery performance. Researchers typically utilize this dataset to explore questions such as how delivery times affect customer satisfaction, what operational improvements can enhance delivery efficiency, and how consumer preferences shape last-mile delivery strategies. Overall, the Cainiao Last-Mile Delivery dataset serves as a vital tool for understanding the complexities of last-mile logistics in the context of e-commerce, providing a foundation for both academic research and practical applications in the field."
  },
  {
    "name": "MSOM Data Challenges",
    "description": "Manufacturing & Service Operations Management challenges",
    "category": "Data Portals",
    "url": "https://pubsonline.informs.org/journal/msom",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "MSOM",
      "operations",
      "INFORMS"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The MSOM Data Challenges dataset focuses on various challenges in manufacturing and service operations management. It provides a platform for researchers and practitioners to analyze operational efficiencies and service delivery mechanisms within these sectors.",
    "use_cases": [
      "Analyzing operational efficiencies in manufacturing",
      "Evaluating service delivery mechanisms",
      "Identifying best practices in operations management"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are the MSOM Data Challenges?",
      "How can I access the MSOM Data Challenges dataset?",
      "What types of operations management challenges are included in the MSOM dataset?",
      "What insights can be gained from analyzing the MSOM Data Challenges?",
      "Are there specific case studies related to the MSOM Data Challenges?",
      "How does the MSOM dataset contribute to operations research?",
      "What methodologies are used in the MSOM Data Challenges?",
      "What are the key variables in the MSOM Data Challenges dataset?"
    ],
    "domain_tags": [
      "manufacturing",
      "service operations"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The MSOM Data Challenges dataset is designed to provide insights into the complexities of manufacturing and service operations management. It encompasses a variety of challenges that practitioners and researchers face in these fields. The dataset is structured in a tabular format, consisting of rows that represent individual challenges and columns that capture various attributes related to each challenge, such as the type of operation, metrics for success, and contextual factors influencing performance. The data collection methodology involves gathering information from industry reports, academic publications, and surveys conducted within the operations management community. While the exact sources are not specified, the dataset aims to reflect real-world scenarios encountered by professionals in the field. Coverage of the dataset is broad, focusing on operational challenges that are relevant across different sectors, although specific temporal or geographic coverage is not explicitly mentioned. Key variables within the dataset may include operational metrics, service quality indicators, and efficiency measures, which are crucial for understanding the performance of manufacturing and service operations. However, users should be aware of potential limitations in data quality, such as inconsistencies in reporting standards or variations in data collection methods across different sources. Common preprocessing steps may involve cleaning the data to address missing values, normalizing metrics for comparison, and transforming categorical variables into a suitable format for analysis. Researchers can utilize this dataset to address a range of research questions, such as identifying factors that contribute to operational success or evaluating the impact of specific interventions on service delivery. The types of analyses supported by the dataset include regression analysis to determine relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize operational performance. Overall, the MSOM Data Challenges dataset serves as a valuable resource for those looking to explore the intricacies of operations management and apply analytical techniques to derive actionable insights."
  },
  {
    "name": "PatentsView",
    "description": "13M+ US patents (1976-present) with citations, inventors, assignees. Full patent text and claims. innovation research at scale",
    "category": "Data Portals",
    "url": "https://patentsview.org/download/data-download-tables",
    "docs_url": "https://patentsview.org/download/data-download-dictionary",
    "github_url": null,
    "tags": [
      "patents",
      "innovation",
      "USPTO",
      "citations",
      "intellectual property"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "innovation",
      "patents",
      "intellectual property"
    ],
    "summary": "PatentsView is a comprehensive dataset containing over 13 million US patents from 1976 to the present. It includes detailed information such as citations, inventors, and assignees, allowing researchers to conduct extensive innovation research and analysis at scale.",
    "use_cases": [
      "Analyzing trends in patent citations to identify influential inventions.",
      "Studying the relationship between patent filings and technological advancements.",
      "Exploring inventor collaboration networks through patent assignments.",
      "Investigating the impact of patents on economic growth and innovation."
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the key variables in the PatentsView dataset?",
      "How can I access US patent data for innovation research?",
      "What insights can be gained from analyzing patent citations?",
      "What is the temporal coverage of the PatentsView dataset?",
      "How many patents are included in the PatentsView dataset?",
      "What types of analyses can be performed using the PatentsView dataset?",
      "Who are the inventors and assignees in the PatentsView dataset?",
      "What is the significance of patent data in understanding innovation trends?"
    ],
    "domain_tags": [
      "technology",
      "intellectual property"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "1976-present",
    "geographic_scope": "US",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/logos/patentsview.png",
    "embedding_text": "The PatentsView dataset is a rich repository of over 13 million US patents, encompassing a wide array of information crucial for understanding innovation dynamics. The dataset spans patents granted from 1976 to the present, providing a longitudinal view of technological advancements and intellectual property trends. Each entry in the dataset includes key variables such as patent numbers, titles, abstracts, full patent texts, claims, citations, inventors, and assignees, which are essential for conducting detailed analyses. The data is structured in a tabular format, making it accessible for various types of data manipulation and analysis. Researchers can utilize this dataset to explore a multitude of research questions, including the impact of patent citations on subsequent innovations, the evolution of technology sectors over time, and the collaboration patterns among inventors and organizations. The collection methodology involves aggregating data from the United States Patent and Trademark Office (USPTO), ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as the possibility of incomplete citation data or variations in patent classification systems. Common preprocessing steps may include cleaning the text data, normalizing inventor names, and filtering patents based on specific criteria. The dataset supports a variety of analytical techniques, including regression analysis, machine learning, and descriptive statistics, making it a versatile tool for both academic and industry researchers. By leveraging the insights gained from the PatentsView dataset, researchers can contribute to a deeper understanding of the role of patents in fostering innovation and economic growth.",
    "benchmark_usage": [
      "Innovation research",
      "Patent analysis",
      "Intellectual property studies"
    ]
  },
  {
    "name": "OpenML",
    "description": "Platform for sharing datasets, tasks, and ML code",
    "category": "Data Portals",
    "url": "https://www.openml.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "ML",
      "open science",
      "benchmarks"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenML is a platform designed for sharing datasets, tasks, and machine learning code. It facilitates collaboration and benchmarking in the machine learning community, allowing users to access a wide variety of datasets and tasks for experimentation and model evaluation.",
    "use_cases": [
      "Benchmarking machine learning models",
      "Collaborative research in machine learning",
      "Accessing diverse datasets for experimentation"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is OpenML?",
      "How can I access datasets on OpenML?",
      "What types of machine learning tasks are available on OpenML?",
      "How does OpenML support open science?",
      "What are the benchmarks available on OpenML?",
      "Can I share my own datasets on OpenML?",
      "What is the purpose of OpenML?",
      "How does OpenML facilitate collaboration in machine learning?"
    ],
    "domain_tags": [
      "technology"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "benchmark_usage": [
      "Common uses include benchmarking machine learning algorithms and sharing best practices in model evaluation."
    ],
    "model_score": 0.0,
    "embedding_text": "OpenML serves as a comprehensive platform for the machine learning community, enabling users to share datasets, tasks, and code in an open and collaborative environment. The data structure on OpenML typically consists of a variety of datasets that may include rows representing individual observations and columns representing various features or attributes of the data. The platform allows researchers to upload datasets that can be utilized for different machine learning tasks, ranging from classification to regression and clustering. Each dataset on OpenML is accompanied by metadata that describes its structure, including the types of variables present, their data types, and any relevant documentation that aids users in understanding the dataset's context and usage.\n\nThe collection methodology for datasets on OpenML varies, as datasets can be contributed by users from diverse backgrounds, including academic researchers, industry professionals, and hobbyists. This crowdsourced approach ensures a rich variety of datasets, but it also means that the quality and completeness of the data can vary significantly. Users are encouraged to provide detailed descriptions and documentation for their datasets, which helps maintain a certain level of quality and usability. However, known limitations may include missing values, inconsistencies in data formatting, and varying levels of data preprocessing that have been applied prior to uploading.\n\nCommon preprocessing steps that may be necessary when working with datasets from OpenML include handling missing values, normalizing or standardizing features, and encoding categorical variables. Researchers often need to perform exploratory data analysis (EDA) to understand the distributions and relationships within the data before applying machine learning algorithms. OpenML supports a wide range of analyses, including descriptive statistics, regression analysis, and various machine learning techniques, making it a versatile resource for both beginners and experienced data scientists.\n\nResearchers typically use OpenML to address a variety of research questions, such as evaluating the performance of different machine learning algorithms on the same dataset, exploring the impact of feature selection on model accuracy, and comparing results across different datasets to identify trends and patterns in machine learning performance. The platform fosters an environment of open science, where findings can be shared and reproduced, contributing to the overall advancement of knowledge in the field of machine learning. By providing access to a wealth of datasets and tasks, OpenML empowers researchers to innovate and collaborate, ultimately driving the field forward.",
    "image_url": "/images/datasets/openml.png"
  },
  {
    "name": "OpenML",
    "description": "ML benchmarking platform with standardized train-test splits for reproducible comparisons",
    "category": "Dataset Aggregators",
    "url": "https://www.openml.org",
    "docs_url": "https://docs.openml.org",
    "github_url": "https://github.com/openml/openml-python",
    "tags": [
      "benchmarking",
      "reproducibility",
      "ML",
      "AutoML"
    ],
    "best_for": "Reproducible ML benchmarking with standardized experimental setups",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenML is a machine learning benchmarking platform that provides standardized train-test splits for reproducible comparisons across various machine learning models and datasets. Users can leverage this platform to evaluate the performance of different algorithms on the same datasets, facilitating a fair comparison and enhancing the reproducibility of machine learning experiments.",
    "use_cases": [
      "Comparing the performance of various machine learning algorithms on standardized datasets.",
      "Evaluating the effectiveness of AutoML techniques using consistent train-test splits.",
      "Conducting reproducible research in machine learning by utilizing shared datasets and benchmarks."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is OpenML and how does it support ML benchmarking?",
      "How can I access datasets on OpenML for reproducible ML comparisons?",
      "What are the features of OpenML for standardized train-test splits?",
      "How does OpenML enhance reproducibility in machine learning research?",
      "What types of datasets are available on OpenML?",
      "How can I use OpenML for AutoML experiments?",
      "What are the benefits of using OpenML for benchmarking machine learning models?",
      "How does OpenML facilitate comparisons between different ML algorithms?"
    ],
    "domain_tags": [
      "technology"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "benchmark_usage": [
      "Standardized comparisons of machine learning models"
    ],
    "model_score": 0.0,
    "image_url": "/images/datasets/openml.png",
    "embedding_text": "OpenML serves as a pivotal platform in the realm of machine learning, designed to facilitate benchmarking through standardized datasets and train-test splits. The platform's architecture is centered around a tabular data structure, where datasets are organized into rows and columns, each representing different observations and features, respectively. This structure allows for a diverse array of variables that can be leveraged for various machine learning tasks. The collection methodology employed by OpenML involves aggregating datasets from numerous contributors, ensuring a rich repository of data that spans multiple domains within machine learning. While specific data sources are not disclosed, the platform emphasizes the importance of reproducibility and transparency in its data collection practices. The coverage of OpenML is not explicitly defined in terms of temporal or geographic dimensions, but it is widely recognized for its comprehensive inclusion of datasets across various machine learning challenges. Key variables within the datasets typically measure performance metrics, feature importance, and algorithmic efficiency, providing researchers with critical insights into the capabilities of different machine learning models. However, users should be aware of potential limitations regarding data quality, as datasets may vary in completeness and accuracy based on their sources. Common preprocessing steps necessary when utilizing OpenML datasets include handling missing values, normalizing data, and encoding categorical variables to prepare for machine learning applications. Researchers often utilize OpenML to address a variety of research questions, such as determining the most effective algorithms for specific tasks, understanding the impact of feature selection on model performance, and exploring the trade-offs between different modeling approaches. The platform supports a range of analyses, including regression, classification, and clustering, enabling users to apply various machine learning techniques to the datasets available. In summary, OpenML is a vital resource for researchers and practitioners in the machine learning field, offering a structured environment for benchmarking and facilitating reproducible research through its extensive collection of datasets and standardized methodologies."
  },
  {
    "name": "Google Dataset Search",
    "description": "Universal search engine for datasets across the web. Meta-tool for discovering research data",
    "category": "Data Portals",
    "url": "https://datasetsearch.research.google.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search engine",
      "datasets",
      "discovery",
      "Google"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Google Dataset Search is a universal search engine designed to help users discover datasets across the web. It serves as a meta-tool for researchers and data enthusiasts to locate relevant research data efficiently.",
    "use_cases": [
      "Finding datasets for academic research projects",
      "Locating data for machine learning model training",
      "Discovering datasets for statistical analysis",
      "Searching for public datasets for data journalism"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available on climate change?",
      "How can I find datasets related to healthcare?",
      "Where can I locate economic datasets for analysis?",
      "What datasets does Google Dataset Search provide for machine learning?",
      "How can I search for datasets on social media trends?",
      "What are the available datasets for educational research?"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "Google Dataset Search is a powerful tool designed to facilitate the discovery of datasets across the internet. It aggregates metadata from various data repositories, making it easier for researchers, data scientists, and curious individuals to find relevant datasets for their projects. The platform operates by indexing datasets and providing a search interface that allows users to input queries related to their specific data needs. The primary structure of the data available through Google Dataset Search includes a variety of datasets that can be categorized into different domains, such as social sciences, health, economics, and more. Each dataset typically contains rows and columns that represent individual records and their associated attributes, respectively. The variables within these datasets can vary widely, measuring different aspects depending on the dataset's focus, such as demographic information, economic indicators, or scientific measurements. The collection methodology for the datasets indexed by Google Dataset Search varies, as it pulls information from numerous sources, including government databases, academic institutions, and private organizations. This diversity in sources contributes to a rich tapestry of data but also introduces variability in data quality and completeness. Users should be aware of potential limitations, such as missing values, outdated information, or inconsistencies in data formats. Common preprocessing steps that researchers might undertake include cleaning the data to handle missing values, normalizing data formats, and transforming variables to suit their analysis needs. Google Dataset Search can support a wide range of research questions, from exploring trends in public health to analyzing economic data for policy development. The types of analyses that can be conducted using datasets found through this platform include regression analysis, machine learning applications, and descriptive statistics, among others. Researchers typically utilize Google Dataset Search in their studies to identify datasets that can provide insights into their hypotheses or to supplement their existing data with additional information. By leveraging this tool, users can save time and enhance the quality of their research by accessing a broader array of datasets that might not be easily discoverable through traditional search methods."
  },
  {
    "name": "Google Dataset Search",
    "description": "Search engine indexing 45M+ datasets from 13,000+ websites using schema.org metadata",
    "category": "Dataset Aggregators",
    "url": "https://datasetsearch.research.google.com",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search",
      "discovery",
      "meta-search",
      "aggregator"
    ],
    "best_for": "Discovering datasets across government portals, research institutions, and commercial providers",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Google Dataset Search is a powerful search engine that indexes over 45 million datasets from more than 13,000 websites using schema.org metadata. It enables users to discover a wide variety of datasets across multiple domains, facilitating research and data analysis.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "How can I find datasets related to climate change?",
      "What datasets are available for machine learning research?",
      "Where can I locate public health datasets?",
      "How do I search for economic datasets using Google Dataset Search?",
      "What datasets are indexed for social science research?",
      "Can I find datasets on sports analytics?",
      "What are the available datasets for educational research?",
      "How to discover datasets for environmental studies?"
    ],
    "domain_tags": [
      "General"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "Google Dataset Search serves as a comprehensive search engine that indexes an extensive array of datasets, exceeding 45 million in total, sourced from over 13,000 websites. This platform utilizes schema.org metadata to enhance the discoverability of datasets across various domains, making it an essential tool for researchers, data scientists, and anyone interested in data-driven insights. The structure of the indexed data is diverse, encompassing various formats and types, which may include tabular data, text-based datasets, images, and more. Each dataset is typically characterized by a set of rows and columns, where rows represent individual data points or records, and columns denote the variables or features of the datasets. The specific variables included can vary widely depending on the dataset's focus, covering aspects such as demographic information, temporal data, geographic identifiers, and other relevant metrics. The collection methodology for the datasets indexed by Google Dataset Search involves aggregating publicly available datasets from numerous sources, ensuring a broad coverage of topics and fields. This aggregation process relies on the structured metadata provided by dataset publishers, which is essential for the effective indexing and retrieval of datasets. However, users should be aware of potential limitations in data quality, as the platform does not directly host the datasets but rather links to them. Consequently, the quality and reliability of the datasets can vary significantly based on their source. Researchers and analysts often engage in common preprocessing steps when utilizing datasets found through Google Dataset Search. These steps may include data cleaning, normalization, and transformation to ensure compatibility with analytical tools and methodologies. The platform supports a wide range of research questions, allowing users to explore various analytical scenarios, including regression analysis, machine learning applications, and descriptive statistics. By facilitating access to a multitude of datasets, Google Dataset Search empowers researchers to conduct studies across diverse fields, fostering innovation and discovery in data science and beyond."
  },
  {
    "name": "Rakuten Data Release",
    "description": "E-commerce, advertising, and multimedia datasets from Rakuten",
    "category": "Data Portals",
    "url": "https://rit.rakuten.com/data_release/#access",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Rakuten",
      "e-commerce",
      "multimedia"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "advertising",
      "multimedia"
    ],
    "summary": "The Rakuten Data Release provides a comprehensive set of datasets related to e-commerce, advertising, and multimedia. Researchers and analysts can utilize this data to explore consumer behavior, advertising effectiveness, and trends in multimedia usage within the e-commerce sector.",
    "use_cases": [
      "Analyzing consumer purchasing patterns in e-commerce",
      "Evaluating the effectiveness of advertising campaigns",
      "Exploring trends in multimedia content consumption",
      "Conducting market research for e-commerce strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available from Rakuten?",
      "How can I analyze e-commerce trends using Rakuten data?",
      "What multimedia datasets does Rakuten provide?",
      "Where can I find advertising data from Rakuten?",
      "What insights can be gained from Rakuten's e-commerce datasets?",
      "How does Rakuten's data support consumer behavior analysis?",
      "What are the key variables in Rakuten's datasets?",
      "What types of analyses can be performed with Rakuten data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/rakuten-data-release.png",
    "embedding_text": "The Rakuten Data Release encompasses a variety of datasets that focus on e-commerce, advertising, and multimedia, making it a valuable resource for researchers and data scientists interested in these domains. The datasets are structured in a tabular format, typically consisting of rows representing individual transactions or interactions, and columns that capture various attributes such as user demographics, product details, transaction amounts, and timestamps. Each dataset is designed to facilitate a range of analyses, from descriptive statistics to more complex machine learning applications. The collection methodology for these datasets involves aggregating data from Rakuten's extensive e-commerce platform, which includes user interactions, purchase history, and advertising metrics. This data is collected through user activity on the platform, ensuring a rich source of information for analysis. However, researchers should be aware of potential limitations in data quality, such as missing values or biases in user behavior that could affect the outcomes of their analyses. Common preprocessing steps may include handling missing data, normalizing variables, and encoding categorical features to prepare the datasets for analysis. Key variables within the datasets may include user IDs, product categories, purchase amounts, and timestamps, each providing insights into consumer behavior and market trends. The Rakuten Data Release supports a variety of research questions, such as understanding the impact of advertising on sales, identifying consumer preferences, and analyzing seasonal trends in e-commerce. Researchers typically use this data to conduct regression analyses, machine learning modeling, and descriptive studies to uncover patterns and insights that can inform business strategies and academic research. Overall, the Rakuten Data Release serves as a comprehensive resource for those looking to delve into the intricacies of e-commerce and its associated fields."
  },
  {
    "name": "Marketing Science Databases",
    "description": "INFORMS conference with data-focused opportunities",
    "category": "Data Portals",
    "url": "https://pubsonline.informs.org/page/mksc/online-databases",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "marketing",
      "INFORMS",
      "databases"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "marketing",
      "data analysis"
    ],
    "summary": "The Marketing Science Databases provide a comprehensive collection of data-focused opportunities presented at the INFORMS conference. Researchers and practitioners can leverage this dataset to explore various marketing strategies, analyze consumer behavior, and enhance decision-making processes in marketing.",
    "use_cases": [
      "Analyzing consumer behavior trends",
      "Evaluating the effectiveness of marketing campaigns",
      "Conducting market segmentation analysis",
      "Exploring pricing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What data is available in the Marketing Science Databases?",
      "How can I access the INFORMS conference data?",
      "What marketing strategies can be analyzed using this dataset?",
      "What insights can be gained from the Marketing Science Databases?",
      "Are there any case studies related to marketing in this dataset?",
      "How does the data support consumer behavior analysis?",
      "What types of marketing data are included in the INFORMS conference?",
      "Can I find examples of data-driven marketing decisions in this dataset?"
    ],
    "domain_tags": [
      "marketing"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Marketing Science Databases serve as a vital resource for researchers and practitioners in the field of marketing, particularly those attending or participating in the INFORMS conference. This dataset encompasses a variety of data structures, typically organized in tabular format, where rows represent individual data entries and columns correspond to various attributes or variables relevant to marketing science. The specific variables included in the dataset may cover aspects such as marketing strategies, consumer demographics, campaign performance metrics, and market trends, allowing for a comprehensive analysis of marketing practices. The collection methodology for this dataset is rooted in the contributions and research presented at the INFORMS conference, where data is often sourced from empirical studies, surveys, and experiments conducted by marketing professionals and academics. This collaborative approach ensures that the dataset is rich in diversity and relevance to current marketing challenges. While the dataset does not explicitly mention temporal or geographic coverage, it is designed to be applicable across various contexts, providing insights that can be generalized or tailored to specific markets or timeframes. Users of the Marketing Science Databases can expect to engage with key variables that measure critical aspects of marketing performance, such as return on investment (ROI), customer acquisition costs, and engagement rates. However, researchers should be mindful of potential limitations in data quality, including biases inherent in self-reported data or variations in data collection methods across different studies. Common preprocessing steps may involve cleaning the data to address missing values, normalizing variables for comparative analysis, and transforming categorical variables into numerical formats suitable for statistical modeling. The dataset supports a wide range of research questions, including inquiries into the effectiveness of different marketing strategies, the impact of consumer behavior on purchasing decisions, and the relationship between marketing expenditures and sales outcomes. Analysts can employ various methodologies, including regression analysis, machine learning techniques, and descriptive statistics, to extract meaningful insights from the data. Researchers typically utilize this dataset to support their studies by identifying patterns, testing hypotheses, and deriving actionable recommendations that can enhance marketing effectiveness and drive business success. Overall, the Marketing Science Databases represent a valuable asset for those looking to deepen their understanding of marketing dynamics and leverage data-driven insights in their decision-making processes."
  },
  {
    "name": "FI-2010 Limit Order Book",
    "description": "4.3M samples of NASDAQ Nordic limit order book data. 10 depth levels, 5 stocks, normalized features. Benchmark for price prediction",
    "category": "Financial Services",
    "url": "https://etsin.fairdata.fi/dataset/73eb48d7-4dbc-4a10-a52a-da745b47a649",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "order book",
      "limit orders",
      "NASDAQ",
      "price prediction",
      "benchmark"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "financial-services",
      "data-analysis",
      "price-prediction"
    ],
    "summary": "The FI-2010 Limit Order Book dataset consists of 4.3 million samples of limit order book data from NASDAQ Nordic, covering 10 depth levels for 5 different stocks. This dataset serves as a benchmark for price prediction, allowing researchers and analysts to explore the dynamics of order books and develop predictive models for stock prices.",
    "use_cases": [
      "Developing predictive models for stock prices",
      "Analyzing the impact of order book dynamics on market behavior",
      "Benchmarking price prediction algorithms",
      "Studying the relationship between order book features and stock volatility"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the FI-2010 Limit Order Book dataset?",
      "How can I use NASDAQ Nordic limit order book data for price prediction?",
      "What features are included in the FI-2010 dataset?",
      "What types of analyses can be performed with limit order book data?",
      "How many samples are in the FI-2010 dataset?",
      "What stocks are covered in the FI-2010 Limit Order Book?",
      "What are the depth levels in the limit order book?",
      "How is the FI-2010 dataset structured?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "massive",
    "benchmark_usage": [
      "Benchmark for price prediction"
    ],
    "model_score": 0.0,
    "image_url": "/images/logos/fairdata.png",
    "embedding_text": "The FI-2010 Limit Order Book dataset is a comprehensive collection of limit order book data from NASDAQ Nordic, encompassing 4.3 million samples that provide insights into the trading behavior of five selected stocks. The dataset is structured in a tabular format, where each row represents a snapshot of the limit order book at a specific point in time, and the columns include various features that capture the state of the order book, such as price levels, order sizes, and timestamps. The data is organized to reflect 10 depth levels, which allows for a detailed analysis of the market's liquidity and the distribution of buy and sell orders. Researchers and analysts can leverage this dataset to explore a multitude of research questions related to market microstructure, price formation, and trading strategies. The collection methodology for this dataset involves the aggregation of real-time order book data from NASDAQ Nordic, ensuring that the dataset reflects actual market conditions. As such, it serves as a valuable resource for both academic research and practical applications in the financial services sector. Key variables within the dataset include order prices, order sizes, and the direction of orders, which together provide a rich context for understanding market dynamics. However, users should be aware of potential limitations in data quality, such as missing values or discrepancies in order book snapshots due to market volatility. Common preprocessing steps may include normalization of features, handling of missing data, and transformation of categorical variables into numerical formats suitable for analysis. The FI-2010 dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics. Researchers typically utilize this dataset to benchmark their price prediction algorithms, assess the effectiveness of trading strategies, and investigate the relationship between order book characteristics and market outcomes. Overall, the FI-2010 Limit Order Book dataset is an essential tool for anyone looking to deepen their understanding of financial markets and enhance their analytical capabilities in the realm of price prediction."
  },
  {
    "name": "RecSys Datasets Collection",
    "description": "Datasets from ACM Recommender Systems challenges",
    "category": "Data Portals",
    "url": "https://github.com/RUCAIBox/RecSysDatasets",
    "docs_url": null,
    "github_url": "https://github.com/RUCAIBox/RecSysDatasets",
    "tags": [
      "RecSys",
      "recommendations",
      "ACM"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The RecSys Datasets Collection comprises various datasets sourced from ACM Recommender Systems challenges. These datasets are designed to facilitate research and development in recommendation systems, allowing users to explore and analyze different aspects of recommendation algorithms and their effectiveness.",
    "use_cases": [
      "Analyzing the performance of different recommendation algorithms",
      "Exploring user behavior patterns in e-commerce",
      "Evaluating the impact of recommendations on consumer choices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the RecSys Datasets Collection?",
      "How can I use ACM Recommender Systems datasets for my research?",
      "What are the key features of the datasets in the RecSys Datasets Collection?",
      "Where can I find datasets for recommendation systems?",
      "What types of analyses can be performed on RecSys datasets?",
      "Are there any challenges associated with using ACM datasets for recommendations?",
      "What insights can be gained from the RecSys Datasets Collection?",
      "How do I access the datasets from ACM Recommender Systems challenges?"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/recsys-datasets-collection.png",
    "embedding_text": "The RecSys Datasets Collection is a comprehensive repository of datasets derived from the ACM Recommender Systems challenges, aimed at advancing research in the field of recommendation systems. This collection includes a variety of datasets that are structured in a tabular format, typically consisting of rows representing individual user interactions or items, and columns that capture various attributes such as user IDs, item IDs, ratings, timestamps, and contextual information. The data structure is designed to facilitate the analysis of user preferences and the effectiveness of different recommendation algorithms. The datasets are collected from real-world applications and competitions, ensuring that they reflect genuine user behavior and preferences in various domains, particularly in e-commerce. Researchers and data scientists can leverage these datasets to conduct a wide range of analyses, including regression analysis, machine learning model training, and descriptive statistics. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the data for analysis. The datasets can address critical research questions such as how different recommendation strategies impact user engagement, the effectiveness of collaborative filtering versus content-based approaches, and the influence of contextual factors on user preferences. However, users should be aware of potential limitations in data quality, such as missing values or biases in user interactions, which may affect the robustness of their findings. Overall, the RecSys Datasets Collection serves as a valuable resource for researchers looking to explore the intricacies of recommendation systems and their applications in real-world scenarios.",
    "domain_tags": [
      "retail"
    ]
  },
  {
    "name": "USASpending Federal Awards",
    "description": "All federal contracts, grants, loans since 2001. 400+ variables, $50T+ in awards. Government procurement analytics",
    "category": "Data Portals",
    "url": "https://www.usaspending.gov/download_center/award_data_archive",
    "docs_url": "https://www.usaspending.gov/data-dictionary",
    "github_url": null,
    "tags": [
      "government",
      "procurement",
      "contracts",
      "grants",
      "federal spending"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The USASpending Federal Awards dataset encompasses all federal contracts, grants, and loans issued since 2001, featuring over 400 variables and more than $50 trillion in awards. This dataset serves as a comprehensive resource for government procurement analytics, enabling users to analyze federal spending patterns and trends.",
    "use_cases": [
      "Analyzing trends in federal spending over time.",
      "Comparing the distribution of federal contracts across different sectors.",
      "Evaluating the impact of federal grants on local economies.",
      "Investigating the relationship between federal loans and business growth."
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the federal awards available in the USASpending dataset?",
      "How can I analyze federal contracts and grants from 2001 onwards?",
      "What variables are included in the USASpending Federal Awards dataset?",
      "Where can I find information on federal spending and procurement?",
      "What types of federal loans are documented in the USASpending dataset?",
      "How has federal spending changed over the years?",
      "What insights can be gained from analyzing government procurement data?",
      "How to access the USASpending Federal Awards data?"
    ],
    "domain_tags": [
      "government",
      "procurement"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2001-present",
    "geographic_scope": "United States",
    "size_category": "massive",
    "model_score": 0.0,
    "embedding_text": "The USASpending Federal Awards dataset is a comprehensive collection of data pertaining to all federal contracts, grants, and loans issued by the United States government since 2001. This dataset includes over 400 variables that capture various aspects of federal spending, amounting to more than $50 trillion in awards. The structure of the dataset is primarily tabular, consisting of rows representing individual awards and columns detailing key attributes such as award type, recipient information, funding agency, and award amounts. Each entry in the dataset provides insights into government procurement activities, making it a valuable resource for researchers, policymakers, and analysts interested in understanding federal spending dynamics. The data is collected through various government agencies that report their financial transactions, ensuring a high level of accuracy and reliability. However, users should be aware of potential limitations, such as variations in reporting standards across agencies and the possibility of missing data for certain awards. Common preprocessing steps may include data cleaning to handle inconsistencies, normalization of award amounts, and filtering for specific types of awards or time periods. Researchers can leverage this dataset to address a variety of research questions, such as analyzing the effectiveness of federal grants in stimulating economic growth, examining the distribution of federal contracts among different industries, and identifying trends in federal spending over time. The dataset supports various types of analyses, including descriptive statistics, regression modeling, and machine learning applications, making it a versatile tool for exploring the complexities of government procurement. Overall, the USASpending Federal Awards dataset serves as an essential resource for those seeking to gain insights into federal financial activities and their implications for the economy."
  },
  {
    "name": "Yongfeng Dataset Collection",
    "description": "E-commerce and recommendation system datasets",
    "category": "Data Portals",
    "url": "https://www.yongfeng.me/dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "recommendations",
      "e-commerce",
      "academic"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "recommendations",
      "consumer-behavior"
    ],
    "summary": "The Yongfeng Dataset Collection consists of various datasets related to e-commerce and recommendation systems. Researchers and practitioners can utilize this collection to analyze consumer behavior, develop recommendation algorithms, and explore trends in online shopping.",
    "use_cases": [
      "Analyzing consumer purchasing patterns",
      "Developing recommendation algorithms",
      "Evaluating the effectiveness of marketing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available in the Yongfeng Dataset Collection?",
      "How can I analyze e-commerce data for recommendations?",
      "What variables are included in the Yongfeng Dataset?",
      "What are common use cases for recommendation system datasets?",
      "How does consumer behavior data influence e-commerce strategies?",
      "What preprocessing steps are needed for e-commerce datasets?",
      "What types of analyses can be performed on the Yongfeng Dataset?",
      "Where can I find datasets related to e-commerce and recommendations?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Yongfeng Dataset Collection is a comprehensive resource for researchers and practitioners interested in the fields of e-commerce and recommendation systems. This collection encompasses various datasets that are structured in a tabular format, making them accessible for analysis using standard data manipulation tools. Each dataset typically contains rows representing individual transactions or user interactions, with columns that capture key variables such as user IDs, item IDs, timestamps, and ratings. These variables are essential for understanding user behavior and preferences in online shopping environments. The collection methodology involves gathering data from real-world e-commerce platforms, ensuring that the datasets reflect actual consumer interactions and trends. This real-world data is invaluable for developing and testing recommendation algorithms, as it provides insights into how users engage with products and services. Researchers can leverage this dataset to address a variety of research questions, such as identifying factors that influence purchasing decisions, evaluating the performance of different recommendation strategies, and exploring the impact of marketing campaigns on consumer behavior. The datasets are suitable for a range of analyses, including regression analysis, machine learning applications, and descriptive statistics. However, users should be aware of potential limitations related to data quality, such as missing values or biases in user behavior, which may affect the outcomes of their analyses. Common preprocessing steps may include data cleaning, normalization, and feature engineering to prepare the datasets for modeling. Overall, the Yongfeng Dataset Collection serves as a vital tool for advancing research in e-commerce and recommendation systems, enabling users to gain deeper insights into consumer behavior and improve the effectiveness of recommendation algorithms."
  },
  {
    "name": "Julian McAuley Datasets",
    "description": "Reviews, recommendations, and social network data",
    "category": "Data Portals",
    "url": "https://cseweb.ucsd.edu/~jmcauley/datasets.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "reviews",
      "recommendations",
      "UCSD",
      "academic"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Julian McAuley Datasets provide a comprehensive collection of reviews, recommendations, and social network data, primarily focused on e-commerce. Researchers can utilize this dataset to analyze consumer behavior, study recommendation systems, and explore social interactions within online platforms.",
    "use_cases": [
      "Analyzing consumer preferences through review data",
      "Developing and testing recommendation algorithms",
      "Studying the impact of social networks on purchasing decisions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the Julian McAuley Datasets?",
      "Where can I find reviews and recommendations datasets?",
      "How can I analyze consumer behavior using the Julian McAuley Datasets?",
      "What types of social network data are available in the Julian McAuley Datasets?",
      "Are there datasets for studying recommendation systems?",
      "What academic datasets are available from UCSD?",
      "How can I access the Julian McAuley Datasets for research?",
      "What insights can be gained from the Julian McAuley Datasets?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/ucsd.png",
    "embedding_text": "The Julian McAuley Datasets consist of a rich collection of data that includes reviews, recommendations, and social network interactions, primarily sourced from e-commerce platforms. The dataset is structured in a tabular format, with rows representing individual reviews or interactions and columns capturing various attributes such as user IDs, item IDs, review text, ratings, and timestamps. This structure allows for straightforward analysis and manipulation using data science tools. The data collection methodology involves scraping publicly available information from online platforms, ensuring that the dataset reflects real-world user interactions and preferences. While the dataset does not specify temporal or geographic coverage, it encompasses a diverse range of consumer behaviors and preferences, making it suitable for various research applications. Key variables within the dataset include user IDs, which identify individual consumers; item IDs, which represent products; review text, providing qualitative insights into consumer opinions; and numerical ratings, quantifying user satisfaction. These variables enable researchers to perform in-depth analyses of consumer sentiment and behavior. However, researchers should be aware of potential limitations, such as biases in user-generated content and the representativeness of the data. Common preprocessing steps may involve cleaning the review text, normalizing ratings, and handling missing values to prepare the data for analysis. The Julian McAuley Datasets can address a variety of research questions, including the effectiveness of recommendation systems, the influence of social networks on purchasing decisions, and patterns in consumer behavior over time. Researchers typically employ techniques such as regression analysis, machine learning, and descriptive statistics to extract insights from the data. By leveraging this dataset, scholars can contribute to the understanding of e-commerce dynamics and enhance the development of more effective recommendation algorithms."
  },
  {
    "name": "Wayfair Search (WANDS)",
    "description": "233k human-annotated query-product judgments, 43k products",
    "category": "E-Commerce",
    "url": "https://github.com/wayfair/WANDS",
    "docs_url": null,
    "github_url": "https://github.com/wayfair/WANDS",
    "tags": [
      "search relevance",
      "annotations",
      "furniture"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Wayfair Search (WANDS) dataset consists of 233,000 human-annotated query-product judgments across 43,000 products, primarily focused on search relevance in the e-commerce sector. Researchers can utilize this dataset to analyze search behavior, improve product recommendations, and enhance user experience in online shopping.",
    "use_cases": [
      "Analyzing search relevance for e-commerce platforms",
      "Improving product recommendation algorithms",
      "Studying consumer behavior in online shopping",
      "Evaluating the effectiveness of search queries"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Wayfair Search (WANDS) dataset?",
      "How can I access the Wayfair Search dataset?",
      "What types of analyses can be performed with the WANDS dataset?",
      "What are the key variables in the Wayfair Search dataset?",
      "How many products are included in the Wayfair Search dataset?",
      "What is the size of the Wayfair Search dataset?",
      "What is the focus of the Wayfair Search dataset?",
      "What kind of judgments are included in the Wayfair Search dataset?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/wayfair-search-wands.png",
    "embedding_text": "The Wayfair Search (WANDS) dataset is a comprehensive collection of 233,000 human-annotated query-product judgments, encompassing 43,000 distinct products primarily within the e-commerce sector. This dataset is structured in a tabular format, where each row represents a unique query-product judgment, and columns include variables such as query text, product identifiers, and relevance scores assigned by human annotators. The data collection methodology involves rigorous annotation processes where human judges evaluate the relevance of products in response to specific search queries, ensuring high-quality judgments that reflect real-world consumer behavior. While the dataset does not explicitly mention temporal or geographic coverage, it provides a rich resource for analyzing trends in search relevance and consumer preferences in the online retail space. Key variables within the dataset measure aspects such as the relevance of products to search queries, which can be crucial for understanding how users interact with e-commerce platforms. Researchers utilizing this dataset may encounter common preprocessing steps, including data cleaning to handle any inconsistencies in annotations, normalization of query text, and potential feature engineering to enhance the dataset's usability for machine learning models. The WANDS dataset supports a variety of analytical approaches, including regression analysis to predict search relevance, machine learning techniques for classification tasks, and descriptive analyses to summarize consumer behavior patterns. Researchers typically leverage this dataset to address research questions related to search optimization, product placement strategies, and the overall effectiveness of search algorithms in improving user experience. By analyzing the judgments provided in this dataset, scholars can gain insights into how product features influence search outcomes and how search queries can be refined to better meet consumer needs, ultimately contributing to advancements in e-commerce strategies and technologies."
  },
  {
    "name": "Ele.me Clickstream",
    "description": "Clickstream data from Ele.me food delivery platform",
    "category": "Food & Delivery",
    "url": "https://tianchi.aliyun.com/dataset/131047",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "clickstream",
      "food delivery",
      "user behavior"
    ],
    "best_for": "Learning food & delivery analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior"
    ],
    "summary": "The Ele.me Clickstream dataset provides detailed clickstream data from the Ele.me food delivery platform, capturing user interactions and behaviors while ordering food online. Researchers can analyze user behavior patterns, preferences, and trends in food delivery, enabling insights into consumer decision-making processes.",
    "use_cases": [
      "Analyzing user behavior patterns in food delivery",
      "Identifying trends in consumer preferences",
      "Studying the impact of promotional strategies on user engagement"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Ele.me Clickstream dataset?",
      "How can I analyze user behavior in food delivery?",
      "What insights can be gained from Ele.me clickstream data?",
      "What are common patterns in food delivery user interactions?",
      "How does clickstream data inform e-commerce strategies?",
      "What variables are included in the Ele.me Clickstream dataset?",
      "How can I use clickstream data for consumer behavior analysis?",
      "What methodologies are suitable for analyzing Ele.me clickstream data?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Ele.me Clickstream dataset is a rich source of information derived from the Ele.me food delivery platform, capturing the intricate details of user interactions as they navigate through the ordering process. This dataset typically consists of rows representing individual user sessions, with columns detailing various variables such as timestamps, user IDs, item IDs, and action types (e.g., clicks, views, purchases). Each entry in the dataset provides a snapshot of user behavior, allowing researchers to track how users engage with the platform over time. The collection methodology for this dataset involves logging user interactions directly from the Ele.me platform, ensuring that the data reflects real-time user behavior in a live environment. This approach provides a comprehensive view of user actions, making it an invaluable resource for analyzing consumer behavior in the food delivery sector. While the dataset does not specify temporal or geographic coverage, it is assumed to encompass a diverse range of user interactions across various demographics, given the broad user base of the Ele.me platform. Key variables in the dataset include user identifiers, timestamps of actions, item identifiers, and the types of interactions (e.g., clicks, purchases), which collectively measure user engagement and preferences. However, researchers should be aware of potential limitations in data quality, such as missing values or inconsistencies in user session tracking, which may affect the analysis. Common preprocessing steps may include data cleaning to handle missing values, normalization of timestamps, and transformation of categorical variables into numerical formats for analysis. The Ele.me Clickstream dataset supports a variety of research questions, such as understanding the factors influencing user decisions in food delivery, evaluating the effectiveness of marketing campaigns, and identifying patterns in user preferences based on demographic factors. Analysts can employ various methodologies, including regression analysis, machine learning techniques, and descriptive statistics, to derive insights from the dataset. Researchers typically use this dataset to explore consumer behavior trends, optimize user experience on the platform, and inform strategic decisions related to product offerings and marketing initiatives. Overall, the Ele.me Clickstream dataset serves as a critical tool for understanding the dynamics of online food delivery and the factors that drive consumer engagement in this rapidly evolving market."
  },
  {
    "name": "CodaLab",
    "description": "Platform for competitions, benchmarks, and reproducible research",
    "category": "Data Portals",
    "url": "https://codalab.lisn.upsaclay.fr/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "benchmarks",
      "reproducibility",
      "competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "CodaLab is a platform designed to facilitate competitions, benchmarks, and reproducible research. It provides a structured environment where researchers and practitioners can engage in various challenges that promote the development and evaluation of algorithms and methodologies in a reproducible manner.",
    "use_cases": [
      "Evaluating machine learning models",
      "Conducting algorithmic competitions",
      "Benchmarking different methodologies",
      "Facilitating reproducible research"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is CodaLab?",
      "How can I participate in competitions on CodaLab?",
      "What benchmarks are available on CodaLab?",
      "How does CodaLab support reproducible research?",
      "What types of data can be found on CodaLab?",
      "What are the benefits of using CodaLab for research?"
    ],
    "domain_tags": [
      "research",
      "technology"
    ],
    "size_category": "medium",
    "benchmark_usage": [
      "Common uses include algorithm evaluation and competition organization"
    ],
    "model_score": 0.0,
    "embedding_text": "CodaLab serves as a versatile platform that supports competitions, benchmarks, and reproducible research, making it an essential tool for researchers and practitioners in the field of data science and machine learning. The platform is structured to allow users to create and participate in competitions that challenge participants to develop innovative algorithms and approaches to solve complex problems. The data structure within CodaLab typically includes a variety of datasets that are organized into rows and columns, where each row represents a unique instance or observation, and each column corresponds to a specific variable or feature relevant to the competition or benchmark task. This structured approach facilitates the easy comparison and evaluation of different methodologies and results. The collection methodology for data on CodaLab varies depending on the specific competition or benchmark, but it often involves aggregating datasets from publicly available sources, research studies, or contributions from the community. This ensures a diverse range of data that can be utilized for various research questions and analyses. While specific temporal and geographic coverage may not be explicitly mentioned, the platform's flexibility allows for a wide range of applications across different domains and timeframes. Key variables within the datasets can include performance metrics, algorithm parameters, and evaluation scores, which measure the effectiveness of the submitted solutions. However, users should be aware of potential limitations in data quality, such as inconsistencies in data collection methods or variations in dataset sizes, which may impact the robustness of the findings. Common preprocessing steps may include data cleaning, normalization, and transformation to ensure that the datasets are suitable for analysis. Researchers typically use CodaLab to address a variety of research questions, such as evaluating the performance of different machine learning models, understanding the impact of various algorithmic approaches, and exploring the reproducibility of research findings. The platform supports a range of analyses, including regression, machine learning, and descriptive statistics, making it a valuable resource for advancing knowledge in the field. Overall, CodaLab is a powerful tool that not only fosters innovation through competition but also emphasizes the importance of reproducibility in research, ultimately contributing to the advancement of data science and machine learning methodologies.",
    "data_modality": "mixed"
  },
  {
    "name": "Stanford GSB Experiment Collection",
    "description": "Datasets for experimentation analysis from Stanford Graduate School of Business",
    "category": "Education",
    "url": "https://github.com/gsbDBI/ExperimentData",
    "docs_url": null,
    "github_url": "https://github.com/gsbDBI/ExperimentData",
    "tags": [
      "Stanford",
      "causal inference",
      "experiments"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Stanford GSB Experiment Collection consists of datasets designed for experimentation analysis, primarily sourced from research conducted at the Stanford Graduate School of Business. Researchers can utilize this collection to explore causal relationships and test hypotheses in various experimental settings.",
    "use_cases": [
      "Analyzing causal effects in business experiments",
      "Testing hypotheses related to consumer behavior",
      "Evaluating the impact of pricing strategies",
      "Conducting regression analysis on experimental data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What datasets are available from Stanford GSB for experimentation?",
      "How can I analyze causal inference using Stanford GSB datasets?",
      "What experiments are included in the Stanford GSB Experiment Collection?",
      "Where can I find datasets for business experimentation?",
      "What types of analyses can be performed with Stanford GSB data?",
      "Are there datasets for educational research from Stanford GSB?",
      "How to access the Stanford GSB Experiment Collection?",
      "What are the key variables in Stanford GSB datasets?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/stanford-gsb-experiment-collection.png",
    "embedding_text": "The Stanford GSB Experiment Collection is a comprehensive repository of datasets curated for the purpose of experimentation analysis, specifically aimed at advancing research in business and economics. This collection is primarily sourced from the Stanford Graduate School of Business, a leading institution known for its rigorous academic research and innovative approaches to business education. The datasets within this collection are structured in a tabular format, which typically includes rows representing individual experimental observations and columns corresponding to various variables that capture key aspects of the experiments. These variables may include treatment conditions, demographic information of participants, response measures, and other relevant metrics that facilitate the analysis of causal relationships. The collection methodology involves rigorous experimental design principles, ensuring that the data collected is robust and suitable for statistical analysis. Researchers often employ randomized controlled trials (RCTs) and other experimental frameworks to generate data that can be used to test specific hypotheses about consumer behavior, decision-making processes, and market dynamics. While the exact temporal and geographic coverage of the datasets is not explicitly mentioned, the experiments are likely to reflect contemporary issues in business and economics, with a focus on diverse demographic groups. Key variables within the datasets may include treatment effects, participant demographics, and outcome measures, which are essential for understanding the implications of the experiments. However, researchers should be aware of potential limitations in data quality, such as sample size constraints or biases in participant selection, which may affect the generalizability of findings. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing variables, and transforming categorical data into numerical formats suitable for analysis. The datasets can address a variety of research questions, such as evaluating the effectiveness of marketing strategies, understanding consumer preferences, and assessing the impact of pricing changes on sales. The types of analyses supported by the data include regression analysis, machine learning applications, and descriptive statistics, allowing researchers to derive meaningful insights from the experimental results. Typically, researchers utilize the Stanford GSB Experiment Collection in studies aimed at exploring causal inference, testing theoretical models, and informing practical applications in business strategy and policy-making. Overall, this dataset collection serves as a valuable resource for scholars and practitioners interested in the intersection of experimentation and business research."
  },
  {
    "name": "Natural Driving in Ohio",
    "description": "ADAS-equipped vehicles with driving behavior events",
    "category": "Transportation & Mobility",
    "url": "https://data.transportation.gov/Automobiles/Advanced-Driver-Assistance-System-ADAS-Equipped-Si/iie8-uenj/about_data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "driving",
      "ADAS",
      "behavior",
      "government"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The dataset 'Natural Driving in Ohio' contains information on ADAS-equipped vehicles and their driving behavior events. Researchers can analyze this data to understand driving patterns, assess the effectiveness of advanced driver-assistance systems, and explore the relationship between vehicle technology and driving behavior.",
    "use_cases": [
      "Analyzing the impact of ADAS on driver behavior and safety.",
      "Evaluating the effectiveness of government policies related to transportation.",
      "Studying the correlation between driving behavior events and traffic incidents.",
      "Conducting regression analysis to predict driving behavior based on vehicle technology."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What driving behavior events are recorded in the Natural Driving in Ohio dataset?",
      "How can ADAS-equipped vehicles improve road safety based on driving behavior data?",
      "What insights can be gained from analyzing driving patterns in Ohio?",
      "What are the key variables in the Natural Driving in Ohio dataset?",
      "How does government regulation impact driving behavior in Ohio?",
      "What types of analyses can be performed using the Natural Driving in Ohio dataset?",
      "What are the limitations of the Natural Driving in Ohio dataset?",
      "How is driving behavior measured in the Natural Driving in Ohio dataset?"
    ],
    "domain_tags": [
      "transportation",
      "government"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Ohio",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/transportation.png",
    "embedding_text": "The 'Natural Driving in Ohio' dataset is a valuable resource for researchers interested in the intersection of transportation technology and driving behavior. This dataset comprises a structured collection of data points related to vehicles equipped with Advanced Driver-Assistance Systems (ADAS). The primary focus is on capturing driving behavior events, which can include a variety of metrics such as acceleration patterns, braking events, lane changes, and other critical driving maneuvers. The data structure typically consists of rows representing individual driving events, with columns detailing various attributes such as timestamp, vehicle speed, GPS coordinates, and specific driving actions taken by the driver. Each row provides a snapshot of a moment in time, allowing for detailed analysis of driving behavior under different conditions.\n\nThe methodology for collecting this data involves the use of onboard sensors and telemetry systems integrated within ADAS-equipped vehicles. These systems continuously monitor and record driving behavior, ensuring a comprehensive dataset that reflects real-world driving scenarios. The data may be sourced from government initiatives aimed at improving road safety, as well as partnerships with automotive manufacturers who provide access to anonymized driving data. This collaboration ensures that the dataset is both rich in detail and relevant to current transportation challenges.\n\nWhile the dataset does not specify temporal coverage, it is assumed to capture contemporary driving behavior, reflecting the latest advancements in vehicle technology. The geographic scope is explicitly defined as Ohio, making it particularly relevant for studies focused on regional transportation dynamics and policy implications. The demographic coverage is not specified, but the dataset likely encompasses a diverse range of driving behaviors across different vehicle types and driver demographics.\n\nKey variables within the dataset measure various aspects of driving behavior, such as frequency and types of driving events, vehicle speed, and environmental conditions at the time of each event. Understanding these variables is crucial for researchers aiming to draw meaningful conclusions about the impact of ADAS on driving safety and efficiency. However, potential limitations include data quality issues such as sensor inaccuracies, missing data points, or biases introduced by the specific vehicles included in the dataset.\n\nCommon preprocessing steps that researchers may need to undertake include data cleaning to address missing or erroneous values, normalization of metrics for comparative analysis, and feature engineering to derive new insights from existing variables. Given the nature of the data, researchers can employ a variety of analytical techniques, including regression analysis to explore relationships between variables, machine learning algorithms for predictive modeling, and descriptive statistics to summarize driving behavior trends.\n\nOverall, the 'Natural Driving in Ohio' dataset serves as a foundational resource for studies aimed at understanding how advanced vehicle technologies influence driving behavior. Researchers typically utilize this dataset to address questions related to road safety, the effectiveness of ADAS features, and the broader implications of government transportation policies. By leveraging this data, they can contribute to the development of safer and more efficient transportation systems."
  },
  {
    "name": "Prosper Loan Data",
    "description": "113K P2P loans with borrower characteristics and outcomes",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/datasets/henryokam/prosper-loan-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "P2P",
      "lending",
      "loans"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "financial-services",
      "lending",
      "peer-to-peer"
    ],
    "summary": "The Prosper Loan Data consists of 113,000 peer-to-peer loans, providing insights into borrower characteristics and loan outcomes. Researchers and analysts can utilize this dataset to explore lending patterns, borrower behavior, and the effectiveness of various loan products.",
    "use_cases": [
      "Analyzing borrower default rates",
      "Exploring the impact of borrower characteristics on loan outcomes",
      "Comparing P2P lending trends over time",
      "Evaluating the effectiveness of different loan products"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Prosper Loan Data?",
      "How can I analyze P2P loans?",
      "What borrower characteristics are included in the dataset?",
      "What outcomes can be measured from the Prosper Loan Data?",
      "How does P2P lending compare to traditional lending?",
      "What trends can be identified in peer-to-peer loans?",
      "What are the key variables in the Prosper Loan Data?",
      "How can I visualize loan outcomes from this dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/prosper-loan-data.jpg",
    "embedding_text": "The Prosper Loan Data is a comprehensive dataset that encompasses 113,000 peer-to-peer loans, offering a rich source of information for researchers and analysts interested in the financial services sector. This dataset includes various borrower characteristics such as credit scores, income levels, and loan amounts, as well as the outcomes of these loans, including whether they were paid back in full, defaulted, or charged off. The data is structured in a tabular format, with rows representing individual loans and columns capturing a range of variables that detail the nature of each loan and the profile of the borrower. Key variables in the dataset include the loan amount, interest rate, term length, borrower credit score, and the purpose of the loan, which can provide insights into lending practices and borrower behavior. The collection methodology for this dataset likely involves aggregating data from the Prosper platform, which facilitates peer-to-peer lending, although specific details on the data sources and collection processes are not provided. Researchers can utilize this dataset to address a variety of research questions, such as examining the factors that contribute to borrower defaults, analyzing the impact of interest rates on loan uptake, and exploring demographic trends in borrowing. The dataset supports various types of analyses, including regression analysis to identify relationships between borrower characteristics and loan outcomes, as well as machine learning techniques to predict loan performance based on historical data. Common preprocessing steps may include handling missing values, normalizing data, and encoding categorical variables to prepare the dataset for analysis. While the dataset provides a wealth of information, researchers should be aware of potential limitations, such as biases in borrower self-reporting and the specific context of the peer-to-peer lending environment, which may not fully represent traditional lending practices. Overall, the Prosper Loan Data serves as a valuable resource for those looking to explore the dynamics of peer-to-peer lending and its implications for the broader financial landscape."
  },
  {
    "name": "Prosper Loans",
    "description": "113K P2P loans with borrower characteristics, credit grades, and loan outcomes. Alternative to LendingClub for P2P lending research",
    "category": "Financial Services",
    "url": "https://www.prosper.com/plp/about/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "P2P lending",
      "loans",
      "credit",
      "fintech"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "financial-services",
      "peer-to-peer-lending",
      "credit-analysis"
    ],
    "summary": "The Prosper Loans dataset contains 113,000 peer-to-peer loans, providing insights into borrower characteristics, credit grades, and loan outcomes. Researchers can utilize this dataset to analyze lending patterns, borrower behavior, and the effectiveness of credit scoring models in the P2P lending space.",
    "use_cases": [
      "Analyzing borrower demographics and their impact on loan performance.",
      "Evaluating the effectiveness of different credit scoring models.",
      "Investigating trends in peer-to-peer lending over time.",
      "Assessing the relationship between loan characteristics and default rates."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the characteristics of borrowers in the Prosper Loans dataset?",
      "How do credit grades influence loan outcomes in peer-to-peer lending?",
      "What trends can be observed in P2P lending over time?",
      "How does the Prosper Loans dataset compare to LendingClub data?",
      "What are the common borrower demographics in the Prosper Loans dataset?",
      "How can machine learning be applied to analyze loan outcomes?",
      "What factors contribute to loan default in P2P lending?",
      "What is the distribution of loan amounts in the Prosper Loans dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/prosper-loans.png",
    "embedding_text": "The Prosper Loans dataset is a comprehensive collection of 113,000 peer-to-peer loans, offering a wealth of information for researchers and analysts interested in the financial services sector, particularly in the realm of P2P lending. This dataset includes various borrower characteristics, credit grades, and loan outcomes, making it an invaluable resource for understanding the dynamics of alternative lending platforms. The data is structured in a tabular format, with each row representing an individual loan and columns detailing key variables such as borrower income, loan amount, interest rate, credit grade, and loan status (e.g., fully paid, defaulted). The dataset's schema allows for a straightforward analysis of relationships between borrower attributes and loan performance metrics. Researchers typically collect this data from the Prosper platform, which aggregates loan information from its users, ensuring a diverse range of borrower profiles and loan types. While the dataset does not specify temporal or geographic coverage, it is assumed to reflect a broad cross-section of borrowers across the United States, given Prosper's operational focus. Key variables within the dataset include borrower credit scores, which measure creditworthiness, and loan amounts, which indicate the financial needs of borrowers. These variables are crucial for conducting analyses related to credit risk and lending decisions. However, researchers should be aware of potential data quality issues, such as missing values or biases in borrower self-reported information. Common preprocessing steps may involve cleaning the data, handling missing values, and normalizing variables for analysis. The dataset supports a variety of analytical approaches, including regression analysis to predict loan outcomes based on borrower characteristics, machine learning techniques for classification tasks, and descriptive statistics to summarize borrower demographics. Researchers often use this dataset to explore questions such as how borrower characteristics influence loan default rates, the effectiveness of credit scoring systems in predicting loan performance, and trends in borrowing behavior over time. Overall, the Prosper Loans dataset serves as a critical resource for understanding the intricacies of peer-to-peer lending and the factors that drive loan performance in this innovative financial landscape."
  },
  {
    "name": "DiDi GAIA Open Data",
    "description": "Billions of GPS points and ride trajectories from China's largest ride-hailing platform. Driver behavior and urban mobility patterns",
    "category": "Transportation & Mobility",
    "url": "https://gaia.didichuxing.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "rideshare",
      "GPS",
      "trajectories",
      "China",
      "DiDi",
      "mobility"
    ],
    "best_for": "Learning transportation & mobility analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "urban mobility",
      "data analysis"
    ],
    "summary": "The DiDi GAIA Open Data dataset consists of billions of GPS points and ride trajectories from DiDi, China's largest ride-hailing service. This dataset can be utilized to analyze driver behavior, urban mobility patterns, and the impact of ridesharing on transportation infrastructure.",
    "use_cases": [
      "Analyzing the impact of ridesharing on traffic congestion",
      "Studying patterns of urban mobility in major Chinese cities",
      "Evaluating driver behavior and its implications for safety",
      "Investigating the relationship between rideshare usage and public transportation"
    ],
    "audience": [
      "Curious-browser",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the DiDi GAIA Open Data dataset?",
      "How can I access GPS points from DiDi's ride-hailing service?",
      "What insights can be derived from analyzing ride trajectories in China?",
      "What are the urban mobility patterns identified in DiDi's data?",
      "How does driver behavior vary across different regions in China?",
      "What are the implications of ridesharing on urban transportation?",
      "How can I use DiDi GAIA data for transportation research?",
      "What types of analyses can be performed with DiDi's ride data?"
    ],
    "domain_tags": [
      "transportation",
      "urban studies",
      "data science"
    ],
    "data_modality": "tabular",
    "geographic_scope": "China",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/logos/didichuxing.png",
    "embedding_text": "The DiDi GAIA Open Data dataset is a comprehensive collection of billions of GPS points and ride trajectories sourced from DiDi, the largest ride-hailing platform in China. This dataset provides a rich resource for researchers and data scientists interested in exploring various aspects of transportation and urban mobility. The data is structured in a tabular format, with rows representing individual ride trajectories and columns containing key variables such as GPS coordinates, timestamps, ride duration, and fare information. The collection methodology involves aggregating data from DiDi's operational systems, ensuring a high volume of data points that reflect real-world usage patterns. The dataset covers a wide geographic scope, specifically focusing on urban areas across China, making it particularly valuable for studies examining the dynamics of transportation in rapidly growing cities.\n\nKey variables in the dataset include GPS coordinates that capture the precise location of rides, timestamps that indicate the start and end times of each ride, and additional metrics such as ride duration and fare amounts. These variables allow researchers to measure and analyze various aspects of driver behavior, including trip frequency, route selection, and time of day effects. However, potential limitations of the dataset include data quality issues related to GPS accuracy, potential biases in ride selection, and the need for preprocessing to handle missing or inconsistent data entries.\n\nCommon preprocessing steps may involve cleaning the data to remove outliers, normalizing GPS coordinates, and aggregating rides by time intervals for analysis. Researchers can leverage this dataset to address a variety of research questions, such as examining the impact of ridesharing on urban traffic patterns, understanding how driver behavior varies across different demographics, and assessing the effectiveness of ridesharing as a complement to public transportation systems.\n\nThe types of analyses supported by the dataset include regression analysis to identify factors influencing ride demand, machine learning techniques for predictive modeling of ride patterns, and descriptive statistics to summarize key trends in urban mobility. Researchers typically use the DiDi GAIA dataset in studies focused on transportation economics, urban planning, and data-driven policy formulation, making it an essential resource for those looking to understand the complexities of modern urban transportation systems."
  },
  {
    "name": "Hashed Multimodal Banking",
    "description": "Banking transactions and product purchases with hashed identifiers",
    "category": "Financial Services",
    "url": "https://github.com/dzhambo/mbd",
    "docs_url": null,
    "github_url": "https://github.com/dzhambo/mbd",
    "tags": [
      "banking",
      "transactions",
      "privacy-preserving"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "financial services",
      "data privacy",
      "transaction analysis"
    ],
    "summary": "The Hashed Multimodal Banking dataset contains banking transactions and product purchases, utilizing hashed identifiers to ensure privacy. Researchers can analyze consumer behavior and transaction patterns while maintaining data confidentiality.",
    "use_cases": [
      "Analyzing consumer spending patterns",
      "Investigating the impact of product purchases on banking behavior",
      "Studying the effectiveness of privacy-preserving techniques in financial datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the privacy-preserving techniques used in banking transactions?",
      "How can hashed identifiers improve data security in financial datasets?",
      "What insights can be derived from analyzing banking transactions?",
      "What are the implications of hashed data in consumer behavior studies?",
      "How do product purchases correlate with banking transactions?",
      "What methods can be used to analyze multimodal banking data?",
      "What trends can be identified in financial services using transaction data?",
      "How does data privacy impact consumer trust in banking?"
    ],
    "domain_tags": [
      "financial services",
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/hashed-multimodal-banking.png",
    "embedding_text": "The Hashed Multimodal Banking dataset is a rich resource for analyzing banking transactions and product purchases while ensuring the privacy of individuals through the use of hashed identifiers. This dataset is structured in a tabular format, consisting of rows representing individual transactions and purchases, with columns detailing various attributes such as transaction amounts, timestamps, product categories, and hashed identifiers. The use of hashed identifiers allows researchers to conduct analyses without compromising the privacy of the individuals involved, making it a valuable asset in the field of financial services research. The collection methodology for this dataset involves aggregating transaction data from banking institutions and retail partners, ensuring a comprehensive coverage of consumer behavior across different sectors. While the specific temporal and geographic coverage is not explicitly mentioned, the dataset is designed to capture a wide range of transaction types and consumer interactions, making it relevant for various demographic groups. Key variables within the dataset include transaction amounts, product categories, and timestamps, which measure consumer spending behavior and purchasing trends. Researchers should be aware of potential limitations regarding data quality, such as missing values or inconsistencies in transaction records, which may require preprocessing steps like data cleaning and normalization before analysis. Common preprocessing steps may include handling missing data, converting timestamps to a standard format, and categorizing product types for easier analysis. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, enabling researchers to explore questions related to consumer behavior, transaction trends, and the effectiveness of privacy-preserving techniques. Researchers typically use this dataset to investigate the relationships between banking transactions and product purchases, assess the impact of privacy measures on consumer trust, and identify trends in financial services that can inform policy and business strategies. Overall, the Hashed Multimodal Banking dataset serves as a crucial tool for understanding the dynamics of consumer behavior in the financial sector while upholding the principles of data privacy."
  },
  {
    "name": "Google BigQuery Crypto",
    "description": "8 complete blockchain histories (Bitcoin, Ethereum, etc.) with daily updates. Transaction-level data for crypto analytics research",
    "category": "Financial Services",
    "url": "https://cloud.google.com/bigquery/public-data",
    "docs_url": "https://cloud.google.com/blog/products/data-analytics/introducing-six-new-cryptocurrencies-in-bigquery-public-datasets",
    "github_url": null,
    "tags": [
      "blockchain",
      "cryptocurrency",
      "Bitcoin",
      "Ethereum",
      "BigQuery"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "blockchain",
      "cryptocurrency",
      "financial services"
    ],
    "summary": "The Google BigQuery Crypto dataset provides comprehensive blockchain histories for major cryptocurrencies such as Bitcoin and Ethereum, updated daily. This dataset enables researchers to conduct in-depth crypto analytics and explore transaction-level data for various analytical purposes.",
    "use_cases": [
      "Analyzing transaction trends in Bitcoin and Ethereum",
      "Conducting research on cryptocurrency market behaviors",
      "Developing predictive models for crypto asset prices"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Google BigQuery Crypto dataset?",
      "How can I access blockchain transaction data?",
      "What cryptocurrencies are included in the Google BigQuery Crypto dataset?",
      "What types of analyses can be performed with crypto transaction data?",
      "How frequently is the Google BigQuery Crypto dataset updated?",
      "What insights can be gained from analyzing Bitcoin and Ethereum transaction histories?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/google-bigquery-crypto.png",
    "embedding_text": "The Google BigQuery Crypto dataset is a rich resource for researchers and analysts interested in the world of cryptocurrencies, particularly Bitcoin and Ethereum. This dataset encompasses complete blockchain histories for these major cryptocurrencies, providing daily updates that ensure the data remains current and relevant for analytical purposes. The structure of the dataset is tabular, consisting of rows that represent individual transactions and columns that capture various attributes of these transactions, such as timestamps, transaction amounts, and involved addresses. The data is sourced directly from the blockchain networks of Bitcoin and Ethereum, ensuring a high level of accuracy and reliability. Researchers can leverage this dataset to explore a wide range of research questions, such as examining transaction trends over time, analyzing the impact of market events on cryptocurrency prices, or investigating the behavior of different user segments within the crypto ecosystem. Typical analyses supported by this dataset include regression analysis, machine learning applications, and descriptive statistics, allowing for a comprehensive understanding of the dynamics within the cryptocurrency markets. However, users should be aware of potential limitations, such as the inherent volatility of cryptocurrency prices and the challenges associated with interpreting blockchain data. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the dataset for analysis. Overall, the Google BigQuery Crypto dataset serves as a foundational tool for those looking to conduct in-depth research in the rapidly evolving field of cryptocurrency."
  },
  {
    "name": "SEC EDGAR Filings",
    "description": "21M+ public company filings since 1994. 10-Ks, 8-Ks, proxy statements. Full text + structured XBRL data",
    "category": "Financial Services",
    "url": "https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent",
    "docs_url": "https://www.sec.gov/os/accessing-edgar-data",
    "github_url": "https://github.com/datasets/edgar",
    "tags": [
      "SEC",
      "corporate filings",
      "10-K",
      "disclosure",
      "large-scale"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "financial-analysis",
      "corporate-governance",
      "regulatory-compliance"
    ],
    "summary": "The SEC EDGAR Filings dataset contains over 21 million public company filings dating back to 1994, including 10-Ks, 8-Ks, and proxy statements. Researchers and analysts can utilize this dataset to conduct financial analysis, assess corporate governance practices, and evaluate regulatory compliance among public companies.",
    "use_cases": [
      "Analyzing trends in corporate financial performance over time.",
      "Evaluating the impact of regulatory changes on corporate disclosures.",
      "Conducting comparative analysis of corporate governance practices.",
      "Assessing the relationship between financial disclosures and stock performance."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest SEC EDGAR filings?",
      "How can I access 10-K reports from public companies?",
      "What types of corporate filings are available in the SEC EDGAR dataset?",
      "How do I analyze proxy statements from the SEC EDGAR database?",
      "What is the historical coverage of SEC filings since 1994?",
      "Where can I find structured XBRL data for public company filings?",
      "What insights can be gained from analyzing SEC EDGAR filings?",
      "How to retrieve large-scale corporate filings data from the SEC EDGAR?"
    ],
    "domain_tags": [
      "financial-services"
    ],
    "data_modality": "mixed",
    "temporal_coverage": "1994-present",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/sec-edgar-filings.png",
    "embedding_text": "The SEC EDGAR Filings dataset is a comprehensive collection of over 21 million public company filings, encompassing a wide array of documents such as 10-Ks, 8-Ks, and proxy statements. This dataset is invaluable for researchers, analysts, and students who are interested in understanding the financial landscape of public companies in the United States. The data is structured in a mixed format, incorporating both full text and structured XBRL data, which allows for detailed analysis and insights. Each filing typically includes key variables such as financial statements, management discussion and analysis, risk factors, and corporate governance disclosures. The collection methodology involves systematic gathering of filings submitted by public companies to the SEC, ensuring that the dataset is up-to-date and comprehensive. The temporal coverage of the dataset extends from 1994 to the present, providing a rich historical context for longitudinal studies. However, while the dataset is extensive, researchers should be aware of potential limitations, such as variations in reporting standards over time and the possibility of incomplete filings. Common preprocessing steps may include data cleaning, normalization of financial figures, and extraction of relevant sections for specific analyses. This dataset supports a variety of research questions, such as examining the relationship between corporate disclosures and market performance, analyzing trends in executive compensation, and assessing the impact of regulatory changes on corporate behavior. The types of analyses that can be conducted using this dataset include regression analysis, machine learning applications, and descriptive statistics. Researchers typically leverage this dataset to gain insights into corporate governance practices, evaluate financial health, and understand compliance with regulatory requirements, making it a cornerstone resource for studies in financial services and corporate governance.",
    "benchmark_usage": [
      "Common uses include financial analysis, compliance assessment, and corporate governance studies."
    ]
  },
  {
    "name": "Computational Neuroscience",
    "description": "Experimental data from neural recordings and behavior",
    "category": "Education",
    "url": "https://crcns.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "neuroscience",
      "neural data",
      "behavior"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Computational Neuroscience dataset contains experimental data derived from neural recordings and associated behavioral observations. Researchers can utilize this dataset to explore the relationships between neural activity and behavior, enabling insights into cognitive processes and neurological conditions.",
    "use_cases": [
      "Analyzing the correlation between neural activity and behavior",
      "Investigating cognitive processes through neural data",
      "Studying neurological conditions using behavioral observations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Computational Neuroscience dataset?",
      "How can I analyze neural recordings?",
      "What behavioral data is included in the dataset?",
      "What insights can be gained from neural data?",
      "Who can use the Computational Neuroscience dataset?",
      "What research questions can be addressed with this dataset?",
      "What methodologies are used in computational neuroscience research?",
      "What are the applications of neural data analysis?"
    ],
    "domain_tags": [
      "healthcare",
      "education"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/crcns.png",
    "embedding_text": "The Computational Neuroscience dataset is a rich collection of experimental data that encompasses neural recordings and behavioral observations, designed to facilitate research in the field of neuroscience. This dataset typically consists of a structured format, where each row represents a unique experimental trial or observation, and the columns include various variables such as neural activity measurements, behavioral responses, timestamps, and possibly participant identifiers. The data is collected through rigorous experimental methodologies, often involving advanced neuroimaging techniques or electrophysiological recordings, ensuring high fidelity in capturing the dynamics of neural processes. Researchers may source this data from controlled laboratory experiments, longitudinal studies, or collaborative research initiatives, which aim to understand the intricate relationships between brain activity and behavior. While the dataset does not specify temporal or geographic coverage, it is generally applicable to a wide range of cognitive and behavioral research questions across diverse populations. Key variables in this dataset may include metrics such as action potentials, local field potentials, reaction times, and accuracy rates, each measuring distinct aspects of neural and behavioral performance. However, researchers should be aware of potential limitations in data quality, such as noise in neural recordings or variability in behavioral responses, which may affect the robustness of analyses. Common preprocessing steps might involve filtering out noise from neural signals, normalizing behavioral data, and aligning timestamps for accurate comparisons. This dataset supports various types of analyses, including regression modeling, machine learning applications, and descriptive statistics, allowing researchers to explore complex interactions between neural and behavioral data. Typical research questions addressed with this dataset include understanding how specific neural patterns correlate with behavioral outcomes, identifying biomarkers for cognitive functions, and examining the effects of experimental manipulations on neural and behavioral responses. Overall, the Computational Neuroscience dataset serves as a valuable resource for researchers aiming to advance our understanding of the neural underpinnings of behavior and cognition."
  },
  {
    "name": "Baidu AI Datasets",
    "description": "AI, NLP, computer vision, and autonomous driving datasets",
    "category": "Data Portals",
    "url": "https://aistudio.baidu.com/datasetoverview",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "Baidu",
      "NLP",
      "computer vision",
      "autonomous"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "AI",
      "NLP",
      "computer vision",
      "autonomous driving"
    ],
    "summary": "The Baidu AI Datasets encompass a variety of datasets focused on artificial intelligence applications, including natural language processing, computer vision, and autonomous driving. Researchers and developers can leverage these datasets to train models, conduct experiments, and enhance AI systems across multiple domains.",
    "use_cases": [
      "Training models for natural language processing tasks",
      "Developing computer vision applications",
      "Conducting research on autonomous driving technologies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What datasets are available for AI research from Baidu?",
      "Where can I find NLP datasets provided by Baidu?",
      "Are there any computer vision datasets from Baidu?",
      "What autonomous driving datasets does Baidu offer?",
      "How can I access Baidu's AI datasets?",
      "What types of data does Baidu provide for AI applications?",
      "Is there a collection of datasets for machine learning on Baidu?"
    ],
    "domain_tags": [
      "technology",
      "transportation"
    ],
    "data_modality": "mixed",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/baidu.png",
    "embedding_text": "The Baidu AI Datasets represent a comprehensive collection of datasets tailored for various artificial intelligence applications, particularly in the fields of natural language processing (NLP), computer vision, and autonomous driving. These datasets are structured to facilitate research and development in AI, providing a rich resource for both academic and industry practitioners. The data structure typically includes rows representing individual data points and columns that delineate various features or attributes pertinent to the datasets. For instance, in NLP datasets, columns may include text data, labels for classification tasks, and metadata such as source or context. In computer vision datasets, images or video frames are often accompanied by annotations that specify object locations, classifications, or other relevant information. The collection methodology for these datasets often involves aggregating data from diverse sources, including web scraping, public repositories, and proprietary databases, ensuring a broad representation of scenarios and use cases. However, researchers should be aware of potential limitations in data quality, such as biases inherent in the source data, incomplete annotations, or variations in data collection methods that may affect the reliability of analyses. Common preprocessing steps may include data cleaning, normalization, and transformation to prepare the datasets for machine learning applications. Researchers can utilize these datasets to address a variety of research questions, such as evaluating the performance of different AI models, exploring the effectiveness of algorithms in real-world scenarios, and conducting comparative studies across different tasks or domains. The datasets support a range of analyses, including regression, machine learning, and descriptive statistics, making them versatile tools for advancing AI research. Typically, researchers leverage these datasets to train and validate models, conduct experiments to refine algorithms, and publish findings that contribute to the broader understanding of AI technologies and their applications."
  },
  {
    "name": "MIMIC-IV",
    "description": "Gold standard for freely accessible critical care EHR data from MIT/Beth Israel. Contains 364,627 unique patients, 546,028 hospitalizations, and 94,458 ICU stays (2008-2022). Includes demographics, vitals, labs, medications, procedures, and clinical notes.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://mimic.mit.edu/",
    "source": "MIT Laboratory for Computational Physiology",
    "type": "EHR Database",
    "access": "Free (PhysioNet credentialing required)",
    "format": "PostgreSQL/CSV",
    "tags": [
      "Healthcare",
      "EHR",
      "ICU",
      "Clinical",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "MIMIC-IV is a comprehensive dataset that provides freely accessible electronic health record (EHR) data from critical care settings. It includes extensive information on patient demographics, vital signs, laboratory results, medications, procedures, and clinical notes, making it a valuable resource for researchers and practitioners in healthcare economics and health technology.",
    "use_cases": [
      "Analyzing patient outcomes in critical care settings",
      "Studying the impact of medications on ICU stays",
      "Exploring demographic trends in critical care admissions",
      "Conducting machine learning analyses on patient data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the MIMIC-IV dataset?",
      "How can I access the MIMIC-IV critical care EHR data?",
      "What types of patient data are included in MIMIC-IV?",
      "What research can be conducted using MIMIC-IV?",
      "How many patients are in the MIMIC-IV dataset?",
      "What time period does the MIMIC-IV dataset cover?",
      "What are the key variables in the MIMIC-IV dataset?",
      "How is the MIMIC-IV dataset structured?"
    ],
    "update_frequency": "Periodic releases",
    "geographic_coverage": "Boston, MA (single hospital)",
    "domain_tags": [
      "healthcare",
      "health-tech"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2008-2022",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/mimic-iv.jpg",
    "embedding_text": "The MIMIC-IV dataset is a rich source of electronic health record (EHR) data specifically designed for research in critical care settings. It encompasses a total of 364,627 unique patients, 546,028 hospitalizations, and 94,458 ICU stays, providing a comprehensive view of patient care from 2008 to 2022. The dataset is structured in a tabular format, featuring numerous rows corresponding to individual patient encounters and columns that represent various clinical variables. Key variables include patient demographics, vital signs, laboratory test results, medication prescriptions, and clinical notes, which together create a detailed picture of patient health and treatment within intensive care units (ICUs). The data is collected from the Beth Israel Deaconess Medical Center in Boston, Massachusetts, and is made available for free, making it an invaluable resource for researchers and healthcare professionals interested in studying critical care practices and outcomes.\n\nThe temporal coverage of the MIMIC-IV dataset spans from 2008 to 2022, allowing researchers to analyze trends and changes in critical care over time. However, the geographic scope is limited to the specific institution from which the data is derived, which may affect the generalizability of findings to other healthcare settings. The dataset includes a diverse range of patients, which enhances its utility for demographic studies and analyses of healthcare disparities.\n\nIn terms of data quality, MIMIC-IV is regarded as a gold standard dataset; however, researchers should be aware of potential limitations, such as missing data in certain variables or variations in data entry practices. Common preprocessing steps may include handling missing values, normalizing data formats, and coding categorical variables for analysis. Researchers can utilize MIMIC-IV for a variety of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it suitable for addressing a wide range of research questions related to patient care, treatment efficacy, and healthcare economics.\n\nTypical research questions that can be explored using the MIMIC-IV dataset include the impact of specific medications on patient outcomes, the relationship between demographic factors and ICU admission rates, and the analysis of vital sign trends among critically ill patients. The dataset supports various analytical approaches, allowing for both exploratory data analysis and hypothesis-driven research. Overall, MIMIC-IV serves as a foundational resource for advancing knowledge in critical care medicine and health technology, facilitating studies that can lead to improved patient care and outcomes."
  },
  {
    "name": "Indian Automobiles (Telangana)",
    "description": "Vehicle sales data for Telangana, India (2023)",
    "category": "Automotive",
    "url": "https://www.kaggle.com/datasets/zubairatha/revving-up-telangana-vehicle-sales-2023",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "vehicles",
      "India",
      "regional sales"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "automotive",
      "sales-analysis",
      "regional-economics"
    ],
    "summary": "This dataset contains vehicle sales data specifically for Telangana, India, in the year 2023. It can be used to analyze trends in automobile sales, understand consumer preferences, and assess the impact of regional economic factors on vehicle purchases.",
    "use_cases": [
      "Analyzing trends in vehicle sales over time",
      "Assessing the impact of economic conditions on automobile purchases",
      "Understanding consumer preferences in the automotive sector",
      "Evaluating the effectiveness of marketing strategies for vehicles"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the vehicle sales trends in Telangana for 2023?",
      "How do regional economic factors influence automobile sales in India?",
      "What types of vehicles are most popular in Telangana?",
      "How can I analyze vehicle sales data for Telangana?",
      "What insights can be drawn from the 2023 vehicle sales data in Telangana?",
      "What is the impact of consumer preferences on vehicle sales in Telangana?",
      "How does vehicle sales in Telangana compare to other regions in India?",
      "What demographic factors affect vehicle sales in Telangana?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2023",
    "geographic_scope": "Telangana, India",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/indian-automobiles-telangana.png",
    "embedding_text": "The Indian Automobiles dataset for Telangana provides a comprehensive view of vehicle sales data for the year 2023. This dataset is structured in a tabular format, consisting of rows and columns that represent various attributes of vehicle sales transactions. Key variables may include the type of vehicle sold, sales volume, price points, and possibly demographic information about the buyers, although specific columns are not detailed in the provided description. The data collection methodology likely involves aggregating sales records from dealerships, manufacturers, or government transportation agencies, ensuring a robust dataset that reflects actual market conditions. However, potential limitations may include data quality issues such as missing entries or inconsistencies in reporting across different sources. Common preprocessing steps might involve cleaning the data to handle missing values, normalizing price data, and categorizing vehicle types for analysis. Researchers can leverage this dataset to address various research questions, such as identifying trends in consumer preferences, analyzing the impact of economic factors on vehicle sales, and evaluating marketing strategies within the automotive sector. The dataset supports a range of analyses, including regression analysis to predict future sales trends, machine learning techniques for classification of vehicle types, and descriptive statistics to summarize sales performance. Overall, this dataset serves as a valuable resource for researchers and analysts interested in the automotive market in Telangana, providing insights that can inform business strategies and economic policies."
  },
  {
    "name": "PEP Experimental Research",
    "description": "Experimental research datasets from Partnership for Economic Policy",
    "category": "Education",
    "url": "https://www.pep-net.org/publications/datasets/experimental-research-datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "development",
      "RCT",
      "policy"
    ],
    "best_for": "Learning education analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The PEP Experimental Research dataset consists of experimental research datasets provided by the Partnership for Economic Policy. It is designed to facilitate analysis and understanding of various economic policies through randomized controlled trials (RCTs). Researchers can utilize this dataset to explore the impacts of specific interventions on economic development.",
    "use_cases": [
      "Analyzing the effectiveness of economic policies",
      "Conducting randomized controlled trials in development economics",
      "Evaluating the impact of specific interventions on target populations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the PEP Experimental Research dataset?",
      "How can I access the PEP Experimental Research datasets?",
      "What types of analyses can be performed with PEP Experimental Research data?",
      "What are the key variables in the PEP Experimental Research dataset?",
      "What is the methodology behind the PEP Experimental Research datasets?",
      "How does the PEP Experimental Research dataset support economic policy analysis?",
      "What are the limitations of the PEP Experimental Research datasets?",
      "What research questions can be addressed using the PEP Experimental Research dataset?"
    ],
    "domain_tags": [
      "education"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The PEP Experimental Research dataset is a comprehensive collection of experimental research datasets curated by the Partnership for Economic Policy, aimed at providing valuable insights into the effectiveness of various economic policies through rigorous analysis. The dataset is structured in a tabular format, consisting of rows and columns that represent individual observations and variables, respectively. Each row corresponds to a unique experimental observation, while the columns capture key variables that measure different aspects of the research, such as treatment effects, demographic information, and outcome measures. The collection methodology involves rigorous randomized controlled trials (RCTs), which are considered the gold standard in experimental research, ensuring that the data collected is reliable and valid for analysis. Researchers typically utilize this dataset to address critical research questions related to the impact of specific interventions on economic development, policy effectiveness, and behavioral responses of individuals or groups to various economic stimuli. The dataset supports a range of analyses, including regression analysis, machine learning applications, and descriptive statistics, allowing researchers to uncover patterns, test hypotheses, and derive meaningful conclusions from the data. However, like any dataset, the PEP Experimental Research dataset has its limitations, including potential biases in sample selection, measurement errors, and the generalizability of findings to broader populations. Common preprocessing steps may include data cleaning, handling missing values, and normalizing variables to prepare the dataset for analysis. Overall, the PEP Experimental Research dataset serves as a crucial resource for researchers in the field of economics, providing a robust foundation for empirical studies aimed at informing policy decisions and enhancing our understanding of economic phenomena."
  },
  {
    "name": "Yandex Datasets",
    "description": "Search ranking, translation quality, and ML task datasets",
    "category": "Data Portals",
    "url": "https://research.yandex.com/datasets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search ranking",
      "translation",
      "ML",
      "Russia"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "search ranking",
      "translation",
      "machine learning"
    ],
    "summary": "Yandex Datasets provide a collection of datasets focused on search ranking, translation quality, and various machine learning tasks. Researchers and practitioners can utilize these datasets to enhance algorithms, improve translation systems, and analyze search engine performance.",
    "use_cases": [
      "Analyzing the effectiveness of search algorithms",
      "Improving machine translation systems",
      "Conducting research on user behavior in search engines"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What datasets does Yandex offer for search ranking?",
      "Where can I find translation quality datasets from Yandex?",
      "Are there any machine learning task datasets available from Yandex?",
      "How can I access Yandex's datasets for research purposes?",
      "What are the key features of Yandex datasets related to ML?",
      "Can I find datasets on search ranking metrics from Yandex?"
    ],
    "domain_tags": [
      "technology",
      "data science"
    ],
    "data_modality": "mixed",
    "geographic_scope": "Russia",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/yandex-datasets.png",
    "embedding_text": "Yandex Datasets encompass a variety of datasets that are particularly valuable for researchers and practitioners in the fields of search ranking, translation quality, and machine learning tasks. The datasets are structured in a tabular format, with rows representing individual data points and columns corresponding to various features and metrics relevant to the tasks at hand. Common variables within these datasets may include search query terms, ranking positions, translation accuracy scores, and other performance indicators that are critical for evaluating search engine effectiveness and translation systems. The collection methodology for these datasets typically involves aggregating data from Yandex's own search engine and translation services, ensuring that the datasets reflect real-world usage and performance metrics. However, users should be aware of potential limitations in data quality, such as biases inherent in user-generated content or variations in data collection methods over time. Preprocessing steps may include cleaning the data to remove duplicates, normalizing text inputs, and transforming categorical variables into numerical formats suitable for analysis. Researchers can leverage Yandex Datasets to address a variety of research questions, such as evaluating the impact of different ranking algorithms on search result relevance or analyzing the effectiveness of translation models across different languages. The datasets support a range of analyses, including regression analysis to predict outcomes based on input features, machine learning techniques for classification and clustering, and descriptive statistics to summarize data characteristics. Overall, Yandex Datasets serve as a rich resource for advancing knowledge in search technologies and machine translation, providing a foundation for innovative research and practical applications in these domains."
  },
  {
    "name": "Makridakis Competitions",
    "description": "Time series data for forecasting competitions (M1-M5)",
    "category": "Data Portals",
    "url": "https://www.mcompetitions.unic.ac.cy/the-dataset/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "time series",
      "forecasting",
      "M competitions"
    ],
    "best_for": "Learning data portals analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "time series analysis"
    ],
    "topic_tags": [
      "forecasting",
      "time series",
      "competitions"
    ],
    "summary": "The Makridakis Competitions dataset contains time series data used for various forecasting competitions, specifically M1 to M5. Researchers and practitioners can utilize this dataset to benchmark forecasting methods and enhance their predictive modeling skills.",
    "use_cases": [
      "Benchmarking forecasting methods",
      "Comparative analysis of time series forecasting techniques",
      "Developing predictive models for time series data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Makridakis Competitions dataset?",
      "How can I access time series data for forecasting?",
      "What are the M competitions in forecasting?",
      "What types of forecasting methods are evaluated in the Makridakis Competitions?",
      "Where can I find datasets for time series analysis?",
      "What are the key features of the Makridakis Competitions dataset?",
      "How to use the Makridakis dataset for machine learning?",
      "What insights can be gained from the M1-M5 competitions?"
    ],
    "domain_tags": [
      "finance",
      "retail",
      "economics"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "model_score": 0.0,
    "embedding_text": "The Makridakis Competitions dataset is a comprehensive collection of time series data specifically designed for evaluating forecasting methods through a series of competitions known as M1 to M5. Each competition presents a unique set of time series data, allowing researchers and practitioners to test and benchmark various forecasting techniques. The dataset is structured in a tabular format, with rows representing individual time series and columns containing key variables that measure different aspects of the data, such as the time index, observed values, and potentially other relevant features. The collection methodology for this dataset involves sourcing time series data from various domains, ensuring a diverse range of applications and contexts for analysis. While the specific details of the data sources are not disclosed, the competitions are well-known in the forecasting community, and the data is curated to provide a robust testing ground for forecasting models.\n\nKey variables in the dataset include the time index, which indicates the temporal aspect of the data, and the observed values, which are the actual measurements taken over time. These variables are crucial for conducting time series analysis, as they allow researchers to explore trends, seasonality, and other temporal patterns. However, like any dataset, the Makridakis Competitions dataset may have limitations in terms of data quality, such as missing values or outliers, which researchers should be aware of when performing their analyses. Common preprocessing steps may include handling missing data, normalizing values, and transforming the data to meet the assumptions of various forecasting models.\n\nResearchers typically use the Makridakis Competitions dataset to address a variety of research questions related to forecasting accuracy and model performance. For instance, they may seek to determine which forecasting methods yield the most accurate predictions across different types of time series data. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for both academic and practical applications in forecasting. Overall, the Makridakis Competitions dataset serves as an essential tool for advancing the field of time series forecasting, providing valuable insights and benchmarks for researchers and practitioners alike.",
    "benchmark_usage": [
      "Evaluating forecasting accuracy",
      "Comparing different forecasting models"
    ]
  },
  {
    "name": "Home Depot Product Search",
    "description": "Human-rated relevance scores (1-3) for search terms and products",
    "category": "E-Commerce",
    "url": "https://www.kaggle.com/datasets/thedevastator/the-home-depot-products-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "search relevance",
      "retail",
      "Kaggle"
    ],
    "best_for": "Practicing customer analytics, demand forecasting, and recommendation systems",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "e-commerce",
      "consumer-behavior",
      "pricing"
    ],
    "summary": "The Home Depot Product Search dataset provides human-rated relevance scores for various search terms and products, allowing researchers to analyze how well products match search queries. This dataset can be utilized to improve search algorithms, enhance product recommendations, and understand consumer behavior in the retail sector.",
    "use_cases": [
      "Analyzing the effectiveness of search algorithms",
      "Improving product recommendation systems",
      "Studying consumer behavior related to search queries",
      "Evaluating the impact of search relevance on sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the relevance scores for search terms related to home improvement?",
      "How do different products rank for the same search query?",
      "What trends can be observed in search relevance over time?",
      "Which search terms yield the highest relevance scores?",
      "How can search relevance scores inform product placement?",
      "What is the relationship between search terms and product categories?",
      "How do relevance scores vary across different consumer demographics?",
      "What are the implications of search relevance on sales performance?"
    ],
    "domain_tags": [
      "retail"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/home-depot-product-search.png",
    "embedding_text": "The Home Depot Product Search dataset is a valuable resource for researchers and practitioners in the e-commerce and retail sectors, providing insight into the relevance of products based on search queries. This dataset consists of human-rated relevance scores ranging from 1 to 3, which indicate how well products align with specific search terms. The data structure is organized in a tabular format, with rows representing individual search queries and corresponding products, while columns include variables such as search term, product ID, and relevance score. The collection methodology likely involves user studies or expert evaluations where participants assess the relevance of products to given search terms, ensuring a human-centered approach to data quality. However, limitations may include potential biases in human ratings and the subjective nature of relevance assessments. Researchers can utilize this dataset to address various research questions, such as the effectiveness of search algorithms, the relationship between search terms and product categories, and consumer behavior trends. Common preprocessing steps may involve normalizing relevance scores, handling missing data, and categorizing products based on their attributes. The dataset supports a range of analyses, including regression analysis to predict relevance scores, machine learning models for classification tasks, and descriptive statistics to summarize search behavior. Overall, this dataset serves as a foundation for enhancing search functionalities, improving user experience, and driving sales in the competitive retail landscape."
  },
  {
    "name": "Russian Car Market",
    "description": "Car sales information in Russia",
    "category": "Automotive",
    "url": "https://www.kaggle.com/datasets/ekibee/car-sales-information",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "cars",
      "Russia",
      "sales"
    ],
    "best_for": "Learning automotive analytics and modeling",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "automotive",
      "market-analysis",
      "sales-trends"
    ],
    "summary": "The Russian Car Market dataset provides comprehensive information on car sales in Russia, detailing various aspects of the automotive market. Researchers and analysts can utilize this data to study sales trends, consumer preferences, and market dynamics within the automotive industry in Russia.",
    "use_cases": [
      "Analyzing sales trends over time",
      "Comparing market performance of different car brands",
      "Studying consumer behavior in car purchases",
      "Evaluating the impact of economic factors on car sales"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the latest car sales trends in Russia?",
      "How do consumer preferences vary in the Russian car market?",
      "What factors influence car sales in Russia?",
      "What are the leading car brands in Russia?",
      "How does the Russian car market compare to other countries?",
      "What is the impact of economic conditions on car sales in Russia?"
    ],
    "domain_tags": [
      "automotive"
    ],
    "data_modality": "tabular",
    "geographic_scope": "Russia",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/russian-car-market.jpg",
    "embedding_text": "The Russian Car Market dataset is a valuable resource for understanding the dynamics of car sales in Russia. It typically consists of a tabular structure with rows representing individual sales transactions and columns capturing various attributes such as car make, model, year of manufacture, sale price, and buyer demographics. The dataset may also include variables that measure sales volume, market share, and seasonal trends. Data collection methodologies often involve aggregating sales data from dealerships, manufacturers, and market research firms, ensuring a comprehensive overview of the automotive landscape in Russia. While the dataset provides rich insights, researchers should be aware of potential limitations, such as data completeness, accuracy, and the influence of external factors like economic fluctuations and regulatory changes. Preprocessing steps may include cleaning data for missing values, normalizing sale prices, and categorizing car models for analysis. The dataset supports a variety of research questions, including inquiries into consumer preferences, brand performance, and the impact of economic conditions on sales. Analysts can employ techniques ranging from descriptive statistics to regression analysis and machine learning models to uncover patterns and predict future trends. Researchers typically use this dataset to inform strategic decisions, guide marketing efforts, and enhance understanding of the automotive market's evolution in response to changing consumer behaviors and economic conditions."
  },
  {
    "name": "National Health Interview Survey (NHIS)",
    "description": "CDC's flagship health survey covering ~35,000 households annually since 1957. Monitors health status, healthcare access, and health behaviors. Free public use files with extensive documentation.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.cdc.gov/nchs/nhis/",
    "source": "CDC National Center for Health Statistics",
    "type": "Survey",
    "access": "Free public use files",
    "format": "SAS/Stata/CSV",
    "tags": [
      "Healthcare",
      "Survey",
      "Health Status",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "healthcare",
      "public health",
      "health economics"
    ],
    "summary": "The National Health Interview Survey (NHIS) is a comprehensive health survey conducted by the CDC that collects data from approximately 35,000 households annually. It provides insights into health status, healthcare access, and health behaviors, making it a valuable resource for researchers and policymakers in the fields of healthcare economics and public health.",
    "use_cases": [
      "Analyzing trends in health behaviors over time",
      "Examining disparities in healthcare access among different demographics",
      "Evaluating the impact of health policies on population health",
      "Investigating the relationship between health status and socioeconomic factors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the National Health Interview Survey?",
      "How can I access the NHIS public use files?",
      "What types of health data does the NHIS collect?",
      "What is the sample size of the NHIS?",
      "How does the NHIS monitor healthcare access?",
      "What are the key variables measured in the NHIS?",
      "What documentation is available for the NHIS dataset?",
      "How can NHIS data be used in health economics research?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (national)",
    "domain_tags": [
      "healthcare",
      "public health",
      "health economics"
    ],
    "data_modality": "tabular",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/national-health-interview-survey-nhis.png",
    "embedding_text": "The National Health Interview Survey (NHIS) is a flagship health survey conducted by the Centers for Disease Control and Prevention (CDC) that has been collecting data since 1957. The NHIS gathers health-related information from approximately 35,000 households each year, making it one of the largest and most comprehensive health surveys in the United States. The dataset is structured in a tabular format, containing numerous rows corresponding to individual respondents and columns representing various health-related variables. Key variables include health status indicators, healthcare access metrics, and health behavior assessments, which provide a multifaceted view of the health landscape in the U.S. The collection methodology involves face-to-face interviews conducted by trained interviewers, ensuring high-quality data collection. The NHIS employs a stratified sampling design to ensure that the sample is representative of the U.S. population, capturing diverse demographic groups across different geographic regions. However, researchers should be aware of potential limitations, such as self-reported data biases and the challenges of longitudinal analysis due to changes in survey design over time. Common preprocessing steps for NHIS data may include handling missing values, recoding categorical variables, and merging datasets for longitudinal studies. The NHIS data supports a wide range of analyses, including descriptive statistics, regression modeling, and machine learning applications. Researchers often utilize this dataset to address critical questions in public health, such as the prevalence of chronic conditions, the effectiveness of health interventions, and the socio-economic determinants of health outcomes. The extensive documentation accompanying the NHIS public use files provides valuable guidance for researchers in navigating the dataset and leveraging its rich information for impactful studies in healthcare economics and public health policy."
  },
  {
    "name": "LendingClub Loans",
    "description": "2.7M loans (2007-2019) with 151 features. Interest rates, credit scores, defaults. The canonical P2P lending dataset for credit risk modeling",
    "category": "Financial Services",
    "url": "https://www.kaggle.com/datasets/wordsforthewise/lending-club",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "P2P lending",
      "credit risk",
      "loans",
      "defaults",
      "fintech"
    ],
    "best_for": "Learning financial services analytics and modeling",
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "financial-services",
      "data-analysis",
      "credit-risk"
    ],
    "summary": "The LendingClub Loans dataset consists of 2.7 million loans issued between 2007 and 2019, featuring 151 attributes that include interest rates, credit scores, and default statuses. This dataset serves as a canonical resource for credit risk modeling and analysis in the peer-to-peer lending sector.",
    "use_cases": [
      "Analyzing the impact of credit scores on loan defaults.",
      "Building predictive models for loan performance.",
      "Investigating trends in peer-to-peer lending over time.",
      "Evaluating the effectiveness of different interest rate strategies."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What are the interest rates in the LendingClub Loans dataset?",
      "How can I analyze default rates using the LendingClub Loans dataset?",
      "What features are included in the LendingClub Loans dataset?",
      "How does credit score affect loan performance in the LendingClub Loans dataset?",
      "What trends can be observed in peer-to-peer lending from 2007 to 2019?",
      "How can I use the LendingClub Loans dataset for machine learning?",
      "What is the size of the LendingClub Loans dataset?",
      "What methodologies are used to collect data in the LendingClub Loans dataset?"
    ],
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2007-2019",
    "size_category": "massive",
    "model_score": 0.0,
    "image_url": "/images/datasets/lendingclub-loans.jpg",
    "embedding_text": "The LendingClub Loans dataset is a comprehensive collection of 2.7 million loans issued from 2007 to 2019, encompassing a wide range of variables that are crucial for understanding credit risk in the peer-to-peer lending landscape. This dataset includes 151 features, such as interest rates, credit scores, loan amounts, and default statuses, making it an invaluable resource for researchers and data scientists interested in financial services and risk assessment. The data is structured in a tabular format, with each row representing an individual loan and each column corresponding to a specific attribute related to that loan. Key variables include the interest rate, which indicates the cost of borrowing; the credit score, which reflects the borrower's creditworthiness; and the default status, which shows whether the loan was paid back as agreed or if it went into default. The collection methodology for this dataset involves aggregating loan data from LendingClub's platform, where users can apply for loans and investors can fund them. This process ensures that the dataset is rich in real-world financial transactions, providing a realistic view of lending practices and borrower behavior. The temporal coverage of the dataset spans over a decade, allowing for longitudinal analysis of trends in lending and borrowing behavior, as well as shifts in economic conditions that may influence loan performance. However, it is important to note that while the dataset is extensive, it may have limitations in terms of data quality, such as missing values or inconsistencies in borrower reporting. Common preprocessing steps include handling missing data, normalizing numerical features, and encoding categorical variables to prepare the dataset for analysis. Researchers can utilize this dataset to address a variety of research questions, such as the relationship between credit scores and loan defaults, the impact of interest rates on borrower behavior, and the effectiveness of different lending strategies over time. The dataset supports various types of analyses, including regression analysis, machine learning modeling, and descriptive statistics, making it a versatile tool for financial analysis. Typically, researchers leverage the LendingClub Loans dataset to build predictive models that forecast loan performance, assess credit risk, and inform lending practices in the fintech industry.",
    "benchmark_usage": [
      "Credit risk modeling",
      "Loan performance analysis"
    ]
  },
  {
    "name": "Behavioral Risk Factor Surveillance System (BRFSS)",
    "description": "World's largest ongoing health survey with 400,000+ adults annually across all states. Covers chronic conditions, risk behaviors, and preventive health. State-level estimates available.",
    "category": "Healthcare Economics & Health-Tech",
    "url": "https://www.cdc.gov/brfss/",
    "source": "CDC",
    "type": "Survey",
    "access": "Free public use files",
    "format": "SAS/ASCII",
    "tags": [
      "Healthcare",
      "Survey",
      "Behavioral",
      "State-level",
      "Free"
    ],
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Behavioral Risk Factor Surveillance System (BRFSS) is the world's largest ongoing health survey, collecting data from over 400,000 adults annually across all states. It covers a wide range of topics including chronic conditions, risk behaviors, and preventive health measures, providing valuable state-level estimates for researchers and policymakers.",
    "use_cases": [
      "Analyzing trends in chronic health conditions across different states.",
      "Examining the relationship between risk behaviors and health outcomes.",
      "Evaluating the effectiveness of preventive health measures at the state level."
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Behavioral Risk Factor Surveillance System?",
      "How can I access the BRFSS dataset?",
      "What types of health behaviors does the BRFSS survey cover?",
      "What are the state-level estimates available in the BRFSS?",
      "How many adults participate in the BRFSS annually?",
      "What chronic conditions are included in the BRFSS data?",
      "How is the BRFSS data collected?",
      "What are the key variables measured in the BRFSS?"
    ],
    "update_frequency": "Annual",
    "geographic_coverage": "United States (state-level)",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "geographic_scope": "United States",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/logos/cdc.png",
    "embedding_text": "The Behavioral Risk Factor Surveillance System (BRFSS) is a comprehensive health survey that serves as a critical resource for understanding health-related behaviors and conditions among adults in the United States. Conducted annually, the BRFSS collects data from over 400,000 adults, making it the largest ongoing health survey globally. The dataset is structured in a tabular format, with rows representing individual survey respondents and columns capturing various health-related variables. Key variables include information on chronic health conditions such as diabetes, hypertension, and obesity, as well as risk behaviors like smoking, alcohol consumption, and physical inactivity. Additionally, the survey gathers data on preventive health measures, including vaccination status and health screenings. The collection methodology involves telephone interviews, which may include both landline and mobile phone respondents, ensuring a diverse demographic representation across different states. This methodology allows for state-level estimates, enabling researchers to analyze health trends and disparities across geographic regions. While the BRFSS provides a wealth of information, researchers should be aware of potential limitations, such as response bias and the reliance on self-reported data, which may affect the accuracy of certain health behaviors. Common preprocessing steps include cleaning the data for missing values, recoding categorical variables, and ensuring the dataset is representative of the population. Researchers utilize the BRFSS data to address various research questions, such as identifying the prevalence of specific health conditions, exploring the impact of socioeconomic factors on health behaviors, and assessing the effectiveness of public health interventions. The dataset supports a range of analyses, including regression modeling, machine learning applications, and descriptive statistics, making it a versatile tool for health economists, public health officials, and data scientists. Overall, the BRFSS is an invaluable dataset that informs health policy and research, providing insights into the health behaviors and conditions of the adult population in the United States."
  },
  {
    "name": "Statcast / Baseball Savant",
    "description": "MLB's official tracking data including pitch velocity, spin rate, exit velocity, launch angle, and player positioning from 2015-present",
    "category": "Sports & Athletics",
    "url": "https://baseballsavant.mlb.com/",
    "docs_url": "https://baseballsavant.mlb.com/csv-docs",
    "github_url": null,
    "tags": [
      "baseball",
      "tracking-data",
      "MLB",
      "pitch-tracking",
      "batted-ball"
    ],
    "best_for": "Modern baseball analytics, player evaluation, and biomechanical analysis",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Statcast / Baseball Savant dataset provides comprehensive tracking data from Major League Baseball (MLB) games, encompassing metrics such as pitch velocity, spin rate, exit velocity, launch angle, and player positioning. This dataset enables analysts and researchers to evaluate player performance, understand game dynamics, and enhance predictive modeling in baseball analytics.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is Statcast data in baseball?",
      "How to access MLB tracking data?",
      "What metrics are included in Baseball Savant?",
      "How can I analyze pitch velocity and spin rate?",
      "What are the applications of launch angle data?",
      "Where can I find player positioning data for MLB?",
      "What is the significance of exit velocity in baseball analytics?",
      "How has Statcast data changed baseball analysis?"
    ],
    "use_cases": [
      "Analyzing player performance trends over multiple seasons",
      "Developing predictive models for player outcomes based on pitch data",
      "Evaluating the effectiveness of different batting techniques",
      "Comparing player statistics across different teams and seasons"
    ],
    "domain_tags": [
      "sports",
      "analytics"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2015-present",
    "size_category": "medium",
    "model_score": 0.0,
    "image_url": "/images/datasets/statcast-baseball-savant.png",
    "embedding_text": "The Statcast / Baseball Savant dataset is a rich repository of tracking data from Major League Baseball (MLB), capturing a wide array of metrics that are crucial for understanding the intricacies of the game. This dataset includes detailed information on pitch velocity, spin rate, exit velocity, launch angle, and player positioning, making it an invaluable resource for analysts, coaches, and researchers alike. The data is structured in a tabular format, with each row representing a unique event or play from a game, and columns detailing various metrics and attributes associated with that event. Key variables include pitch characteristics such as velocity and spin rate, which are essential for evaluating pitcher performance, as well as batted-ball metrics like exit velocity and launch angle that provide insights into hitting effectiveness. Player positioning data adds another layer of depth, allowing for analysis of defensive strategies and player movements on the field. The collection methodology for this dataset relies on advanced tracking technology used during MLB games, which captures high-resolution data in real-time. This technology ensures a high level of accuracy and detail, although it is important to note that data quality may vary based on the specific game conditions and the technology used. Researchers utilizing this dataset often engage in common preprocessing steps such as data cleaning, normalization, and feature engineering to prepare the data for analysis. The dataset supports a variety of analytical approaches, including regression analysis, machine learning models, and descriptive statistics, enabling users to explore research questions related to player performance, game strategy, and overall team effectiveness. Typical research questions might include inquiries into how specific pitching techniques affect batting outcomes or how player positioning impacts defensive success. Overall, the Statcast / Baseball Savant dataset serves as a cornerstone for modern baseball analytics, providing the necessary data to drive insights and enhance understanding of the game."
  },
  {
    "name": "LOBSTER (Limit Order Book System)",
    "description": "High-frequency limit order book tick data for NASDAQ stocks reconstructed from ITCH messages. Gold standard for market microstructure research.",
    "category": "Financial Services",
    "url": "https://lobsterdata.com/",
    "docs_url": "https://lobsterdata.com/info/DataStructure.php",
    "github_url": null,
    "tags": [
      "high-frequency",
      "limit-order-book",
      "NASDAQ",
      "tick-data",
      "market-microstructure"
    ],
    "best_for": "Market microstructure research, algorithmic trading backtesting, LOB simulation validation",
    "model_score": 0.0,
    "difficulty": "intermediate",
    "prerequisites": [
      "pandas-dataframes",
      "time-series-analysis"
    ],
    "topic_tags": [
      "financial-markets",
      "algorithmic-trading"
    ],
    "summary": "LOBSTER (Limit Order Book System) provides high-frequency limit order book tick data for NASDAQ stocks, reconstructed from ITCH messages. This dataset serves as a gold standard for market microstructure research, allowing researchers to analyze trading behaviors and market dynamics in detail.",
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the LOBSTER dataset?",
      "How can I access high-frequency limit order book data?",
      "What are the applications of NASDAQ tick data?",
      "How is market microstructure analyzed using LOBSTER?",
      "What variables are included in the LOBSTER dataset?",
      "How does LOBSTER reconstruct data from ITCH messages?",
      "What research questions can be addressed with limit order book data?",
      "What preprocessing is needed for high-frequency trading data?"
    ],
    "use_cases": [
      "Analyzing the impact of order book dynamics on price formation.",
      "Studying the behavior of market participants in high-frequency trading.",
      "Investigating liquidity and its effects on market efficiency.",
      "Evaluating trading strategies based on limit order book data."
    ],
    "embedding_text": "The LOBSTER (Limit Order Book System) dataset is a comprehensive collection of high-frequency limit order book tick data specifically for NASDAQ stocks, reconstructed from ITCH messages. This dataset is widely recognized as a gold standard for market microstructure research, providing invaluable insights into the intricacies of trading behavior and market dynamics. The data structure consists of rows representing individual tick data points, with columns that include variables such as timestamp, price, volume, order type, and order status. Each entry captures the state of the limit order book at a specific moment, allowing for detailed temporal analysis of market activities.\n\nThe collection methodology involves the reconstruction of data from ITCH messages, which are the raw feeds from NASDAQ that contain detailed information about orders, cancellations, and trades. This meticulous process ensures that the dataset reflects the actual trading environment, making it a reliable resource for researchers and analysts. The dataset's coverage is primarily focused on the NASDAQ exchange, and while it does not specify temporal or geographic limitations, it is understood to encompass a wide range of trading activities across various stocks listed on NASDAQ.\n\nKey variables within the LOBSTER dataset measure critical aspects of market behavior, including the price levels at which orders are placed, the volume of orders, and the types of orders (e.g., market or limit orders). These variables are essential for analyzing how liquidity is provided in the market and how it affects price movements. However, researchers should be aware of potential limitations in data quality, such as missing data points or discrepancies that may arise from the reconstruction process. Common preprocessing steps include filtering out irrelevant data, handling missing values, and normalizing the data for analysis.\n\nResearchers typically use the LOBSTER dataset to address a variety of research questions related to market microstructure. These questions may involve examining the relationship between order book dynamics and price volatility, understanding the behavior of different market participants, or evaluating the effectiveness of trading strategies in high-frequency trading environments. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile tool for both academic and practical applications in finance.\n\nIn summary, the LOBSTER dataset stands out as a premier resource for those interested in the detailed workings of financial markets, particularly in the context of high-frequency trading and market microstructure. Its rich data structure, combined with the rigorous methodology of data collection, provides a robust foundation for conducting advanced research and analysis in the field of finance.",
    "domain_tags": [
      "fintech"
    ],
    "data_modality": "time-series",
    "size_category": "medium",
    "benchmark_usage": [
      "Commonly used in market microstructure research"
    ]
  },
  {
    "name": "Call Centre Queue Simulation",
    "description": "One year of simulated call center data with arrival times, handle times, and outcomes. Ideal for estimating arrival rates and validating Erlang-C staffing models.",
    "category": "Operations & Service",
    "url": "https://www.kaggle.com/datasets/donovanbangs/call-centre-queue-simulation",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "call-center",
      "simulation",
      "Erlang-C",
      "staffing",
      "Kaggle"
    ],
    "best_for": "Queueing theory - estimate arrival rates \u03bb(t), fit service time distributions, validate Erlang-C formulas",
    "image_url": "/images/datasets/call-centre-queue-simulation.png",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes"
    ],
    "topic_tags": [],
    "summary": "The Call Centre Queue Simulation dataset contains one year of simulated call center data, including arrival times, handle times, and outcomes. This dataset is particularly useful for estimating arrival rates and validating Erlang-C staffing models, making it a valuable resource for operations research and service management studies.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the arrival rate in the call center simulation?",
      "How can I validate Erlang-C staffing models using simulated data?",
      "What are the average handle times in the call center dataset?",
      "How does call volume vary over the year in the simulation?",
      "What outcomes are measured in the call center simulation?",
      "How can I analyze queueing behavior from this dataset?",
      "What insights can be drawn from simulated call center data?",
      "What statistical methods can be applied to this call center dataset?"
    ],
    "use_cases": [
      "Estimating call arrival rates",
      "Validating staffing models using Erlang-C",
      "Analyzing call handling efficiency",
      "Simulating different staffing scenarios"
    ],
    "embedding_text": "The Call Centre Queue Simulation dataset is a comprehensive collection of simulated call center data designed to aid researchers and practitioners in the field of operations and service management. This dataset encompasses one year of data, capturing critical variables such as arrival times, handle times, and outcomes associated with call center operations. The data is structured in a tabular format, with rows representing individual calls and columns detailing various attributes of each call, including timestamps for arrivals and completions, duration of calls, and the outcomes of each interaction. This structured approach allows for straightforward analysis and manipulation using tools like pandas in Python. The collection methodology employed in generating this dataset involves simulation techniques that mimic real-world call center dynamics, ensuring that the data reflects realistic patterns of call arrivals and service times. While the dataset does not specify geographic or demographic coverage, it is designed to be applicable across various call center environments, making it versatile for different research contexts. Key variables within the dataset include arrival times, which measure the timing of incoming calls, handle times that quantify the duration of each call, and outcomes that categorize the results of the calls, such as successful resolutions or escalations. These variables are essential for understanding call center performance and operational efficiency. However, as with any simulated data, there are inherent limitations regarding the accuracy of the simulation compared to real-world scenarios. Researchers should be cautious about generalizing findings from this dataset to actual call centers without considering the nuances of real operational environments. Common preprocessing steps may include cleaning the data to handle any anomalies or inconsistencies, transforming timestamps into more analyzable formats, and aggregating data to derive insights at different time intervals. The dataset supports a variety of analyses, including regression modeling to predict call handling times, machine learning approaches to classify call outcomes, and descriptive statistics to summarize call center performance metrics. Researchers typically utilize this dataset to address questions related to staffing efficiency, customer wait times, and overall service quality in call centers. By leveraging the insights gained from this dataset, organizations can make informed decisions about staffing levels, operational improvements, and customer service strategies, ultimately enhancing their service delivery and operational effectiveness.",
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "null",
    "geographic_scope": "null",
    "size_category": "medium",
    "benchmark_usage": [
      "Estimating arrival rates",
      "Validating Erlang-C staffing models"
    ]
  },
  {
    "name": "Call Center Data",
    "description": "Daily call center performance metrics including call volumes, handle times, and agent performance.",
    "category": "Operations & Service",
    "url": "https://www.kaggle.com/datasets/satvicoder/call-center-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "call-center",
      "performance-metrics",
      "operations",
      "Kaggle"
    ],
    "best_for": "Queueing theory - analyze daily patterns, agent utilization, service level optimization",
    "image_url": "/images/datasets/call-center-data.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Call Center Data provides daily performance metrics from a call center, including key indicators such as call volumes, handle times, and agent performance. This dataset can be utilized to analyze operational efficiency, improve customer service strategies, and enhance agent training programs.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the daily call volumes in the call center dataset?",
      "How can I analyze agent performance using call center data?",
      "What metrics are included in the call center performance dataset?",
      "How do handle times vary across different agents?",
      "What insights can be drawn from call center performance metrics?",
      "How can I visualize call center data for better understanding?",
      "What statistical methods can be applied to call center performance data?",
      "Where can I find datasets related to call center operations?"
    ],
    "use_cases": [
      "Analyzing trends in call volumes over time.",
      "Evaluating the impact of handle times on customer satisfaction.",
      "Comparing agent performance to identify training needs.",
      "Optimizing call routing based on performance metrics."
    ],
    "embedding_text": "The Call Center Data is structured in a tabular format, consisting of rows and columns that capture various performance metrics relevant to call center operations. Each row typically represents a daily record, while columns include variables such as call volumes, average handle times, and individual agent performance metrics. This dataset is invaluable for organizations looking to enhance their operational efficiency and customer service quality. The collection methodology for this dataset involves systematic recording of call center activities, likely sourced from internal call management systems or customer relationship management (CRM) platforms. As such, it reflects real-time operational data, providing insights into daily performance trends. However, it is essential to note that while the dataset may offer a comprehensive overview of call center operations, it may also have limitations regarding data quality, such as missing values or inaccuracies in agent performance reporting. Common preprocessing steps for this dataset may include handling missing data, normalizing metrics for comparative analysis, and aggregating data to derive insights over specific time periods. Researchers can leverage this dataset to address various research questions, such as identifying factors that influence call handling efficiency, understanding the relationship between call volumes and customer satisfaction, and evaluating the effectiveness of training programs for agents. The types of analyses supported by this dataset range from descriptive statistics to more complex regression analyses and machine learning applications aimed at predicting performance outcomes. Typically, researchers utilize this dataset in studies focused on operational improvements, customer experience enhancements, and workforce management strategies, making it a critical resource for both academic and practical applications in the field of operations and service management.",
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Queue Waiting Time Prediction",
    "description": "Call center data with arrival time, service start/end times, waiting time, and queue length. Perfect for validating queueing theory formulas.",
    "category": "Operations & Service",
    "url": "https://www.kaggle.com/datasets/sanjeebtiwary/queue-waiting-time-prediction",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "waiting-time",
      "service-times",
      "queue-length",
      "Erlang-C",
      "Kaggle"
    ],
    "best_for": "Queueing theory - direct Erlang-C validation, compare predicted vs actual wait times",
    "image_url": "/images/datasets/queue-waiting-time-prediction.jpg",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [],
    "summary": "The Queue Waiting Time Prediction dataset contains call center data that includes variables such as arrival time, service start and end times, waiting time, and queue length. This dataset is ideal for validating queueing theory formulas and can be used to analyze service efficiency and customer experience in call centers.",
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What is the average waiting time in a call center?",
      "How does queue length affect service times?",
      "What patterns can be identified in call center arrival times?",
      "How can queueing theory be validated using real data?",
      "What is the relationship between service start times and customer satisfaction?",
      "How can machine learning be applied to predict waiting times?",
      "What are the peak hours for call center traffic?",
      "How does the Erlang-C model apply to this dataset?"
    ],
    "use_cases": [
      "Analyzing the impact of arrival times on waiting times",
      "Validating queueing theory formulas",
      "Predicting service times based on historical data",
      "Assessing the efficiency of call center operations"
    ],
    "embedding_text": "The Queue Waiting Time Prediction dataset is a valuable resource for researchers and practitioners interested in the dynamics of call center operations. This dataset is structured in a tabular format, comprising rows that represent individual call instances and columns that capture key variables such as arrival time, service start time, service end time, waiting time, and queue length. Each row provides insights into the operational efficiency of the call center, allowing for detailed analysis of customer wait times and service delivery. The collection methodology for this dataset typically involves logging call center interactions, where data is captured in real-time or through batch processing at regular intervals. Sources may include internal call center management systems or external datasets available from platforms like Kaggle. The coverage of this dataset is not explicitly mentioned in terms of temporal or geographic scope, indicating that it may represent a snapshot of operations over a specific period or a generalized dataset applicable to various call centers. Key variables in this dataset include arrival time, which measures when a customer initiates contact; service start time and service end time, which indicate the duration of service; waiting time, which reflects the time a customer spends in the queue before being attended to; and queue length, which provides insights into the volume of customers awaiting service. Understanding these variables is crucial for analyzing call center performance and customer satisfaction. Data quality is an important consideration, as inaccuracies in logging times or service metrics can affect the reliability of analyses. Common preprocessing steps may include handling missing values, normalizing time formats, and aggregating data to derive meaningful insights. Researchers can utilize this dataset to address various research questions, such as the impact of queue length on waiting times, the effectiveness of service strategies, and the validation of queueing theory models like Erlang-C. The dataset supports a range of analyses, including regression analysis to predict waiting times based on historical data, machine learning applications for forecasting service demand, and descriptive statistics to summarize operational performance. In studies, researchers typically use this dataset to explore patterns in call center operations, validate theoretical models, and develop strategies for improving service efficiency and customer experience.",
    "domain_tags": [
      "operations",
      "service"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "ER Wait Time",
    "description": "Simulated emergency room data with wait times, patient outcomes, and satisfaction scores for healthcare queueing analysis.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/rivalytics/er-wait-time",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "emergency-room",
      "wait-times",
      "healthcare",
      "simulation",
      "Kaggle"
    ],
    "best_for": "Queueing theory - ER capacity planning, priority queue modeling, patient flow optimization",
    "image_url": "/images/datasets/er-wait-time.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The ER Wait Time dataset provides simulated emergency room data that includes essential metrics such as wait times, patient outcomes, and satisfaction scores. This dataset can be utilized for healthcare queueing analysis, allowing researchers and practitioners to explore factors affecting patient flow and satisfaction in emergency care settings.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the average wait times in emergency rooms?",
      "How do patient outcomes correlate with wait times?",
      "What factors influence patient satisfaction in emergency care?",
      "How can simulation data improve healthcare queueing?",
      "What trends can be observed in emergency room wait times?",
      "How does patient flow impact emergency room efficiency?",
      "What are the implications of wait times on patient care?",
      "How can data analysis optimize emergency room operations?"
    ],
    "use_cases": [
      "Analyzing the impact of wait times on patient satisfaction",
      "Simulating emergency room scenarios to improve patient outcomes",
      "Evaluating the efficiency of healthcare queueing systems",
      "Identifying trends in emergency room usage over time"
    ],
    "embedding_text": "The ER Wait Time dataset is a comprehensive collection of simulated emergency room data designed for healthcare queueing analysis. This dataset encompasses various dimensions of emergency care, including critical metrics such as wait times, patient outcomes, and satisfaction scores. The structure of the dataset is primarily tabular, comprising rows that represent individual patient encounters and columns that capture key variables such as wait time, treatment duration, patient demographics, and satisfaction ratings. Each variable is meticulously crafted to reflect realistic scenarios encountered in emergency departments, making the dataset a valuable resource for both academic research and practical applications in healthcare management.\n\nThe collection methodology for this dataset involves simulation techniques that generate synthetic data based on established healthcare models. This approach allows for the creation of a diverse range of scenarios, reflecting different patient flows and outcomes without compromising patient privacy or requiring real-world data access. The dataset is particularly useful for researchers interested in understanding the dynamics of emergency room operations and the factors that influence patient experiences.\n\nKey variables in the dataset include wait times, which measure the duration patients spend waiting for treatment, and satisfaction scores, which gauge patient perceptions of their care experience. Other relevant variables may include demographic information such as age, gender, and presenting complaints, which can help analysts identify trends and disparities in care. The quality of the data is generally high due to its simulated nature, although users should be aware of the limitations inherent in synthetic datasets, such as the potential lack of real-world variability and the need for careful interpretation of results.\n\nCommon preprocessing steps for this dataset may involve cleaning the data to handle any inconsistencies, normalizing variables for comparative analysis, and possibly transforming variables to meet the assumptions of specific statistical tests. Researchers can leverage this dataset to address a variety of research questions, such as examining the relationship between wait times and patient satisfaction, evaluating the effectiveness of different triage protocols, or exploring how demographic factors influence patient outcomes in emergency settings.\n\nThe types of analyses supported by the ER Wait Time dataset are diverse, ranging from descriptive statistics that summarize the data to more complex regression analyses and machine learning models that predict outcomes based on input variables. Researchers typically use this dataset in studies aimed at improving emergency department efficiency, enhancing patient care quality, and informing policy decisions related to healthcare resource allocation. By utilizing the insights gained from this dataset, healthcare practitioners can make informed decisions that ultimately lead to better patient experiences and outcomes in emergency care.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Hospital Emergency Dataset",
    "description": "Patient care and operational efficiency data from hospital emergency departments for healthcare operations research.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/xavierberge/hospital-emergency-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "emergency",
      "hospital",
      "operations",
      "patient-flow",
      "Kaggle"
    ],
    "best_for": "Queueing theory - bed requirements, treatment time distributions, boarding delay analysis",
    "image_url": "/images/datasets/hospital-emergency-dataset.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Hospital Emergency Dataset contains patient care and operational efficiency data from hospital emergency departments, which can be utilized for healthcare operations research. Researchers can analyze this data to improve patient flow, optimize resource allocation, and enhance overall operational efficiency in emergency departments.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Hospital Emergency Dataset?",
      "How can I access the Hospital Emergency Dataset?",
      "What variables are included in the Hospital Emergency Dataset?",
      "What analyses can be performed with the Hospital Emergency Dataset?",
      "Is there a Kaggle version of the Hospital Emergency Dataset?",
      "What insights can be gained from the Hospital Emergency Dataset?",
      "How does the Hospital Emergency Dataset improve healthcare operations?",
      "What are the key metrics in the Hospital Emergency Dataset?"
    ],
    "use_cases": [
      "Analyzing patient wait times in emergency departments",
      "Evaluating the impact of staffing levels on patient outcomes",
      "Optimizing the patient flow process in hospitals",
      "Assessing operational efficiency in emergency care settings"
    ],
    "embedding_text": "The Hospital Emergency Dataset is a comprehensive collection of data focused on patient care and operational efficiency within hospital emergency departments. This dataset is structured in a tabular format, consisting of various rows and columns that represent different patient encounters and operational metrics. Key variables may include patient demographics, time stamps for arrival and treatment, wait times, treatment outcomes, and resource utilization metrics. The data is typically collected through hospital information systems, which record patient interactions and operational processes in real-time. Researchers can leverage this dataset to explore a variety of research questions, such as identifying factors that contribute to long wait times, evaluating the effectiveness of different treatment protocols, and assessing how variations in staffing levels impact patient outcomes. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing time stamps for analysis, and categorizing qualitative variables for statistical modeling. The dataset supports various types of analyses, including regression analysis to determine the relationships between variables, machine learning techniques for predictive modeling, and descriptive statistics to summarize patient care trends. Researchers often utilize this dataset to inform healthcare policy decisions, improve operational workflows, and enhance the overall quality of emergency care services. However, it is essential to acknowledge potential limitations in data quality, such as inconsistencies in data entry or variations in reporting practices across different hospitals. Overall, the Hospital Emergency Dataset serves as a valuable resource for healthcare operations research, providing insights that can lead to improved patient care and operational efficiency in emergency departments.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Hospital Triage and Patient History",
    "description": "Triage data with timestamps and patient history for modeling priority queues in emergency medicine.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/maalona/hospital-triage-and-patient-history-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "triage",
      "patient-flow",
      "timestamps",
      "priority-queue",
      "Kaggle"
    ],
    "best_for": "Queueing theory - priority queue modeling (high acuity preempts), arrival rate estimation by acuity",
    "image_url": "/images/datasets/hospital-triage-and-patient-history.jpg",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Hospital Triage and Patient History dataset contains triage data with timestamps and patient history, which can be utilized for modeling priority queues in emergency medicine. This dataset allows researchers and practitioners to analyze patient flow and optimize triage processes, ultimately improving emergency care efficiency.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "What is the Hospital Triage and Patient History dataset?",
      "How can I analyze triage data for emergency medicine?",
      "What variables are included in the triage dataset?",
      "How does patient history impact triage decisions?",
      "What are the timestamps in the triage dataset used for?",
      "How can I model priority queues using this dataset?",
      "What insights can be gained from analyzing patient flow in hospitals?",
      "Where can I find datasets related to healthcare triage?"
    ],
    "use_cases": [
      "Analyzing patient flow in emergency departments",
      "Modeling triage processes to improve response times",
      "Evaluating the impact of patient history on triage decisions"
    ],
    "embedding_text": "The Hospital Triage and Patient History dataset is a structured collection of data that captures essential information regarding patient triage in emergency medical settings. This dataset typically consists of rows representing individual patient encounters and columns detailing various attributes such as timestamps, patient demographics, medical history, and triage assessments. The data structure is designed to facilitate the analysis of patient flow and the efficiency of triage processes, making it an invaluable resource for healthcare researchers and practitioners. The collection methodology for this dataset may involve gathering data from hospital records, electronic health systems, or direct observations in emergency departments. It is crucial to ensure that the data is anonymized to protect patient privacy while maintaining the integrity of the information. Coverage in terms of temporal aspects is not explicitly mentioned, but the dataset likely includes data collected over a specific period, capturing variations in patient flow and triage decisions over time. Geographic coverage is also not specified, indicating that the dataset may be applicable to various healthcare settings without being limited to a particular region. Key variables in the dataset include timestamps that indicate when patients arrived, were triaged, and received treatment, as well as demographic information such as age, gender, and medical history. These variables are essential for measuring the efficiency of triage processes and understanding how different factors influence patient outcomes. Data quality is a critical consideration, as inaccuracies in patient records or missing data can impact the reliability of analyses. Researchers may need to perform common preprocessing steps, such as cleaning the data, handling missing values, and normalizing timestamps to ensure that the dataset is suitable for analysis. This dataset can address several research questions, such as how patient demographics affect triage decisions, the relationship between wait times and patient outcomes, and the effectiveness of different triage protocols. It supports various types of analyses, including regression modeling to identify predictors of patient flow, machine learning techniques for predictive analytics, and descriptive statistics to summarize patient characteristics and triage patterns. Researchers typically use this dataset in studies aimed at improving emergency care processes, optimizing resource allocation in hospitals, and enhancing overall patient outcomes in emergency medical settings. By leveraging the insights gained from this dataset, healthcare professionals can make informed decisions that lead to better triage practices and improved patient care.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Synthetic Patient Wait Time Records",
    "description": "Hospital wait time records for analyzing patient flow and queue dynamics in healthcare settings.",
    "category": "Healthcare",
    "url": "https://www.kaggle.com/datasets/bharathreddybollu/hospital-wait-time-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "wait-times",
      "hospital",
      "synthetic",
      "patient-flow",
      "Kaggle"
    ],
    "best_for": "Queueing theory - wait time prediction, capacity planning for target service levels",
    "image_url": "/images/datasets/synthetic-patient-wait-time-records.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Synthetic Patient Wait Time Records dataset contains hospital wait time records designed for analyzing patient flow and queue dynamics in healthcare settings. Researchers can utilize this dataset to explore various aspects of patient wait times, identify bottlenecks in healthcare delivery, and improve overall patient experience.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "hospital wait time dataset",
      "synthetic patient flow records",
      "analyze queue dynamics in healthcare",
      "patient wait time analysis",
      "Kaggle healthcare datasets",
      "hospital queueing data",
      "synthetic data for patient wait times"
    ],
    "use_cases": [
      "Analyzing patient wait times to identify trends",
      "Optimizing hospital resource allocation",
      "Studying the impact of wait times on patient satisfaction"
    ],
    "embedding_text": "The Synthetic Patient Wait Time Records dataset is a structured collection of hospital wait time records aimed at providing insights into patient flow and queue dynamics within healthcare environments. This dataset typically comprises rows representing individual patient visits and columns detailing various attributes such as wait times, timestamps, and potentially other relevant metrics. The data is synthesized to mimic real-world scenarios, allowing for robust analysis without compromising patient privacy. The collection methodology involves generating synthetic data that reflects realistic wait time distributions and patterns observed in actual healthcare settings. This approach ensures that researchers can engage with the data without ethical concerns associated with using real patient information. The dataset is designed to support a variety of analyses, including regression analysis, machine learning applications, and descriptive statistics. Key variables within the dataset may include wait time duration, service type, patient demographics, and timestamps, all of which are crucial for understanding patient flow dynamics. However, as with any synthetic dataset, there are limitations regarding the accuracy of the representation of real-world conditions, and researchers should be aware of potential discrepancies. Common preprocessing steps might involve cleaning the data, handling missing values, and normalizing wait times for comparative analysis. Researchers can leverage this dataset to address critical questions such as how wait times influence patient satisfaction, what factors contribute to increased wait times, and how different hospital departments manage patient flow. Overall, the Synthetic Patient Wait Time Records dataset serves as a valuable resource for healthcare analysts and data scientists interested in improving patient care through data-driven insights.",
    "domain_tags": [
      "healthcare"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Server Logs",
    "description": "Apache server logs with timestamps and response times for modeling web server queues.",
    "category": "Technology & Infrastructure",
    "url": "https://www.kaggle.com/datasets/vishnu0399/server-logs",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "server-logs",
      "response-time",
      "web",
      "Apache",
      "Kaggle"
    ],
    "best_for": "Queueing theory - estimate request arrival rates, M/M/c or M/G/1 modeling, SLA planning",
    "image_url": "/images/datasets/server-logs.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Server Logs dataset consists of Apache server logs that include timestamps and response times, making it suitable for modeling web server queues. This dataset can be utilized to analyze server performance, optimize response times, and improve overall web service efficiency.",
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What are Apache server logs?",
      "How can I analyze response times from server logs?",
      "What insights can be gained from web server queue modeling?",
      "How to preprocess Apache server logs for analysis?",
      "What tools are useful for analyzing server logs?",
      "How to visualize response times from server logs?",
      "What are common issues in server log data analysis?",
      "How do timestamps in server logs affect performance analysis?"
    ],
    "use_cases": [
      "Analyzing server performance metrics",
      "Optimizing web service response times",
      "Modeling web server queues",
      "Identifying traffic patterns in web usage"
    ],
    "embedding_text": "The Server Logs dataset is a collection of Apache server logs that provide detailed insights into web server performance. Each log entry typically includes a timestamp, the response time for requests, and various other metrics that are crucial for understanding server behavior under different loads. The data structure is primarily tabular, with rows representing individual log entries and columns capturing key variables such as request type, response time, IP address, and user agent. This dataset is invaluable for researchers and data scientists interested in modeling web server queues, as it allows for the examination of how servers respond to varying levels of traffic and the efficiency of request handling. \n\nThe collection methodology for this dataset involves capturing logs directly from Apache web servers, which are widely used in the industry. These logs are generated automatically by the server software and can be configured to include a variety of information depending on the server's settings. The primary data source is the Apache server itself, which records each request made to the server along with the corresponding response times. \n\nWhile the dataset does not explicitly mention temporal or geographic coverage, it is typically understood that server logs can span a wide range of time frames depending on the server's configuration and usage patterns. Similarly, the geographic scope is often broad, as web servers can serve users from various locations worldwide. However, without specific details, these aspects remain undefined. \n\nKey variables in the dataset include timestamps, which measure when requests were made, and response times, which indicate how long it took for the server to respond to each request. These variables are critical for analyzing server performance and identifying potential bottlenecks in web service delivery. Data quality can vary based on server configuration and logging practices; common limitations include missing data, inconsistent log formats, and the potential for noise in the data due to bot traffic or other anomalies. \n\nCommon preprocessing steps for this dataset may include cleaning the data to remove incomplete or erroneous entries, converting timestamps into a standard format, and aggregating response times to analyze performance trends over time. Researchers can address various research questions using this dataset, such as how server response times vary with different levels of traffic, what patterns emerge in user requests, and how to optimize server configurations for better performance. \n\nThe types of analyses supported by this dataset include regression analysis to model the relationship between traffic and response times, machine learning techniques for predictive modeling, and descriptive statistics to summarize key performance metrics. Researchers typically use this dataset in studies focused on improving web service efficiency, understanding user behavior, and optimizing server resource allocation. Overall, the Server Logs dataset is a rich resource for anyone looking to delve into the intricacies of web server performance and response dynamics.",
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Web Log Dataset",
    "description": "Web server traffic logs for analyzing request patterns and server queue dynamics.",
    "category": "Technology & Infrastructure",
    "url": "https://www.kaggle.com/datasets/shawon10/web-log-dataset",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "web-logs",
      "traffic",
      "requests",
      "server",
      "Kaggle"
    ],
    "best_for": "Queueing theory - traffic analysis, identify bursts deviating from Poisson, capacity planning",
    "image_url": "/images/datasets/web-log-dataset.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Web Log Dataset consists of web server traffic logs that capture request patterns and server queue dynamics. This dataset can be utilized to analyze user behavior, optimize server performance, and improve web application efficiency.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the patterns in web server traffic logs?",
      "How can I analyze request patterns using web logs?",
      "What insights can be gained from server queue dynamics?",
      "How do web server logs help in understanding user behavior?",
      "What tools can I use to analyze web log datasets?",
      "How to visualize web traffic data?",
      "What are common metrics derived from web server logs?",
      "How can web logs be used to improve server performance?"
    ],
    "use_cases": [
      "Analyzing user request patterns",
      "Optimizing server queue dynamics",
      "Improving web application performance"
    ],
    "embedding_text": "The Web Log Dataset is a collection of web server traffic logs designed to facilitate the analysis of request patterns and server queue dynamics. This dataset typically consists of rows representing individual requests made to a web server, with columns capturing various attributes such as timestamps, request types, response times, and user identifiers. The data structure allows for comprehensive analysis of how users interact with web applications, providing insights into peak usage times, common request types, and potential bottlenecks in server performance. The collection methodology for this dataset often involves logging mechanisms integrated into web servers, which capture detailed information about each request processed. This data can originate from various sources, including commercial websites, educational institutions, or public web services, depending on the specific use case. Coverage in terms of temporal and geographic aspects is generally broad, as web logs can span multiple time periods and user demographics, but specific details are not always explicitly mentioned in the dataset description. Key variables within the dataset may include request timestamps, HTTP methods (GET, POST), response codes, and user agent strings, each measuring different aspects of web traffic and user engagement. However, researchers should be aware of potential data quality issues, such as incomplete logs or inconsistencies in logging formats, which can affect the reliability of analyses. Common preprocessing steps for this dataset may involve cleaning the data to remove erroneous entries, aggregating requests by time intervals, and transforming categorical variables for analysis. Researchers typically leverage this dataset to address a variety of research questions, such as identifying trends in user behavior, assessing server load during peak times, and evaluating the effectiveness of web application features. The types of analyses supported by this dataset include regression analysis to predict server response times based on traffic patterns, machine learning techniques for clustering user behavior, and descriptive statistics to summarize key metrics of web traffic. Overall, the Web Log Dataset serves as a valuable resource for data scientists and researchers aiming to enhance their understanding of web traffic dynamics and improve the performance of web applications.",
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Synthetic Distributed System Logs",
    "description": "Log data from distributed systems showing request patterns and processing times across multiple nodes.",
    "category": "Technology & Infrastructure",
    "url": "https://www.kaggle.com/datasets/shubhampatil1999/synthetic-log-data-of-distributed-system",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "distributed-systems",
      "logs",
      "synthetic",
      "microservices",
      "Kaggle"
    ],
    "best_for": "Queueing theory - network of queues, load balancing analysis, service time estimation",
    "image_url": "/images/datasets/synthetic-distributed-system-logs.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The Synthetic Distributed System Logs dataset contains log data from distributed systems, capturing request patterns and processing times across multiple nodes. This dataset is useful for analyzing system performance, understanding request handling in microservices, and simulating distributed system behavior.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "synthetic distributed system logs dataset",
      "distributed systems log analysis",
      "request patterns in microservices",
      "processing times in distributed systems",
      "queueing analysis in logs",
      "Kaggle synthetic logs dataset"
    ],
    "use_cases": [
      "Analyzing request patterns in distributed systems",
      "Evaluating processing times across multiple nodes",
      "Simulating load and performance in microservices"
    ],
    "embedding_text": "The Synthetic Distributed System Logs dataset is a comprehensive collection of log data generated from distributed systems, specifically designed to illustrate request patterns and processing times across various nodes. This dataset is structured in a tabular format, consisting of multiple rows and columns that capture essential variables related to system operations. Each row typically represents a unique log entry, while the columns may include variables such as timestamp, request type, processing time, node identifier, and status codes. The collection methodology for this dataset involves simulating a distributed system environment where synthetic logs are generated to mimic real-world scenarios. This approach allows researchers and practitioners to analyze system behavior without the constraints of using sensitive or proprietary data. The dataset does not specify temporal or geographic coverage, making it versatile for various analytical applications. Key variables within the dataset measure critical aspects of distributed system performance, such as the time taken to process requests, the frequency of different request types, and the overall health of the nodes involved. However, users should be aware of potential limitations in data quality, as synthetic data may not capture all the complexities and anomalies present in real-world logs. Common preprocessing steps that may be necessary include cleaning the data to remove any inconsistencies, normalizing processing times for comparative analysis, and transforming categorical variables into a suitable format for machine learning algorithms. Researchers can leverage this dataset to address a variety of research questions, such as how request patterns change under different load conditions, the impact of node failures on processing times, and the effectiveness of load balancing strategies in distributed systems. The dataset supports various types of analyses, including regression analysis to identify relationships between variables, machine learning for predictive modeling, and descriptive statistics to summarize key performance metrics. Typically, researchers utilize this dataset in studies focused on optimizing distributed system architectures, enhancing microservice performance, and developing strategies for efficient resource allocation in cloud environments.",
    "domain_tags": [
      "technology",
      "infrastructure"
    ],
    "data_modality": "tabular",
    "size_category": "medium"
  },
  {
    "name": "Disney California Adventure Wait Times",
    "description": "Historical ride wait time data from Disney California Adventure for theme park queueing analysis.",
    "category": "Entertainment & Media",
    "url": "https://www.kaggle.com/datasets/tivory27/disney-california-adventure-wait-times",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "theme-park",
      "wait-times",
      "Disney",
      "attractions",
      "Kaggle"
    ],
    "best_for": "Queueing theory - back out arrival rates from wait times, FastPass priority queue modeling",
    "image_url": "/images/datasets/disney-california-adventure-wait-times.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "This dataset contains historical ride wait time data from Disney California Adventure, which can be utilized for theme park queueing analysis. Researchers and analysts can use this data to understand visitor patterns, optimize ride operations, and enhance the overall guest experience at the park.",
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Disney California Adventure wait times dataset",
      "historical theme park queueing data",
      "ride wait time analysis Disney",
      "Disney attractions wait times",
      "Kaggle Disney California Adventure dataset",
      "theme park analytics dataset",
      "Disney wait times historical data"
    ],
    "use_cases": [
      "Analyzing peak wait times for rides",
      "Optimizing staffing and operations based on wait times",
      "Studying visitor behavior patterns",
      "Improving guest experience through data-driven insights"
    ],
    "embedding_text": "The Disney California Adventure Wait Times dataset provides a comprehensive collection of historical ride wait time data, specifically designed for theme park queueing analysis. This dataset is structured in a tabular format, where each row represents a specific time point during the operational hours of the park, and columns include variables such as ride names, wait times, timestamps, and possibly other contextual information related to the rides. The collection methodology for this dataset typically involves systematic data gathering from the park's operational systems or third-party aggregators that track real-time wait times. This data is crucial for understanding the dynamics of visitor flow and ride popularity throughout the day. While the dataset does not explicitly mention temporal coverage, it is implied that the data spans multiple operational days or seasons, allowing for longitudinal analysis of wait times. The geographic scope is limited to the Disney California Adventure park located in Anaheim, California, making it highly relevant for studies focused on this specific location. Key variables in the dataset include ride names, which identify the attractions, and wait times, which measure the duration guests spend in line for each ride. These variables are essential for analyzing visitor behavior and operational efficiency. However, researchers should be aware of potential limitations in data quality, such as missing values during peak operational hours or discrepancies in reported wait times due to varying data collection methods. Common preprocessing steps may include cleaning the data to handle missing values, normalizing wait times for comparative analysis, and aggregating data to analyze trends over time. This dataset can address various research questions, such as identifying peak wait times for specific rides, understanding the impact of special events on visitor flow, and optimizing ride operations based on historical data. Analysts can employ various types of analyses, including regression analysis to predict wait times based on historical trends, machine learning models to classify visitor patterns, and descriptive statistics to summarize wait time distributions. Researchers typically use this dataset in studies aimed at enhancing the guest experience, improving operational efficiency, and informing strategic decisions related to ride management and park operations.",
    "domain_tags": [
      "entertainment",
      "media"
    ],
    "data_modality": "time-series",
    "size_category": "medium"
  },
  {
    "name": "TouringPlans Disney World Data",
    "description": "Posted vs actual wait times for Disney World attractions from 2012-present. Premium dataset for validating queue estimates.",
    "category": "Entertainment & Media",
    "url": "https://touringplans.com/walt-disney-world/crowd-calendar#DataSets",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "theme-park",
      "wait-times",
      "Disney",
      "posted-vs-actual"
    ],
    "best_for": "Queueing theory - compare posted vs actual wait times, optimal touring plan optimization",
    "image_url": "/images/logos/touringplans.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "data-analysis"
    ],
    "summary": "The TouringPlans Disney World Data provides insights into the wait times for Disney World attractions, comparing posted wait times against actual wait times from 2012 to the present. This dataset is valuable for analyzing queue estimates and understanding visitor behavior in theme parks.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the actual wait times for Disney World attractions?",
      "How do posted wait times compare to actual wait times at Disney World?",
      "What trends can be observed in Disney World wait times from 2012 to present?",
      "How can I validate queue estimates using Disney World data?",
      "What is the average wait time for attractions at Disney World?",
      "Are there specific times of year when wait times are longer at Disney World?",
      "What factors influence the differences between posted and actual wait times at Disney World?"
    ],
    "use_cases": [
      "Analyzing the accuracy of posted wait times versus actual wait times.",
      "Studying visitor behavior and patterns in attraction queues.",
      "Evaluating the impact of seasonal events on wait times."
    ],
    "embedding_text": "The TouringPlans Disney World Data is a comprehensive dataset that captures the dynamics of wait times at Disney World attractions, providing a rich resource for researchers and analysts interested in theme park operations and visitor experiences. The dataset spans from 2012 to the present, offering a longitudinal view of how wait times fluctuate over time. This temporal coverage allows for the analysis of trends and patterns in visitor behavior, particularly in relation to posted versus actual wait times. The data structure is organized in a tabular format, consisting of rows that represent individual observations of wait times for various attractions, with columns detailing key variables such as attraction name, posted wait time, actual wait time, date, and time of day. These variables are crucial for understanding the discrepancies between expected and real-world experiences, and they enable researchers to perform a variety of analyses. The collection methodology involves aggregating data from visitor reports and possibly automated tracking systems within the park, ensuring a robust dataset that reflects real-time conditions. However, it is important to note that data quality may vary based on the volume of reports and the accuracy of self-reported wait times. Common preprocessing steps may include cleaning the data to handle missing values, normalizing wait times for comparative analysis, and segmenting the data by time of year or day of the week to identify patterns. Researchers can leverage this dataset to address a range of questions, such as the effectiveness of queue management strategies, the impact of special events on wait times, and the overall visitor experience at Disney World. The dataset supports various types of analyses, including regression analysis to model wait time predictions, machine learning techniques for clustering visitor patterns, and descriptive statistics to summarize wait time distributions. By utilizing the TouringPlans Disney World Data, analysts can gain valuable insights into the operational efficiency of theme parks and the factors that influence guest satisfaction.",
    "domain_tags": [
      "entertainment"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2012-present",
    "geographic_scope": "Disney World",
    "size_category": "medium"
  },
  {
    "name": "Queue-Times API",
    "description": "Real-time wait time data API for 80+ theme parks worldwide. Live queueing data for attractions.",
    "category": "Entertainment & Media",
    "url": "https://queue-times.com/en-US/pages/api",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "theme-park",
      "wait-times",
      "API",
      "real-time",
      "global"
    ],
    "best_for": "Queueing theory - real-time queue monitoring, cross-park comparison, demand forecasting",
    "image_url": "/images/logos/queue-times.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "entertainment",
      "data-analysis",
      "real-time-data"
    ],
    "summary": "The Queue-Times API provides real-time wait time data for over 80 theme parks worldwide, enabling users to access live queueing data for various attractions. This dataset can be utilized for analyzing visitor patterns, optimizing park operations, and enhancing user experience through informed decision-making.",
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What is the Queue-Times API?",
      "How can I access real-time wait time data for theme parks?",
      "What attractions are covered by the Queue-Times API?",
      "How does the Queue-Times API collect data?",
      "What are the benefits of using the Queue-Times API for park operations?",
      "Can I use the Queue-Times API for data analysis?",
      "What are the key features of the Queue-Times API?",
      "How accurate is the data provided by the Queue-Times API?"
    ],
    "use_cases": [
      "Analyzing visitor wait times to improve park operations.",
      "Studying the impact of wait times on visitor satisfaction.",
      "Optimizing attraction management based on real-time data.",
      "Forecasting visitor patterns based on historical wait time data."
    ],
    "embedding_text": "The Queue-Times API is a specialized data source that provides real-time wait time information for over 80 theme parks around the globe. This dataset is structured in a time-series format, where each entry typically includes variables such as park name, attraction name, current wait time, and timestamp. The API collects data through a combination of user inputs and automated monitoring systems, ensuring that the information is as accurate and up-to-date as possible. While the exact data collection methodology is proprietary, it generally involves aggregating data from various sources, including user submissions and park management systems. The coverage of this dataset is global, encompassing theme parks from different continents and regions, making it a valuable resource for researchers and analysts interested in the entertainment and tourism sectors. Key variables in the dataset include the wait time for each attraction, which measures the duration visitors are expected to wait in line, and the timestamp, which indicates when the data was recorded. These variables allow for in-depth analysis of visitor behavior and operational efficiency. However, users should be aware of potential limitations, such as data inaccuracies that can arise from fluctuating visitor numbers and varying reporting practices across different parks. Common preprocessing steps may include cleaning the data to remove outliers, normalizing wait times for comparative analysis, and aggregating data over specific time intervals for trend analysis. Researchers can leverage this dataset to address various research questions, such as how wait times affect visitor satisfaction, the correlation between wait times and park attendance, and the effectiveness of crowd management strategies. The dataset supports a range of analytical techniques, including regression analysis to identify trends, machine learning for predictive modeling, and descriptive statistics for summarizing visitor experiences. In studies, researchers typically use the Queue-Times API data to inform operational decisions, enhance visitor experiences, and contribute to the broader understanding of consumer behavior in the entertainment industry.",
    "domain_tags": [
      "entertainment",
      "tourism"
    ],
    "data_modality": "time-series",
    "geographic_scope": "global",
    "size_category": "medium"
  },
  {
    "name": "2015 Flight Delays and Cancellations",
    "description": "5M+ US flights from 2015 with departure/arrival times and delay information. Model runways as single-server queues.",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/usdot/flight-delays",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "flights",
      "delays",
      "aviation",
      "DOT",
      "Kaggle"
    ],
    "best_for": "Queueing theory - runway queueing, delay propagation cascades, system stability analysis",
    "image_url": "/images/datasets/2015-flight-delays-and-cancellations.jpg",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "basic-statistics"
    ],
    "topic_tags": [
      "transportation",
      "data-analysis",
      "queueing-theory"
    ],
    "summary": "The 2015 Flight Delays and Cancellations dataset contains information on over 5 million flights in the United States, including departure and arrival times as well as delay information. This dataset can be utilized to model airport runways as single-server queues, analyze flight patterns, and study the factors contributing to delays and cancellations.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the causes of flight delays in 2015?",
      "How can we model airport runways as queues?",
      "What is the average delay time for US flights in 2015?",
      "How many flights were canceled in 2015?",
      "What patterns can be observed in flight delays across different airlines?",
      "How do weather conditions affect flight delays?",
      "What is the distribution of flight delays throughout the year?",
      "How do delays vary by airport?"
    ],
    "use_cases": [
      "Analyzing the impact of weather on flight delays",
      "Modeling airport operations using queueing theory",
      "Identifying trends in flight cancellations",
      "Evaluating airline performance based on delay metrics"
    ],
    "embedding_text": "The 2015 Flight Delays and Cancellations dataset is a comprehensive collection of data pertaining to over 5 million flights that occurred in the United States during the year 2015. This dataset is structured in a tabular format, with each row representing a unique flight and columns containing various attributes such as flight number, departure and arrival times, delay durations, cancellation status, and other relevant variables. Key variables include the scheduled departure and arrival times, actual departure and arrival times, and the total delay time, which allows for a thorough analysis of flight performance and operational efficiency. The data is sourced from the U.S. Department of Transportation (DOT) and is made available through platforms such as Kaggle, ensuring a high level of credibility and reliability. The collection methodology involves aggregating flight data from various airlines and airports across the country, providing a rich dataset that reflects the complexities of air travel in the U.S. during that year. Researchers and analysts can utilize this dataset to address a variety of research questions, such as identifying the primary causes of flight delays, understanding the impact of different factors like weather and airport congestion on flight performance, and evaluating the effectiveness of airline operations. The dataset supports various types of analyses, including regression analysis, machine learning applications, and descriptive statistics, making it a versatile resource for data scientists and transportation researchers alike. However, it is important to note that the dataset may have limitations, such as missing values or inconsistencies in reporting, which may require common preprocessing steps like data cleaning and normalization before conducting analyses. Overall, the 2015 Flight Delays and Cancellations dataset serves as a valuable tool for understanding the dynamics of air travel and improving operational strategies within the aviation industry.",
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2015",
    "geographic_scope": "United States",
    "size_category": "massive",
    "benchmark_usage": [
      "Modeling runways as single-server queues",
      "Analyzing flight delay patterns"
    ]
  },
  {
    "name": "Flight Delay Dataset 2018-2022",
    "description": "Recent flight delay data covering 2018-2022 for analyzing aviation queueing and delay patterns.",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/robikscube/flight-delay-dataset-20182022",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "flights",
      "delays",
      "aviation",
      "recent",
      "Kaggle"
    ],
    "best_for": "Queueing theory - recent delay patterns, airport congestion analysis, network effects",
    "image_url": "/images/datasets/flight-delay-dataset-2018-2022.png",
    "difficulty": "beginner",
    "prerequisites": [
      "pandas-dataframes",
      "regression-analysis"
    ],
    "topic_tags": [
      "transportation",
      "aviation",
      "data-analysis"
    ],
    "summary": "The Flight Delay Dataset 2018-2022 provides recent data on flight delays from 2018 to 2022, enabling users to analyze patterns in aviation queueing and delays. This dataset is particularly useful for researchers and analysts interested in understanding the factors contributing to flight delays and improving operational efficiency in the aviation sector.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the flight delay patterns from 2018 to 2022?",
      "How can I analyze aviation queueing using flight delay data?",
      "What factors contribute to flight delays in recent years?",
      "Where can I find recent flight delay data for analysis?",
      "What insights can be gained from the Flight Delay Dataset?",
      "How do delays vary by airline and time of year?",
      "What is the impact of weather on flight delays?",
      "How can I visualize flight delay trends over time?"
    ],
    "use_cases": [
      "Analyzing the impact of weather on flight delays",
      "Identifying trends in aviation delays over time",
      "Evaluating the effectiveness of airline policies on reducing delays",
      "Studying the correlation between flight schedules and delay occurrences"
    ],
    "embedding_text": "The Flight Delay Dataset 2018-2022 is a comprehensive collection of recent flight delay data that spans five years, from 2018 to 2022. This dataset is structured in a tabular format, consisting of rows and columns that represent individual flight records and various attributes associated with each flight. Key variables typically include flight identifiers, departure and arrival times, delay durations, causes of delays, and additional contextual information such as airline, flight route, and aircraft type. The data is collected from various aviation authorities and airline databases, ensuring a robust and reliable source for analysis. Researchers and analysts can leverage this dataset to explore a multitude of research questions, such as the factors influencing flight delays, the impact of seasonal variations on aviation performance, and the effectiveness of different airlines in managing delays. Common preprocessing steps may involve cleaning the data to handle missing values, normalizing time formats, and filtering records to focus on specific airlines or time periods. The dataset supports a variety of analytical approaches, including descriptive statistics to summarize delay patterns, regression analysis to identify predictors of delays, and machine learning techniques to build predictive models. Researchers often utilize this dataset to inform policy decisions, enhance operational efficiencies, and contribute to the broader understanding of aviation dynamics. However, it is important to acknowledge potential limitations in data quality, such as discrepancies in reporting standards across airlines and the influence of external factors like weather that may not be fully captured in the dataset. Overall, the Flight Delay Dataset 2018-2022 serves as a valuable resource for those interested in the intricacies of flight operations and the ongoing challenges faced by the aviation industry.",
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2018-2022",
    "size_category": "medium"
  },
  {
    "name": "Flight Delay and Cancellation 2019-2023",
    "description": "5 years of flight delay and cancellation data including COVID-era disruptions for aviation operations research.",
    "category": "Transportation & Mobility",
    "url": "https://www.kaggle.com/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "queueing",
      "flights",
      "delays",
      "cancellations",
      "COVID",
      "Kaggle"
    ],
    "best_for": "Queueing theory - system recovery analysis, capacity constraints, arrival rate instability",
    "image_url": "/images/datasets/flight-delay-and-cancellation-2019-2023.png",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "transportation",
      "aviation",
      "data-analysis"
    ],
    "summary": "The Flight Delay and Cancellation dataset provides comprehensive data on flight delays and cancellations from 2019 to 2023, including disruptions caused by the COVID-19 pandemic. Researchers can utilize this dataset to analyze trends in aviation performance, assess the impact of external factors on flight operations, and develop predictive models for future delays and cancellations.",
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "What are the trends in flight delays from 2019 to 2023?",
      "How did COVID-19 affect flight cancellations?",
      "What factors contribute to flight delays in the aviation industry?",
      "Can machine learning predict flight cancellations based on historical data?",
      "What is the average delay time for flights during peak travel seasons?",
      "How do weather conditions impact flight delays and cancellations?"
    ],
    "use_cases": [
      "Analyzing the impact of COVID-19 on flight operations.",
      "Developing predictive models for flight delays based on historical data.",
      "Assessing the effectiveness of airline policies on reducing cancellations."
    ],
    "embedding_text": "The Flight Delay and Cancellation dataset spans five years, from 2019 to 2023, and encompasses a wide range of variables related to flight operations, specifically delays and cancellations. The dataset is structured in a tabular format, with rows representing individual flight records and columns capturing key variables such as flight number, departure and arrival times, delay durations, cancellation status, and the reasons for delays or cancellations. This rich data structure allows for a detailed analysis of flight performance over time. The data is collected from various aviation authorities and airlines, ensuring a comprehensive coverage of flight operations during both normal and disrupted periods, particularly during the COVID-19 pandemic, which had a significant impact on air travel. Key variables in the dataset include the scheduled departure and arrival times, actual departure and arrival times, delay durations, and cancellation indicators. These variables enable researchers to measure the punctuality of flights, identify patterns in delays, and understand the factors contributing to cancellations. However, like any dataset, it has its limitations; for instance, there may be inconsistencies in reporting practices among airlines, and certain external factors, such as weather conditions or air traffic control decisions, may not be fully captured. Common preprocessing steps for this dataset may include handling missing values, normalizing time formats, and categorizing delay reasons for more straightforward analysis. Researchers can leverage this dataset to address various research questions, such as the impact of external factors on flight delays, trends in cancellations over time, and the effectiveness of airline policies aimed at improving on-time performance. The dataset supports a range of analyses, including regression analysis to identify predictors of delays, machine learning models for predictive analytics, and descriptive statistics to summarize flight performance trends. Typically, researchers utilize this dataset in studies focused on operational efficiency in the aviation industry, the economic implications of flight disruptions, and the broader impact of external events, such as pandemics, on air travel behavior.",
    "domain_tags": [
      "transportation",
      "aviation"
    ],
    "data_modality": "tabular",
    "temporal_coverage": "2019-2023",
    "size_category": "medium"
  },
  {
    "name": "NYC EMS Incident Dispatch Data",
    "description": "4.83M+ ambulance responses with second-level timestamps for dispatch, en route, and on-scene times. Ideal for M/M/c modeling of ambulance fleets.",
    "category": "Healthcare",
    "url": "https://data.cityofnewyork.us/Public-Safety/EMS-Incident-Dispatch-Data/76xm-jjuj",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "EMS",
      "ambulance",
      "emergency",
      "dispatch",
      "NYC",
      "open-data"
    ],
    "best_for": "Ambulance response time modeling, geographic pooling analysis, fleet optimization"
  },
  {
    "name": "NEMSIS Public Release Research Dataset",
    "description": "49M+ EMS activations nationally with minute-level timestamps. The definitive dataset for US emergency medical services research.",
    "category": "Healthcare",
    "url": "https://nemsis.org/using-ems-data/request-research-data/",
    "docs_url": "https://nemsis.org/technical-resources/version-3/version-3-data-dictionaries/",
    "github_url": null,
    "tags": [
      "EMS",
      "emergency",
      "ambulance",
      "national",
      "healthcare"
    ],
    "best_for": "National EMS patterns, response time benchmarking, capacity planning",
    "image_url": "/images/datasets/nemsis-public-release-research-dataset.jpg"
  },
  {
    "name": "TfL Unified API",
    "description": "Transport for London real-time and historical transit data including arrivals, departures, and service disruptions across all modes.",
    "category": "Transportation & Mobility",
    "url": "https://api.tfl.gov.uk/",
    "docs_url": "https://api-portal.tfl.gov.uk/docs",
    "github_url": null,
    "tags": [
      "transit",
      "London",
      "API",
      "real-time",
      "public-transport"
    ],
    "best_for": "Platform crowding analysis, headway modeling, service reliability",
    "image_url": "/images/logos/tfl.gov.png"
  },
  {
    "name": "MTA Open Data",
    "description": "NYC subway and bus real-time feeds and historical performance data including ridership, delays, and service metrics.",
    "category": "Transportation & Mobility",
    "url": "https://new.mta.info/open-data",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "transit",
      "NYC",
      "subway",
      "bus",
      "GTFS",
      "real-time"
    ],
    "best_for": "Train frequency modeling, station crowding, transit network analysis",
    "image_url": "/images/datasets/mta-open-data.jpg"
  },
  {
    "name": "Bank of Israel Call Center (Technion)",
    "description": "250K+ calls with second-level timestamps from an Israeli bank. The most extensively validated call center dataset in OR literature with ~20% abandonment.",
    "category": "Operations & Service",
    "url": "https://iew.technion.ac.il/serveng/callcenterdata/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "call-center",
      "abandonment",
      "Erlang",
      "service-times",
      "operations-research"
    ],
    "best_for": "M/M/c+G modeling, Erlang-A validation, patience distribution estimation"
  },
  {
    "name": "DataMOCCA US Bank Database",
    "description": "220M calls across 2.5 years from multiple call center sites with skills-based routing. Accessible via SEEStat software.",
    "category": "Operations & Service",
    "url": "https://ie.technion.ac.il/serveng/References/DataMOCCA.pdf",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "call-center",
      "skills-routing",
      "multi-site",
      "large-scale"
    ],
    "best_for": "Multi-site pooling effects, skills-based routing analysis, large-scale staffing"
  },
  {
    "name": "Polish Supermarket POS Dataset",
    "description": "Transaction-level checkout data with timestamps showing start/end times, basket size, and payment method. Distinguishes staffed vs self-service.",
    "category": "E-Commerce",
    "url": "https://www.mdpi.com/2306-5729/4/2/67",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "retail",
      "checkout",
      "POS",
      "self-service",
      "transactions"
    ],
    "best_for": "Checkout queue modeling, pooling effects between queue types, service time estimation",
    "image_url": "/images/datasets/polish-supermarket-pos-dataset.jpg"
  },
  {
    "name": "CAIDA Anonymized Internet Traces",
    "description": "Backbone packet captures from 10-100 Gbps commercial links with nanosecond precision. 80-200GB per hourly trace.",
    "category": "Technology & Infrastructure",
    "url": "https://www.caida.org/catalog/datasets/passive_dataset/",
    "docs_url": "https://www.caida.org/catalog/datasets/passive_dataset_download/",
    "github_url": null,
    "tags": [
      "network",
      "packets",
      "backbone",
      "high-frequency",
      "internet"
    ],
    "best_for": "Heavy traffic analysis, self-similar traffic validation, M/G/1 heavy-tailed service",
    "image_url": "/images/datasets/caida-anonymized-internet-traces.png"
  },
  {
    "name": "Google Cluster Workload Traces 2019",
    "description": "2.4 TiB of job scheduling data from 8 Borg clusters (~12,000 machines each) with microsecond timestamps for submissions and completions.",
    "category": "Technology & Infrastructure",
    "url": "https://github.com/google/cluster-data",
    "docs_url": "https://github.com/google/cluster-data/blob/master/ClusterData2019.md",
    "github_url": "https://github.com/google/cluster-data",
    "tags": [
      "cluster",
      "scheduling",
      "jobs",
      "Google",
      "Borg",
      "cloud"
    ],
    "best_for": "G/G/c multi-server queues, priority scheduling, resource contention modeling",
    "image_url": "/images/datasets/google-cluster-workload-traces-2019.png"
  },
  {
    "name": "MAWI Traffic Archive",
    "description": "Daily 15-minute packet traces from Japan's WIDE backbone with microsecond precision. Free download without registration.",
    "category": "Technology & Infrastructure",
    "url": "https://mawi.wide.ad.jp/mawi/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "network",
      "packets",
      "Japan",
      "backbone",
      "free"
    ],
    "best_for": "Daily traffic patterns, long-term trend analysis, accessible network queueing data"
  },
  {
    "name": "Alibaba Cluster Trace v2018",
    "description": "280 GB of container scheduling data from Alibaba production clusters with job DAGs and resource utilization.",
    "category": "Technology & Infrastructure",
    "url": "https://github.com/alibaba/clusterdata",
    "docs_url": null,
    "github_url": "https://github.com/alibaba/clusterdata",
    "tags": [
      "cluster",
      "containers",
      "scheduling",
      "Alibaba",
      "cloud"
    ],
    "best_for": "Container orchestration queues, DAG scheduling, cloud workload characterization",
    "image_url": "/images/datasets/alibaba-cluster-trace-v2018.png"
  },
  {
    "name": "Bosch Production Line Performance",
    "description": "1.18M parts tracked across 52 workstations with 1,156 timestamp features. One of the largest real manufacturing datasets.",
    "category": "Manufacturing",
    "url": "https://www.kaggle.com/c/bosch-production-line-performance",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "manufacturing",
      "production-line",
      "timestamps",
      "quality",
      "Kaggle"
    ],
    "best_for": "Tandem queue analysis, bottleneck identification, work-in-progress dynamics",
    "image_url": "/images/datasets/bosch-production-line-performance.png"
  },
  {
    "name": "IMF PortWatch",
    "description": "Daily port activity across 1,985 ports worldwide using AIS satellite data from 90,000+ vessels. Free API access.",
    "category": "Logistics & Supply Chain",
    "url": "https://portwatch.imf.org/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "ports",
      "shipping",
      "maritime",
      "global",
      "AIS",
      "API"
    ],
    "best_for": "Port congestion analysis, global supply chain monitoring, vessel queueing",
    "image_url": "/images/datasets/imf-portwatch.png"
  },
  {
    "name": "Port of Los Angeles Port Optimizer",
    "description": "Container dwell times and truck turn times from the largest US port since 2021. Real operational metrics.",
    "category": "Logistics & Supply Chain",
    "url": "https://www.portoflosangeles.org/business/supply-chain/port-optimizer",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "port",
      "containers",
      "trucks",
      "dwell-time",
      "Los-Angeles"
    ],
    "best_for": "Container terminal queueing, truck appointment systems, port efficiency",
    "image_url": "/images/logos/portoflosangeles.png"
  },
  {
    "name": "OR-Library Job Shop Benchmarks",
    "description": "82+ classic job shop scheduling problem instances used as benchmarks in operations research literature.",
    "category": "Manufacturing",
    "url": "https://people.brunel.ac.uk/~mastjjb/jeb/orlib/jobshopinfo.html",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "job-shop",
      "scheduling",
      "benchmarks",
      "operations-research",
      "classic"
    ],
    "best_for": "Scheduling algorithm validation, theoretical queueing model testing"
  },
  {
    "name": "CoasterQueues",
    "description": "10-minute interval wait time data for 48 theme parks worldwide. CC-BY licensed CSV downloads.",
    "category": "Entertainment & Media",
    "url": "https://coasterqueues.com/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "theme-park",
      "wait-times",
      "attractions",
      "CC-BY"
    ],
    "best_for": "Theme park demand patterns, attraction capacity planning, historical wait analysis"
  },
  {
    "name": "Telecom Italia Big Data Challenge",
    "description": "Milan call detail records aggregated by 10-minute intervals and geographic grid squares. Ideal for Erlang B/C validation.",
    "category": "Telecommunications",
    "url": "https://dandelion.eu/datamine/open-big-data/",
    "docs_url": null,
    "github_url": null,
    "tags": [
      "telecom",
      "CDR",
      "Milan",
      "cellular",
      "grid"
    ],
    "best_for": "Telecommunications traffic engineering, Erlang formula validation, spatial demand",
    "image_url": "/images/logos/dandelion.png"
  }
]
