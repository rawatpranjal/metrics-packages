[
  {
    "name": "BayesianBandits",
    "description": "Lightweight microframework for Bayesian bandits (Thompson Sampling) with support for contextual/restless/delayed rewards.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://rukulkarni.com/projects/bayesianbandits/",
    "github_url": "https://github.com/IntelyCare/bayesianbandits",
    "url": "https://github.com/IntelyCare/bayesianbandits",
    "install": "pip install bayesianbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "Bayesian"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "BayesianBandits is a lightweight microframework designed for implementing Bayesian bandits using Thompson Sampling. It is useful for researchers and practitioners in the field of adaptive experimentation, particularly those interested in A/B testing and contextual bandits.",
    "use_cases": [
      "Optimizing online marketing campaigns",
      "Personalizing content recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian bandits",
      "how to implement Thompson Sampling in python",
      "A/B testing framework in python",
      "contextual bandits python library"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "TensorFlow Probability"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ContextualBandits",
    "description": "Implements a wide range of contextual bandit algorithms (linear, tree-based, neural) and off-policy evaluation methods.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://contextual-bandits.readthedocs.io/",
    "github_url": "https://github.com/david-cortes/contextualbandits",
    "url": "https://github.com/david-cortes/contextualbandits",
    "install": "pip install contextualbandits",
    "tags": [
      "A/B testing",
      "experimentation",
      "machine learning"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "machine learning"
    ],
    "summary": "ContextualBandits implements a variety of contextual bandit algorithms and off-policy evaluation methods. It is useful for researchers and practitioners in the fields of machine learning and experimentation.",
    "use_cases": [
      "personalized recommendations",
      "dynamic pricing strategies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits",
      "how to implement A/B testing in python",
      "contextual bandit algorithms in python",
      "off-policy evaluation methods python",
      "experimentation tools in python",
      "machine learning library for bandits"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MABWiser",
    "description": "Production-ready, scikit-learn style library for contextual & stochastic bandits with parallelism and simulation tools.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://fidelity.github.io/mabwiser/",
    "github_url": "https://github.com/fidelity/mabwiser",
    "url": "https://github.com/fidelity/mabwiser",
    "install": "pip install mabwiser",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "adaptive experimentation",
      "bandit algorithms"
    ],
    "summary": "MABWiser is a production-ready library designed for contextual and stochastic bandits, offering tools for parallelism and simulation. It is suitable for data scientists and researchers involved in experimentation and A/B testing.",
    "use_cases": [
      "Running A/B tests with contextual bandits",
      "Simulating bandit algorithms for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for contextual bandits",
      "how to perform A/B testing in python",
      "stochastic bandits library python",
      "parallelism in experimentation python",
      "MABWiser documentation",
      "scikit-learn style bandits library"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "bandit",
      "PyTorch-Bandits"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Open Bandit Pipeline (OBP)",
    "description": "Framework for **offline evaluation (OPE)** of bandit policies using logged data. Implements IPS, DR, DM estimators.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://zr-obp.readthedocs.io/en/latest/",
    "github_url": "https://github.com/st-tech/zr-obp",
    "url": "https://github.com/st-tech/zr-obp",
    "install": "pip install obp",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bandit-algorithms"
    ],
    "summary": "Open Bandit Pipeline (OBP) is a framework designed for offline evaluation of bandit policies using logged data. It is particularly useful for researchers and practitioners in the field of adaptive experimentation who need to assess the performance of various bandit algorithms.",
    "use_cases": [
      "Evaluating the effectiveness of different bandit algorithms",
      "Analyzing logged data from previous experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for offline evaluation of bandit policies",
      "how to implement IPS estimator in python",
      "bandit algorithms evaluation in python",
      "offline policy evaluation with logged data",
      "using Open Bandit Pipeline for A/B testing",
      "how to analyze bandit policies in python"
    ],
    "primary_use_cases": [
      "offline evaluation of bandit policies",
      "implementing IPS, DR, DM estimators"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyXAB",
    "description": "Library for advanced bandit problems: X-armed bandits (continuous/structured action spaces) and online optimization.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://pyxab.readthedocs.io/en/latest/",
    "github_url": "https://github.com/huanzhang12/pyxab",
    "url": "https://github.com/huanzhang12/pyxab",
    "install": "pip install pyxab",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "PyXAB is a library designed for tackling advanced bandit problems, including X-armed bandits with continuous and structured action spaces. It is useful for researchers and practitioners in the field of adaptive experimentation and online optimization.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for bandit problems",
      "how to implement X-armed bandits in python",
      "online optimization library python",
      "advanced experimentation library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SMPyBandits",
    "description": "Comprehensive research framework for single/multi-player MAB algorithms (stochastic, adversarial, contextual).",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://smpybandits.github.io/",
    "github_url": "https://github.com/SMPyBandits/SMPyBandits",
    "url": "https://github.com/SMPyBandits/SMPyBandits",
    "install": "pip install SMPyBandits",
    "tags": [
      "A/B testing",
      "experimentation"
    ],
    "best_for": "Online A/B testing, multi-armed bandits, adaptive allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "SMPyBandits is a comprehensive research framework designed for implementing single and multi-player Multi-Armed Bandit (MAB) algorithms, including stochastic, adversarial, and contextual settings. It is primarily used by researchers and practitioners in the field of adaptive experimentation and bandit algorithms.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for multi-armed bandits",
      "how to implement A/B testing in python",
      "MAB algorithms in python",
      "contextual bandits framework python",
      "stochastic bandits library python",
      "adversarial bandits implementation python"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "multi-player MAB algorithms"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "abracadabra",
    "description": "Sequential testing with always-valid inference. Supports continuous monitoring of A/B tests.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://github.com/quizlet/abracadabra",
    "github_url": "https://github.com/quizlet/abracadabra",
    "url": "https://pypi.org/project/abracadabra/",
    "install": "pip install abracadabra",
    "tags": [
      "sequential testing",
      "A/B testing",
      "always-valid"
    ],
    "best_for": "Continuous experiment monitoring",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "adaptive-experimentation"
    ],
    "summary": "Abracadabra is a Python package designed for sequential testing with always-valid inference, allowing for continuous monitoring of A/B tests. It is useful for data scientists and researchers involved in experimental design and analysis.",
    "use_cases": [
      "Monitoring A/B tests in real-time",
      "Conducting sequential analysis for marketing experiments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for sequential testing",
      "how to do A/B testing in python",
      "continuous monitoring of A/B tests",
      "always-valid inference in experiments"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "crewai",
    "description": "Framework for orchestrating role-playing autonomous AI agents. Multi-agent collaboration made intuitive.",
    "category": "Agentic AI",
    "docs_url": "https://docs.crewai.com/",
    "github_url": "https://github.com/crewAIInc/crewAI",
    "url": "https://www.crewai.com/",
    "install": "pip install crewai",
    "tags": [
      "agents",
      "multi-agent",
      "orchestration",
      "roles"
    ],
    "best_for": "Role-based multi-agent teams",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "crewai is a framework designed for orchestrating role-playing autonomous AI agents, making multi-agent collaboration intuitive. It is suitable for developers and researchers interested in agent-based systems.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for multi-agent collaboration",
      "how to orchestrate AI agents in python",
      "framework for role-playing AI agents",
      "autonomous AI agents in python",
      "python multi-agent orchestration",
      "collaborative AI agents framework"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "langchain",
    "description": "Framework for developing LLM-powered applications. Chains, tools, memory, and retrieval.",
    "category": "Agentic AI",
    "docs_url": "https://python.langchain.com/",
    "github_url": "https://github.com/langchain-ai/langchain",
    "url": "https://python.langchain.com/",
    "install": "pip install langchain",
    "tags": [
      "LLM",
      "chains",
      "tools",
      "RAG"
    ],
    "best_for": "LLM framework \u2014 chains, tools, memory",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Langchain is a framework designed for developing applications powered by large language models (LLMs). It is used by developers and data scientists looking to create sophisticated AI-driven solutions.",
    "use_cases": [
      "Building conversational agents",
      "Creating data retrieval systems"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LLM applications",
      "how to build chains in python",
      "tools for memory in langchain",
      "RAG in langchain",
      "developing AI applications with langchain",
      "langchain framework documentation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "langgraph",
    "description": "Framework for building stateful, multi-actor LLM applications. Graph-based agent workflows with persistence.",
    "category": "Agentic AI",
    "docs_url": "https://langchain-ai.github.io/langgraph/",
    "github_url": "https://github.com/langchain-ai/langgraph",
    "url": "https://langchain-ai.github.io/langgraph/",
    "install": "pip install langgraph",
    "tags": [
      "agents",
      "LLM",
      "workflows",
      "multi-agent"
    ],
    "best_for": "Production agent workflows with state management",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Langgraph is a framework designed for building stateful, multi-actor LLM applications. It enables users to create graph-based agent workflows with persistence, making it suitable for developers working on complex AI systems.",
    "use_cases": [
      "Developing chatbots with multiple interacting agents",
      "Creating AI-driven applications that require state management"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for building LLM applications",
      "how to create multi-agent workflows in python",
      "stateful LLM application framework",
      "graph-based agent workflows in python",
      "langgraph documentation",
      "examples of langgraph usage"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "openai-agents",
    "description": "OpenAI's lightweight, production-ready SDK for building agentic AI applications. Fast prototyping.",
    "category": "Agentic AI",
    "docs_url": "https://openai.github.io/openai-agents-python/",
    "github_url": "https://github.com/openai/openai-agents-python",
    "url": "https://openai.github.io/openai-agents-python/",
    "install": "pip install openai-agents",
    "tags": [
      "agents",
      "OpenAI",
      "tools",
      "lightweight"
    ],
    "best_for": "OpenAI's lightweight SDK \u2014 fast prototyping",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "OpenAI's openai-agents is a lightweight SDK designed for building agentic AI applications, enabling fast prototyping. It is suitable for developers looking to create AI-driven solutions efficiently.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for agentic AI",
      "how to build AI applications in python",
      "OpenAI SDK for agents",
      "lightweight AI tools in python",
      "fast prototyping AI applications",
      "using openai-agents for AI development"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "bsts",
    "description": "Bayesian Structural Time Series providing the foundation for CausalImpact. Supports spike-and-slab variable selection, multiple state components (trend, seasonality, regression), and non-Gaussian outcomes. Developed at Google.",
    "category": "Bayesian Causal Inference",
    "docs_url": "https://cran.r-project.org/web/packages/bsts/bsts.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bsts",
    "install": "install.packages(\"bsts\")",
    "tags": [
      "Bayesian",
      "structural-time-series",
      "spike-and-slab",
      "state-space",
      "Google"
    ],
    "best_for": "Bayesian structural time series with spike-and-slab selection\u2014foundation for CausalImpact",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian"
    ],
    "summary": "The bsts package provides a framework for Bayesian Structural Time Series modeling, which is foundational for causal impact analysis. It is particularly useful for users interested in understanding the effects of interventions over time.",
    "use_cases": [
      "Analyzing the impact of marketing campaigns on sales",
      "Forecasting future trends based on historical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "bayesian structural time series R",
      "how to perform causal impact analysis in R",
      "R package for spike-and-slab variable selection",
      "time series modeling with bsts",
      "bayesian analysis for time series",
      "structural time series in R"
    ],
    "primary_use_cases": [
      "causal impact analysis",
      "time series forecasting"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Bambi",
    "description": "High-level interface for building Bayesian GLMMs, built on top of PyMC. Uses formula syntax similar to R's `lme4`.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://bambinos.github.io/bambi/",
    "github_url": "https://github.com/bambinos/bambi",
    "url": "https://github.com/bambinos/bambi",
    "install": "pip install bambi",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "Bambi provides a high-level interface for building Bayesian Generalized Linear Mixed Models (GLMMs) using a formula syntax similar to R's `lme4`. It is designed for users who want to perform Bayesian inference in a user-friendly manner.",
    "use_cases": [
      "Modeling hierarchical data",
      "Analyzing longitudinal data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian GLMMs",
      "how to build Bayesian models in python",
      "Bambi package for Bayesian inference",
      "Bayesian econometrics in python",
      "using PyMC for GLMMs",
      "high-level Bayesian modeling in python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "LightweightMMM",
    "description": "Bayesian Marketing Mix Modeling (see Marketing Mix Models section).",
    "category": "Bayesian Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/google/lightweight_mmm",
    "url": "https://github.com/google/lightweight_mmm",
    "install": "pip install lightweight_mmm",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing-mix-modeling"
    ],
    "summary": "LightweightMMM is a package designed for Bayesian Marketing Mix Modeling, allowing users to analyze the effectiveness of marketing strategies. It is primarily used by data scientists and marketing analysts looking to optimize their marketing spend.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Optimizing advertising spend"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Marketing Mix Modeling",
      "how to perform marketing mix analysis in python",
      "Bayesian inference for marketing",
      "LightweightMMM usage examples",
      "best practices for marketing mix models in python",
      "how to analyze marketing effectiveness with python"
    ],
    "primary_use_cases": [
      "marketing mix modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "PyMC3",
      "Stan"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "NumPyro",
    "description": "Probabilistic programming library built on JAX for scalable Bayesian inference, often faster than PyMC.",
    "category": "Bayesian Econometrics",
    "docs_url": "https://num.pyro.ai/",
    "github_url": "https://github.com/pyro-ppl/numpyro",
    "url": "https://github.com/pyro-ppl/numpyro",
    "install": "pip install numpyro",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "NumPyro is a probabilistic programming library built on JAX that facilitates scalable Bayesian inference. It is designed for users looking for efficient and faster alternatives to traditional Bayesian frameworks like PyMC.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for probabilistic programming",
      "how to do Bayesian inference in python",
      "NumPyro tutorial",
      "scalable Bayesian inference python",
      "JAX probabilistic programming",
      "compare NumPyro and PyMC"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "related_packages": [
      "PyMC"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyMC",
    "description": "Flexible probabilistic programming library for Bayesian modeling and inference using MCMC algorithms (NUTS).",
    "category": "Bayesian Econometrics",
    "docs_url": "https://www.pymc.io/",
    "github_url": "https://github.com/pymc-devs/pymc",
    "url": "https://github.com/pymc-devs/pymc",
    "install": "pip install pymc",
    "tags": [
      "Bayesian",
      "inference"
    ],
    "best_for": "Uncertainty quantification, prior-informed inference, probabilistic modeling",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "PyMC is a flexible probabilistic programming library designed for Bayesian modeling and inference using Markov Chain Monte Carlo (MCMC) algorithms, specifically the No-U-Turn Sampler (NUTS). It is used by data scientists and statisticians for building complex models and conducting inference.",
    "use_cases": [
      "modeling complex data distributions",
      "conducting Bayesian inference"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian modeling",
      "how to perform inference in Python",
      "Bayesian inference with PyMC",
      "MCMC algorithms in Python",
      "using NUTS in PyMC",
      "probabilistic programming in Python"
    ],
    "primary_use_cases": [
      "Bayesian modeling",
      "inference using MCMC"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Stan",
      "TensorFlow Probability"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "bayesplot",
    "description": "Extensive library of ggplot2-based plotting functions for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks supporting the applied Bayesian workflow for any MCMC-fitted model.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/bayesplot/",
    "github_url": "https://github.com/stan-dev/bayesplot",
    "url": "https://cran.r-project.org/package=bayesplot",
    "install": "install.packages(\"bayesplot\")",
    "tags": [
      "visualization",
      "MCMC-diagnostics",
      "posterior-predictive-checks",
      "ggplot2",
      "Bayesian"
    ],
    "best_for": "Diagnostic plots and posterior visualization for MCMC-based Bayesian models, implementing Gabry et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "bayesplot is an extensive library of ggplot2-based plotting functions designed for posterior analysis, MCMC diagnostics, and prior/posterior predictive checks. It is used by statisticians and data scientists who work with MCMC-fitted models in Bayesian analysis.",
    "use_cases": [
      "Visualizing MCMC diagnostics",
      "Creating posterior predictive checks",
      "Analyzing Bayesian model outputs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for MCMC diagnostics",
      "how to visualize posterior distributions in R",
      "bayesian plotting functions in R",
      "ggplot2 for Bayesian analysis",
      "posterior predictive checks in R",
      "R package for MCMC analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "brms",
    "description": "High-level interface for fitting Bayesian generalized multilevel models using Stan, with lme4-style formula syntax supporting linear, count, survival, ordinal, zero-inflated, hurdle, and mixture models with flexible prior specification.",
    "category": "Bayesian Inference",
    "docs_url": "https://paul-buerkner.github.io/brms/",
    "github_url": "https://github.com/paul-buerkner/brms",
    "url": "https://cran.r-project.org/package=brms",
    "install": "install.packages(\"brms\")",
    "tags": [
      "Bayesian",
      "multilevel-models",
      "Stan",
      "regression",
      "distributional-regression"
    ],
    "best_for": "Complex hierarchical Bayesian regression with familiar R formula syntax, implementing B\u00fcrkner (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "The brms package provides a high-level interface for fitting Bayesian generalized multilevel models using Stan. It is particularly useful for statisticians and data scientists who need to specify complex models with flexible prior distributions.",
    "use_cases": [
      "Fitting linear mixed models",
      "Conducting Bayesian regression analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian multilevel models",
      "how to fit a Bayesian model in R",
      "Stan interface for R",
      "Bayesian regression in R",
      "multilevel modeling with brms",
      "brms package documentation"
    ],
    "primary_use_cases": [
      "Fitting Bayesian generalized multilevel models",
      "Specifying flexible prior distributions"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rstan",
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "rstan",
    "description": "Core R interface to the Stan probabilistic programming language, providing full Bayesian inference via NUTS/HMC, approximate inference via ADVI, and penalized maximum likelihood via L-BFGS for custom Bayesian models.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstan/",
    "github_url": "https://github.com/stan-dev/rstan",
    "url": "https://cran.r-project.org/package=rstan",
    "install": "install.packages(\"rstan\")",
    "tags": [
      "Stan",
      "MCMC",
      "HMC",
      "probabilistic-programming",
      "Bayesian"
    ],
    "best_for": "Custom Bayesian models requiring direct Stan language access for maximum flexibility, implementing Carpenter et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "rstan is the core R interface to the Stan probabilistic programming language, enabling full Bayesian inference and approximate inference for custom Bayesian models. It is used by statisticians and data scientists who require advanced modeling techniques.",
    "use_cases": [
      "Bayesian data analysis",
      "Statistical modeling of complex data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian inference",
      "how to perform MCMC in R",
      "Stan interface for R",
      "Bayesian modeling in R",
      "R probabilistic programming",
      "using HMC with R",
      "L-BFGS in R for Bayesian models"
    ],
    "primary_use_cases": [
      "Bayesian data analysis",
      "Statistical modeling"
    ],
    "api_complexity": "advanced",
    "related_packages": [
      "brms",
      "rstanarm"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "rstanarm",
    "description": "Pre-compiled Bayesian regression models using Stan that mimic familiar R functions (lm, glm, lmer) with customary formula syntax, weakly informative default priors, and zero model compilation time.",
    "category": "Bayesian Inference",
    "docs_url": "https://mc-stan.org/rstanarm/",
    "github_url": "https://github.com/stan-dev/rstanarm",
    "url": "https://cran.r-project.org/package=rstanarm",
    "install": "install.packages(\"rstanarm\")",
    "tags": [
      "Bayesian",
      "Stan",
      "regression",
      "mixed-effects",
      "pre-compiled"
    ],
    "best_for": "Applied Bayesian regression with minimal learning curve for lm/glm/lmer users",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian"
    ],
    "summary": "rstanarm provides pre-compiled Bayesian regression models that mimic familiar R functions, allowing users to perform regression analysis with minimal setup time. It is primarily used by statisticians and data scientists who require Bayesian modeling capabilities.",
    "use_cases": [
      "Performing Bayesian regression analysis",
      "Conducting mixed-effects modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian regression",
      "how to use rstanarm for mixed-effects models",
      "Bayesian modeling in R",
      "pre-compiled Bayesian models in R",
      "rstanarm documentation",
      "install rstanarm package",
      "examples of rstanarm usage"
    ],
    "primary_use_cases": [
      "Bayesian regression analysis",
      "mixed-effects modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "rstan",
      "brms"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "boot",
    "description": "Classic bootstrap methods implementing the approaches described in Davison & Hinkley (1997). Provides functions for both parametric and nonparametric resampling with various confidence interval methods.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://cran.r-project.org/web/packages/boot/boot.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=boot",
    "install": "install.packages(\"boot\")",
    "tags": [
      "bootstrap",
      "resampling",
      "confidence-intervals",
      "nonparametric",
      "parametric"
    ],
    "best_for": "Classic bootstrap methods from Davison & Hinkley (1997) for general resampling inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bootstrap",
      "resampling"
    ],
    "summary": "The 'boot' package provides classic bootstrap methods for statistical inference as described in Davison & Hinkley (1997). It is used by statisticians and data scientists for both parametric and nonparametric resampling to create confidence intervals.",
    "use_cases": [
      "Estimating confidence intervals for a sample mean",
      "Conducting hypothesis tests using bootstrap methods"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for bootstrap methods",
      "how to perform resampling in R",
      "confidence intervals in R",
      "nonparametric bootstrap in R",
      "parametric bootstrap techniques R",
      "statistical inference R package"
    ],
    "primary_use_cases": [
      "confidence interval estimation",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Davison & Hinkley (1997)",
    "maintenance_status": "active"
  },
  {
    "name": "fwildclusterboot",
    "description": "Fast wild cluster bootstrap implementation following Roodman et al. (2019)\u2014up to 1000\u00d7 faster than alternatives. Critical for panel data with few clusters. Integrates with fixest and lfe for efficient inference.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://s3alfisc.github.io/fwildclusterboot/",
    "github_url": "https://github.com/s3alfisc/fwildclusterboot",
    "url": "https://cran.r-project.org/package=fwildclusterboot",
    "install": "install.packages(\"fwildclusterboot\")",
    "tags": [
      "wild-bootstrap",
      "cluster-robust",
      "few-clusters",
      "panel-data",
      "fixest"
    ],
    "best_for": "Fast wild cluster bootstrap for panel data with few clusters, implementing Roodman et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fwildclusterboot is a fast wild cluster bootstrap implementation that significantly speeds up analysis for panel data with few clusters. It is particularly useful for researchers and data scientists working with econometric models.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for wild bootstrap",
      "how to perform cluster-robust inference in R",
      "fast bootstrap methods for panel data",
      "wild cluster bootstrap implementation R",
      "R package for efficient inference with fixest",
      "bootstrap methods for few clusters in R"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "fixest",
      "lfe"
    ],
    "implements_paper": "Roodman et al. (2019)",
    "maintenance_status": "active"
  },
  {
    "name": "rsample",
    "description": "Modern tidyverse-compatible resampling infrastructure. Provides functions for creating resamples (bootstrap, cross-validation, time series splits) that integrate seamlessly with tidymodels workflows.",
    "category": "Bootstrap & Inference",
    "docs_url": "https://rsample.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/rsample",
    "url": "https://cran.r-project.org/package=rsample",
    "install": "install.packages(\"rsample\")",
    "tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "tidymodels",
      "time-series-cv"
    ],
    "best_for": "Tidyverse-native resampling for bootstrap, cross-validation, and time series splits",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "resampling",
      "cross-validation",
      "bootstrap",
      "time-series"
    ],
    "summary": "The rsample package provides modern resampling infrastructure that is compatible with the tidyverse. It offers functions for creating various types of resamples, such as bootstrap and cross-validation, which integrate seamlessly with tidymodels workflows.",
    "use_cases": [
      "Creating bootstrap samples for model training",
      "Performing time series cross-validation for predictive modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for resampling",
      "how to perform cross-validation in R",
      "bootstrap methods in R",
      "tidymodels resampling functions",
      "time series cross-validation in R",
      "R package for model evaluation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidymodels"
    ],
    "related_packages": [
      "boot",
      "caret"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "bunching",
    "description": "Implements Kleven-Waseem style bunching estimation for kink and notch designs. Calculates parametric elasticities from bunching at tax thresholds with publication-ready output.",
    "category": "Bunching Estimation",
    "docs_url": "https://cran.r-project.org/web/packages/bunching/bunching.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bunching",
    "install": "install.packages(\"bunching\")",
    "tags": [
      "bunching",
      "kink-design",
      "notch-design",
      "tax-research",
      "elasticity"
    ],
    "best_for": "Kleven-Waseem bunching estimation at kinks and notches for tax research",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bunching",
      "tax-research",
      "elasticity"
    ],
    "summary": "The 'bunching' package implements Kleven-Waseem style bunching estimation for kink and notch designs. It is used by researchers and analysts in tax policy to calculate parametric elasticities from bunching at tax thresholds.",
    "use_cases": [
      "Estimating tax elasticities at specific income thresholds",
      "Analyzing the effects of tax policy changes on taxpayer behavior"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for bunching estimation",
      "how to calculate elasticities in R",
      "Kleven-Waseem bunching method in R",
      "R tax threshold analysis package",
      "bunching analysis for tax research",
      "notch design estimation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "bnlearn",
    "description": "Bayesian network structure learning, parameter estimation, and inference. Implements constraint-based (PC, GS), score-based (HC, TABU), and hybrid algorithms for DAG learning with discrete and continuous data.",
    "category": "Causal Discovery",
    "docs_url": "https://www.bnlearn.com/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=bnlearn",
    "install": "install.packages(\"bnlearn\")",
    "tags": [
      "Bayesian-networks",
      "structure-learning",
      "parameter-estimation",
      "probabilistic-graphical-models",
      "inference"
    ],
    "best_for": "Bayesian network learning and inference with constraint-based and score-based algorithms",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "bnlearn is a package for Bayesian network structure learning, parameter estimation, and inference. It is used by data scientists and researchers interested in causal discovery and probabilistic graphical models.",
    "use_cases": [
      "Learning the structure of a Bayesian network from data",
      "Estimating parameters of a Bayesian network"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Bayesian networks",
      "how to perform structure learning in R",
      "Bayesian network inference in R",
      "parameter estimation for Bayesian networks R",
      "constraint-based algorithms for DAG learning R",
      "score-based learning in R"
    ],
    "primary_use_cases": [
      "Bayesian network structure learning",
      "parameter estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "dagitty",
    "description": "Analysis of structural causal models represented as DAGs. Computes adjustment sets, identifies instrumental variables, tests conditional independencies, and finds minimal sufficient adjustment sets for causal identification.",
    "category": "Causal Discovery",
    "docs_url": "http://www.dagitty.net/",
    "github_url": "https://github.com/jtextor/dagitty",
    "url": "https://cran.r-project.org/package=dagitty",
    "install": "install.packages(\"dagitty\")",
    "tags": [
      "DAG",
      "causal-graphs",
      "adjustment-sets",
      "d-separation",
      "instrumental-variables"
    ],
    "best_for": "DAG-based causal analysis with adjustment set computation and d-separation testing",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Dagitty is a tool for analyzing structural causal models represented as directed acyclic graphs (DAGs). It is used by researchers and practitioners in the field of causal inference to compute adjustment sets and identify instrumental variables.",
    "use_cases": [
      "Identifying instrumental variables for causal analysis",
      "Testing conditional independencies in datasets"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal graphs",
      "how to compute adjustment sets in R",
      "DAG analysis in R",
      "R library for instrumental variables",
      "conditional independence testing in R",
      "R package for d-separation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ggdag",
    "description": "Visualize and analyze causal DAGs using ggplot2. Provides tidy interface to dagitty with publication-quality DAG plots, path highlighting, and adjustment set visualization.",
    "category": "Causal Discovery",
    "docs_url": "https://ggdag.malco.io/",
    "github_url": "https://github.com/malcolmbarrett/ggdag",
    "url": "https://cran.r-project.org/package=ggdag",
    "install": "install.packages(\"ggdag\")",
    "tags": [
      "DAG",
      "visualization",
      "ggplot2",
      "causal-diagrams",
      "adjustment-sets"
    ],
    "best_for": "Publication-quality DAG visualization using ggplot2 with dagitty integration",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ggdag package allows users to visualize and analyze causal Directed Acyclic Graphs (DAGs) using ggplot2. It is particularly useful for researchers and practitioners in causal discovery who need publication-quality DAG plots and path highlighting.",
    "use_cases": [
      "Creating publication-quality causal DAGs",
      "Highlighting paths in causal diagrams"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal DAG visualization",
      "how to create DAGs in R",
      "ggplot2 DAG plotting",
      "analyze causal diagrams in R",
      "visualize adjustment sets R",
      "path highlighting in causal analysis R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pcalg",
    "description": "Causal structure learning from observational data using the PC algorithm and variants. Estimates Markov equivalence class of DAGs from conditional independence tests with intervention support.",
    "category": "Causal Discovery",
    "docs_url": "https://cran.r-project.org/web/packages/pcalg/pcalg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pcalg",
    "install": "install.packages(\"pcalg\")",
    "tags": [
      "causal-discovery",
      "PC-algorithm",
      "structure-learning",
      "DAG",
      "conditional-independence"
    ],
    "best_for": "Causal structure learning from observational data using PC algorithm",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The pcalg package provides tools for causal structure learning from observational data using the PC algorithm and its variants. It is primarily used by researchers and practitioners in the field of causal inference.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal structure learning",
      "how to use PC algorithm in R",
      "conditional independence tests in R",
      "DAG estimation in R",
      "causal discovery with pcalg",
      "intervention support in causal analysis"
    ],
    "primary_use_cases": [
      "causal structure learning",
      "Markov equivalence class estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Ananke",
    "description": "Causal inference using graphical models (DAGs), including identification theory and effect estimation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://ananke.readthedocs.io/",
    "github_url": "[https://github.com/py-why/Ananke](https://github.com/ghosthamlet/ananke",
    "url": "[https://github.com/py-why/Ananke](https://github.com/ghosthamlet/ananke",
    "install": "pip install ananke-causal",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "Ananke is a Python package designed for causal inference using graphical models, specifically directed acyclic graphs (DAGs). It focuses on identification theory and effect estimation, making it useful for researchers and data scientists working in causal analysis.",
    "use_cases": [
      "Estimating causal effects in observational studies",
      "Conducting A/B tests using causal models"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate effects using DAGs in python",
      "causal discovery tools in python",
      "graphical models for causal analysis python",
      "python package for identification theory",
      "DAGs in causal inference python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "CausalInference"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Benchpress",
    "description": "Benchmarking 41+ structure learning algorithms for causal discovery. Standardized evaluation framework.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://benchpressdocs.readthedocs.io/",
    "github_url": "https://github.com/felixleopoldo/benchpress",
    "url": "https://github.com/felixleopoldo/benchpress",
    "install": "pip install benchpress",
    "tags": [
      "causal discovery",
      "benchmarking",
      "structure learning"
    ],
    "best_for": "Comparing causal discovery algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphical-models"
    ],
    "summary": "Benchpress is a benchmarking tool for evaluating over 41 structure learning algorithms used in causal discovery. It provides a standardized evaluation framework for researchers and practitioners in the field.",
    "use_cases": [
      "Comparing different structure learning algorithms",
      "Evaluating causal discovery methods in research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to benchmark structure learning algorithms in python",
      "evaluate structure learning algorithms python",
      "benchmarking causal discovery methods python",
      "python causal inference library",
      "structure learning algorithms evaluation python"
    ],
    "primary_use_cases": [
      "benchmarking structure learning algorithms",
      "evaluating causal discovery methods"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Causal Discovery Toolbox (CDT)",
    "description": "Implements algorithms for causal discovery (recovering causal graph structure) from observational data.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html",
    "github_url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "url": "https://github.com/FenTechSolutions/CausalDiscoveryToolbox",
    "install": "pip install cdt",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "The Causal Discovery Toolbox (CDT) implements algorithms for recovering causal graph structures from observational data. It is used by researchers and practitioners in fields such as statistics, data science, and economics to understand causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in observational studies",
      "Building causal models for decision-making"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to recover causal graphs in python",
      "causal inference tools in python",
      "causal discovery algorithms python",
      "graphs in causal inference python",
      "observational data causal analysis python"
    ],
    "primary_use_cases": [
      "causal graph structure recovery"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalNex",
    "description": "Uses Bayesian Networks for causal reasoning, combining ML with expert knowledge to model relationships.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/microsoft/causalnex",
    "url": "https://github.com/microsoft/causalnex",
    "install": "pip install causalnex",
    "tags": [
      "causal inference",
      "graphs",
      "Bayesian"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "CausalNex is a Python package that utilizes Bayesian Networks for causal reasoning, effectively combining machine learning with expert knowledge to model complex relationships. It is used by data scientists and researchers interested in understanding causal relationships in their data.",
    "use_cases": [
      "Analyzing the impact of marketing strategies on sales",
      "Understanding the causal relationships in healthcare data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to model relationships with Bayesian networks in python",
      "CausalNex documentation",
      "causal discovery tools in python",
      "using Bayesian networks for causal reasoning",
      "best practices for causal inference in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pgmpy",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "LiNGAM",
    "description": "Specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://lingam.readthedocs.io/",
    "github_url": "https://github.com/cdt15/lingam",
    "url": "https://github.com/cdt15/lingam",
    "install": "pip install lingam",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "LiNGAM is a specialized package for learning non-Gaussian linear causal models, implementing various versions of the LiNGAM algorithm including ICA-based methods. It is used by researchers and practitioners interested in causal discovery and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in observational data",
      "Developing models for causal inference in social sciences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to learn non-Gaussian linear causal models in python",
      "LiNGAM algorithm implementation in python",
      "causal discovery tools in python",
      "graphs in causal inference python",
      "ICA-based methods for causal models python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MCD",
    "description": "Mixture of Causal Graphs discovery for heterogeneous time series (ICML 2024). Finds time-varying causal structures.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/causal-disentanglement/mcd",
    "url": "https://pypi.org/project/MCD/",
    "install": "pip install mcd",
    "tags": [
      "causal discovery",
      "time series",
      "heterogeneous"
    ],
    "best_for": "Time-varying causal structure discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "graphical-models"
    ],
    "summary": "MCD is a Python package designed for discovering time-varying causal structures in heterogeneous time series data. It is useful for researchers and practitioners in the fields of causal discovery and graphical models.",
    "use_cases": [
      "Analyzing causal relationships in economic data",
      "Studying the impact of interventions over time"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to analyze time series with causal graphs",
      "MCD package for heterogeneous time series",
      "discovering causal structures in Python",
      "time-varying causal analysis library",
      "causal graphs in Python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SDCI",
    "description": "State-dependent causal inference for conditionally stationary processes (ICML 2025). Handles regime-switching causal graphs.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/causal-disentanglement/sdci",
    "url": "https://pypi.org/project/SDCI/",
    "install": "pip install sdci",
    "tags": [
      "causal discovery",
      "time series",
      "regime switching"
    ],
    "best_for": "State-dependent causal discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "regime-switching"
    ],
    "summary": "SDCI is a Python package designed for state-dependent causal inference in conditionally stationary processes. It is particularly useful for researchers and practitioners working with regime-switching causal graphs.",
    "use_cases": [
      "Analyzing causal relationships in time series data",
      "Studying the effects of regime changes on causal inference"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to perform regime switching analysis in python",
      "time series causal inference python",
      "SDCI package usage",
      "regime-switching causal graphs in python",
      "causal inference for time series"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Tigramite",
    "description": "Specialized package for causal inference in time series data implementing PCMCI, PCMCIplus, LPCMCI algorithms with conditional independence tests.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://tigramite.readthedocs.io/",
    "github_url": "https://github.com/jakobrunge/tigramite",
    "url": "https://github.com/jakobrunge/tigramite",
    "install": "pip install tigramite",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series"
    ],
    "summary": "Tigramite is a specialized package for causal inference in time series data that implements PCMCI, PCMCIplus, and LPCMCI algorithms along with conditional independence tests. It is used by researchers and data scientists working on causal discovery in temporal datasets.",
    "use_cases": [
      "Analyzing causal relationships in economic time series",
      "Evaluating the impact of interventions over time"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal discovery in time series",
      "Tigramite package usage",
      "conditional independence tests in Python",
      "time series causal inference library",
      "implementing PCMCI in Python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "causal-learn",
    "description": "Comprehensive Python package serving as Python translation and extension of Java-based Tetrad toolkit for causal discovery algorithms.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://causal-learn.readthedocs.io/",
    "github_url": "https://github.com/py-why/causal-learn",
    "url": "https://github.com/py-why/causal-learn",
    "install": "pip install causal-learn",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "Causal-learn is a comprehensive Python package that serves as a translation and extension of the Java-based Tetrad toolkit for causal discovery algorithms. It is used by researchers and practitioners in the field of causal inference to analyze and model causal relationships.",
    "use_cases": [
      "Analyzing causal relationships in datasets",
      "Developing causal models for research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to perform causal inference in python",
      "causal graphs in python",
      "Tetrad toolkit in python",
      "causal analysis with python",
      "causal-learn package usage"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "Tetrad",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "causal-llm-bfs",
    "description": "LLM + BFS hybrid for efficient causal graph discovery. Uses language models to guide structure search.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://github.com/superkaiba/causal-llm-bfs",
    "github_url": "https://github.com/superkaiba/causal-llm-bfs",
    "url": "https://github.com/superkaiba/causal-llm-bfs",
    "install": "pip install causal-llm-bfs",
    "tags": [
      "causal discovery",
      "LLM",
      "graphs"
    ],
    "best_for": "LLM-assisted causal discovery",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "The causal-llm-bfs package combines language models with breadth-first search techniques to enhance the discovery of causal graphs. It is useful for researchers and practitioners in causal inference and data science.",
    "use_cases": [
      "Discovering causal relationships in datasets",
      "Guiding structure search in complex data environments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to use LLM for graph discovery",
      "efficient causal graph discovery in python",
      "BFS algorithm for causal graphs",
      "causal inference with language models",
      "graphical models in python"
    ],
    "primary_use_cases": [
      "causal graph discovery"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gCastle",
    "description": "Huawei Noah's Ark Lab end-to-end causal structure learning toolchain emphasizing gradient-based methods with GPU acceleration (NOTEARS, GOLEM).",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": "https://gcastle.readthedocs.io/",
    "github_url": "https://github.com/huawei-noah/trustworthyAI",
    "url": "https://github.com/huawei-noah/trustworthyAI",
    "install": "pip install gcastle",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "gCastle is an end-to-end causal structure learning toolchain developed by Huawei Noah's Ark Lab, focusing on gradient-based methods with GPU acceleration. It is used by researchers and practitioners in the field of causal inference and graphical models.",
    "use_cases": [
      "learning causal structures from data",
      "analyzing causal relationships in complex systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do causal structure learning in python",
      "gradient-based methods for graphs in python",
      "GPU acceleration for causal inference python",
      "causal discovery tools in python",
      "Huawei Noah's Ark Lab causal tools"
    ],
    "primary_use_cases": [
      "causal structure learning",
      "gradient-based causal inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "py-tetrad",
    "description": "Python interface to Tetrad Java library using JPype, providing direct access to Tetrad's causal discovery algorithms with efficient data translation.",
    "category": "Causal Discovery & Graphical Models",
    "docs_url": null,
    "github_url": "https://github.com/py-why/py-tetrad",
    "url": "https://github.com/py-why/py-tetrad",
    "install": "Available on GitHub (installation via git clone)",
    "tags": [
      "causal inference",
      "graphs"
    ],
    "best_for": "Learning causal structure from data, DAG estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "JPype"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "py-tetrad is a Python interface to the Tetrad Java library that provides direct access to Tetrad's causal discovery algorithms. It is used by data scientists and researchers interested in causal inference and graphical models.",
    "use_cases": [
      "Causal analysis in research studies",
      "Graphical model construction for data analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal discovery",
      "how to use Tetrad in Python",
      "causal inference with py-tetrad",
      "graphs in causal analysis Python",
      "implementing causal algorithms in Python",
      "Tetrad Java library interface Python"
    ],
    "primary_use_cases": [
      "causal discovery",
      "graphical model estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CATENets",
    "description": "JAX-accelerated neural network CATE estimators implementing SNet, FlexTENet, TARNet, CFRNet, and DragonNet architectures.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/AliciaCurth/CATENets",
    "url": "https://github.com/AliciaCurth/CATENets",
    "install": "pip install catenets",
    "tags": [
      "causal inference",
      "deep learning",
      "JAX"
    ],
    "best_for": "GPU-accelerated neural CATE estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "deep-learning"
    ],
    "summary": "CATENets is a library for JAX-accelerated neural network CATE estimators that implements various architectures such as SNet, FlexTENet, TARNet, CFRNet, and DragonNet. It is designed for researchers and practitioners in causal inference seeking advanced deep learning solutions.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate CATE in python",
      "deep learning for causal inference",
      "JAX neural network estimators",
      "CATE estimators in python",
      "using JAX for deep learning",
      "neural networks for causal analysis",
      "implementing TARNet in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "JAX"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalInference",
    "description": "Implements classical causal inference methods like propensity score matching, inverse probability weighting, stratification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalinferenceinpython.org",
    "github_url": "https://github.com/laurencium/causalinference",
    "url": "https://github.com/laurencium/causalinference",
    "install": "pip install CausalInference",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalInference implements classical causal inference methods such as propensity score matching, inverse probability weighting, and stratification. It is used by data scientists and researchers to analyze causal relationships in data.",
    "use_cases": [
      "Analyzing treatment effects in observational studies",
      "Evaluating the impact of interventions in healthcare"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do propensity score matching in python",
      "inverse probability weighting python package",
      "stratification methods in python",
      "matching techniques in causal analysis",
      "causal inference tools for data science"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "inverse probability weighting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalLib",
    "description": "IBM-developed package that provides a scikit-learn-inspired API for causal inference with meta-algorithms supporting arbitrary machine learning models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causallib.readthedocs.io/",
    "github_url": "https://github.com/IBM/causallib",
    "url": "https://github.com/IBM/causallib",
    "install": "pip install causallib",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalLib is an IBM-developed package that provides a scikit-learn-inspired API for causal inference. It is designed for users who need to apply causal inference techniques using various machine learning models.",
    "use_cases": [
      "Estimating causal effects in observational studies",
      "Conducting A/B tests using machine learning models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do matching in python",
      "causal inference with machine learning",
      "scikit-learn inspired causal inference library",
      "using CausalLib for A/B testing",
      "meta-algorithms for causal inference in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalML",
    "description": "Focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causalml.readthedocs.io/",
    "github_url": "https://github.com/uber/causalml",
    "url": "https://github.com/uber/causalml",
    "install": "pip install causalml",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalML focuses on uplift modeling and heterogeneous treatment effect estimation using machine learning techniques. It is used by data scientists and researchers interested in causal inference.",
    "use_cases": [
      "Estimating the effect of marketing campaigns",
      "Personalizing treatment recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "machine learning for causal inference",
      "CausalML documentation",
      "best practices for matching in python",
      "python library for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "heterogeneous treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CausalMatch",
    "description": "Implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with ML flexibility for propensity score estimation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/bytedance/CausalMatch",
    "github_url": null,
    "url": "https://github.com/bytedance/CausalMatch",
    "install": "pip install causalmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalMatch implements Propensity Score Matching (PSM) and Coarsened Exact Matching (CEM) with machine learning flexibility for propensity score estimation. It is useful for researchers and data scientists interested in causal inference methodologies.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Conducting A/B tests with matched samples"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for propensity score matching",
      "how to perform causal inference in python",
      "CausalMatch documentation",
      "examples of Coarsened Exact Matching in python",
      "best practices for matching in causal analysis",
      "python library for causal inference"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "coarsened exact matching"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalPlayground",
    "description": "Python library for causal research that addresses the scarcity of real-world datasets with known causal relations. Provides fine-grained control over structural causal models.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-playground.readthedocs.io/",
    "github_url": "https://github.com/causal-playground/causal-playground",
    "url": "https://github.com/causal-playground/causal-playground",
    "install": "pip install causal-playground",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CausalPlayground is a Python library designed for causal research, focusing on the use of real-world datasets with known causal relations. It is suitable for researchers and data scientists interested in structural causal models.",
    "use_cases": [
      "analyzing causal relationships in observational data",
      "designing experiments for A/B testing"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal research",
      "how to use structural causal models in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalPy",
    "description": "Developed by PyMC Labs, focuses specifically on causal inference in quasi-experimental settings. Specializes in scenarios where randomization is impossible or expensive.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://github.com/pymc-labs/pymc-marketing",
    "install": "pip install CausalPy",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "CausalPy is a Python library developed by PyMC Labs that focuses on causal inference in quasi-experimental settings. It is designed for scenarios where randomization is impossible or expensive, making it useful for researchers and data scientists working in fields that require causal analysis.",
    "use_cases": [
      "Analyzing the impact of a policy change",
      "Evaluating the effectiveness of a marketing campaign"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to do causal analysis in python",
      "matching techniques in python",
      "quasi-experimental methods python",
      "CausalPy documentation",
      "causal inference tools in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoWhy",
      "EconML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "DoWhy",
    "description": "End-to-end framework for causal inference based on causal graphs (DAGs) and potential outcomes. Covers identification, estimation, refutation.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://www.pywhy.org/dowhy/",
    "github_url": "https://github.com/py-why/dowhy",
    "url": "https://github.com/py-why/dowhy",
    "install": "pip install dowhy",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DoWhy is an end-to-end framework for causal inference that utilizes causal graphs (DAGs) and potential outcomes. It is designed for researchers and practitioners looking to identify, estimate, and refute causal relationships.",
    "use_cases": [
      "Estimating causal effects from observational data",
      "Conducting A/B tests using causal inference methods"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform causal analysis in python",
      "DoWhy package usage",
      "causal graphs in python",
      "potential outcomes framework python",
      "causal inference tools python"
    ],
    "primary_use_cases": [
      "causal effect estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "EconML",
      "CausalML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "KECENI",
    "description": "Doubly robust, non-parametric estimation of node-wise counterfactual means under network interference (arXiv 2024).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/HeejongBong/KECENI",
    "url": "https://pypi.org/project/KECENI/",
    "install": "pip install keceni",
    "tags": [
      "networks",
      "spillovers",
      "causal inference"
    ],
    "best_for": "Network interference with node heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "networks"
    ],
    "summary": "KECENI provides a method for doubly robust, non-parametric estimation of counterfactual means in the presence of network interference. It is particularly useful for researchers and practitioners in causal inference who are dealing with complex network data.",
    "use_cases": [
      "Estimating treatment effects in networked populations",
      "Analyzing spillover effects in social networks"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate counterfactual means in python",
      "network interference estimation python",
      "doubly robust estimation in python",
      "spillover effects analysis python",
      "non-parametric causal inference python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "NetworkCausalTree",
    "description": "Estimates both direct treatment effects and spillover effects under clustered network interference (Bargagli-Stoffi et al. 2025).",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "url": "https://github.com/fbargaglistoffi/NetworkCausalTree",
    "install": "pip install networkcausaltree",
    "tags": [
      "causal inference",
      "networks",
      "spillovers"
    ],
    "best_for": "Treatment effects with network interference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "networks",
      "spillovers"
    ],
    "summary": "NetworkCausalTree estimates both direct treatment effects and spillover effects in the context of clustered network interference. It is useful for researchers and practitioners in causal inference who are dealing with network data.",
    "use_cases": [
      "Estimating treatment effects in social networks",
      "Analyzing spillover effects in public health interventions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate spillover effects in python",
      "network interference analysis in python",
      "direct treatment effects estimation python",
      "causal inference with networks",
      "spillover effects in clustered networks"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Bargagli-Stoffi et al. (2025)",
    "maintenance_status": "active"
  },
  {
    "name": "PySensemakr",
    "description": "Implements Cinelli-Hazlett framework for assessing robustness to unobserved confounding. Computes confounder strength needed to invalidate results.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/carloscinelli/PySensemakr",
    "github_url": "https://github.com/carloscinelli/PySensemakr",
    "url": "https://github.com/carloscinelli/PySensemakr",
    "install": "pip install pysensemakr",
    "tags": [
      "causal inference",
      "sensitivity analysis",
      "robustness"
    ],
    "best_for": "Sensitivity analysis with publication-ready contour plots",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "sensitivity-analysis",
      "robustness"
    ],
    "summary": "PySensemakr implements the Cinelli-Hazlett framework to assess robustness against unobserved confounding in causal inference studies. It is useful for researchers and data scientists who need to evaluate the strength of confounders that could potentially invalidate their results.",
    "use_cases": [
      "Assessing the robustness of causal estimates",
      "Evaluating the impact of unobserved confounding on study results"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to perform sensitivity analysis in python",
      "robustness assessment in python",
      "Cinelli-Hazlett framework implementation",
      "confounder strength analysis python",
      "evaluate unobserved confounding python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "aipyw",
    "description": "Minimal, fast AIPW (Augmented Inverse Probability Weighting) implementation for discrete treatments. Sklearn-compatible with cross-fitting.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/aipyw",
    "url": "https://github.com/apoorvalal/aipyw",
    "install": "pip install aipyw",
    "tags": [
      "causal inference",
      "AIPW",
      "treatment effects"
    ],
    "best_for": "Fast AIPW estimation with sklearn models",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "Aipyw is a minimal and fast implementation of Augmented Inverse Probability Weighting (AIPW) for discrete treatments. It is designed to be compatible with Scikit-learn and supports cross-fitting, making it suitable for users interested in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing A/B test results"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for AIPW",
      "how to do causal inference in python",
      "AIPW implementation in python",
      "fast AIPW for discrete treatments",
      "cross-fitting in causal inference python",
      "scikit-learn compatible AIPW"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "causal-curve",
    "description": "Continuous treatment dose-response curve estimation. GPS and TMLE methods for continuous treatments.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://causal-curve.readthedocs.io/",
    "github_url": "https://github.com/ronikobrosly/causal-curve",
    "url": "https://github.com/ronikobrosly/causal-curve",
    "install": "pip install causal-curve",
    "tags": [
      "dose-response",
      "continuous treatment",
      "GPS"
    ],
    "best_for": "Dose-response curves for continuous treatments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "dose-response"
    ],
    "summary": "The causal-curve package provides methods for estimating continuous treatment dose-response curves using Generalized Propensity Score (GPS) and Targeted Maximum Likelihood Estimation (TMLE). It is useful for researchers and practitioners in causal inference who are working with continuous treatments.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing the impact of dosage levels in drug studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for dose-response estimation",
      "how to estimate continuous treatment effects in python",
      "GPS methods in python",
      "TMLE methods for continuous treatments",
      "causal inference tools in python",
      "continuous treatment analysis python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fastmatch",
    "description": "Fast k-nearest-neighbor matching for large datasets using Facebook's FAISS library.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/py-econometrics/fastmatch",
    "github_url": null,
    "url": "https://github.com/py-econometrics/fastmatch",
    "install": "pip install fastmatch",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "Fastmatch is a Python package designed for efficient k-nearest-neighbor matching on large datasets, leveraging Facebook's FAISS library. It is particularly useful for researchers and data scientists working in causal inference and matching scenarios.",
    "use_cases": [
      "Matching large datasets for causal analysis",
      "Conducting A/B tests with efficient neighbor matching"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for k-nearest-neighbor matching",
      "how to perform causal inference in python",
      "fastmatch package documentation",
      "efficient matching for large datasets in python",
      "using FAISS for matching in python",
      "best practices for causal inference with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mcf (Modified Causal Forest)",
    "description": "Comprehensive Python implementation for heterogeneous treatment effect estimation. Handles binary/multiple discrete treatments with optimal policy learning via Policy Trees.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://mcfpy.github.io/mcf/",
    "github_url": "https://github.com/MCFpy/mcf",
    "url": "https://github.com/MCFpy/mcf",
    "install": "pip install mcf",
    "tags": [
      "causal inference",
      "treatment effects",
      "policy learning"
    ],
    "best_for": "CATE estimation with policy tree optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "policy-learning"
    ],
    "summary": "The mcf package provides a comprehensive Python implementation for estimating heterogeneous treatment effects. It is designed for researchers and practitioners interested in causal inference and optimal policy learning.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Optimizing marketing strategies based on treatment outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate treatment effects in python",
      "policy learning with python",
      "modified causal forest implementation",
      "A/B testing in python",
      "python library for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "policy learning"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pydtr",
    "description": "Dynamic treatment regimes using Iterative Q-Learning. Scikit-learn compatible for multi-stage optimal treatment sequencing.",
    "category": "Causal Inference & Matching",
    "docs_url": null,
    "github_url": "https://github.com/fullflu/pydtr",
    "url": "https://pypi.org/project/pydtr/",
    "install": "pip install pydtr",
    "tags": [
      "dynamic treatment",
      "reinforcement learning",
      "causal inference"
    ],
    "best_for": "Multi-stage dynamic treatment regimes",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "reinforcement-learning"
    ],
    "summary": "pydtr is a Python package designed for implementing dynamic treatment regimes using Iterative Q-Learning. It is compatible with Scikit-learn and is used for multi-stage optimal treatment sequencing in causal inference contexts.",
    "use_cases": [
      "optimizing treatment strategies in clinical trials",
      "analyzing multi-stage decision processes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dynamic treatment regimes",
      "how to implement iterative Q-learning in python",
      "scikit-learn compatible treatment sequencing",
      "dynamic treatment using reinforcement learning in python"
    ],
    "primary_use_cases": [
      "dynamic treatment optimization",
      "multi-stage treatment sequencing"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "scikit-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pyregadj",
    "description": "Regression and ML adjustments to treatment effects in RCTs. Implements List et al. (2024) methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://github.com/vyasenov/pyregadj",
    "github_url": "https://github.com/vyasenov/pyregadj",
    "url": "https://github.com/vyasenov/pyregadj",
    "install": "pip install pyregadj",
    "tags": [
      "RCT",
      "regression adjustment",
      "treatment effects"
    ],
    "best_for": "Covariate adjustment in experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "pyregadj provides methods for regression and machine learning adjustments to treatment effects in randomized controlled trials (RCTs). It is useful for researchers and data scientists working in causal inference.",
    "use_cases": [
      "Adjusting treatment effects in clinical trials",
      "Analyzing data from randomized controlled trials"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression adjustment",
      "how to analyze treatment effects in RCTs with python",
      "machine learning adjustments for causal inference in python",
      "pyregadj documentation",
      "RCT analysis tools in python",
      "methods for treatment effects in python"
    ],
    "primary_use_cases": [
      "regression adjustment",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "List et al. (2024)",
    "maintenance_status": "active"
  },
  {
    "name": "scikit-uplift",
    "description": "Focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://scikit-uplift.readthedocs.io/en/latest/",
    "github_url": "https://github.com/maks-sh/scikit-uplift",
    "url": "https://github.com/maks-sh/scikit-uplift",
    "install": "pip install scikit-uplift",
    "tags": [
      "causal inference",
      "matching"
    ],
    "best_for": "Estimating treatment effects, propensity score matching, observational studies",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "matching"
    ],
    "summary": "scikit-uplift focuses on uplift modeling and estimating heterogeneous treatment effects using various ML-based methods. It is used by data scientists and researchers interested in causal inference and treatment effect estimation.",
    "use_cases": [
      "Estimating the effect of marketing campaigns",
      "Personalizing treatment recommendations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for uplift modeling",
      "how to estimate treatment effects in python",
      "causal inference library in python",
      "matching methods in python",
      "scikit-uplift documentation",
      "uplift modeling techniques python"
    ],
    "primary_use_cases": [
      "uplift modeling",
      "heterogeneous treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "y0",
    "description": "Causal inference framework providing tools for causal graph manipulation and effect identification.",
    "category": "Causal Inference & Matching",
    "docs_url": "https://y0.readthedocs.io/",
    "github_url": "https://github.com/y0-causal-inference/y0",
    "url": "https://github.com/y0-causal-inference/y0",
    "install": "pip install y0",
    "tags": [
      "causal inference",
      "graphs",
      "identification"
    ],
    "best_for": "Causal graph manipulation and do-calculus",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "graphs"
    ],
    "summary": "y0 is a causal inference framework that provides tools for manipulating causal graphs and identifying effects. It is used by data scientists and researchers interested in causal analysis.",
    "use_cases": [
      "Analyzing the impact of interventions",
      "Understanding causal relationships in data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to manipulate causal graphs in python",
      "tools for effect identification in python",
      "causal inference framework python",
      "python causal graphs library",
      "identifying effects with python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ATbounds",
    "description": "Implements modern treatment effect bounds beyond basic Manski worst-case scenarios. Provides tighter bounds using monotonicity, mean independence, and other assumptions following Lee and Weidner (2021).",
    "category": "Causal Inference (Bounds)",
    "docs_url": "https://cran.r-project.org/web/packages/ATbounds/ATbounds.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ATbounds",
    "install": "install.packages(\"ATbounds\")",
    "tags": [
      "partial-identification",
      "bounds",
      "treatment-effects",
      "Manski",
      "monotonicity"
    ],
    "best_for": "Modern treatment effect bounds with tighter identification under various assumptions, implementing Lee & Weidner (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "ATbounds implements modern treatment effect bounds that go beyond basic Manski worst-case scenarios, providing tighter bounds using various assumptions. It is useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Estimating treatment effects under monotonicity assumptions",
      "Analyzing data with mean independence assumptions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for treatment effect bounds",
      "how to implement causal inference in R",
      "bounds for treatment effects in R",
      "R library for partial identification",
      "Manski bounds implementation in R",
      "causal inference tools in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Lee and Weidner (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "CausalGPS",
    "description": "Machine learning-based generalized propensity score estimation for continuous treatments. Uses SuperLearner ensemble methods for flexible estimation of dose-response curves.",
    "category": "Causal Inference (Continuous Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/CausalGPS/CausalGPS.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CausalGPS",
    "install": "install.packages(\"CausalGPS\")",
    "tags": [
      "GPS",
      "continuous-treatment",
      "machine-learning",
      "SuperLearner",
      "dose-response"
    ],
    "best_for": "ML-based generalized propensity scores for continuous treatment dose-response estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "CausalGPS is a machine learning package designed for generalized propensity score estimation for continuous treatments. It employs SuperLearner ensemble methods to provide flexible estimation of dose-response curves, making it useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing dose-response relationships in clinical trials"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized propensity score",
      "how to estimate dose-response curves in R",
      "machine learning for continuous treatments R",
      "CausalGPS documentation",
      "SuperLearner for causal inference",
      "R library for dose-response analysis"
    ],
    "primary_use_cases": [
      "generalized propensity score estimation",
      "dose-response curve estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DRDID",
    "description": "Implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. Handles both panel data and repeated cross-sections in the canonical 2x2 DiD setting with covariates, providing robustness against model misspecification.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://psantanna.com/DRDID/",
    "github_url": "https://github.com/pedrohcgs/DRDID",
    "url": "https://cran.r-project.org/package=DRDID",
    "install": "install.packages(\"DRDID\")",
    "tags": [
      "doubly-robust",
      "difference-in-differences",
      "inverse-probability-weighting",
      "ATT",
      "covariates"
    ],
    "best_for": "Two-period DiD with covariates requiring robust estimation against model misspecification, implementing Sant'Anna & Zhao (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DRDID implements locally efficient doubly robust DiD estimators that combine inverse probability weighting and outcome regression for improved statistical properties. It is used by researchers and practitioners working with panel data and repeated cross-sections in causal inference.",
    "use_cases": [
      "Estimating treatment effects in policy evaluations",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for doubly robust DiD",
      "how to implement difference-in-differences in R",
      "R library for causal inference with covariates",
      "inverse probability weighting in R",
      "outcome regression for DiD in R",
      "statistical properties of DiD estimators in R"
    ],
    "primary_use_cases": [
      "causal inference analysis",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "HonestDiD",
    "description": "Constructs robust confidence intervals for DiD and event-study designs under violations of parallel trends. Allows researchers to conduct sensitivity analysis by relaxing the parallel trends assumption using smoothness or relative magnitude restrictions on pre-trend violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/asheshrambachan/HonestDiD",
    "github_url": "https://github.com/asheshrambachan/HonestDiD",
    "url": "https://cran.r-project.org/package=HonestDiD",
    "install": "install.packages(\"HonestDiD\")",
    "tags": [
      "sensitivity-analysis",
      "parallel-trends",
      "robust-inference",
      "confidence-intervals",
      "event-study"
    ],
    "best_for": "Assessing how treatment effect conclusions change under plausible parallel trends violations, implementing Rambachan & Roth (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "HonestDiD constructs robust confidence intervals for Difference-in-Differences (DiD) and event-study designs, addressing violations of parallel trends. It is primarily used by researchers conducting sensitivity analysis in causal inference.",
    "use_cases": [
      "Analyzing the impact of policy changes using DiD",
      "Conducting sensitivity analysis for pre-trend violations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal inference",
      "how to conduct sensitivity analysis in R",
      "robust confidence intervals for DiD",
      "event-study analysis in R",
      "parallel trends violation analysis",
      "R library for event-study designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "bacondecomp",
    "description": "Performs Goodman-Bacon decomposition showing how two-way fixed effects (TWFE) estimates are weighted averages of all possible 2\u00d72 DiD comparisons. Essential for diagnosing negative weights problems in staggered adoption designs.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/bacondecomp/bacondecomp.pdf",
    "github_url": "https://github.com/evanjflack/bacondecomp",
    "url": "https://cran.r-project.org/package=bacondecomp",
    "install": "install.packages(\"bacondecomp\")",
    "tags": [
      "DiD",
      "TWFE",
      "Goodman-Bacon",
      "decomposition",
      "staggered-adoption"
    ],
    "best_for": "Goodman-Bacon decomposition for diagnosing negative weights in TWFE staggered DiD designs",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "decomposition"
    ],
    "summary": "The bacondecomp package performs Goodman-Bacon decomposition, which helps in understanding how two-way fixed effects estimates are weighted averages of all possible 2\u00d72 DiD comparisons. It is essential for diagnosing negative weights problems in staggered adoption designs.",
    "use_cases": [
      "Analyzing staggered adoption designs",
      "Diagnosing negative weights in DiD estimates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Goodman-Bacon decomposition",
      "how to diagnose negative weights in staggered adoption",
      "two-way fixed effects in R",
      "R library for causal inference",
      "Goodman-Bacon decomposition in R",
      "staggered adoption analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "did",
    "description": "Implements group-time average treatment effects (ATT(g,t)) for staggered DiD designs with multiple periods and variation in treatment timing. Provides flexible aggregation into event-study plots or overall treatment effect estimates, addressing the well-documented negative weighting issues with conventional TWFE under staggered adoption.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://bcallaway11.github.io/did/",
    "github_url": "https://github.com/bcallaway11/did",
    "url": "https://cran.r-project.org/package=did",
    "install": "install.packages(\"did\")",
    "tags": [
      "difference-in-differences",
      "staggered-adoption",
      "event-study",
      "treatment-effects",
      "panel-data"
    ],
    "best_for": "Staggered rollout designs where different units adopt treatment at different times, implementing the Callaway & Sant'Anna (2021) estimator",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The 'did' package implements group-time average treatment effects for staggered DiD designs, allowing for flexible aggregation into event-study plots and overall treatment effect estimates. It is primarily used by researchers and practitioners in causal inference to address issues with conventional methods under staggered adoption.",
    "use_cases": [
      "Analyzing treatment effects in policy evaluations",
      "Creating event-study plots for staggered adoption scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for group-time average treatment effects",
      "how to create event-study plots in R",
      "difference-in-differences analysis in R",
      "staggered adoption treatment effects R",
      "R library for causal inference",
      "R package for panel data treatment effects"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "didimputation",
    "description": "Implements the imputation-based DiD estimator that first estimates Y(0) counterfactuals from untreated observations using two-way fixed effects, then imputes treatment effects for treated units. Avoids negative weighting problems of conventional TWFE under heterogeneous treatment effects.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://github.com/kylebutts/didimputation",
    "github_url": "https://github.com/kylebutts/didimputation",
    "url": "https://cran.r-project.org/package=didimputation",
    "install": "install.packages(\"didimputation\")",
    "tags": [
      "imputation",
      "two-way-fixed-effects",
      "event-study",
      "counterfactual",
      "robust-estimation"
    ],
    "best_for": "Event-study designs where imputation-based correction for TWFE bias is preferred, implementing Borusyak, Jaravel & Spiess (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "counterfactual",
      "robust-estimation"
    ],
    "summary": "The didimputation package implements an imputation-based DiD estimator that estimates Y(0) counterfactuals from untreated observations using two-way fixed effects. It is designed for researchers and practitioners who need to analyze treatment effects while avoiding negative weighting issues in heterogeneous treatment scenarios.",
    "use_cases": [
      "Estimating treatment effects in policy evaluation",
      "Analyzing the impact of interventions in social sciences"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for DiD estimation",
      "how to impute treatment effects in R",
      "two-way fixed effects in R",
      "counterfactual analysis in R",
      "robust estimation methods in R",
      "event study analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fastdid",
    "description": "High-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets with millions of observations. Reduces computation time from hours to seconds while supporting time-varying covariates and multiple events per unit.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://tsailintung.github.io/fastdid",
    "github_url": "https://github.com/TsaiLintung/fastdid",
    "url": "https://cran.r-project.org/package=fastdid",
    "install": "install.packages(\"fastdid\")",
    "tags": [
      "high-performance",
      "large-scale",
      "staggered-DiD",
      "time-varying-covariates",
      "fast-computation"
    ],
    "best_for": "Large-scale applications where standard did package is computationally prohibitive, with support for time-varying covariates",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "fastdid is a high-performance implementation of Callaway & Sant'Anna estimators optimized for large datasets. It is designed for researchers and practitioners who need to conduct causal inference analysis efficiently.",
    "use_cases": [
      "Estimating treatment effects in large observational studies",
      "Analyzing the impact of policy changes over time"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to implement Callaway & Sant'Anna estimators in R",
      "fast computation for DiD analysis in R",
      "large-scale causal inference R package",
      "time-varying covariates in R",
      "high-performance DiD estimators R"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fect",
    "description": "Fixed Effects Counterfactual Estimators (v2.0+) incorporating gsynth functionality. Supports treatment switching on/off with carryover effects, matrix completion methods, and Rambachan & Roth sensitivity analysis for parallel trends violations.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://yiqingxu.org/packages/fect/",
    "github_url": "https://github.com/xuyiqing/fect",
    "url": "https://cran.r-project.org/package=fect",
    "install": "install.packages(\"fect\")",
    "tags": [
      "counterfactual",
      "matrix-completion",
      "interactive-fixed-effects",
      "sensitivity-analysis",
      "carryover"
    ],
    "best_for": "Counterfactual estimation with interactive fixed effects, treatment switching, and sensitivity analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The fect package provides Fixed Effects Counterfactual Estimators that incorporate gsynth functionality, allowing for treatment switching and carryover effects. It is used by researchers and practitioners in causal inference, particularly in the context of DiD analyses.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing panel data with fixed effects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for fixed effects counterfactual estimators",
      "how to perform sensitivity analysis in R",
      "R treatment switching analysis",
      "matrix completion methods in R",
      "R package for carryover effects",
      "R gsynth functionality"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "staggered",
    "description": "Provides the efficient estimator for randomized staggered rollout designs, offering optimal weighting schemes for treatment effect estimation. Also implements Callaway & Sant'Anna and Sun & Abraham estimators with design-based Fisher inference for randomized experiments.",
    "category": "Causal Inference (DiD)",
    "docs_url": "https://cran.r-project.org/web/packages/staggered/readme/README.html",
    "github_url": "https://github.com/jonathandroth/staggered",
    "url": "https://cran.r-project.org/package=staggered",
    "install": "install.packages(\"staggered\")",
    "tags": [
      "staggered-rollout",
      "randomized-experiments",
      "efficient-estimation",
      "event-study",
      "fisher-inference"
    ],
    "best_for": "Randomized experiments with staggered treatment timing where efficiency gains matter, implementing Roth & Sant'Anna (2023)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The staggered package provides efficient estimators for randomized staggered rollout designs, implementing optimal weighting schemes for treatment effect estimation. It is useful for researchers and practitioners conducting randomized experiments.",
    "use_cases": [
      "Estimating treatment effects in randomized staggered rollout designs",
      "Conducting event studies with optimal weighting schemes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for staggered rollout designs",
      "how to estimate treatment effects in R",
      "efficient estimation for randomized experiments in R",
      "Callaway & Sant'Anna estimator R package",
      "Sun & Abraham estimator in R",
      "Fisher inference for randomized experiments R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DTRreg",
    "description": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions. Implements methods for finding optimal treatment rules that adapt over time based on patient characteristics.",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DTRreg/DTRreg.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DTRreg",
    "install": "install.packages(\"DTRreg\")",
    "tags": [
      "dynamic-treatment",
      "G-estimation",
      "sequential-decisions",
      "optimal-treatment",
      "personalization"
    ],
    "best_for": "Dynamic treatment regime estimation via G-estimation for sequential treatment decisions",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DTRreg is a package for estimating dynamic treatment regimes using G-estimation for sequential treatment decisions. It is used by researchers and practitioners in causal inference to find optimal treatment rules that adapt over time based on patient characteristics.",
    "use_cases": [
      "Estimating optimal treatment strategies in clinical trials",
      "Adapting treatment plans based on patient responses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic treatment regimes",
      "how to perform G-estimation in R",
      "sequential treatment decisions in R",
      "optimal treatment rules R package",
      "personalization in treatment decisions R",
      "dynamic treatment estimation R"
    ],
    "primary_use_cases": [
      "dynamic treatment estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DynTxRegime",
    "description": "Comprehensive package for dynamic treatment regimes implementing Q-learning, value search, and outcome-weighted learning methods. Accompanies the textbook 'Dynamic Treatment Regimes' (Tsiatis et al., 2020).",
    "category": "Causal Inference (Dynamic Treatment)",
    "docs_url": "https://cran.r-project.org/web/packages/DynTxRegime/DynTxRegime.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=DynTxRegime",
    "install": "install.packages(\"DynTxRegime\")",
    "tags": [
      "dynamic-treatment",
      "Q-learning",
      "value-search",
      "reinforcement-learning",
      "personalized-medicine"
    ],
    "best_for": "Comprehensive dynamic treatment regimes with Q-learning and value search, from Tsiatis et al. (2020) textbook",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DynTxRegime is a comprehensive R package designed for implementing dynamic treatment regimes using Q-learning, value search, and outcome-weighted learning methods. It is particularly useful for researchers and practitioners in the field of personalized medicine.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for dynamic treatment regimes",
      "how to implement Q-learning in R",
      "outcome-weighted learning methods in R",
      "personalized medicine R package",
      "value search in dynamic treatment",
      "causal inference R tools"
    ],
    "primary_use_cases": [
      "dynamic treatment regimes",
      "Q-learning implementation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tsiatis et al. (2020)",
    "maintenance_status": "active"
  },
  {
    "name": "eventstudyr",
    "description": "Implements event study best practices from Freyaldenhoven et al. (2021) including sup-t confidence bands for uniform inference and formal pre-trend testing. Provides robust methods for dynamic treatment effect estimation.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/eventstudyr/eventstudyr.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=eventstudyr",
    "install": "install.packages(\"eventstudyr\")",
    "tags": [
      "event-study",
      "pre-trends",
      "sup-t-bands",
      "uniform-inference",
      "dynamic-effects"
    ],
    "best_for": "Event study best practices with sup-t confidence bands and formal pre-trend testing, implementing Freyaldenhoven et al. (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "event-study"
    ],
    "summary": "The eventstudyr package implements event study best practices, including sup-t confidence bands for uniform inference and formal pre-trend testing. It is designed for researchers and practitioners interested in dynamic treatment effect estimation.",
    "use_cases": [
      "Analyzing the impact of a policy change on stock prices",
      "Evaluating the effect of a new marketing campaign on sales"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for event study analysis",
      "how to implement pre-trend testing in R",
      "event study methods in R",
      "dynamic treatment effects R package",
      "sup-t confidence bands R",
      "uniform inference in R"
    ],
    "primary_use_cases": [
      "dynamic treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Freyaldenhoven et al. (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "fixes",
    "description": "Streamlined event study workflows with simple run_es() and plot_es() functions built on fixest. New 2025 package providing convenient wrappers for common event study specifications.",
    "category": "Causal Inference (Event Study)",
    "docs_url": "https://cran.r-project.org/web/packages/fixes/fixes.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=fixes",
    "install": "install.packages(\"fixes\")",
    "tags": [
      "event-study",
      "fixest",
      "DiD",
      "streamlined",
      "visualization"
    ],
    "best_for": "Streamlined event study workflows with simple run_es() and plot_es() functions on fixest",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "event-study"
    ],
    "summary": "The 'fixes' package streamlines event study workflows by providing simple functions like run_es() and plot_es() built on the fixest framework. It is designed for users looking for convenient wrappers for common event study specifications.",
    "use_cases": [
      "Analyzing the impact of a policy change on stock prices",
      "Visualizing event study results for academic research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for event study",
      "how to conduct event studies in R",
      "fixest event study tutorial",
      "visualization of event studies in R",
      "streamlined event study workflows R",
      "event study analysis with fixes package"
    ],
    "api_complexity": "simple",
    "framework_compatibility": [
      "fixest"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "inferference",
    "description": "Computes inverse probability weighted (IPW) causal effects under partial interference following Tchetgen Tchetgen and VanderWeele (2012). Handles spillover effects within groups while maintaining independence across groups.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/inferference/inferference.pdf",
    "github_url": "https://github.com/bsaul/inferference",
    "url": "https://cran.r-project.org/package=inferference",
    "install": "install.packages(\"inferference\")",
    "tags": [
      "interference",
      "spillovers",
      "IPW",
      "partial-interference",
      "SUTVA-violations"
    ],
    "best_for": "IPW causal effects under partial interference with within-group spillovers, implementing Tchetgen Tchetgen & VanderWeele (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The 'inferference' package computes inverse probability weighted (IPW) causal effects under partial interference, addressing spillover effects within groups while maintaining independence across groups. It is useful for researchers and practitioners in causal inference who are dealing with complex group interactions.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to compute IPW causal effects in R",
      "spillover effects analysis in R",
      "partial interference methods in R",
      "Tchetgen Tchetgen and VanderWeele causal effects",
      "R package for handling SUTVA violations"
    ],
    "primary_use_cases": [
      "causal effect estimation under partial interference",
      "analyzing spillover effects"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tchetgen Tchetgen and VanderWeele (2012)",
    "maintenance_status": "active"
  },
  {
    "name": "latenetwork",
    "description": "Handles both noncompliance AND network interference of unknown form following Hoshino and Yanagi (2023 JASA). Provides valid inference when treatment effects spill over through network connections.",
    "category": "Causal Inference (Interference)",
    "docs_url": "https://cran.r-project.org/web/packages/latenetwork/latenetwork.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=latenetwork",
    "install": "install.packages(\"latenetwork\")",
    "tags": [
      "network-interference",
      "noncompliance",
      "LATE",
      "spillovers",
      "IV"
    ],
    "best_for": "LATE estimation with network interference and noncompliance, implementing Hoshino & Yanagi (2023 JASA)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "network-interference"
    ],
    "summary": "The latenetwork package handles noncompliance and network interference of unknown form, providing valid inference when treatment effects spill over through network connections. It is useful for researchers and practitioners in causal inference who need to account for these complexities in their analyses.",
    "use_cases": [
      "Analyzing treatment effects in social networks",
      "Estimating causal effects in experimental designs with noncompliance"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for network interference",
      "how to handle noncompliance in R",
      "R causal inference package",
      "spillover effects analysis in R",
      "network effects in treatment studies",
      "Hoshino and Yanagi causal inference R package"
    ],
    "primary_use_cases": [
      "valid inference in causal studies",
      "analyzing spillover effects"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Hoshino and Yanagi (2023)",
    "maintenance_status": "active"
  },
  {
    "name": "EValue",
    "description": "Conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. Computes E-values representing the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://louisahsmith.github.io/evalue/",
    "github_url": "https://github.com/mayamathur/evalue_package",
    "url": "https://cran.r-project.org/package=EValue",
    "install": "install.packages(\"EValue\")",
    "tags": [
      "E-value",
      "unmeasured-confounding",
      "sensitivity-analysis",
      "selection-bias",
      "meta-analysis"
    ],
    "best_for": "Quantifying the minimum confounding strength on the risk ratio scale needed to explain away observed treatment-outcome associations, implementing VanderWeele & Ding (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EValue conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error in observational studies and meta-analyses. It computes E-values that represent the minimum strength of association unmeasured confounders would need to fully explain away an observed effect.",
    "use_cases": [
      "Assessing the impact of unmeasured confounding in observational studies",
      "Evaluating selection bias in meta-analyses"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for sensitivity analysis",
      "how to compute E-values in R",
      "unmeasured confounding analysis R",
      "selection bias analysis in R",
      "meta-analysis tools in R",
      "E-value computation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SuperLearner",
    "description": "Implements the Super Learner algorithm for optimal ensemble prediction via cross-validation. Creates weighted combinations of multiple ML algorithms (XGBoost, Random Forest, glmnet, neural networks, SVM, BART) with guaranteed asymptotic optimality.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html",
    "github_url": "https://github.com/ecpolley/SuperLearner",
    "url": "https://cran.r-project.org/package=SuperLearner",
    "install": "install.packages(\"SuperLearner\")",
    "tags": [
      "ensemble-learning",
      "cross-validation",
      "stacking",
      "prediction",
      "model-selection"
    ],
    "best_for": "Building optimal prediction ensembles for nuisance parameter estimation (propensity scores, outcome models) in causal inference, implementing van der Laan, Polley & Hubbard (2007)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "ensemble-learning"
    ],
    "summary": "SuperLearner implements the Super Learner algorithm for optimal ensemble prediction through cross-validation. It creates weighted combinations of various machine learning algorithms, making it suitable for users looking to improve prediction accuracy.",
    "use_cases": [
      "Combining predictions from multiple machine learning models",
      "Improving prediction accuracy in complex datasets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for ensemble learning",
      "how to implement Super Learner in R",
      "cross-validation techniques in R",
      "optimal ensemble prediction R package",
      "weighted combinations of ML algorithms R",
      "using SuperLearner for model selection"
    ],
    "primary_use_cases": [
      "model-selection"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret",
      "mlr"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "causalToolbox",
    "description": "Implements meta-learner algorithms (S-learner, T-learner, X-learner) for heterogeneous treatment effect estimation using flexible base learners including honest Random Forests and BART for personalized CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://rdrr.io/github/soerenkuenzel/causalToolbox/",
    "github_url": "https://github.com/forestry-labs/causalToolbox",
    "url": "https://github.com/forestry-labs/causalToolbox",
    "install": "devtools::install_github(\"forestry-labs/causalToolbox\")",
    "tags": [
      "metalearners",
      "X-learner",
      "T-learner",
      "S-learner",
      "CATE"
    ],
    "best_for": "Comparing and benchmarking different CATE meta-learner strategies (S/T/X-learner) with BART or RF base learners, implementing K\u00fcnzel et al. (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "causalToolbox implements meta-learner algorithms for estimating heterogeneous treatment effects using flexible base learners. It is useful for researchers and practitioners in causal inference looking to personalize treatment effect estimation.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to estimate treatment effects in R",
      "R meta-learner algorithms",
      "personalized CATE estimation in R",
      "using Random Forests for causal inference",
      "BART for treatment effect estimation"
    ],
    "primary_use_cases": [
      "heterogeneous treatment effect estimation",
      "personalized CATE estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "causalweight",
    "description": "Semiparametric causal inference methods based on inverse probability weighting and double machine learning for average treatment effects, causal mediation analysis (direct/indirect effects), and dynamic treatment evaluation. Supports LATE estimation with instrumental variables.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/causalweight/causalweight.pdf",
    "github_url": "https://github.com/hbodory/causalweight",
    "url": "https://cran.r-project.org/package=causalweight",
    "install": "install.packages(\"causalweight\")",
    "tags": [
      "inverse-probability-weighting",
      "causal-mediation",
      "double-machine-learning",
      "LATE",
      "instrumental-variables"
    ],
    "best_for": "Mediation analysis and LATE estimation using weighting-based approaches with flexible nuisance estimation, implementing Huber (2014) and Fr\u00f6lich & Huber (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The causalweight package provides semiparametric causal inference methods using inverse probability weighting and double machine learning techniques. It is designed for researchers and practitioners interested in estimating average treatment effects, causal mediation analysis, and dynamic treatment evaluation.",
    "use_cases": [
      "Estimating average treatment effects in clinical trials",
      "Conducting causal mediation analysis in social sciences"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference",
      "how to perform causal mediation analysis in R",
      "R package for double machine learning",
      "LATE estimation in R",
      "inverse probability weighting in R",
      "instrumental variables in R"
    ],
    "primary_use_cases": [
      "average treatment effects estimation",
      "causal mediation analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ddml",
    "description": "Streamlined double/debiased machine learning estimation with emphasis on (short-)stacking to combine multiple base learners, increasing robustness to unknown data generating processes. Designed as a complement to DoubleML with simpler syntax.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://thomaswiemann.com/ddml/",
    "github_url": "https://github.com/thomaswiemann/ddml",
    "url": "https://cran.r-project.org/package=ddml",
    "install": "install.packages(\"ddml\")",
    "tags": [
      "double-machine-learning",
      "stacking",
      "model-averaging",
      "treatment-effects",
      "causal-inference"
    ],
    "best_for": "Quick, robust DML estimation using short-stacking to ensemble multiple ML learners without extensive tuning, implementing Ahrens, Hansen, Schaffer & Wiemann (2024)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "ddml is a package designed for streamlined double/debiased machine learning estimation, focusing on stacking to enhance robustness against unknown data generating processes. It is particularly useful for researchers and practitioners in causal inference who seek a simpler syntax compared to DoubleML.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Combining multiple machine learning models for improved predictions"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for double machine learning",
      "how to perform causal inference in R",
      "stacking models in R",
      "treatment effects estimation in R",
      "double machine learning R tutorial",
      "robust machine learning methods R"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "DoubleML"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "grf",
    "description": "Forest-based statistical estimation and inference for heterogeneous treatment effects, supporting multiple treatment arms, instrumental variables, survival outcomes, and quantile regression\u2014all with honest estimation and valid confidence intervals. The most widely-used R package for CATE estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://grf-labs.github.io/grf/",
    "github_url": "https://github.com/grf-labs/grf",
    "url": "https://cran.r-project.org/package=grf",
    "install": "install.packages(\"grf\")",
    "tags": [
      "causal-forest",
      "heterogeneous-treatment-effects",
      "CATE",
      "machine-learning",
      "econometrics"
    ],
    "best_for": "Estimating individual-level treatment effects (CATE) with valid statistical inference in RCTs or observational studies, implementing Athey, Tibshirani & Wager (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "The 'grf' package provides forest-based statistical estimation and inference for heterogeneous treatment effects, making it suitable for various complex scenarios such as multiple treatment arms and instrumental variables. It is widely used by researchers and practitioners in fields like econometrics and machine learning for causal analysis.",
    "use_cases": [
      "Estimating causal effects in clinical trials",
      "Analyzing treatment effects in observational studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for causal forest",
      "how to estimate treatment effects in R",
      "R package for heterogeneous treatment effects",
      "using grf for CATE estimation",
      "forest-based inference in R",
      "R package for quantile regression"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "hdm",
    "description": "High-dimensional statistical methods featuring heteroscedasticity-robust LASSO with theoretically-grounded penalty selection, post-double-selection inference, and treatment effect estimation under sparsity assumptions for high-dimensional controls.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/hdm/vignettes/hdm.html",
    "github_url": "https://github.com/MartinSpindler/hdm",
    "url": "https://cran.r-project.org/package=hdm",
    "install": "install.packages(\"hdm\")",
    "tags": [
      "lasso",
      "post-double-selection",
      "high-dimensional",
      "instrumental-variables",
      "sparsity"
    ],
    "best_for": "Post-double-selection LASSO inference and treatment effect estimation when the true model is sparse, implementing Belloni, Chernozhukov & Hansen (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "The hdm package provides high-dimensional statistical methods that focus on heteroscedasticity-robust LASSO, enabling users to perform post-double-selection inference and treatment effect estimation under sparsity assumptions. It is primarily used by researchers and practitioners in causal inference and statistics.",
    "use_cases": [
      "Estimating treatment effects in high-dimensional settings",
      "Conducting post-double-selection inference for causal analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for high-dimensional LASSO",
      "how to perform treatment effect estimation in R",
      "heteroscedasticity-robust methods in R",
      "post-double-selection inference R package",
      "sparsity assumptions in causal inference",
      "instrumental variables in high-dimensional settings"
    ],
    "primary_use_cases": [
      "treatment effect estimation",
      "post-double-selection inference"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ltmle",
    "description": "Targeted maximum likelihood estimation for treatment/censoring-specific mean outcomes with time-varying treatments and confounders. Supports longitudinal settings, marginal structural models, and dynamic treatment regimes alongside IPTW and G-computation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://joshuaschwab.github.io/ltmle/",
    "github_url": "https://github.com/joshuaschwab/ltmle",
    "url": "https://cran.r-project.org/package=ltmle",
    "install": "install.packages(\"ltmle\")",
    "tags": [
      "TMLE",
      "longitudinal",
      "time-varying-treatment",
      "dynamic-regimes",
      "MSM"
    ],
    "best_for": "Causal inference with time-varying treatments, time-varying confounders, and right-censored longitudinal data, implementing Lendle et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "longitudinal-data"
    ],
    "summary": "The ltmle package provides targeted maximum likelihood estimation for treatment and censoring-specific mean outcomes in longitudinal settings. It is particularly useful for researchers dealing with time-varying treatments and confounders in causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing longitudinal health data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for targeted maximum likelihood estimation",
      "how to use ltmle for longitudinal data analysis",
      "time-varying treatment analysis in R",
      "causal inference with ltmle",
      "dynamic treatment regimes in R",
      "marginal structural models R package"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "sensemakr",
    "description": "Suite of sensitivity analysis tools extending the traditional omitted variable bias framework, computing robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to assess how strong unmeasured confounders would need to be to overturn conclusions.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://carloscinelli.com/sensemakr/",
    "github_url": "https://github.com/carloscinelli/sensemakr",
    "url": "https://cran.r-project.org/package=sensemakr",
    "install": "install.packages(\"sensemakr\")",
    "tags": [
      "sensitivity-analysis",
      "omitted-variable-bias",
      "robustness-value",
      "causal-inference",
      "regression"
    ],
    "best_for": "Assessing how strong unmeasured confounders would need to be to overturn regression-based causal conclusions, implementing Cinelli & Hazlett (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "sensitivity-analysis",
      "regression"
    ],
    "summary": "sensemakr is a suite of sensitivity analysis tools that extends the traditional omitted variable bias framework. It computes robustness values, bias-adjusted estimates, and sensitivity contour plots for OLS regression to evaluate the impact of unmeasured confounders on conclusions.",
    "use_cases": [
      "Assessing the robustness of regression results",
      "Evaluating the impact of unmeasured confounders on study conclusions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for sensitivity analysis",
      "how to assess omitted variable bias in R",
      "tools for robustness analysis in regression",
      "sensitivity contour plots in R",
      "bias-adjusted estimates in R",
      "methods for causal inference in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tmle",
    "description": "Implements targeted maximum likelihood estimation for point treatment effects with binary or continuous outcomes. Estimates ATE, ATT, ATC, and supports marginal structural models. Integrates SuperLearner for data-adaptive nuisance parameter estimation.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://cran.r-project.org/web/packages/tmle/tmle.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tmle",
    "install": "install.packages(\"tmle\")",
    "tags": [
      "TMLE",
      "causal-inference",
      "ATE",
      "doubly-robust",
      "propensity-score"
    ],
    "best_for": "Estimating point treatment effects (ATE/ATT/ATC) in observational studies with binary treatments, implementing Gruber & van der Laan (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The tmle package implements targeted maximum likelihood estimation for point treatment effects, supporting both binary and continuous outcomes. It is used by statisticians and data scientists working on causal inference problems.",
    "use_cases": [
      "Estimating average treatment effects in clinical trials",
      "Analyzing the impact of a new policy on health outcomes"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for targeted maximum likelihood estimation",
      "how to estimate ATE in R",
      "R package for causal inference",
      "tmle R documentation",
      "how to use SuperLearner in R",
      "tmle for binary outcomes",
      "tmle for continuous outcomes"
    ],
    "primary_use_cases": [
      "estimating average treatment effects (ATE)",
      "supporting marginal structural models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tmle3",
    "description": "A modular, extensible framework for targeted minimum loss-based estimation supporting custom TMLE parameters through a unified interface. Part of the tlverse ecosystem, designed to be as general as the mathematical TMLE framework itself for complex analyses.",
    "category": "Causal Inference (ML)",
    "docs_url": "https://tlverse.org/tmle3/",
    "github_url": "https://github.com/tlverse/tmle3",
    "url": "https://github.com/tlverse/tmle3",
    "install": "remotes::install_github(\"tlverse/tmle3\")",
    "tags": [
      "TMLE",
      "tlverse",
      "modular",
      "extensible",
      "stochastic-interventions"
    ],
    "best_for": "Complex TMLE analyses requiring custom parameters, mediation, stochastic interventions, or optimal treatment regimes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "tmle3 is a modular and extensible framework designed for targeted minimum loss-based estimation, allowing users to support custom TMLE parameters through a unified interface. It is part of the tlverse ecosystem and is suitable for complex analyses in causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for targeted minimum loss-based estimation",
      "how to use tmle3 for causal inference",
      "tmle3 examples in R",
      "tlverse ecosystem packages",
      "modular framework for TMLE in R",
      "tmle3 documentation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tlverse"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "CBPS",
    "description": "Implements Covariate Balancing Propensity Score, which estimates propensity scores by jointly optimizing treatment prediction and covariate balance via generalized method of moments (GMM). Supports binary, multi-valued, and continuous treatments, as well as longitudinal settings for marginal structural models.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/CBPS/CBPS.pdf",
    "github_url": "https://github.com/kosukeimai/CBPS",
    "url": "https://cran.r-project.org/package=CBPS",
    "install": "install.packages(\"CBPS\")",
    "tags": [
      "propensity-score",
      "covariate-balance",
      "GMM",
      "weighting",
      "treatment-effects"
    ],
    "best_for": "When propensity score model specification is uncertain and you want simultaneous balance optimization, implementing Imai & Ratkovic (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CBPS implements the Covariate Balancing Propensity Score method, which estimates propensity scores by optimizing treatment prediction and covariate balance using generalized method of moments (GMM). It is useful for researchers and practitioners in causal inference who need to analyze treatment effects.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Balancing covariates in experimental designs"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for covariate balancing",
      "how to estimate propensity scores in R",
      "R package for treatment effects analysis",
      "propensity score matching in R",
      "GMM for treatment prediction R",
      "longitudinal treatment effects R"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MatchIt",
    "description": "Comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. Provides a unified interface to multiple matching methods including nearest neighbor, optimal pair, optimal full, genetic, exact, coarsened exact (CEM), cardinality matching, and subclassification with propensity score estimation via GLM, GAM, random forest, and BART.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://kosukeimai.github.io/MatchIt/",
    "github_url": "https://github.com/kosukeimai/MatchIt",
    "url": "https://cran.r-project.org/package=MatchIt",
    "install": "install.packages(\"MatchIt\")",
    "tags": [
      "propensity-score-matching",
      "causal-inference",
      "observational-studies",
      "covariate-balance",
      "treatment-effects"
    ],
    "best_for": "Preprocessing observational data via matching to reduce confounding before estimating causal treatment effects, implementing Ho et al. (2007, 2011)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "MatchIt is a comprehensive matching package that selects matched samples of treated and control groups with similar covariate distributions. It provides a unified interface to multiple matching methods, making it useful for researchers conducting observational studies.",
    "use_cases": [
      "Selecting matched samples for treatment studies",
      "Improving covariate balance in observational research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for propensity score matching",
      "how to perform matching in R",
      "matching methods in R",
      "causal inference with R",
      "observational studies R package",
      "covariate balance in R"
    ],
    "primary_use_cases": [
      "propensity score matching",
      "treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "WeightIt",
    "description": "Unified interface for generating balancing weights for causal effect estimation in observational studies. Supports binary, multi-category, and continuous treatments for point and longitudinal/marginal structural models. Methods include inverse probability weighting (IPW), entropy balancing, covariate balancing propensity score (CBPS), energy balancing, stable balancing weights, BART, and SuperLearner.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/WeightIt/",
    "github_url": "https://github.com/ngreifer/WeightIt",
    "url": "https://cran.r-project.org/package=WeightIt",
    "install": "install.packages(\"WeightIt\")",
    "tags": [
      "propensity-score-weighting",
      "inverse-probability-weighting",
      "entropy-balancing",
      "CBPS",
      "marginal-structural-models"
    ],
    "best_for": "Generating balancing weights using modern weighting methods (IPW, entropy balancing, CBPS, etc.) for point or longitudinal treatments",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "WeightIt provides a unified interface for generating balancing weights used in causal effect estimation for observational studies. It is utilized by researchers and practitioners in fields such as statistics and data science to ensure balanced treatment groups in various types of analyses.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Creating balanced datasets for analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for causal effect estimation",
      "how to generate balancing weights in R",
      "R library for inverse probability weighting",
      "entropy balancing in R",
      "propensity score weighting R",
      "marginal structural models in R"
    ],
    "primary_use_cases": [
      "causal effect estimation",
      "balancing weights generation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "cobalt",
    "description": "Generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. Provides unified balance assessment across multiple R packages (MatchIt, WeightIt, twang, Matching, optmatch, CBPS, ebal, cem, sbw, designmatch). Supports multi-category, continuous, and longitudinal treatments with clustered and multiply imputed data.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://ngreifer.github.io/cobalt/",
    "github_url": "https://github.com/ngreifer/cobalt",
    "url": "https://cran.r-project.org/package=cobalt",
    "install": "install.packages(\"cobalt\")",
    "tags": [
      "covariate-balance",
      "balance-diagnostics",
      "love-plot",
      "standardized-mean-difference",
      "balance-tables"
    ],
    "best_for": "Assessing and visualizing covariate balance before/after matching or weighting to validate causal inference preprocessing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The cobalt package generates standardized balance tables and plots for covariates after preprocessing via matching, weighting, or subclassification. It is used by researchers and practitioners in causal inference to assess balance across multiple R packages.",
    "use_cases": [
      "Assessing covariate balance after matching",
      "Visualizing balance diagnostics for treatment groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for covariate balance",
      "how to assess balance in causal inference R",
      "R balance diagnostics tools",
      "generate love plot in R",
      "standardized mean difference in R",
      "R package for matching and weighting"
    ],
    "primary_use_cases": [
      "balance assessment across multiple R packages",
      "generating balance tables and plots"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "MatchIt",
      "WeightIt",
      "twang",
      "Matching",
      "optmatch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ebal",
    "description": "Implements entropy balancing, a reweighting method that finds weights for control units such that specified covariate moment conditions (means, variances) are exactly satisfied while staying as close as possible to uniform weights by minimizing Kullback-Leibler divergence. Primarily designed for ATT estimation.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://cran.r-project.org/web/packages/ebal/ebal.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=ebal",
    "install": "install.packages(\"ebal\")",
    "tags": [
      "entropy-balancing",
      "reweighting",
      "covariate-balance",
      "observational-studies",
      "ATT"
    ],
    "best_for": "When you need exact covariate balance on specified moments (means, variances) with minimal weight dispersion, implementing Hainmueller (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ebal package implements entropy balancing, a reweighting method that finds weights for control units to satisfy specified covariate moment conditions while minimizing Kullback-Leibler divergence. It is primarily designed for Average Treatment Effect on the Treated (ATT) estimation.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Balancing covariates in experimental designs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for entropy balancing",
      "how to perform reweighting in R",
      "R package for covariate balance",
      "entropy balancing for observational studies",
      "ATT estimation in R",
      "how to minimize Kullback-Leibler divergence in R",
      "R methods for causal inference",
      "reweighting methods in R"
    ],
    "primary_use_cases": [
      "ATT estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "optmatch",
    "description": "Distance-based bipartite matching using minimum cost network flow algorithms, oriented to matching treatment and control groups in observational studies. Provides optimal full matching and pair matching with support for propensity score distances, Mahalanobis distance, calipers, and exact matching constraints.",
    "category": "Causal Inference (Matching)",
    "docs_url": "https://markmfredrickson.github.io/optmatch",
    "github_url": "https://github.com/markmfredrickson/optmatch",
    "url": "https://cran.r-project.org/package=optmatch",
    "install": "install.packages(\"optmatch\")",
    "tags": [
      "optimal-matching",
      "propensity-score",
      "network-flow",
      "observational-studies",
      "full-matching"
    ],
    "best_for": "When you need mathematically optimal matching solutions that minimize total matched distance with flexible control:treatment ratios (full matching), implementing Hansen & Klopfer (2006)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The optmatch package provides distance-based bipartite matching using minimum cost network flow algorithms, specifically designed for matching treatment and control groups in observational studies. It is used by researchers and data scientists working in causal inference to achieve optimal matching.",
    "use_cases": [
      "Matching treatment and control groups in observational studies",
      "Conducting optimal full matching for causal analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for optimal matching",
      "how to perform full matching in R",
      "distance-based matching in R",
      "matching treatment and control groups in R",
      "R library for network flow algorithms",
      "observational studies matching in R"
    ],
    "primary_use_cases": [
      "optimal full matching",
      "pair matching"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CMAverse",
    "description": "Unified interface for six causal mediation approaches including traditional regression, inverse odds weighting, and g-formula. Supports multiple sequential mediators and exposure-mediator interactions.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://bs1125.github.io/CMAverse/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=CMAverse",
    "install": "install.packages(\"CMAverse\")",
    "tags": [
      "mediation",
      "g-formula",
      "multiple-mediators",
      "causal-mechanisms",
      "unified-interface"
    ],
    "best_for": "Unified causal mediation analysis with six approaches and multiple sequential mediators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "CMAverse provides a unified interface for various causal mediation approaches, allowing users to analyze complex relationships involving multiple mediators and exposure-mediator interactions. It is useful for researchers and practitioners in causal inference who need to implement mediation analysis.",
    "use_cases": [
      "Analyzing the effect of a treatment on an outcome through multiple mediators",
      "Evaluating the impact of exposure-mediator interactions in a study"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal mediation analysis",
      "how to use CMAverse for mediation",
      "CMAverse examples",
      "causal mediation approaches in R",
      "R unified interface for mediation",
      "multiple mediators analysis in R",
      "g-formula in R",
      "inverse odds weighting in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mediation",
    "description": "Estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. Implements Tingley et al. (2014 JSS) methods for understanding causal mechanisms.",
    "category": "Causal Inference (Mediation)",
    "docs_url": "https://cran.r-project.org/web/packages/mediation/mediation.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mediation",
    "install": "install.packages(\"mediation\")",
    "tags": [
      "mediation",
      "ACME",
      "causal-mechanisms",
      "sensitivity-analysis",
      "indirect-effects"
    ],
    "best_for": "Average Causal Mediation Effects with sensitivity analysis, implementing Tingley et al. (2014 JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The mediation package estimates Average Causal Mediation Effects (ACME) with sensitivity analysis for unmeasured confounding. It is used by researchers and practitioners interested in understanding causal mechanisms in their data.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for causal mediation analysis",
      "how to estimate ACME in R",
      "sensitivity analysis for mediation effects in R",
      "understanding causal mechanisms R package"
    ],
    "primary_use_cases": [
      "estimating average causal mediation effects",
      "sensitivity analysis for unmeasured confounding"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Tingley et al. (2014)",
    "maintenance_status": "active"
  },
  {
    "name": "PStrata",
    "description": "Principal stratification analysis for noncompliance and truncation-by-death using both Bayesian (Stan) and frequentist estimation. Implements Liu and Li (2023) methods for causal inference with post-treatment complications.",
    "category": "Causal Inference (Principal Stratification)",
    "docs_url": "https://cran.r-project.org/web/packages/PStrata/PStrata.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=PStrata",
    "install": "install.packages(\"PStrata\")",
    "tags": [
      "principal-stratification",
      "noncompliance",
      "truncation-by-death",
      "Bayesian",
      "Stan"
    ],
    "best_for": "Principal stratification for noncompliance and truncation-by-death with Bayesian/frequentist estimation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "bayesian"
    ],
    "summary": "PStrata is designed for principal stratification analysis, focusing on noncompliance and truncation-by-death. It utilizes both Bayesian and frequentist estimation methods, making it suitable for researchers dealing with causal inference in complex scenarios.",
    "use_cases": [
      "Analyzing noncompliance in clinical trials",
      "Estimating causal effects with truncation-by-death"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for principal stratification",
      "how to perform causal inference in R",
      "Bayesian analysis for noncompliance",
      "truncation-by-death analysis in R",
      "PStrata package usage",
      "methods for causal inference with post-treatment complications"
    ],
    "primary_use_cases": [
      "principal stratification analysis",
      "causal inference with post-treatment complications"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Liu and Li (2023)",
    "maintenance_status": "active"
  },
  {
    "name": "rddapp",
    "description": "Supports multi-assignment RDD with two running variables, power analysis for RDD designs, and includes a Shiny interface for interactive analysis. Handles both sharp and fuzzy designs with bandwidth selection.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddapp/rddapp.pdf",
    "github_url": "https://github.com/felixthoemmes/rddapp",
    "url": "https://cran.r-project.org/package=rddapp",
    "install": "install.packages(\"rddapp\")",
    "tags": [
      "RDD",
      "multi-assignment",
      "power-analysis",
      "Shiny",
      "fuzzy-RDD"
    ],
    "best_for": "Multi-assignment RDD with two running variables and power analysis with Shiny interface",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rddapp package supports multi-assignment regression discontinuity designs (RDD) with two running variables and provides power analysis for RDD designs. It includes a Shiny interface for interactive analysis, making it useful for researchers and practitioners in causal inference.",
    "use_cases": [
      "Analyzing the impact of policy changes using RDD",
      "Conducting power analysis for experimental designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for multi-assignment RDD",
      "how to perform power analysis for RDD in R",
      "interactive analysis for RDD using Shiny",
      "fuzzy RDD analysis in R",
      "bandwidth selection for RDD",
      "causal inference tools in R"
    ],
    "primary_use_cases": [
      "multi-assignment RDD analysis",
      "power analysis for RDD designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rddensity",
    "description": "Implements manipulation testing (density discontinuity testing) procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. Provides rddensity() for hypothesis testing, rdbwdensity() for bandwidth selection, and rdplotdensity() for density plots with confidence bands.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rddensity/",
    "github_url": "https://github.com/rdpackages/rddensity",
    "url": "https://cran.r-project.org/package=rddensity",
    "install": "install.packages(\"rddensity\")",
    "tags": [
      "manipulation-testing",
      "density-discontinuity",
      "McCrary-test",
      "falsification",
      "sorting"
    ],
    "best_for": "Testing RDD validity by detecting bunching/manipulation around the cutoff (McCrary-type tests), implementing Cattaneo, Jansson & Ma (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "rddensity implements manipulation testing procedures using local polynomial density estimators to detect perfect self-selection around a cutoff. It is used primarily by researchers and practitioners in causal inference to test hypotheses and visualize density plots.",
    "use_cases": [
      "Testing for self-selection around a cutoff",
      "Visualizing density plots with confidence bands"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for manipulation testing",
      "how to perform density discontinuity testing in R",
      "R density plots with confidence bands",
      "local polynomial density estimators in R",
      "hypothesis testing for self-selection in R",
      "bandwidth selection in R for density tests"
    ],
    "primary_use_cases": [
      "density discontinuity testing",
      "hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rddtools",
    "description": "Regression discontinuity design toolkit with clustered inference for geographic discontinuities. Provides bandwidth selection, specification tests, and visualization tools.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://cran.r-project.org/web/packages/rddtools/rddtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=rddtools",
    "install": "install.packages(\"rddtools\")",
    "tags": [
      "RDD",
      "clustered-inference",
      "bandwidth-selection",
      "geographic-discontinuity",
      "visualization"
    ],
    "best_for": "RDD with clustered inference for geographic discontinuities",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rddtools package provides a toolkit for regression discontinuity design, focusing on clustered inference for geographic discontinuities. It is useful for researchers and practitioners who need to perform causal inference analyses with a focus on bandwidth selection and visualization.",
    "use_cases": [
      "Analyzing the impact of policy changes at geographic boundaries",
      "Evaluating educational interventions using RDD"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for regression discontinuity design",
      "how to visualize geographic discontinuities in R",
      "bandwidth selection in RDD",
      "clustered inference in R",
      "specification tests for RDD",
      "tools for causal inference in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdlocrand",
    "description": "Provides tools for RD analysis under local randomization: rdrandinf() performs hypothesis testing using randomization inference, rdwinselect() selects a window around the cutoff where randomization likely holds, rdsensitivity() assesses sensitivity to different windows, and rdrbounds() constructs Rosenbaum bounds for unobserved confounders.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdlocrand/",
    "github_url": "https://github.com/rdpackages/rdlocrand",
    "url": "https://cran.r-project.org/package=rdlocrand",
    "install": "install.packages(\"rdlocrand\")",
    "tags": [
      "local-randomization",
      "randomization-inference",
      "finite-sample",
      "window-selection",
      "sensitivity-analysis"
    ],
    "best_for": "Finite-sample inference in RDD when local randomization assumption is plausible near the cutoff, implementing Cattaneo, Frandsen & Titiunik (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdlocrand package provides tools for regression discontinuity analysis under local randomization. It is useful for researchers and practitioners who need to perform hypothesis testing and sensitivity analysis in causal inference settings.",
    "use_cases": [
      "Performing hypothesis testing using randomization inference",
      "Assessing sensitivity to different windows in regression discontinuity designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for randomization inference",
      "how to perform sensitivity analysis in R",
      "tools for regression discontinuity analysis in R",
      "R local randomization methods",
      "hypothesis testing with randomization in R",
      "window selection for RDD in R"
    ],
    "primary_use_cases": [
      "hypothesis testing",
      "sensitivity analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdmulti",
    "description": "Provides tools for RD designs with multiple cutoffs or scores: rdmc() estimates pooled and cutoff-specific effects in multi-cutoff designs, rdmcplot() draws RD plots for multi-cutoff designs, and rdms() estimates effects in cumulative cutoffs or multi-score (geographic/boundary) designs.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdmulti/",
    "github_url": "https://github.com/rdpackages/rdmulti",
    "url": "https://cran.r-project.org/package=rdmulti",
    "install": "install.packages(\"rdmulti\")",
    "tags": [
      "multiple-cutoffs",
      "multi-score",
      "geographic-RD",
      "pooled-effects",
      "extrapolation"
    ],
    "best_for": "RDD with multiple cutoffs (e.g., different thresholds across regions) or multiple running variables (geographic boundaries), implementing Cattaneo, Titiunik & Vazquez-Bare (2020)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdmulti package provides tools for regression discontinuity designs with multiple cutoffs or scores. It is useful for researchers and practitioners in causal inference who need to estimate effects in complex RD designs.",
    "use_cases": [
      "Estimating pooled effects in multi-cutoff designs",
      "Visualizing regression discontinuity plots for multiple cutoffs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for multiple cutoffs",
      "how to estimate effects in multi-score designs",
      "tools for regression discontinuity designs in R",
      "rdmc() function usage",
      "creating RD plots with rdmcplot()",
      "cumulative cutoffs analysis in R",
      "geographic RD analysis tools"
    ],
    "primary_use_cases": [
      "estimating effects in multi-cutoff designs",
      "drawing RD plots for multi-cutoff designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "rdpower",
    "description": "Provides tools for power, sample size, and minimum detectable effects (MDE) calculations in RD designs using robust bias-corrected local polynomial inference: rdpower() calculates power, rdsampsi() calculates required sample size for desired power, and rdmde() computes minimum detectable effects.",
    "category": "Causal Inference (RDD)",
    "docs_url": "https://rdpackages.github.io/rdpower/",
    "github_url": "https://github.com/rdpackages/rdpower",
    "url": "https://cran.r-project.org/package=rdpower",
    "install": "install.packages(\"rdpower\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "MDE",
      "study-design",
      "ex-ante-analysis"
    ],
    "best_for": "Planning RDD studies\u2014calculating required sample sizes, statistical power, or minimum detectable effects, implementing Cattaneo, Titiunik & Vazquez-Bare (2019)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The rdpower package provides tools for power, sample size, and minimum detectable effects calculations in regression discontinuity designs. It is useful for researchers and practitioners working in causal inference who need to determine the necessary sample sizes and power for their studies.",
    "use_cases": [
      "Calculating power for a regression discontinuity design study",
      "Determining sample size needed for desired power in an RD analysis"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for power analysis",
      "how to calculate sample size in R",
      "minimum detectable effects in R",
      "tools for study design in R",
      "power calculations for RD designs",
      "rdpower package documentation",
      "R package for ex-ante analysis"
    ],
    "primary_use_cases": [
      "power calculation",
      "sample size determination",
      "minimum detectable effects computation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "SCtools",
    "description": "Automates placebo tests and multi-treated-unit ATT calculations for synthetic control. Provides utilities for generating in-space and in-time placebos with visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/SCtools/SCtools.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=SCtools",
    "install": "install.packages(\"SCtools\")",
    "tags": [
      "synthetic-control",
      "placebo-tests",
      "multi-unit",
      "ATT",
      "visualization"
    ],
    "best_for": "Automated placebo tests and multi-treated-unit ATT calculations for synthetic control",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "visualization"
    ],
    "summary": "SCtools automates placebo tests and multi-treated-unit ATT calculations for synthetic control. It provides utilities for generating in-space and in-time placebos with visualization, making it useful for researchers in causal inference.",
    "use_cases": [
      "Conducting placebo tests for causal inference studies",
      "Calculating ATT for multiple treated units"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to perform placebo tests in R",
      "multi-treated-unit ATT calculations in R",
      "visualization tools for synthetic control",
      "automate placebo tests in R",
      "R utilities for synthetic control"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Synth",
    "description": "The original synthetic control method implementation for comparative case studies. Constructs a weighted combination of comparison units to create a synthetic counterfactual for estimating effects of interventions on a single treated unit, as used in seminal studies of California tobacco program and German reunification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://web.stanford.edu/~jhain/",
    "github_url": "https://github.com/cran/Synth",
    "url": "https://cran.r-project.org/package=Synth",
    "install": "install.packages(\"Synth\")",
    "tags": [
      "synthetic-control",
      "comparative-case-studies",
      "counterfactual",
      "policy-evaluation",
      "single-unit-treatment"
    ],
    "best_for": "Classic single-treated-unit policy evaluations, implementing Abadie, Diamond & Hainmueller (2010, 2011, 2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "comparative-case-studies",
      "policy-evaluation"
    ],
    "summary": "Synth is an implementation of the synthetic control method for comparative case studies. It constructs a synthetic counterfactual to estimate the effects of interventions on a single treated unit, making it useful for researchers in policy evaluation.",
    "use_cases": [
      "Evaluating the impact of the California tobacco program",
      "Analyzing the effects of German reunification"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control",
      "how to perform comparative case studies in R",
      "synthetic counterfactual analysis R",
      "policy evaluation methods in R",
      "single unit treatment effects R",
      "synthetic control method tutorial"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "augsynth",
    "description": "Implements the Augmented Synthetic Control Method, which uses an outcome model (ridge regression by default) to correct for bias when pre-treatment fit is imperfect. Uniquely supports staggered adoption across multiple treated units via multisynth() function.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://github.com/ebenmichael/augsynth/blob/master/vignettes/singlesynth-vignette.md",
    "github_url": "https://github.com/ebenmichael/augsynth",
    "url": "https://github.com/ebenmichael/augsynth",
    "install": "devtools::install_github(\"ebenmichael/augsynth\")",
    "tags": [
      "augmented-synthetic-control",
      "bias-correction",
      "staggered-adoption",
      "ridge-regression",
      "imperfect-fit"
    ],
    "best_for": "SC applications with imperfect pre-treatment fit or staggered adoption across units, implementing Ben-Michael, Feller & Rothstein (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "AugSynth implements the Augmented Synthetic Control Method to correct for bias in pre-treatment fit using an outcome model. It is particularly useful for researchers dealing with staggered adoption across multiple treated units.",
    "use_cases": [
      "Analyzing the impact of policy changes across different regions",
      "Evaluating treatment effects in staggered adoption scenarios"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for augmented synthetic control",
      "how to correct bias in synthetic control in R",
      "R library for staggered adoption analysis",
      "implementing ridge regression in synthetic control",
      "R synthetic control method for causal inference",
      "using multisynth function in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gsynth",
    "description": "Implements generalized synthetic control with interactive fixed effects, extending SCM to multiple treated units with variable treatment timing. Uses factor models to impute counterfactuals, handling unbalanced panels and complex treatment patterns with latent factor structures.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://yiqingxu.org/packages/gsynth/",
    "github_url": "https://github.com/xuyiqing/gsynth",
    "url": "https://cran.r-project.org/package=gsynth",
    "install": "install.packages(\"gsynth\")",
    "tags": [
      "generalized-synthetic-control",
      "interactive-fixed-effects",
      "factor-models",
      "multiple-treated-units",
      "unbalanced-panels"
    ],
    "best_for": "Multiple treated units with staggered treatment timing and latent factor structures, implementing Xu (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The gsynth package implements generalized synthetic control methods with interactive fixed effects, allowing for the analysis of multiple treated units with varying treatment timings. It is primarily used by researchers and practitioners in causal inference to handle complex treatment patterns in unbalanced panel data.",
    "use_cases": [
      "Analyzing the impact of policy changes across multiple regions",
      "Evaluating treatment effects in economic studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized synthetic control",
      "how to implement interactive fixed effects in R",
      "R library for causal inference with unbalanced panels",
      "generalized synthetic control methods in R",
      "factor models for counterfactuals in R",
      "R package for multiple treated units analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "microsynth",
    "description": "Extends synthetic control method to micro-level data with many units. Implements permutation inference and handles high-dimensional settings where traditional SCM struggles.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/microsynth/microsynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=microsynth",
    "install": "install.packages(\"microsynth\")",
    "tags": [
      "synthetic-control",
      "micro-data",
      "permutation-inference",
      "high-dimensional",
      "many-units"
    ],
    "best_for": "Synthetic control for micro-level data with many units and permutation inference",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "Microsynth extends the synthetic control method to micro-level data with many units, allowing for permutation inference and handling high-dimensional settings. It is useful for researchers and practitioners in causal inference who work with complex datasets.",
    "use_cases": [
      "Analyzing treatment effects in observational studies",
      "Evaluating policy impacts on micro-level data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to perform permutation inference in R",
      "high-dimensional causal inference in R",
      "synthetic control for micro-data",
      "R library for many units analysis",
      "advanced synthetic control methods in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pensynth",
    "description": "Implements penalized synthetic control method from Abadie & L'Hour (2021). Adds regularization to improve pre-treatment fit and reduce interpolation bias in sparse donor pools.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/pensynth/pensynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=pensynth",
    "install": "install.packages(\"pensynth\")",
    "tags": [
      "synthetic-control",
      "penalized",
      "regularization",
      "interpolation-bias",
      "sparse-donors"
    ],
    "best_for": "Penalized synthetic control with regularization, implementing Abadie & L'Hour (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "synthetic-control"
    ],
    "summary": "The pensynth package implements a penalized synthetic control method to enhance pre-treatment fit and mitigate interpolation bias in sparse donor pools. It is primarily used by researchers and practitioners in causal inference who require improved estimation techniques.",
    "use_cases": [
      "Evaluating treatment effects in observational studies",
      "Improving causal inference in economic research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control",
      "how to implement penalized synthetic control in R",
      "synthetic control method for sparse donors",
      "penalized synthetic control example",
      "R package for interpolation bias in causal inference",
      "best practices for synthetic control in R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Abadie & L'Hour (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "scpi",
    "description": "Provides rigorous prediction intervals for synthetic control methods following Cattaneo et al. (2021, 2025). Supports staggered adoption designs with valid uncertainty quantification.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://nppackages.github.io/scpi/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=scpi",
    "install": "install.packages(\"scpi\")",
    "tags": [
      "synthetic-control",
      "prediction-intervals",
      "uncertainty-quantification",
      "staggered-adoption",
      "inference"
    ],
    "best_for": "Rigorous prediction intervals for synthetic control, implementing Cattaneo et al. (2021, 2025)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "uncertainty-quantification"
    ],
    "summary": "The scpi package provides rigorous prediction intervals for synthetic control methods, allowing users to perform valid uncertainty quantification in staggered adoption designs. It is particularly useful for researchers and practitioners in causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic control methods",
      "how to create prediction intervals in R",
      "uncertainty quantification in R",
      "staggered adoption designs in R",
      "Cattaneo 2021 synthetic control",
      "Cattaneo 2025 prediction intervals"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Cattaneo et al. (2021)",
    "maintenance_status": "active"
  },
  {
    "name": "synthdid",
    "description": "Implements synthetic difference-in-differences, a hybrid method combining insights from both DiD and synthetic control that reweights and matches pre-treatment trends. Provides improved robustness properties compared to either method alone by combining their strengths.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://synth-inference.github.io/synthdid/",
    "github_url": "https://github.com/synth-inference/synthdid",
    "url": "https://cran.r-project.org/package=synthdid",
    "install": "install.packages(\"synthdid\")",
    "tags": [
      "synthetic-control",
      "difference-in-differences",
      "hybrid-estimator",
      "panel-data",
      "robust-estimation"
    ],
    "best_for": "Settings where neither pure DiD nor pure SC is ideal, implementing Arkhangelsky, Athey, Hirshberg, Imbens & Wager (2021)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "synthetic-control"
    ],
    "summary": "The synthdid package implements synthetic difference-in-differences, a method that combines insights from both difference-in-differences and synthetic control. It is used by researchers and practitioners in causal inference to improve robustness in estimating treatment effects.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for synthetic difference-in-differences",
      "how to implement synthetic control in R",
      "difference-in-differences method in R",
      "hybrid estimator for causal inference R",
      "robust estimation techniques in R",
      "panel data analysis in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "tidysynth",
    "description": "Brings synthetic control method into the tidyverse with cleaner syntax and built-in placebo inference. Provides pipe-friendly workflows for SCM estimation and visualization.",
    "category": "Causal Inference (Synthetic Control)",
    "docs_url": "https://cran.r-project.org/web/packages/tidysynth/tidysynth.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=tidysynth",
    "install": "install.packages(\"tidysynth\")",
    "tags": [
      "synthetic-control",
      "tidyverse",
      "placebo-inference",
      "causal-inference",
      "policy-evaluation"
    ],
    "best_for": "Tidyverse-friendly synthetic control method with clean syntax and built-in placebo inference",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "policy-evaluation"
    ],
    "summary": "tidysynth brings the synthetic control method into the tidyverse, providing a cleaner syntax and built-in placebo inference. It offers pipe-friendly workflows for estimating and visualizing synthetic control models, making it accessible for users in the data science community.",
    "use_cases": [
      "Evaluating the impact of a policy intervention",
      "Comparing treatment and control groups in observational studies"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for synthetic control",
      "how to use synthetic control in R",
      "tidyverse package for causal inference",
      "visualizing synthetic control results in R",
      "placebo inference in R",
      "synthetic control method tidyverse"
    ],
    "primary_use_cases": [
      "synthetic control estimation",
      "visualization of causal effects"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "tidyverse"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "MAPIE",
    "description": "Scikit-learn-contrib library for conformal prediction intervals. Provides model-agnostic uncertainty quantification for regression and classification.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://mapie.readthedocs.io/",
    "github_url": "https://github.com/scikit-learn-contrib/MAPIE",
    "url": "https://github.com/scikit-learn-contrib/MAPIE",
    "install": "pip install mapie",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "intervals"
    ],
    "best_for": "Model-agnostic prediction intervals",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "conformal-prediction",
      "uncertainty-quantification"
    ],
    "summary": "MAPIE is a library that provides model-agnostic uncertainty quantification for regression and classification tasks through conformal prediction intervals. It is useful for data scientists and researchers who need to assess the uncertainty of their predictions.",
    "use_cases": [
      "Estimating prediction intervals for regression models",
      "Providing uncertainty quantification for classification tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to quantify uncertainty in predictions using python",
      "scikit-learn conformal prediction intervals",
      "MAPIE library for uncertainty quantification",
      "predictive intervals in python",
      "model-agnostic uncertainty estimation python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "TorchCP",
    "description": "PyTorch-native conformal prediction for DNNs, GNNs, and LLMs with GPU acceleration.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://torchcp.readthedocs.io/",
    "github_url": "https://github.com/ml-stat-Sustech/TorchCP",
    "url": "https://github.com/ml-stat-Sustech/TorchCP",
    "install": "pip install torchcp",
    "tags": [
      "conformal prediction",
      "PyTorch",
      "deep learning"
    ],
    "best_for": "Conformal prediction for neural networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "TorchCP is a PyTorch-native library designed for conformal prediction in deep neural networks, graph neural networks, and large language models, providing GPU acceleration. It is useful for practitioners in machine learning who need to quantify uncertainty in their models.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to do conformal prediction in PyTorch",
      "PyTorch GPU acceleration for deep learning",
      "conformal prediction for DNNs",
      "using TorchCP for uncertainty quantification",
      "TorchCP examples",
      "installing TorchCP",
      "TorchCP documentation"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "crepes",
    "description": "Lightweight library for conformal regressors and predictive systems. Simple API for calibrated prediction intervals.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/henrikbostrom/crepes",
    "github_url": "https://github.com/henrikbostrom/crepes",
    "url": "https://github.com/henrikbostrom/crepes",
    "install": "pip install crepes",
    "tags": [
      "conformal prediction",
      "regression",
      "intervals"
    ],
    "best_for": "Simple conformal regressors",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Crepes is a lightweight library designed for conformal regressors and predictive systems, providing a simple API for generating calibrated prediction intervals. It is useful for data scientists and statisticians who need reliable uncertainty quantification in their predictions.",
    "use_cases": [
      "Generating calibrated prediction intervals for regression models",
      "Implementing conformal prediction methods in data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to create prediction intervals in python",
      "lightweight library for regression in python",
      "calibrated prediction intervals python",
      "conformal regressors in python",
      "predictive systems library python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "fortuna",
    "description": "AWS library for uncertainty quantification in deep learning. Bayesian and conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://aws-fortuna.readthedocs.io/",
    "github_url": "https://github.com/awslabs/fortuna",
    "url": "https://github.com/awslabs/fortuna",
    "install": "pip install fortuna",
    "tags": [
      "uncertainty",
      "Bayesian",
      "deep learning"
    ],
    "best_for": "Deep learning uncertainty quantification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bayesian",
      "uncertainty"
    ],
    "summary": "Fortuna is an AWS library designed for uncertainty quantification in deep learning, utilizing Bayesian and conformal methods. It is primarily used by data scientists and researchers working on uncertainty in machine learning models.",
    "use_cases": [
      "Quantifying uncertainty in deep learning models",
      "Implementing Bayesian methods for model predictions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for uncertainty quantification",
      "how to implement Bayesian methods in python",
      "AWS library for deep learning uncertainty",
      "conformal prediction in python",
      "deep learning uncertainty quantification",
      "Bayesian deep learning library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "puncc",
    "description": "IRT Lab's library for predictive uncertainty with conformal prediction. Supports various conformal methods.",
    "category": "Conformal Prediction & Uncertainty",
    "docs_url": "https://github.com/deel-ai/puncc",
    "github_url": "https://github.com/deel-ai/puncc",
    "url": "https://github.com/deel-ai/puncc",
    "install": "pip install puncc",
    "tags": [
      "conformal prediction",
      "uncertainty",
      "calibration"
    ],
    "best_for": "Calibrated prediction sets",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "puncc is a library developed by IRT Lab for predictive uncertainty using conformal prediction methods. It is designed for users interested in applying various conformal methods for uncertainty quantification.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for conformal prediction",
      "how to implement uncertainty calibration in python",
      "predictive uncertainty library python",
      "conformal prediction methods in python",
      "IRT Lab conformal prediction library",
      "using puncc for uncertainty quantification"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "cjoint",
    "description": "Estimates Average Marginal Component Effects (AMCEs) for conjoint experiments following Hainmueller, Hopkins & Yamamoto (2014). Handles multi-dimensional preferences with clustered standard errors.",
    "category": "Conjoint Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/cjoint/cjoint.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=cjoint",
    "install": "install.packages(\"cjoint\")",
    "tags": [
      "conjoint",
      "AMCE",
      "survey-experiments",
      "preferences",
      "political-science"
    ],
    "best_for": "AMCE estimation for conjoint experiments, implementing Hainmueller, Hopkins & Yamamoto (2014)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "conjoint-analysis",
      "survey-experiments",
      "political-science"
    ],
    "summary": "The cjoint package estimates Average Marginal Component Effects (AMCEs) for conjoint experiments, allowing researchers to analyze multi-dimensional preferences with clustered standard errors. It is primarily used by social scientists conducting survey experiments.",
    "use_cases": [
      "Analyzing voter preferences in political surveys",
      "Evaluating consumer choices in market research"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for conjoint analysis",
      "how to estimate AMCEs in R",
      "R package for survey experiments",
      "multi-dimensional preferences analysis in R",
      "clustered standard errors in R",
      "political science conjoint analysis R"
    ],
    "primary_use_cases": [
      "estimating average marginal component effects",
      "analyzing multi-dimensional preferences"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Hainmueller, Hopkins & Yamamoto (2014)",
    "maintenance_status": "active"
  },
  {
    "name": "cregg",
    "description": "Tidy interface for conjoint analysis with visualization. Provides functions for calculating and plotting marginal means and AMCEs with ggplot2-based output for publication-ready figures.",
    "category": "Conjoint Analysis",
    "docs_url": "https://thomasleeper.com/cregg/",
    "github_url": "https://github.com/leeper/cregg",
    "url": "https://cran.r-project.org/package=cregg",
    "install": "install.packages(\"cregg\")",
    "tags": [
      "conjoint",
      "visualization",
      "marginal-means",
      "ggplot2",
      "survey-experiments"
    ],
    "best_for": "Tidy conjoint analysis with ggplot2 visualization for marginal means and AMCEs",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The cregg package provides a tidy interface for conducting conjoint analysis and visualizing the results. It is designed for users who need to calculate and plot marginal means and Average Marginal Component Effects (AMCEs) using ggplot2 for publication-ready figures.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R package for conjoint analysis",
      "how to visualize marginal means in R",
      "conjoint analysis with ggplot2",
      "R library for survey experiments",
      "calculate AMCEs in R",
      "tidy interface for conjoint analysis"
    ],
    "primary_use_cases": [
      "calculating marginal means",
      "plotting AMCEs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "H2O Sparkling Water",
    "description": "H2O's distributed ML engine on Spark with GLM/GAM that provides p-values, confidence intervals, and Tweedie/Gamma distributions.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.h2o.ai/sparkling-water/3.3/latest-stable/doc/index.html",
    "github_url": "https://github.com/h2oai/sparkling-water",
    "url": "https://github.com/h2oai/sparkling-water",
    "install": "pip install h2o_pysparkling_3.4",
    "tags": [
      "spark",
      "GLM",
      "GAM",
      "distributed",
      "p-values"
    ],
    "best_for": "Econometric inference (p-values, CIs) at Spark scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "H2O Sparkling Water is a distributed machine learning engine that integrates with Spark, providing functionalities for Generalized Linear Models (GLM) and Generalized Additive Models (GAM). It is used by data scientists and machine learning practitioners who require scalable solutions for statistical modeling and inference.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "H2O Sparkling Water library",
      "how to use GLM in Spark",
      "distributed machine learning with Spark",
      "H2O Sparkling Water examples",
      "GAM implementation in Spark",
      "p-values in machine learning",
      "Tweedie distribution in Spark",
      "H2O machine learning engine"
    ],
    "primary_use_cases": [
      "statistical modeling",
      "machine learning inference"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Linregress",
    "description": "Simple linear regression for Rust with R-style formula syntax, standard errors, t-stats, and p-values.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://docs.rs/linregress",
    "github_url": "https://github.com/n1m3/linregress",
    "url": "https://crates.io/crates/linregress",
    "install": "cargo add linregress",
    "tags": [
      "rust",
      "regression",
      "OLS",
      "statistics"
    ],
    "best_for": "Simple no-frills OLS regression in Rust",
    "language": "Rust",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "statistics"
    ],
    "summary": "Linregress is a Rust library that provides simple linear regression capabilities using R-style formula syntax. It is useful for statisticians and data scientists looking to perform regression analysis in Rust.",
    "use_cases": [
      "Performing linear regression analysis on datasets",
      "Calculating standard errors and p-values for regression models"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "rust library for linear regression",
      "how to perform OLS in Rust",
      "statistics package in Rust",
      "Rust regression analysis",
      "simple linear regression Rust",
      "Rust library for statistics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "Polars",
    "description": "Blazingly fast DataFrame library for Rust and Python with SQL-like syntax, lazy evaluation, and excellent time series handling.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://pola.rs/",
    "github_url": "https://github.com/pola-rs/polars",
    "url": "https://crates.io/crates/polars",
    "install": "cargo add polars",
    "tags": [
      "rust",
      "dataframe",
      "data manipulation",
      "performance"
    ],
    "best_for": "High-performance data manipulation (pandas alternative)",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "data manipulation",
      "time-series"
    ],
    "summary": "Polars is a high-performance DataFrame library designed for Rust and Python, offering SQL-like syntax and lazy evaluation. It is particularly useful for data manipulation and handling time series data efficiently.",
    "use_cases": [
      "Data analysis with large datasets",
      "Time series data processing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for data manipulation",
      "how to handle time series in python",
      "fast dataframe library for python",
      "rust dataframe library",
      "sql-like syntax for data analysis",
      "lazy evaluation in data processing",
      "performance data manipulation library"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "pandas",
      "dask"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Scikit-learn",
    "description": "Foundational ML library with regression models (incl. regularized), model selection, cross-validation, evaluation metrics.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://scikit-learn.org/",
    "github_url": "https://github.com/scikit-learn/scikit-learn",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "install": "pip install scikit-learn",
    "tags": [
      "regression",
      "linear models"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "Scikit-learn is a foundational machine learning library in Python that provides tools for regression models, model selection, cross-validation, and evaluation metrics. It is widely used by data scientists and machine learning practitioners for building and evaluating predictive models.",
    "use_cases": [
      "Building regression models",
      "Evaluating model performance"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression",
      "how to do model selection in python",
      "cross-validation in scikit-learn",
      "evaluation metrics for machine learning in python",
      "implementing linear models in python",
      "scikit-learn documentation",
      "best practices for using scikit-learn"
    ],
    "primary_use_cases": [
      "Building regression models",
      "Model selection"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "TensorFlow",
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Statsmodels",
    "description": "Comprehensive library for estimating statistical models (OLS, GLM, etc.), conducting tests, and data exploration. Core tool.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://www.statsmodels.org/",
    "github_url": "https://github.com/statsmodels/statsmodels",
    "url": "https://github.com/statsmodels/statsmodels",
    "install": "pip install statsmodels",
    "tags": [
      "regression",
      "linear models"
    ],
    "best_for": "OLS regression, basic econometrics, data manipulation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "linear models"
    ],
    "summary": "Statsmodels is a comprehensive library for estimating statistical models such as OLS and GLM, conducting statistical tests, and exploring data. It is widely used by data scientists and statisticians for various statistical analyses.",
    "use_cases": [
      "Estimating linear regression models",
      "Conducting hypothesis tests on statistical data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for statistical models",
      "how to conduct regression analysis in python",
      "python library for data exploration",
      "how to perform OLS in python",
      "python GLM examples",
      "best python libraries for regression"
    ],
    "primary_use_cases": [
      "linear regression analysis",
      "generalized linear model fitting"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "stats",
      "pandas"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "appelpy",
    "description": "Applied Econometrics Library bridging Stata-like syntax with Python. Built on statsmodels with convenient API.",
    "category": "Core Libraries & Linear Models",
    "docs_url": "https://appelpy.readthedocs.io/",
    "github_url": "https://github.com/mfarragher/appelpy",
    "url": "https://github.com/mfarragher/appelpy",
    "install": "pip install appelpy",
    "tags": [
      "regression",
      "linear models",
      "Stata"
    ],
    "best_for": "Stata-like econometrics workflow in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "statsmodels"
    ],
    "topic_tags": [
      "regression",
      "linear models"
    ],
    "summary": "Appelpy is an Applied Econometrics Library that bridges Stata-like syntax with Python, providing a convenient API built on statsmodels. It is designed for users who are familiar with econometric modeling and want to leverage Python's capabilities.",
    "use_cases": [
      "Conducting regression analysis",
      "Performing econometric modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for applied econometrics",
      "how to perform regression in python",
      "Stata-like syntax in python",
      "linear models in python",
      "applying econometrics with python",
      "python statsmodels tutorial"
    ],
    "primary_use_cases": [
      "regression analysis",
      "econometric modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "statsmodels",
      "pandas"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "collapse",
    "description": "High-performance data transformation package designed by an economist. Provides fast grouped operations, time series functions, and panel data tools with 10-100\u00d7 speedups over dplyr on large data.",
    "category": "Data Workflow",
    "docs_url": "https://sebkrantz.github.io/collapse/",
    "github_url": "https://github.com/SebKrantz/collapse",
    "url": "https://cran.r-project.org/package=collapse",
    "install": "install.packages(\"collapse\")",
    "tags": [
      "data-transformation",
      "high-performance",
      "panel-data",
      "time-series",
      "grouped-operations"
    ],
    "best_for": "High-performance data transformation optimized for economists\u201410-100\u00d7 faster than dplyr",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "data-transformation",
      "time-series",
      "panel-data"
    ],
    "summary": "The 'collapse' package is a high-performance data transformation tool designed for economists, providing fast grouped operations, time series functions, and panel data tools. It is particularly useful for users dealing with large datasets who require efficient data manipulation.",
    "use_cases": [
      "Transforming large datasets efficiently",
      "Conducting time series analysis",
      "Performing grouped operations on panel data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for high-performance data transformation",
      "how to perform grouped operations in R",
      "R time series functions",
      "efficient panel data tools in R",
      "data manipulation package for economists",
      "fast data transformation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "data.table",
    "description": "Extension of data.frame providing fast aggregation of large data (100GB+), ordered joins, and memory-efficient operations. Uses reference semantics for in-place modification with concise syntax [:=, .SD, by=].",
    "category": "Data Workflow",
    "docs_url": "https://rdatatable.gitlab.io/data.table/",
    "github_url": "https://github.com/Rdatatable/data.table",
    "url": "https://cran.r-project.org/package=data.table",
    "install": "install.packages(\"data.table\")",
    "tags": [
      "data-manipulation",
      "fast",
      "large-data",
      "reference-semantics",
      "aggregation"
    ],
    "best_for": "Fast operations on large datasets (100GB+) with memory-efficient reference semantics",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "data.table is an extension of data.frame in R that provides fast aggregation of large datasets, ordered joins, and memory-efficient operations. It is commonly used by data scientists and statisticians working with large data sets.",
    "use_cases": [
      "Aggregating large datasets for analysis",
      "Performing ordered joins on large data tables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for fast data manipulation",
      "how to aggregate large data in R",
      "R package for memory-efficient data operations",
      "data.table vs data.frame in R",
      "efficient joins in R",
      "R data manipulation tools"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "dplyr",
      "data.frame"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "haven",
    "description": "Import and export Stata, SPSS, and SAS data files preserving variable labels and value labels. Handles .dta, .sav, .sas7bdat, and .xpt formats with labelled vectors for metadata.",
    "category": "Data Workflow",
    "docs_url": "https://haven.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/haven",
    "url": "https://cran.r-project.org/package=haven",
    "install": "install.packages(\"haven\")",
    "tags": [
      "Stata",
      "SPSS",
      "SAS",
      "data-import",
      "labelled-data"
    ],
    "best_for": "Reading and writing Stata, SPSS, and SAS files with preserved labels",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The haven package allows users to import and export data files from Stata, SPSS, and SAS while preserving variable and value labels. It is useful for data analysts and researchers who work with these statistical software formats.",
    "use_cases": [
      "Importing .dta files from Stata",
      "Exporting .sav files to SPSS"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for importing Stata data",
      "how to export SPSS files in R",
      "R haven package documentation",
      "import SAS data into R",
      "R data import tools",
      "preserve variable labels in R",
      "R package for labelled data"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "tidyverse",
    "description": "Meta-package installing core tidyverse packages: ggplot2 (visualization), dplyr (manipulation), tidyr (tidying), readr (import), purrr (functional programming), tibble (data frames), stringr (strings), and forcats (factors).",
    "category": "Data Workflow",
    "docs_url": "https://www.tidyverse.org/",
    "github_url": "https://github.com/tidyverse/tidyverse",
    "url": "https://cran.r-project.org/package=tidyverse",
    "install": "install.packages(\"tidyverse\")",
    "tags": [
      "tidyverse",
      "data-science",
      "dplyr",
      "ggplot2",
      "meta-package"
    ],
    "best_for": "Core tidyverse ecosystem for consistent data science workflows",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The tidyverse is a collection of R packages designed for data science, providing tools for data manipulation, visualization, and import. It is widely used by data scientists and statisticians to streamline their workflow.",
    "use_cases": [
      "Creating visualizations with ggplot2",
      "Data manipulation with dplyr"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for data manipulation",
      "how to visualize data in R",
      "R package for data import",
      "functional programming in R",
      "R tools for data frames",
      "tidyverse package overview"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "ggplot2",
      "dplyr",
      "tidyr",
      "readr",
      "purrr",
      "tibble",
      "stringr",
      "forcats"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "AER",
    "description": "Companion package to 'Applied Econometrics with R' (Kleiber & Zeileis) plus datasets from Stock & Watson. Provides ivreg() for instrumental variables, tobit(), and econometric testing functions.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/AER/AER.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=AER",
    "install": "install.packages(\"AER\")",
    "tags": [
      "datasets",
      "textbook",
      "instrumental-variables",
      "Stock-Watson",
      "Kleiber-Zeileis"
    ],
    "best_for": "Datasets and functions from 'Applied Econometrics with R' plus Stock & Watson data",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "instrumental-variables"
    ],
    "summary": "AER is a companion package to 'Applied Econometrics with R' that provides functions for instrumental variables and econometric testing. It is used by students and practitioners in econometrics for analysis and testing.",
    "use_cases": [
      "Performing instrumental variable regression",
      "Conducting tobit analysis",
      "Testing econometric hypotheses"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for instrumental variables",
      "how to perform econometric testing in R",
      "datasets for applied econometrics",
      "R package for tobit model",
      "AER R package documentation",
      "how to use ivreg in R",
      "Stock-Watson datasets in R"
    ],
    "primary_use_cases": [
      "instrumental variable regression",
      "tobit model analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "causaldata",
    "description": "Unified collection of datasets from three major causal inference textbooks: 'The Effect' (Huntington-Klein), 'Causal Inference: The Mixtape' (Cunningham), and 'Causal Inference: What If?' (Hern\u00e1n & Robins).",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/causaldata/causaldata.pdf",
    "github_url": "https://github.com/NickCH-K/causaldata",
    "url": "https://cran.r-project.org/package=causaldata",
    "install": "install.packages(\"causaldata\")",
    "tags": [
      "datasets",
      "causal-inference",
      "textbook",
      "The-Effect",
      "Mixtape"
    ],
    "best_for": "Datasets from The Effect, Causal Inference: The Mixtape, and What If? textbooks",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The causaldata package provides a unified collection of datasets from three major causal inference textbooks, making it easier for users to access and utilize these resources for causal analysis. It is primarily used by students and practitioners in the field of causal inference.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for causal inference datasets",
      "how to use datasets from The Effect in R",
      "datasets for causal analysis in R",
      "R package for causal inference textbooks",
      "access causal inference datasets in R",
      "causaldata R package usage"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "wooldridge",
    "description": "All 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). Includes wage equations, crime data, housing prices, and classic econometrics teaching examples.",
    "category": "Datasets",
    "docs_url": "https://cran.r-project.org/web/packages/wooldridge/wooldridge.pdf",
    "github_url": "https://github.com/JustinMShea/wooldridge",
    "url": "https://cran.r-project.org/package=wooldridge",
    "install": "install.packages(\"wooldridge\")",
    "tags": [
      "datasets",
      "textbook",
      "teaching",
      "Wooldridge",
      "econometrics"
    ],
    "best_for": "115 datasets from Wooldridge's 'Introductory Econometrics' for teaching and examples",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "econometrics",
      "datasets",
      "teaching"
    ],
    "summary": "The wooldridge package provides access to all 115 datasets from Wooldridge's 'Introductory Econometrics: A Modern Approach' (7th edition). It is primarily used by students and educators in econometrics for teaching and learning purposes.",
    "use_cases": [
      "Teaching econometrics using real datasets",
      "Analyzing wage equations for educational purposes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "R library for econometrics datasets",
      "how to access Wooldridge datasets in R",
      "datasets for teaching econometrics",
      "Wooldridge econometrics examples in R",
      "R package for wage equations",
      "crime data analysis in R",
      "housing prices datasets in R",
      "econometrics teaching resources in R"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "FactorAnalyzer",
    "description": "Specialized library for Exploratory (EFA) and Confirmatory (CFA) Factor Analysis with rotation options for interpretability.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://factor-analyzer.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EducationalTestingService/factor_analyzer",
    "url": "https://github.com/EducationalTestingService/factor_analyzer",
    "install": "pip install factor_analyzer",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "FactorAnalyzer is a specialized library designed for Exploratory and Confirmatory Factor Analysis, providing various rotation options for better interpretability. It is used by data scientists and researchers who need to analyze complex datasets and uncover latent structures.",
    "use_cases": [
      "Analyzing survey data to identify underlying factors",
      "Reducing dimensionality in large datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for factor analysis",
      "how to perform EFA in python",
      "CFA library in python",
      "factor analysis with rotation options python",
      "exploratory factor analysis python",
      "confirmatory factor analysis python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "factor_analyzer"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "openTSNE",
    "description": "Optimized, parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) for large datasets.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://opentsne.readthedocs.io/en/stable/",
    "github_url": "https://github.com/pavlin-policar/openTSNE",
    "url": "https://github.com/pavlin-policar/openTSNE",
    "install": "pip install opentsne",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "openTSNE is an optimized and parallel implementation of t-distributed Stochastic Neighbor Embedding (t-SNE) designed for handling large datasets. It is commonly used by data scientists and researchers for visualizing high-dimensional data in a lower-dimensional space.",
    "use_cases": [
      "Visualizing clusters in high-dimensional data",
      "Reducing dimensionality for machine learning preprocessing"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dimensionality reduction",
      "how to use t-SNE in python",
      "visualizing high-dimensional data in python",
      "openTSNE tutorial",
      "parallel t-SNE implementation python",
      "large dataset t-SNE python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn",
      "umap-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "umap-learn",
    "description": "Fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear reduction.",
    "category": "Dimensionality Reduction",
    "docs_url": "https://umap-learn.readthedocs.io/en/latest/",
    "github_url": "https://github.com/lmcinnes/umap",
    "url": "https://github.com/lmcinnes/umap",
    "install": "pip install umap-learn",
    "tags": [
      "machine learning",
      "dimensionality"
    ],
    "best_for": "Feature extraction, PCA, high-dimensional data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "umap-learn is a fast and scalable implementation of Uniform Manifold Approximation and Projection (UMAP) for non-linear dimensionality reduction. It is used by data scientists and machine learning practitioners to visualize high-dimensional data in lower dimensions.",
    "use_cases": [
      "Visualizing high-dimensional datasets",
      "Reducing dimensions for clustering algorithms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for dimensionality reduction",
      "how to use UMAP in python",
      "fast UMAP implementation python",
      "scalable UMAP for machine learning",
      "visualize high-dimensional data python",
      "UMAP for non-linear reduction python"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "scikit-learn"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Biogeme",
    "description": "Maximum likelihood estimation of parametric models, with strong support for complex discrete choice models.",
    "category": "Discrete Choice Models",
    "docs_url": "https://biogeme.epfl.ch/index.html",
    "github_url": "https://github.com/michelbierlaire/biogeme",
    "url": "https://github.com/michelbierlaire/biogeme",
    "install": "pip install biogeme",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "discrete-choice",
      "statistical-modeling"
    ],
    "summary": "Biogeme is a Python package designed for maximum likelihood estimation of parametric models, particularly excelling in complex discrete choice models. It is used by researchers and practitioners in fields such as transportation, marketing, and economics to analyze choice behavior.",
    "use_cases": [
      "Estimating consumer preferences in marketing",
      "Analyzing transportation mode choice"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to estimate parametric models in python",
      "maximum likelihood estimation in python",
      "discrete choice analysis with Biogeme",
      "Biogeme tutorial",
      "complex discrete choice models in python"
    ],
    "primary_use_cases": [
      "maximum likelihood estimation",
      "discrete choice modeling"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "choicepy",
      "pylogit"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "PyBLP",
    "description": "Tools for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method.",
    "category": "Discrete Choice Models",
    "docs_url": "https://pyblp.readthedocs.io/",
    "github_url": "https://github.com/jeffgortmaker/pyblp",
    "url": "https://github.com/jeffgortmaker/pyblp",
    "install": "pip install pyblp",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "PyBLP is a Python package designed for estimating demand for differentiated products using the Berry-Levinsohn-Pakes (BLP) method. It is primarily used by researchers and practitioners in economics and data science who are interested in discrete choice modeling.",
    "use_cases": [
      "Estimating demand for consumer products",
      "Analyzing market competition"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for estimating demand",
      "how to use BLP method in python",
      "tools for discrete choice modeling in python",
      "python package for differentiated products demand estimation",
      "how to estimate demand using PyBLP",
      "BLP method implementation in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "PyLogit",
    "description": "Flexible implementation of conditional/multinomial logit models with utilities for data preparation.",
    "category": "Discrete Choice Models",
    "docs_url": null,
    "github_url": "https://github.com/timothyb0912/pylogit",
    "url": "https://github.com/timothyb0912/pylogit",
    "install": "pip install pylogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [
      "discrete choice"
    ],
    "summary": "PyLogit is a flexible implementation of conditional and multinomial logit models, providing utilities for data preparation. It is primarily used by data scientists and researchers working on discrete choice modeling.",
    "use_cases": [
      "Estimating consumer preferences",
      "Analyzing survey data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to implement multinomial logit in python",
      "PyLogit documentation",
      "conditional logit model python",
      "data preparation for logit models",
      "discrete choice analysis python",
      "logit model examples in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "XLogit",
    "description": "Fast estimation of Multinomial Logit and Mixed Logit models, optimized for performance.",
    "category": "Discrete Choice Models",
    "docs_url": "https://xlogit.readthedocs.io/",
    "github_url": "https://github.com/arteagac/xlogit",
    "url": "https://github.com/arteagac/xlogit",
    "install": "pip install xlogit",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "discrete choice"
    ],
    "summary": "XLogit is a Python package designed for the fast estimation of Multinomial Logit and Mixed Logit models, focusing on performance optimization. It is primarily used by data scientists and researchers working in the field of discrete choice modeling.",
    "use_cases": [
      "Estimating consumer choice behavior",
      "Analyzing survey data for preference modeling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Multinomial Logit",
      "how to estimate Mixed Logit models in python",
      "XLogit package documentation",
      "discrete choice modeling in python",
      "fast estimation of logit models python",
      "performance optimization in logit models"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "torch-choice",
    "description": "PyTorch framework for flexible estimation of complex discrete choice models, leveraging GPU acceleration.",
    "category": "Discrete Choice Models",
    "docs_url": "https://gsbdbi.github.io/torch-choice/",
    "github_url": "https://github.com/gsbDBI/torch-choice",
    "url": "https://github.com/gsbDBI/torch-choice",
    "install": "pip install torch-choice",
    "tags": [
      "discrete choice",
      "logit"
    ],
    "best_for": "Logit/probit models, consumer choice, demand estimation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "torch-choice is a PyTorch framework designed for flexible estimation of complex discrete choice models, utilizing GPU acceleration. It is primarily used by data scientists and researchers working on discrete choice analysis.",
    "use_cases": [
      "Estimating consumer preferences in marketing",
      "Modeling transportation choices"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for discrete choice models",
      "how to estimate complex discrete choice models in python",
      "torch-choice usage examples",
      "PyTorch discrete choice modeling",
      "GPU acceleration for discrete choice models",
      "discrete choice analysis in python"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "PyTorch"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "DoubleML",
    "description": "Implements the double/debiased ML framework (Chernozhukov et al.) for estimating causal parameters (ATE, LATE, POM) with ML nuisances.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://docs.doubleml.org/",
    "github_url": "https://github.com/DoubleML/doubleml-for-py",
    "url": "https://github.com/DoubleML/doubleml-for-py",
    "install": "pip install DoubleML",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "DoubleML implements the double/debiased ML framework for estimating causal parameters such as ATE, LATE, and POM using machine learning nuisances. It is used by data scientists and researchers focused on causal inference.",
    "use_cases": [
      "Estimating average treatment effects",
      "Conducting A/B test analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to estimate ATE in python",
      "double ML framework python",
      "machine learning for causal parameters",
      "using DoubleML for LATE estimation",
      "python package for debiased ML"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Chernozhukov et al. (2018)",
    "related_packages": [
      "causalml",
      "econml"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Doubly-Debiased-Lasso",
    "description": "High-dimensional inference under hidden confounding. Doubly debiased Lasso for valid inference.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "github_url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "url": "https://github.com/zijguo/Doubly-Debiased-Lasso",
    "install": "Install from GitHub",
    "tags": [
      "high-dimensional",
      "Lasso",
      "debiased"
    ],
    "best_for": "High-dim inference with confounding",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "high-dimensional"
    ],
    "summary": "Doubly-Debiased-Lasso is a Python package designed for high-dimensional inference under hidden confounding. It is primarily used by data scientists and researchers who need valid inference methods in causal analysis.",
    "use_cases": [
      "Estimating causal effects in high-dimensional settings",
      "Conducting valid inference in the presence of confounding variables"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for high-dimensional inference",
      "how to use Lasso for causal inference in python",
      "Doubly Debiased Lasso tutorial",
      "valid inference methods in python",
      "high-dimensional data analysis python",
      "causal inference library python"
    ],
    "primary_use_cases": [
      "causal effect estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "EconML",
    "description": "Microsoft toolkit for estimating heterogeneous treatment effects using DML, causal forests, meta-learners, and orthogonal ML methods.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://econml.azurewebsites.net/",
    "github_url": "https://github.com/py-why/EconML",
    "url": "https://github.com/py-why/EconML",
    "install": "pip install econml",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "EconML is a Microsoft toolkit designed for estimating heterogeneous treatment effects using advanced machine learning methods. It is primarily used by data scientists and researchers in the field of causal inference.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing A/B test results",
      "Evaluating policy impacts using observational data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for estimating treatment effects",
      "how to use causal forests in python",
      "EconML documentation",
      "machine learning for causal inference in python",
      "DML methods in python",
      "meta-learners for treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "dowhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "SynapseML",
    "description": "Microsoft's distributed ML library with native Double ML (DoubleMLEstimator) for heterogeneous treatment effects at scale.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://microsoft.github.io/SynapseML/",
    "github_url": "https://github.com/microsoft/SynapseML",
    "url": "https://github.com/microsoft/SynapseML",
    "install": "pip install synapseml",
    "tags": [
      "spark",
      "causal inference",
      "double ML",
      "distributed"
    ],
    "best_for": "Causal inference at 100M+ rows on Spark clusters",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "distributed-machine-learning"
    ],
    "summary": "SynapseML is a distributed machine learning library developed by Microsoft that provides native support for Double ML, specifically designed for estimating heterogeneous treatment effects at scale. It is used by data scientists and researchers working in causal inference and machine learning.",
    "use_cases": [
      "Estimating treatment effects in clinical trials",
      "Analyzing the impact of marketing campaigns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for distributed machine learning",
      "how to perform causal inference in Spark",
      "Double ML implementation in Python",
      "Microsoft ML library for treatment effects",
      "using SynapseML for A/B testing",
      "causal inference with Spark library",
      "distributed ML for heterogeneous treatment effects"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "Spark"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "pydoublelasso",
    "description": "Double\u2011post\u00a0Lasso estimator for high\u2011dimensional treatment effects (Belloni\u2011Chernozhukov\u2011Hansen\u202f2014).",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pydoublelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pydoublelasso/",
    "install": "pip install pydoublelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "machine-learning"
    ],
    "summary": "The pydoublelasso package provides a Double-post Lasso estimator for analyzing high-dimensional treatment effects, based on the methodology established by Belloni, Chernozhukov, and Hansen in 2014. It is primarily used by researchers and practitioners in causal inference and econometrics.",
    "use_cases": [
      "Estimating treatment effects in observational studies",
      "Analyzing high-dimensional data for causal inference"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for double post lasso",
      "how to estimate treatment effects in python",
      "pydoublelasso documentation",
      "double lasso estimator python",
      "causal inference python package",
      "high-dimensional treatment effects in python"
    ],
    "primary_use_cases": [
      "high-dimensional treatment effect estimation"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Belloni-Chernozhukov-Hansen (2014)",
    "maintenance_status": "active"
  },
  {
    "name": "pyhtelasso",
    "description": "Debiased\u2011Lasso detector of heterogeneous treatment effects in randomized experiments.",
    "category": "Double/Debiased Machine Learning (DML)",
    "docs_url": "https://pypi.org/project/pyhtelasso/",
    "github_url": null,
    "url": "https://pypi.org/project/pyhtelasso/",
    "install": "pip install pyhtelasso",
    "tags": [
      "machine learning",
      "causal inference"
    ],
    "best_for": "High-dimensional controls, ML-based causal inference",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "pyhtelasso is a Python package designed for detecting heterogeneous treatment effects in randomized experiments using a debiased Lasso approach. It is useful for researchers and practitioners in the field of causal inference and machine learning.",
    "use_cases": [
      "Analyzing treatment effects in clinical trials",
      "Evaluating policy interventions in social sciences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for debiased Lasso",
      "how to detect heterogeneous treatment effects in python",
      "causal inference tools in python",
      "randomized experiments analysis python",
      "machine learning for treatment effects",
      "debiased Lasso implementation python"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "DeclareDesign",
    "description": "Ex ante experimental design declaration and diagnosis. Enables researchers to formally describe their research design, diagnose statistical properties via simulation, and improve designs before data collection.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/",
    "github_url": "https://github.com/DeclareDesign/DeclareDesign",
    "url": "https://cran.r-project.org/package=DeclareDesign",
    "install": "install.packages(\"DeclareDesign\")",
    "tags": [
      "experimental-design",
      "pre-registration",
      "power-analysis",
      "simulation",
      "design-diagnosis"
    ],
    "best_for": "Ex ante experimental design declaration and diagnosis via simulation",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "experimental-design"
    ],
    "summary": "DeclareDesign is a package that allows researchers to formally describe their research design and diagnose its statistical properties through simulation. It is primarily used by researchers looking to improve their experimental designs before data collection.",
    "use_cases": [
      "Improving experimental designs before data collection",
      "Diagnosing statistical properties of research designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for experimental design",
      "how to diagnose research design in R",
      "pre-registration tools for experiments",
      "power analysis in R",
      "simulation for experimental design",
      "design diagnosis R package"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "contextual",
    "description": "Multi-armed bandit algorithms including Thompson Sampling, UCB, and LinUCB. Directly applicable to adaptive A/B testing and recommendation optimization with simulation and evaluation tools.",
    "category": "Experimental Design",
    "docs_url": "https://nth-iteration-labs.github.io/contextual/",
    "github_url": "https://github.com/Nth-iteration-labs/contextual",
    "url": "https://cran.r-project.org/package=contextual",
    "install": "install.packages(\"contextual\")",
    "tags": [
      "bandits",
      "Thompson-sampling",
      "UCB",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "best_for": "Multi-armed bandits for adaptive A/B testing with Thompson Sampling, UCB, LinUCB",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "bandits",
      "adaptive-experiments",
      "A/B-testing"
    ],
    "summary": "The 'contextual' package implements multi-armed bandit algorithms such as Thompson Sampling, UCB, and LinUCB. It is designed for adaptive A/B testing and recommendation optimization, providing simulation and evaluation tools for users.",
    "use_cases": [
      "Adaptive A/B testing",
      "Recommendation system optimization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for multi-armed bandits",
      "how to implement Thompson Sampling in R",
      "adaptive A/B testing in R",
      "recommendation optimization with R",
      "UCB algorithm in R",
      "evaluate bandit algorithms in R"
    ],
    "primary_use_cases": [
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fabricatr",
    "description": "Simulates realistic social science data for power analysis and design testing. Creates hierarchical data structures with correlated variables matching real-world patterns.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/fabricatr/",
    "github_url": "https://github.com/DeclareDesign/fabricatr",
    "url": "https://cran.r-project.org/package=fabricatr",
    "install": "install.packages(\"fabricatr\")",
    "tags": [
      "data-simulation",
      "power-analysis",
      "hierarchical-data",
      "synthetic-data",
      "design-testing"
    ],
    "best_for": "Simulating realistic hierarchical data for experimental power analysis and design testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fabricatr simulates realistic social science data for power analysis and design testing. It is used by researchers and data scientists to create hierarchical data structures with correlated variables that reflect real-world patterns.",
    "use_cases": [
      "Simulating data for social science research",
      "Testing design methodologies in experimental studies"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R library for data simulation",
      "how to simulate social science data in R",
      "R package for power analysis",
      "creating hierarchical data in R",
      "synthetic data generation in R",
      "design testing with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "randomizr",
    "description": "Proper randomization procedures for experiments with known assignment probabilities. Implements simple, complete, block, and cluster randomization with exact probability calculations for IPW estimation.",
    "category": "Experimental Design",
    "docs_url": "https://declaredesign.org/r/randomizr/",
    "github_url": "https://github.com/DeclareDesign/randomizr",
    "url": "https://cran.r-project.org/package=randomizr",
    "install": "install.packages(\"randomizr\")",
    "tags": [
      "randomization",
      "block-randomization",
      "cluster-randomization",
      "assignment-probability",
      "experiments"
    ],
    "best_for": "Proper experimental randomization with exact assignment probabilities for IPW",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "experimental-design"
    ],
    "summary": "The randomizr package provides proper randomization procedures for experiments with known assignment probabilities. It is used by researchers and practitioners conducting experiments that require randomization techniques.",
    "use_cases": [
      "Conducting randomized controlled trials",
      "Implementing block randomization in experiments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package for randomization",
      "how to perform block randomization in R",
      "cluster randomization techniques in R",
      "randomization procedures for experiments in R"
    ],
    "primary_use_cases": [
      "block randomization",
      "cluster randomization"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Nashpy",
    "description": "Computation of Nash equilibria for 2-player games. Support enumeration and Lemke-Howson algorithm.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://nashpy.readthedocs.io/",
    "github_url": "https://github.com/drvinceknight/Nashpy",
    "url": "https://github.com/drvinceknight/Nashpy",
    "install": "pip install nashpy",
    "tags": [
      "game theory",
      "Nash equilibrium"
    ],
    "best_for": "2-player Nash equilibrium computation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "game theory"
    ],
    "summary": "Nashpy is a Python library designed for the computation of Nash equilibria in 2-player games. It supports enumeration methods and the Lemke-Howson algorithm, making it useful for researchers and practitioners in game theory.",
    "use_cases": [
      "Computing Nash equilibria for strategic games",
      "Analyzing 2-player game scenarios",
      "Research in game theory applications"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Nash equilibria",
      "how to compute Nash equilibria in python",
      "Nashpy documentation",
      "game theory library python",
      "Lemke-Howson algorithm python",
      "enumerate Nash equilibria python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "OpenSpiel",
    "description": "DeepMind's 70+ game environments with multi-agent RL algorithms including Alpha-Rank, Neural Fictitious Self-Play, and CFR variants.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://openspiel.readthedocs.io/",
    "github_url": "https://github.com/deepmind/open_spiel",
    "url": "https://github.com/deepmind/open_spiel",
    "install": "pip install open_spiel",
    "tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "best_for": "Multi-agent RL and game-theoretic algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "game theory",
      "reinforcement learning",
      "multi-agent"
    ],
    "summary": "OpenSpiel is a library developed by DeepMind that provides over 70 game environments for multi-agent reinforcement learning. It includes various algorithms such as Alpha-Rank, Neural Fictitious Self-Play, and CFR variants, making it suitable for researchers and practitioners in the field of game theory and AI.",
    "use_cases": [
      "Testing multi-agent reinforcement learning algorithms",
      "Simulating game environments for research",
      "Developing AI strategies for competitive games"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for game theory",
      "how to implement multi-agent reinforcement learning in python",
      "DeepMind OpenSpiel tutorial",
      "game environments for reinforcement learning in python",
      "reinforcement learning algorithms in OpenSpiel",
      "multi-agent game theory library python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "fairpy",
    "description": "Fair division algorithms from academic papers. Implements cake-cutting and item allocation procedures.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://fairpy.readthedocs.io/",
    "github_url": "https://github.com/erelsgl/fairpy",
    "url": "https://github.com/erelsgl/fairpy",
    "install": "pip install fairpy",
    "tags": [
      "fair division",
      "allocation",
      "mechanism design"
    ],
    "best_for": "Fair division and cake-cutting algorithms",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "game theory",
      "mechanism design"
    ],
    "summary": "Fairpy provides implementations of fair division algorithms based on academic research, focusing on cake-cutting and item allocation procedures. It is useful for researchers and practitioners in game theory and mechanism design.",
    "use_cases": [
      "Dividing resources fairly among multiple parties",
      "Allocating items to participants in a fair manner"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for fair division",
      "how to implement cake-cutting in python",
      "item allocation algorithms in python",
      "fair allocation procedures python",
      "mechanism design library python",
      "fairpy documentation"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "fairpyx",
    "description": "Course-seat allocation with capacity constraints. Practical fair division for university course assignment.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": null,
    "github_url": "https://github.com/ariel-research/fairpyx",
    "url": "https://github.com/ariel-research/fairpyx",
    "install": "pip install fairpyx",
    "tags": [
      "fair division",
      "course allocation",
      "mechanism design"
    ],
    "best_for": "Course-seat allocation with constraints",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "fairpyx is a Python package designed for practical fair division in university course assignment, focusing on course-seat allocation with capacity constraints. It is useful for universities and educational institutions looking to optimize course assignments fairly.",
    "use_cases": [
      "Allocating seats in university courses fairly",
      "Optimizing course assignments based on student preferences"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for course allocation",
      "how to implement fair division in python",
      "course-seat allocation in python",
      "mechanism design for course assignment",
      "fairpyx documentation",
      "python fair division package",
      "university course assignment tools",
      "capacity constraints in course allocation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "pygambit",
    "description": "N-player extensive form games with Alan Turing Institute support. Computes Nash, perfect, and sequential equilibria.",
    "category": "Game Theory & Mechanism Design",
    "docs_url": "https://gambitproject.readthedocs.io/",
    "github_url": "https://github.com/gambitproject/gambit",
    "url": "https://github.com/gambitproject/gambit",
    "install": "pip install pygambit",
    "tags": [
      "game theory",
      "extensive form",
      "equilibrium"
    ],
    "best_for": "N-player extensive form game solving",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "pygambit is a Python package designed for analyzing N-player extensive form games, providing tools to compute Nash, perfect, and sequential equilibria. It is particularly useful for researchers and practitioners in game theory and mechanism design.",
    "use_cases": [
      "Analyzing strategic interactions in economics",
      "Studying competitive behaviors in multi-agent systems"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for game theory",
      "how to compute Nash equilibrium in python",
      "extensive form games analysis python",
      "tools for game theory in python",
      "sequential equilibria computation python",
      "pygambit documentation",
      "N-player game analysis python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "gamlss",
    "description": "Distributional regression where all parameters of a response distribution (location, scale, shape) can be modeled as functions of predictors, supporting 100+ distributions including highly skewed and kurtotic continuous and discrete distributions.",
    "category": "Generalized Additive Models",
    "docs_url": "https://www.gamlss.com/",
    "github_url": "https://github.com/gamlss-dev/gamlss",
    "url": "https://cran.r-project.org/package=gamlss",
    "install": "install.packages(\"gamlss\")",
    "tags": [
      "distributional-regression",
      "location-scale-shape",
      "flexible-distributions",
      "centile-estimation",
      "beyond-mean-modeling"
    ],
    "best_for": "Modeling non-normal responses where variance, skewness, or kurtosis depend on predictors, implementing Rigby & Stasinopoulos (2005)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'gamlss' package allows users to model all parameters of a response distribution as functions of predictors, supporting over 100 distributions. It is useful for statisticians and data scientists who need to analyze complex data distributions beyond traditional mean modeling.",
    "use_cases": [
      "Modeling highly skewed data distributions",
      "Estimating centiles for non-normal data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for distributional regression",
      "how to model location scale shape in R",
      "R flexible distributions library",
      "gamlss usage examples",
      "R centile estimation package",
      "advanced regression modeling in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mgcv",
    "description": "The definitive GAM implementation providing generalized additive (mixed) models with automatic smoothness estimation via REML/GCV/ML, supporting thin plate splines, tensor products, multiple distributions, and scalable fitting for large datasets.",
    "category": "Generalized Additive Models",
    "docs_url": "https://cran.r-project.org/web/packages/mgcv/mgcv.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mgcv",
    "install": "install.packages(\"mgcv\")",
    "tags": [
      "GAM",
      "splines",
      "smoothing",
      "penalized-regression",
      "mixed-models"
    ],
    "best_for": "Flexible nonparametric regression with automatic smoothing parameter selection, implementing Wood (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The mgcv package provides a comprehensive implementation of generalized additive models (GAMs) and mixed models in R, allowing users to estimate smooth functions and fit complex models to data. It is widely used by statisticians and data scientists for tasks involving non-linear relationships and large datasets.",
    "use_cases": [
      "Modeling non-linear relationships in data",
      "Analyzing large datasets with smooth functions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for generalized additive models",
      "how to fit GAMs in R",
      "R package for smoothness estimation",
      "using mgcv for mixed models",
      "automated smoothness estimation in R",
      "GAM implementation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "GeoLift",
    "description": "Meta's end-to-end synthetic control for geo experiments with multi-cell testing and power calculations.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": "https://facebookincubator.github.io/GeoLift/",
    "github_url": "https://github.com/facebookincubator/GeoLift",
    "url": "https://github.com/facebookincubator/GeoLift",
    "install": "pip install geolift",
    "tags": [
      "geo-experiments",
      "synthetic control",
      "incrementality"
    ],
    "best_for": "Meta's geo-level incrementality measurement",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "geo-experiments",
      "incrementality"
    ],
    "summary": "GeoLift is a Python package designed for conducting geo experiments using synthetic control methods. It is particularly useful for researchers and data scientists looking to measure lift and perform multi-cell testing.",
    "use_cases": [
      "Evaluating marketing campaign effectiveness",
      "Assessing policy impact in different regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to measure lift in python",
      "synthetic control methods in python",
      "incrementality testing in python",
      "multi-cell testing in python",
      "python package for causal inference"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "matched_markets",
    "description": "Google's time-based regression with greedy search for optimal geo experiment groups.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/matched_markets",
    "url": "https://github.com/google/matched_markets",
    "install": "pip install matched-markets",
    "tags": [
      "geo-experiments",
      "market matching",
      "incrementality"
    ],
    "best_for": "Optimal geo experiment group selection",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "geo-experiments"
    ],
    "summary": "The matched_markets package provides a time-based regression method with a greedy search algorithm to create optimal groups for geo experiments. It is useful for data scientists and researchers looking to measure lift and incrementality in marketing experiments.",
    "use_cases": [
      "Optimizing geo experiment groups",
      "Measuring marketing lift",
      "Conducting A/B tests with geographic considerations"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to measure incrementality in python",
      "optimal market matching in python",
      "time-based regression for experiments",
      "greedy search for geo experiments",
      "python package for lift measurement",
      "market matching algorithms in python",
      "geo experiment analysis in python"
    ],
    "primary_use_cases": [
      "geo experiment design",
      "incrementality measurement"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "trimmed_match",
    "description": "Google's robust analysis for paired geo experiments using trimmed statistics. Handles outliers in geo-level data.",
    "category": "Geo-Experiments & Lift Measurement",
    "docs_url": null,
    "github_url": "https://github.com/google/trimmed_match",
    "url": "https://github.com/google/trimmed_match",
    "install": "pip install trimmed-match",
    "tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "best_for": "Robust paired geo experiment analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "geo-experiments",
      "robust statistics",
      "incrementality"
    ],
    "summary": "trimmed_match is a Python package designed for robust analysis of paired geo experiments using trimmed statistics. It is particularly useful for handling outliers in geo-level data, making it suitable for data scientists and researchers in the field of causal inference.",
    "use_cases": [
      "Analyzing the impact of marketing campaigns across different regions",
      "Evaluating the effectiveness of policy changes in urban areas"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for geo experiments",
      "how to analyze paired geo data in python",
      "robust statistics in geo-level analysis",
      "incrementality measurement in python",
      "handling outliers in geo experiments",
      "A/B testing with trimmed statistics in python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "Awesome Economics",
    "description": "Curated list of economics resources including datasets, software, courses, and blogs.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/antontarasenko/awesome-economics",
    "url": "https://github.com/antontarasenko/awesome-economics",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Comprehensive economics resource directory",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Awesome Economics is a curated list of resources for economics, including datasets, software, courses, and blogs. It is useful for anyone looking to explore various economics-related tools and information.",
    "use_cases": [],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for economics resources",
      "how to find economics datasets in python",
      "best economics courses in python",
      "python tools for economics blogs"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "Computational Methods for practitioners",
    "description": "Open-source textbook by Richard Evans on computational methods for researchers using Python.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://opensourceecon.github.io/CompMethods/",
    "github_url": "https://github.com/OpenSourceEcon/CompMethods",
    "url": "https://opensourceecon.github.io/CompMethods/",
    "install": "",
    "tags": [
      "education",
      "computation",
      "textbook"
    ],
    "best_for": "Comprehensive computational economics course",
    "language": "Python",
    "difficulty": "beginner|intermediate",
    "prerequisites": [],
    "topic_tags": [
      "computation",
      "education"
    ],
    "summary": "This package is an open-source textbook designed for researchers to learn computational methods using Python. It is suitable for those interested in applying these methods in various research contexts.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for computational methods",
      "how to use Python for research",
      "open-source textbook on Python",
      "learning computational methods in Python",
      "Python tools for researchers",
      "educational resources for Python"
    ],
    "api_complexity": "simple"
  },
  {
    "name": "Econ Project Templates",
    "description": "Cookiecutter templates for reproducible economics research projects. Standardized project structure.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://econ-project-templates.readthedocs.io/",
    "github_url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "url": "https://github.com/OpenSourceEconomics/econ-project-templates",
    "install": "",
    "tags": [
      "reproducibility",
      "templates",
      "workflow"
    ],
    "best_for": "Starting reproducible economics projects",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "Econ Project Templates provides cookiecutter templates designed for reproducible economics research projects, offering a standardized project structure. It is useful for researchers in economics looking to streamline their project setup.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for reproducible economics research",
      "how to create a standardized project structure in python",
      "cookiecutter templates for economics",
      "economics research project setup in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "First Course in Causal Inference (Python)",
    "description": "Python implementation of Peng Ding's textbook 'A First Course in Causal Inference'. Educational resource with code examples.",
    "category": "Inference & Reporting Tools",
    "docs_url": "https://github.com/apoorvalal/ding_causalInference_python",
    "github_url": "https://github.com/apoorvalal/ding_causalInference_python",
    "url": "https://github.com/apoorvalal/ding_causalInference_python",
    "install": "",
    "tags": [
      "causal inference",
      "education",
      "textbook"
    ],
    "best_for": "Learning causal inference with Peng Ding's textbook",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "education"
    ],
    "summary": "This package provides a Python implementation of Peng Ding's textbook 'A First Course in Causal Inference', offering educational resources and code examples for learning causal inference. It is designed for students and practitioners interested in understanding causal analysis.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for causal inference",
      "how to learn causal inference in python",
      "educational resources for causal analysis in python",
      "examples of causal inference in python"
    ],
    "api_complexity": "simple"
  },
  {
    "name": "Python Packages for Applied Economists",
    "description": "Curated collection of Python packages for applied researchers organized by functionality.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "url": "https://github.com/clibassi/python-packages-for-applied-economists",
    "install": "",
    "tags": [
      "curated list",
      "resources"
    ],
    "best_for": "Discovering econometrics packages by use case",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "This package provides a curated collection of Python packages specifically designed for applied researchers. It is useful for those looking to enhance their research with various tools organized by functionality.",
    "use_cases": [],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for applied economists",
      "how to analyze data in python",
      "python packages for statistical analysis",
      "best python tools for econometrics",
      "resources for econometric analysis in python",
      "python libraries for data reporting"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "clusterbootstraps",
    "description": "Wild cluster bootstrap and pairs cluster bootstrap implementations for clustered standard errors.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/BingkunLin/clusterbootstraps",
    "url": "https://pypi.org/project/clusterbootstraps/",
    "install": "pip install clusterbootstraps",
    "tags": [
      "bootstrap",
      "clustered errors",
      "inference"
    ],
    "best_for": "Alternative cluster bootstrap implementations",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The clusterbootstraps package provides implementations for wild cluster bootstrap and pairs cluster bootstrap methods, which are useful for calculating clustered standard errors. It is primarily used by data scientists and statisticians working with clustered data.",
    "use_cases": [],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for cluster bootstrap",
      "how to calculate clustered standard errors in python",
      "wild cluster bootstrap implementation python",
      "pairs cluster bootstrap python",
      "bootstrapping techniques in python",
      "clustered errors analysis python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "maketables",
    "description": "Publication-ready regression tables for pyfixest, statsmodels, linearmodels. Outputs HTML (great-tables), LaTeX, Word.",
    "category": "Inference & Reporting Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/maketables",
    "url": "https://github.com/py-econometrics/maketables",
    "install": "pip install maketables",
    "tags": [
      "reporting",
      "tables",
      "visualization"
    ],
    "best_for": "Multi-format regression tables from pyfixest/statsmodels",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "maketables is a Python package that generates publication-ready regression tables for various statistical modeling libraries such as pyfixest, statsmodels, and linearmodels. It is useful for researchers and data scientists who need to present their regression results in a clear and professional format.",
    "use_cases": [
      "Generating regression tables for academic publications",
      "Creating HTML reports for data analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for regression tables",
      "how to create publication-ready tables in python",
      "maketables package documentation",
      "best python libraries for reporting",
      "generate LaTeX tables in python",
      "visualization tools for regression results"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "ShiftShareSE",
    "description": "Implements correct standard errors for Bartik/shift-share instrumental variables designs following Ad\u00e3o, Koles\u00e1r, and Morales (2019 QJE). Standard clustered SEs are typically incorrect for shift-share\u2014this package provides econometrically valid inference.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ShiftShareSE/ShiftShareSE.pdf",
    "github_url": "https://github.com/kolesarm/ShiftShareSE",
    "url": "https://cran.r-project.org/package=ShiftShareSE",
    "install": "install.packages(\"ShiftShareSE\")",
    "tags": [
      "shift-share",
      "Bartik",
      "instrumental-variables",
      "standard-errors",
      "regional-economics"
    ],
    "best_for": "Correct standard errors for Bartik/shift-share IV designs, implementing Ad\u00e3o, Koles\u00e1r & Morales (2019 QJE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "econometrics"
    ],
    "summary": "ShiftShareSE implements correct standard errors for Bartik/shift-share instrumental variables designs, providing econometrically valid inference. It is primarily used by researchers and practitioners in regional economics.",
    "use_cases": [
      "Estimating the impact of regional economic policies",
      "Analyzing labor market effects using shift-share designs"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for shift-share analysis",
      "how to implement Bartik IV in R",
      "standard errors for shift-share designs in R",
      "econometric inference with R package",
      "Bartik instrumental variables R",
      "shift-share standard errors R package"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Ad\u00e3o, Koles\u00e1r, and Morales (2019)",
    "maintenance_status": "active"
  },
  {
    "name": "gmm",
    "description": "Generalized Method of Moments estimation implementing two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. Supports linear and nonlinear moment conditions.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/gmm/gmm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=gmm",
    "install": "install.packages(\"gmm\")",
    "tags": [
      "GMM",
      "method-of-moments",
      "HAC",
      "instrumental-variables",
      "CUE"
    ],
    "best_for": "Generalized Method of Moments estimation with two-step, iterated, and CUE estimators",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "instrumental-variables"
    ],
    "summary": "The gmm package provides tools for Generalized Method of Moments estimation, including two-step GMM, iterated GMM, and continuous updated estimator (CUE) with HAC covariance matrices. It is used by statisticians and data scientists for estimating models with linear and nonlinear moment conditions.",
    "use_cases": [
      "Estimating parameters in econometric models",
      "Conducting hypothesis tests with GMM",
      "Analyzing time series data with HAC covariance"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for GMM estimation",
      "how to perform two-step GMM in R",
      "HAC covariance matrices in R",
      "GMM method-of-moments R",
      "instrumental variables estimation R",
      "CUE estimation in R"
    ],
    "primary_use_cases": [
      "parameter estimation using GMM",
      "hypothesis testing with moment conditions"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ivmodel",
    "description": "Specialized package for weak instrument diagnostics implementing Anderson-Rubin tests, k-class estimators (LIML, Fuller), and sensitivity analysis following Jiang et al. (2015). Essential when instrument strength is questionable.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/ivmodel/ivmodel.pdf",
    "github_url": "https://github.com/hyunseungkang/ivmodel",
    "url": "https://cran.r-project.org/package=ivmodel",
    "install": "install.packages(\"ivmodel\")",
    "tags": [
      "instrumental-variables",
      "weak-instruments",
      "Anderson-Rubin",
      "LIML",
      "sensitivity-analysis"
    ],
    "best_for": "Weak instrument diagnostics with Anderson-Rubin tests and k-class estimators (LIML, Fuller)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference"
    ],
    "summary": "The ivmodel package provides tools for weak instrument diagnostics, including Anderson-Rubin tests and k-class estimators like LIML and Fuller. It is essential for researchers dealing with questionable instrument strength in their econometric models.",
    "use_cases": [
      "Evaluating instrument strength in econometric models",
      "Conducting sensitivity analysis for instrumental variable estimates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for weak instrument diagnostics",
      "how to perform Anderson-Rubin tests in R",
      "R LIML estimator package",
      "sensitivity analysis in R",
      "diagnosing weak instruments R",
      "ivmodel package usage",
      "econometrics package for R"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Jiang et al. (2015)",
    "maintenance_status": "active"
  },
  {
    "name": "ivreg",
    "description": "Modern implementation of two-stage least squares (2SLS) instrumental variables regression with comprehensive diagnostics including hat values, studentized residuals, and component-plus-residual plots. Successor to AER's ivreg() function with superior diagnostic tools.",
    "category": "Instrumental Variables",
    "docs_url": "https://zeileis.github.io/ivreg/",
    "github_url": "https://github.com/zeileis/ivreg",
    "url": "https://cran.r-project.org/package=ivreg",
    "install": "install.packages(\"ivreg\")",
    "tags": [
      "instrumental-variables",
      "2SLS",
      "IV-regression",
      "endogeneity",
      "diagnostics"
    ],
    "best_for": "Modern 2SLS instrumental variables regression with comprehensive diagnostic tools",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "diagnostics"
    ],
    "summary": "The ivreg package provides a modern implementation of two-stage least squares (2SLS) instrumental variables regression, offering comprehensive diagnostics such as hat values and studentized residuals. It is designed for users needing advanced regression analysis tools, particularly in the context of endogeneity.",
    "use_cases": [
      "Analyzing the impact of policy changes using instrumental variables",
      "Estimating causal relationships in economics",
      "Conducting regression analysis with endogeneity concerns"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for instrumental variables regression",
      "how to perform 2SLS in R",
      "diagnostics for IV regression in R",
      "R library for endogeneity correction",
      "hat values in R ivreg",
      "studentized residuals in R",
      "component-plus-residual plots in R"
    ],
    "primary_use_cases": [
      "instrumental variables regression",
      "diagnostic analysis for regression models"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "AER"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "momentfit",
    "description": "Modern S4-based implementation of Generalized Method of Moments supporting systems of equations, nonlinear moment conditions, and hypothesis testing. Successor to gmm package with object-oriented design.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/momentfit/momentfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=momentfit",
    "install": "install.packages(\"momentfit\")",
    "tags": [
      "GMM",
      "S4-class",
      "systems-estimation",
      "moment-conditions",
      "hypothesis-testing"
    ],
    "best_for": "Modern object-oriented GMM estimation for systems of equations",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "instrumental-variables",
      "hypothesis-testing"
    ],
    "summary": "Momentfit is a modern S4-based implementation of the Generalized Method of Moments (GMM) that supports systems of equations and nonlinear moment conditions. It is designed for users who need to perform hypothesis testing in econometric models.",
    "use_cases": [
      "Estimating parameters in econometric models",
      "Testing hypotheses in economic research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for Generalized Method of Moments",
      "how to perform hypothesis testing in R",
      "R GMM implementation",
      "S4 class in R for econometrics",
      "nonlinear moment conditions in R",
      "systems of equations in R"
    ],
    "primary_use_cases": [
      "parameter estimation in econometrics",
      "hypothesis testing in econometric models"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "gmm"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "systemfit",
    "description": "Simultaneous systems estimation implementing Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). Critical for demand systems and structural macro models.",
    "category": "Instrumental Variables",
    "docs_url": "https://cran.r-project.org/web/packages/systemfit/systemfit.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=systemfit",
    "install": "install.packages(\"systemfit\")",
    "tags": [
      "SUR",
      "2SLS",
      "3SLS",
      "systems-estimation",
      "demand-systems"
    ],
    "best_for": "Simultaneous equation systems: SUR, 2SLS, and 3SLS estimation for demand systems and structural models",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "instrumental-variables",
      "econometrics"
    ],
    "summary": "The systemfit package allows for simultaneous estimation of systems of equations using methods such as Seemingly Unrelated Regression (SUR), two-stage least squares (2SLS), and three-stage least squares (3SLS). It is particularly useful for economists and researchers working on demand systems and structural macroeconomic models.",
    "use_cases": [
      "Estimating demand systems for consumer goods",
      "Analyzing structural macroeconomic models",
      "Conducting policy impact assessments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for simultaneous systems estimation",
      "how to perform SUR in R",
      "R 2SLS implementation",
      "best practices for demand systems in R",
      "three-stage least squares R tutorial",
      "econometric modeling with R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "py-econometrics `gmm`",
    "description": "Lightweight package for setting up and estimating custom GMM models based on user-defined moment conditions.",
    "category": "Instrumental Variables (IV) & GMM",
    "docs_url": "https://github.com/py-econometrics/gmm",
    "github_url": null,
    "url": "https://github.com/py-econometrics/gmm",
    "install": "pip install gmm",
    "tags": [
      "IV",
      "GMM"
    ],
    "best_for": "Endogeneity correction, 2SLS, moment estimation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "py-econometrics `gmm` is a lightweight package designed for setting up and estimating custom Generalized Method of Moments (GMM) models based on user-defined moment conditions. It is useful for researchers and practitioners in econometrics who need to implement GMM estimation tailored to their specific requirements.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for GMM estimation",
      "how to set up custom GMM models in python",
      "GMM models in econometrics python",
      "estimating moment conditions in python",
      "lightweight GMM package python",
      "using GMM for econometric analysis in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "CausalMotifs",
    "description": "Meta's library for estimating heterogeneous spillover effects in A/B tests. Handles network interference.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/facebookresearch/CausalMotifs",
    "github_url": "https://github.com/facebookresearch/CausalMotifs",
    "url": "https://github.com/facebookresearch/CausalMotifs",
    "install": "pip install causal-motifs",
    "tags": [
      "network interference",
      "spillovers",
      "A/B testing"
    ],
    "best_for": "Spillover effects in social networks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "network-analysis"
    ],
    "summary": "CausalMotifs is a library designed to estimate heterogeneous spillover effects in A/B tests, particularly in the presence of network interference. It is useful for researchers and data scientists conducting experiments where interactions between subjects may affect outcomes.",
    "use_cases": [
      "Estimating spillover effects in marketing campaigns",
      "Analyzing the impact of social networks on treatment effects"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for estimating spillover effects",
      "how to analyze A/B tests with network interference in python",
      "CausalMotifs documentation",
      "best practices for A/B testing in python",
      "network interference analysis tools",
      "spillover effects in experiments python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "spilled_t",
    "description": "Treatment and spillover effect estimation under network interference. Separates direct and indirect effects.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/mpleung/spilled_t",
    "github_url": "https://github.com/mpleung/spilled_t",
    "url": "https://github.com/mpleung/spilled_t",
    "install": "pip install spilled_t",
    "tags": [
      "network interference",
      "spillovers",
      "treatment effects"
    ],
    "best_for": "Separating direct and spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "network-analysis"
    ],
    "summary": "The 'spilled_t' package estimates treatment and spillover effects in the presence of network interference. It is useful for researchers and practitioners who need to separate direct and indirect effects in their analyses.",
    "use_cases": [
      "Estimating treatment effects in social networks",
      "Analyzing the impact of interventions in connected populations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for treatment effect estimation",
      "how to analyze spillover effects in python",
      "network interference analysis in python",
      "estimating indirect effects with python",
      "spillover effect estimation library",
      "causal inference tools in python"
    ],
    "primary_use_cases": [
      "treatment effect estimation",
      "spillover effect analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "testinterference",
    "description": "Statistical tests for SUTVA violations and spillover hypotheses. Detects network interference in experiments.",
    "category": "Interference & Spillovers",
    "docs_url": "https://github.com/tkhdyanagi/testinterference",
    "github_url": "https://github.com/tkhdyanagi/testinterference",
    "url": "https://github.com/tkhdyanagi/testinterference",
    "install": "pip install testinterference",
    "tags": [
      "SUTVA",
      "spillovers",
      "hypothesis testing"
    ],
    "best_for": "Testing for spillover effects",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "hypothesis-testing"
    ],
    "summary": "The testinterference package provides statistical tests to identify violations of the Stable Unit Treatment Value Assumption (SUTVA) and to analyze spillover effects in experimental designs. It is useful for researchers and practitioners in the field of causal inference who are investigating network interference in experiments.",
    "use_cases": [
      "Analyzing network interference in randomized controlled trials",
      "Testing for SUTVA violations in experimental data"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for SUTVA violations",
      "how to test for spillover effects in python",
      "statistical tests for interference in experiments",
      "detecting network interference in python",
      "hypothesis testing for spillovers",
      "python package for causal inference"
    ],
    "primary_use_cases": [
      "SUTVA violation detection",
      "spillover hypothesis testing"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "glmnet",
    "description": "Efficient procedures for fitting regularized generalized linear models via penalized maximum likelihood. Implements LASSO, ridge regression, and elastic net with extremely fast coordinate descent algorithms. Foundation for high-dimensional regression and causal ML.",
    "category": "Machine Learning",
    "docs_url": "https://glmnet.stanford.edu/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=glmnet",
    "install": "install.packages(\"glmnet\")",
    "tags": [
      "LASSO",
      "ridge",
      "elastic-net",
      "regularization",
      "high-dimensional"
    ],
    "best_for": "LASSO, ridge, and elastic net regularization\u2014foundation for high-dimensional regression and causal ML",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "glmnet provides efficient procedures for fitting regularized generalized linear models through penalized maximum likelihood. It is widely used by data scientists and statisticians for high-dimensional regression and causal machine learning applications.",
    "use_cases": [
      "Fitting LASSO models for feature selection",
      "Applying ridge regression for multicollinearity issues"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for LASSO",
      "how to perform ridge regression in R",
      "glmnet tutorial",
      "R elastic net example",
      "high-dimensional regression in R",
      "causal ML with glmnet"
    ],
    "primary_use_cases": [
      "high-dimensional regression",
      "causal ML"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret",
      "glm",
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "ranger",
    "description": "Fast implementation of random forests particularly suited for high-dimensional data. Provides survival forests, classification, and regression with efficient memory usage. Core backend for grf's causal forests.",
    "category": "Machine Learning",
    "docs_url": "https://cran.r-project.org/web/packages/ranger/ranger.pdf",
    "github_url": "https://github.com/imbs-hl/ranger",
    "url": "https://cran.r-project.org/package=ranger",
    "install": "install.packages(\"ranger\")",
    "tags": [
      "random-forests",
      "survival-forests",
      "high-dimensional",
      "fast",
      "causal-forests"
    ],
    "best_for": "Fast random forests for high-dimensional data\u2014backend for grf causal forests",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "machine-learning",
      "random-forests",
      "survival-analysis"
    ],
    "summary": "Ranger is a fast implementation of random forests that is particularly suited for high-dimensional data. It provides capabilities for survival forests, classification, and regression, making it useful for data scientists and statisticians working with complex datasets.",
    "use_cases": [
      "Predicting outcomes in high-dimensional datasets",
      "Conducting survival analysis with random forests"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for random forests",
      "how to implement survival forests in R",
      "fast random forests in R",
      "R package for high-dimensional data analysis"
    ],
    "primary_use_cases": [
      "causal forest estimation"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "randomForest",
      "grf"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "tidymodels",
    "description": "Modern framework for modeling and machine learning using tidyverse principles. Meta-package including parsnip (model specification), recipes (preprocessing), workflows, tune (hyperparameter tuning), and yardstick (metrics). Successor to caret.",
    "category": "Machine Learning",
    "docs_url": "https://www.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/tidymodels",
    "url": "https://cran.r-project.org/package=tidymodels",
    "install": "install.packages(\"tidymodels\")",
    "tags": [
      "machine-learning",
      "tidyverse",
      "modeling-framework",
      "hyperparameter-tuning",
      "preprocessing"
    ],
    "best_for": "Modern tidyverse-native ML framework with reproducible workflows\u2014successor to caret",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "tidymodels is a modern framework for modeling and machine learning that adheres to tidyverse principles. It is designed for data scientists and analysts who want to streamline their modeling processes using a cohesive set of tools.",
    "use_cases": [
      "Building predictive models",
      "Performing hyperparameter tuning",
      "Preprocessing data for analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for machine learning",
      "how to do hyperparameter tuning in R",
      "tidyverse modeling framework",
      "best practices for preprocessing in R",
      "R package for model specification",
      "how to use tidymodels for machine learning"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "caret"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "xgboost",
    "description": "Extreme Gradient Boosting implementing state-of-the-art gradient boosted decision trees. Highly efficient, scalable, and portable with interfaces to R, Python, and other languages. Essential for prediction in double ML workflows.",
    "category": "Machine Learning",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://cran.r-project.org/package=xgboost",
    "install": "install.packages(\"xgboost\")",
    "tags": [
      "gradient-boosting",
      "XGBoost",
      "prediction",
      "machine-learning",
      "ensemble"
    ],
    "best_for": "State-of-the-art gradient boosting for prediction in causal ML and double ML workflows",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics"
    ],
    "topic_tags": [
      "machine-learning",
      "ensemble"
    ],
    "summary": "XGBoost is an implementation of gradient boosted decision trees designed for efficiency and scalability. It is widely used in machine learning tasks, particularly for prediction in double ML workflows.",
    "use_cases": [
      "predicting outcomes in machine learning competitions",
      "building predictive models for business analytics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for gradient boosting",
      "how to use XGBoost in R",
      "XGBoost for prediction",
      "best practices for XGBoost",
      "XGBoost tutorial",
      "gradient boosting with XGBoost"
    ],
    "primary_use_cases": [
      "prediction in double ML workflows"
    ],
    "api_complexity": "intermediate",
    "framework_compatibility": [
      "R",
      "Python"
    ],
    "related_packages": [
      "LightGBM",
      "CatBoost"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "emmeans",
    "description": "Estimated Marginal Means (least-squares means) for factorial designs. Computes adjusted means and contrasts for balanced and unbalanced designs, with support for mixed models and Bayesian models.",
    "category": "Marginal Effects",
    "docs_url": "https://rvlenth.github.io/emmeans/",
    "github_url": "https://github.com/rvlenth/emmeans",
    "url": "https://cran.r-project.org/package=emmeans",
    "install": "install.packages(\"emmeans\")",
    "tags": [
      "marginal-means",
      "least-squares-means",
      "factorial-designs",
      "contrasts",
      "mixed-models"
    ],
    "best_for": "Estimated marginal means for factorial designs with interaction interpretation",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "marginal-effects",
      "mixed-models"
    ],
    "summary": "The emmeans package computes estimated marginal means, also known as least-squares means, for factorial designs. It is used by statisticians and data scientists to analyze and interpret the effects of factors in both balanced and unbalanced designs, including mixed and Bayesian models.",
    "use_cases": [
      "Analyzing the effects of different treatments in an experiment",
      "Comparing group means in a factorial design"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for estimated marginal means",
      "how to compute least-squares means in R",
      "R contrasts for factorial designs",
      "mixed models analysis in R",
      "bayesian models with emmeans",
      "marginal effects in R"
    ],
    "primary_use_cases": [
      "A/B test analysis",
      "contrasts in factorial designs"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "marginaleffects",
    "description": "Modern standard for interpreting regression results\u2014up to 1000\u00d7 faster than margins. Computes marginal effects, predictions, contrasts, and slopes for 100+ model classes. Published in JSS 2024.",
    "category": "Marginal Effects",
    "docs_url": "https://marginaleffects.com/",
    "github_url": "https://github.com/vincentarelbundock/marginaleffects",
    "url": "https://cran.r-project.org/package=marginaleffects",
    "install": "install.packages(\"marginaleffects\")",
    "tags": [
      "marginal-effects",
      "predictions",
      "contrasts",
      "interpretation",
      "slopes"
    ],
    "best_for": "Modern marginal effects interpretation\u20141000\u00d7 faster than margins with 100+ model support, JSS 2024",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "causal-inference",
      "interpretation"
    ],
    "summary": "The marginaleffects package provides a modern standard for interpreting regression results, allowing users to compute marginal effects, predictions, contrasts, and slopes for over 100 model classes. It is designed for statisticians and data scientists who need efficient and accurate interpretation of regression outputs.",
    "use_cases": [
      "Interpreting regression results quickly",
      "Comparing model predictions",
      "Analyzing contrasts between groups"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R library for marginal effects",
      "how to compute predictions in R",
      "R package for regression interpretation",
      "marginal effects in R",
      "how to use contrasts in R",
      "R slopes analysis package"
    ],
    "primary_use_cases": [
      "computing marginal effects",
      "making predictions from regression models"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "JSS (2024)",
    "maintenance_status": "active"
  },
  {
    "name": "Lifetimes",
    "description": "Analyze customer lifetime value (CLV) using probabilistic models (BG/NBD, Pareto/NBD) to predict purchases.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://lifetimes.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifetimes",
    "url": "https://github.com/CamDavidsonPilon/lifetimes",
    "install": "pip install lifetimes",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "analytics"
    ],
    "summary": "Lifetimes is a Python package designed to analyze customer lifetime value (CLV) using probabilistic models such as BG/NBD and Pareto/NBD. It is used by data scientists and marketers to predict customer purchases and optimize marketing strategies.",
    "use_cases": [
      "Predicting customer purchases over time",
      "Estimating customer lifetime value for marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for customer lifetime value analysis",
      "how to predict purchases in python",
      "BG/NBD model implementation in python",
      "Pareto/NBD model for marketing",
      "analyze customer behavior with python",
      "customer analytics library python"
    ],
    "primary_use_cases": [
      "customer lifetime value analysis",
      "purchase prediction"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "MaMiMo",
    "description": "Lightweight Python library focused specifically on Marketing Mix Modeling implementation.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/Garve/mamimo",
    "url": "https://github.com/Garve/mamimo",
    "install": "pip install mamimo",
    "tags": [
      "marketing",
      "analytics"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [
      "marketing",
      "analytics"
    ],
    "summary": "MaMiMo is a lightweight Python library designed for implementing Marketing Mix Modeling. It is particularly useful for analysts and data scientists working in marketing analytics.",
    "use_cases": [],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for marketing mix modeling",
      "how to do marketing analytics in python",
      "lightweight python library for MMM",
      "best practices for marketing mix modeling in python",
      "implementing marketing mix models in python",
      "python tools for business analytics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "PyMC Marketing",
    "description": "Collection of Bayesian marketing models built with PyMC, including MMM, CLV, and attribution.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": "https://www.pymc-marketing.io/",
    "github_url": "https://github.com/pymc-labs/pymc-marketing",
    "url": "https://github.com/pymc-labs/pymc-marketing",
    "install": "pip install pymc-marketing",
    "tags": [
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing",
      "analytics"
    ],
    "summary": "PyMC Marketing is a collection of Bayesian marketing models built with PyMC, designed for tasks such as Marketing Mix Modeling (MMM), Customer Lifetime Value (CLV) estimation, and attribution analysis. It is useful for data scientists and marketers looking to apply Bayesian methods to their marketing strategies.",
    "use_cases": [
      "Estimating the impact of marketing channels",
      "Analyzing customer lifetime value",
      "Attributing sales to different marketing efforts"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian marketing models",
      "how to perform MMM in python",
      "customer lifetime value estimation with PyMC",
      "attribution modeling in python",
      "bayesian analytics for marketing",
      "marketing mix models in python"
    ],
    "primary_use_cases": [
      "Marketing Mix Modeling",
      "Customer Lifetime Value estimation",
      "Attribution analysis"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "mmm_stan",
    "description": "Python/STAN implementation of Bayesian Marketing Mix Models.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/sibylhe/mmm_stan",
    "url": "https://github.com/sibylhe/mmm_stan",
    "install": "GitHub Repository",
    "tags": [
      "marketing",
      "analytics",
      "Bayesian"
    ],
    "best_for": "Marketing ROI, media mix optimization, attribution",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "bayesian",
      "marketing",
      "analytics"
    ],
    "summary": "mmm_stan is a Python implementation of Bayesian Marketing Mix Models, designed to help businesses analyze and optimize their marketing strategies. It is primarily used by data scientists and marketers looking to leverage Bayesian methods for marketing analytics.",
    "use_cases": [
      "optimizing marketing spend",
      "analyzing the effectiveness of advertising campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for Bayesian Marketing Mix Models",
      "how to analyze marketing data with Python",
      "Bayesian analytics for marketing",
      "marketing mix modeling in Python",
      "using STAN for marketing analysis",
      "Bayesian methods for business analytics"
    ],
    "primary_use_cases": [
      "marketing mix modeling",
      "Bayesian analysis of marketing data"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "ziln_cltv",
    "description": "Google's Zero-Inflated Lognormal loss for heavily-tailed LTV distributions. Outputs both predicted LTV and churn probability.",
    "category": "Marketing Mix Models (MMM) & Business Analytics",
    "docs_url": null,
    "github_url": "https://github.com/google/lifetime_value",
    "url": "https://github.com/google/lifetime_value",
    "install": "pip install lifetime-value",
    "tags": [
      "LTV",
      "customer analytics",
      "churn"
    ],
    "best_for": "Customer LTV with zero-inflated distributions",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "customer analytics",
      "churn prediction"
    ],
    "summary": "The ziln_cltv package provides Google's Zero-Inflated Lognormal loss model for predicting customer Lifetime Value (LTV) and churn probability, specifically designed for heavily-tailed distributions. It is useful for marketers and data scientists looking to analyze customer behavior and retention.",
    "use_cases": [
      "Predicting customer lifetime value for subscription services",
      "Analyzing churn rates in e-commerce",
      "Estimating LTV for marketing campaigns"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for LTV prediction",
      "how to predict churn probability in python",
      "zero-inflated lognormal model python",
      "customer analytics tools in python",
      "python package for marketing mix models",
      "predicting customer lifetime value using python",
      "churn analysis with python",
      "business analytics library in python"
    ],
    "primary_use_cases": [
      "customer lifetime value prediction",
      "churn probability estimation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "algmatch",
    "description": "Student-Project Allocation with lecturer preferences. Extends matching to three-sided markets.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": null,
    "url": "https://pypi.org/project/algmatch/",
    "install": "pip install algmatch",
    "tags": [
      "matching",
      "market design",
      "allocation"
    ],
    "best_for": "Student-project-lecturer allocation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "algmatch is a Python package designed for student-project allocation that incorporates lecturer preferences, extending the matching process to three-sided markets. It is useful for educational institutions and researchers involved in allocation problems.",
    "use_cases": [
      "Allocating students to projects based on preferences",
      "Facilitating lecturer involvement in project assignments"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python library for student-project allocation",
      "how to match students with projects in python",
      "python matching algorithms for three-sided markets",
      "allocation problems in python",
      "lecturer preferences in project allocation python",
      "student project matching library",
      "market design algorithms in python",
      "how to implement matching in python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "deep-opt-auctions",
    "description": "Neural network optimal auction design. Implements RegretNet, RochetNet for mechanism design.",
    "category": "Matching & Market Design",
    "docs_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "github_url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "url": "https://github.com/saisrivatsan/deep-opt-auctions",
    "install": "Install from GitHub",
    "tags": [
      "auctions",
      "mechanism design",
      "deep learning"
    ],
    "best_for": "Neural network auction design",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The deep-opt-auctions package provides neural network-based solutions for optimal auction design, specifically implementing RegretNet and RochetNet for mechanism design. It is useful for researchers and practitioners in the fields of economics and machine learning.",
    "use_cases": [
      "Designing optimal auctions using neural networks",
      "Analyzing auction mechanisms with deep learning"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for optimal auction design",
      "how to implement neural networks for auctions in python",
      "deep learning for mechanism design",
      "auctions with deep learning in python",
      "RegretNet implementation in python",
      "RochetNet auction design python"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "kep_solver",
    "description": "Kidney exchange optimization with hierarchical objectives. Production-ready for kidney paired donation.",
    "category": "Matching & Market Design",
    "docs_url": "https://kep-solver.readthedocs.io/en/latest/",
    "github_url": "https://gitlab.com/wpettersson/kep_solver",
    "url": "https://pypi.org/project/kep_solver/",
    "install": "pip install kep-solver",
    "tags": [
      "matching",
      "market design",
      "kidney exchange"
    ],
    "best_for": "Kidney exchange program optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "The kep_solver package provides optimization solutions for kidney exchange programs, focusing on hierarchical objectives. It is designed for use in kidney paired donation scenarios, making it suitable for healthcare professionals and researchers in the field of organ transplantation.",
    "use_cases": [
      "Optimizing kidney paired donation matches",
      "Improving efficiency in organ transplantation programs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for kidney exchange optimization",
      "how to optimize kidney paired donation in python",
      "matching algorithms for kidney exchange",
      "market design tools for healthcare",
      "hierarchical objectives in optimization",
      "python matching and market design library"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "matching",
    "description": "Implements Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates using Gale-Shapley (JOSS paper).",
    "category": "Matching & Market Design",
    "docs_url": "https://daffidwilde.github.io/matching/",
    "github_url": "https://github.com/daffidwilde/matching",
    "url": "https://github.com/daffidwilde/matching",
    "install": "pip install matching",
    "tags": [
      "matching",
      "market design",
      "Gale-Shapley"
    ],
    "best_for": "Classic two-sided matching algorithms",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The matching package implements algorithms for Stable Marriage, Hospital-Resident, Student-Allocation, and Stable Roommates problems using the Gale-Shapley method. It is useful for researchers and practitioners in matching and market design.",
    "use_cases": [
      "Allocating students to schools",
      "Matching residents to hospitals",
      "Organizing stable marriages",
      "Creating roommate assignments"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for stable marriage",
      "how to implement Gale-Shapley in python",
      "matching algorithms in python",
      "hospital-resident matching python",
      "student allocation algorithms python",
      "stable roommates implementation python"
    ],
    "api_complexity": "intermediate",
    "implements_paper": "Gale & Shapley (1962)",
    "maintenance_status": "active"
  },
  {
    "name": "scarfmatch",
    "description": "Matching with couples using Scarf's algorithm. Essential for NRMP-style medical residency matching.",
    "category": "Matching & Market Design",
    "docs_url": null,
    "github_url": "https://github.com/dwtang/scarf",
    "url": "https://pypi.org/project/scarfmatch/",
    "install": "pip install scarfmatch",
    "tags": [
      "matching",
      "market design",
      "couples"
    ],
    "best_for": "Residency matching with couples",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas"
    ],
    "topic_tags": [],
    "summary": "Scarfmatch implements Scarf's algorithm for matching couples, which is essential for NRMP-style medical residency matching. It is designed for users involved in market design and matching processes.",
    "use_cases": [
      "Matching couples for medical residency",
      "Designing matching algorithms for market scenarios"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python library for matching couples",
      "how to use Scarf's algorithm in python",
      "NRMP-style matching in python",
      "market design algorithms in python"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "glmmTMB",
    "description": "Fit generalized linear mixed models with extensions including zero-inflation, hurdle models, heteroscedasticity, and autocorrelation using Template Model Builder (TMB) with automatic differentiation and Laplace approximation.",
    "category": "Mixed Effects",
    "docs_url": "https://glmmtmb.github.io/glmmTMB/",
    "github_url": "https://github.com/glmmTMB/glmmTMB",
    "url": "https://cran.r-project.org/package=glmmTMB",
    "install": "install.packages(\"glmmTMB\")",
    "tags": [
      "GLMM",
      "zero-inflation",
      "negative-binomial",
      "TMB",
      "overdispersion"
    ],
    "best_for": "Zero-inflated, overdispersed, or complex GLMMs beyond lme4 capabilities, implementing Brooks et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "glmmTMB is an R package that fits generalized linear mixed models with various extensions such as zero-inflation and hurdle models. It is used by statisticians and data scientists for modeling complex data structures.",
    "use_cases": [
      "Modeling count data with excess zeros",
      "Analyzing longitudinal data with random effects"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for generalized linear mixed models",
      "how to fit mixed effects models in R",
      "zero-inflation models in R",
      "hurdle models R package",
      "TMB for mixed models",
      "R package for overdispersion handling",
      "fit autocorrelation in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "lme4",
    "description": "Fit linear and generalized linear mixed-effects models using S4 classes with Eigen C++ library for efficient computation, supporting arbitrarily nested and crossed random effects structures for hierarchical and longitudinal data.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lme4/vignettes/",
    "github_url": "https://github.com/lme4/lme4",
    "url": "https://cran.r-project.org/package=lme4",
    "install": "install.packages(\"lme4\")",
    "tags": [
      "linear-mixed-models",
      "GLMM",
      "random-effects",
      "hierarchical-models",
      "repeated-measures"
    ],
    "best_for": "Standard linear and generalized linear mixed-effects modeling with crossed/nested random effects, implementing Bates et al. (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The lme4 package is designed to fit linear and generalized linear mixed-effects models using S4 classes. It is utilized by statisticians and data scientists for analyzing hierarchical and longitudinal data with complex random effects structures.",
    "use_cases": [
      "Analyzing longitudinal data",
      "Modeling hierarchical data structures"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for linear mixed models",
      "how to fit generalized linear mixed models in R",
      "lme4 documentation",
      "lme4 examples",
      "R mixed effects modeling",
      "lme4 random effects structures"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "nlme"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "lmerTest",
    "description": "Provides p-values for lme4 model fits via Satterthwaite's or Kenward-Roger degrees of freedom methods, with Type I/II/III ANOVA tables, model selection tools (step, drop1), and least-squares means calculations.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf",
    "github_url": "https://github.com/runehaubo/lmerTestR",
    "url": "https://cran.r-project.org/package=lmerTest",
    "install": "install.packages(\"lmerTest\")",
    "tags": [
      "p-values",
      "Satterthwaite",
      "Kenward-Roger",
      "ANOVA",
      "hypothesis-testing"
    ],
    "best_for": "Getting p-values and formal hypothesis tests for lme4 linear mixed models, implementing Kuznetsova et al. (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "hypothesis-testing",
      "mixed-effects-models"
    ],
    "summary": "lmerTest provides p-values for lme4 model fits using Satterthwaite's or Kenward-Roger degrees of freedom methods. It is used by statisticians and data scientists for model selection and ANOVA analysis in mixed effects models.",
    "use_cases": [
      "Conducting ANOVA for mixed effects models",
      "Performing model selection for linear mixed models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed effects models",
      "how to get p-values for lme4 models in R",
      "ANOVA tables in R",
      "model selection tools in R",
      "Satterthwaite method in R",
      "Kenward-Roger method in R"
    ],
    "primary_use_cases": [
      "ANOVA analysis for mixed effects models",
      "p-value calculation for lme4 model fits"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "nlme",
    "description": "Fit Gaussian linear and nonlinear mixed-effects models with flexible correlation structures, variance functions for heteroscedasticity, and nested random effects. Ships with base R and offers more variance-covariance flexibility than lme4.",
    "category": "Mixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/nlme/nlme.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=nlme",
    "install": "install.packages(\"nlme\")",
    "tags": [
      "nonlinear-mixed-models",
      "autocorrelation",
      "heteroscedasticity",
      "repeated-measures",
      "longitudinal"
    ],
    "best_for": "Models requiring custom correlation structures, variance functions, or nonlinear mixed effects, implementing Pinheiro & Bates (2000)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "mixed-effects-models",
      "longitudinal-data"
    ],
    "summary": "The nlme package is designed to fit Gaussian linear and nonlinear mixed-effects models, accommodating flexible correlation structures and variance functions for heteroscedasticity. It is widely used by statisticians and data scientists working with complex hierarchical data.",
    "use_cases": [
      "Modeling repeated measures data",
      "Analyzing longitudinal studies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for mixed-effects models",
      "how to fit nonlinear mixed models in R",
      "R nlme package documentation",
      "Gaussian mixed-effects models in R",
      "nlme vs lme4 in R",
      "how to handle heteroscedasticity in R"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "lme4"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "car",
    "description": "Functions accompanying 'An R Companion to Applied Regression.' Provides advanced regression diagnostics including variance inflation factors (VIF), Type II/III ANOVA, influence measures, linear hypothesis testing, power transformations (Box-Cox), and comprehensive diagnostic plots.",
    "category": "Model Diagnostics",
    "docs_url": "https://www.john-fox.ca/Companion/index.html",
    "github_url": null,
    "url": "https://cran.r-project.org/package=car",
    "install": "install.packages(\"car\")",
    "tags": [
      "regression-diagnostics",
      "VIF",
      "ANOVA",
      "hypothesis-testing",
      "influence-diagnostics"
    ],
    "best_for": "Classical regression diagnostics: VIF for multicollinearity, Type II/III ANOVA, linear hypothesis tests, from Fox & Weisberg (2019)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'car' package provides functions for advanced regression diagnostics, including variance inflation factors, ANOVA, and influence measures. It is primarily used by statisticians and data scientists for performing thorough regression analysis.",
    "use_cases": [
      "Assessing multicollinearity in regression models",
      "Evaluating the influence of data points on regression results"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for regression diagnostics",
      "how to calculate VIF in R",
      "ANOVA analysis in R",
      "influence measures in regression R",
      "Box-Cox transformation R",
      "diagnostic plots in R"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "performance",
    "description": "Utilities for computing indices of model quality and goodness of fit, including R\u00b2, RMSE, ICC, AIC/BIC. Provides functions to check models for overdispersion, zero-inflation, multicollinearity (VIF), convergence, and singularity. Supports mixed effects and Bayesian models.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/performance/",
    "github_url": "https://github.com/easystats/performance",
    "url": "https://cran.r-project.org/package=performance",
    "install": "install.packages(\"performance\")",
    "tags": [
      "model-diagnostics",
      "R-squared",
      "assumption-checking",
      "VIF",
      "goodness-of-fit"
    ],
    "best_for": "Comprehensive model quality assessment, especially the check_model() visual diagnostic panel, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [],
    "topic_tags": [
      "model-diagnostics",
      "goodness-of-fit"
    ],
    "summary": "The 'performance' package provides utilities for assessing model quality and goodness of fit through various indices such as R\u00b2 and RMSE. It is useful for statisticians and data scientists working with mixed effects and Bayesian models.",
    "use_cases": [
      "Evaluating the fit of a mixed effects model",
      "Checking for zero-inflation in count data models"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "R package for model diagnostics",
      "how to compute R-squared in R",
      "functions for checking overdispersion in R",
      "R utilities for goodness of fit",
      "how to assess multicollinearity in R",
      "R package for Bayesian model evaluation"
    ],
    "api_complexity": "intermediate",
    "maintenance_status": "active"
  },
  {
    "name": "see",
    "description": "Visualization toolbox for the easystats ecosystem built on ggplot2. Provides publication-ready plotting methods for model parameters, predictions, and performance diagnostics from all easystats packages via simple plot() calls.",
    "category": "Model Diagnostics",
    "docs_url": "https://easystats.github.io/see/",
    "github_url": "https://github.com/easystats/see",
    "url": "https://cran.r-project.org/package=see",
    "install": "install.packages(\"see\")",
    "tags": [
      "visualization",
      "ggplot2",
      "diagnostic-plots",
      "publication-ready",
      "easystats"
    ],
    "best_for": "Publication-ready visualizations of model diagnostics with a simple plot() interface, implementing L\u00fcdecke et al. (2021, JOSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [],
    "topic_tags": [],
    "summary": "The 'see' package is a visualization toolbox designed for the easystats ecosystem, leveraging ggplot2 to create publication-ready plots. It is useful for users looking to visualize model parameters, predictions, and performance diagnostics easily.",
    "use_cases": [
      "Creating publication-ready plots for model outputs",
      "Visualizing predictions from statistical models"
    ],
    "audience": [
      "Curious-browser"
    ],
    "synthetic_questions": [
      "visualization toolbox for easystats",
      "ggplot2 diagnostic plots",
      "how to visualize model parameters in R",
      "publication-ready plots in R",
      "easystats visualization methods",
      "R package for performance diagnostics"
    ],
    "api_complexity": "simple",
    "maintenance_status": "active"
  },
  {
    "name": "CausalNLP",
    "description": "Causal inference for text data. Estimate treatment effects from unstructured text using NLP.",
    "category": "Natural Language Processing for Economics",
    "docs_url": null,
    "github_url": "https://github.com/amaiya/causalnlp",
    "url": "https://github.com/amaiya/causalnlp",
    "install": "pip install causalnlp",
    "tags": [
      "NLP",
      "causal inference",
      "text"
    ],
    "best_for": "Causal effects from text data",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [
      "causal-inference",
      "natural-language-processing"
    ],
    "summary": "CausalNLP is a package designed for causal inference in text data, enabling users to estimate treatment effects from unstructured text using natural language processing techniques. It is particularly useful for researchers and practitioners in economics who work with text data.",
    "use_cases": [
      "Analyzing the impact of a marketing campaign based on customer reviews",
      "Estimating the effect of policy changes on public sentiment from social media posts"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for causal inference in text",
      "how to estimate treatment effects from text in python",
      "NLP for causal analysis in python",
      "causal inference tools for text data",
      "using CausalNLP for text analysis",
      "text data treatment effects estimation python"
    ],
    "primary_use_cases": [
      "causal forest estimation",
      "A/B test analysis"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "causalml",
      "DoWhy"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Gensim",
    "description": "Library focused on topic modeling (LDA, LSI) and document similarity analysis.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://radimrehurek.com/gensim/",
    "github_url": "https://github.com/RaRe-Technologies/gensim",
    "url": "https://github.com/RaRe-Technologies/gensim",
    "install": "pip install gensim",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn"
    ],
    "topic_tags": [],
    "summary": "Gensim is a library focused on topic modeling and document similarity analysis. It is widely used by researchers and practitioners in the field of natural language processing to analyze large text corpora.",
    "use_cases": [
      "Analyzing customer feedback for insights",
      "Building recommendation systems based on text data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for topic modeling",
      "how to do document similarity analysis in python",
      "Gensim tutorial",
      "Gensim LDA example",
      "text analysis with Gensim",
      "using Gensim for NLP"
    ],
    "api_complexity": "intermediate",
    "related_packages": [
      "spaCy",
      "NLTK"
    ],
    "maintenance_status": "active"
  },
  {
    "name": "Transformers",
    "description": "Access to thousands of pre-trained models for NLP tasks like text classification, summarization, embeddings, etc.",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://huggingface.co/transformers/",
    "github_url": "https://github.com/huggingface/transformers",
    "url": "https://github.com/huggingface/transformers",
    "install": "pip install transformers",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "pytorch-tensors",
      "neural-network-fundamentals"
    ],
    "topic_tags": [
      "transformers",
      "pre-trained-models",
      "text-classification",
      "language-models",
      "huggingface"
    ],
    "summary": "Transformers is a Python library providing access to thousands of pre-trained language models for various NLP tasks. It's widely used by data scientists and researchers to quickly implement state-of-the-art text analysis without training models from scratch. The library supports tasks like sentiment analysis, text summarization, and embedding generation with minimal code.",
    "use_cases": [
      "Analyzing customer feedback sentiment for product development decisions",
      "Extracting key insights from earnings call transcripts for investment research"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to classify text sentiment using pre-trained models",
      "Best Python library for NLP text analysis",
      "Hugging Face transformers tutorial for beginners",
      "Pre-trained models for economic text classification"
    ]
  },
  {
    "name": "spaCy",
    "description": "Industrial-strength NLP library for efficient text processing pipelines (NER, POS tagging, etc.).",
    "category": "Natural Language Processing for Economics",
    "docs_url": "https://spacy.io/",
    "github_url": "https://github.com/explosion/spaCy",
    "url": "https://github.com/explosion/spaCy",
    "install": "pip install spacy",
    "tags": [
      "NLP",
      "text analysis"
    ],
    "best_for": "Text analysis, sentiment analysis, document classification",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "regular-expressions",
      "basic-linguistics"
    ],
    "topic_tags": [
      "natural-language-processing",
      "text-mining",
      "named-entity-recognition",
      "python-library",
      "sentiment-analysis"
    ],
    "summary": "spaCy is a production-ready Python library for advanced natural language processing tasks including named entity recognition, part-of-speech tagging, and dependency parsing. It's designed for speed and efficiency in processing large volumes of text data. Tech economists use it to extract structured information from unstructured text sources like earnings calls, patent filings, and regulatory documents.",
    "use_cases": [
      "Extracting company names and financial metrics from SEC filings to build datasets for corporate finance research",
      "Processing job postings to identify skill requirements and wage premiums for labor economics analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for named entity recognition in economics research",
      "how to extract companies and locations from earnings transcripts",
      "fast NLP preprocessing for economic text analysis",
      "spacy vs nltk for financial document processing"
    ]
  },
  {
    "name": "ggraph",
    "description": "Grammar of graphics for network data built on ggplot2. Provides layouts, geometries, and faceting specifically designed for network visualization with publication-quality output.",
    "category": "Network Analysis",
    "docs_url": "https://ggraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/ggraph",
    "url": "https://cran.r-project.org/package=ggraph",
    "install": "install.packages(\"ggraph\")",
    "tags": [
      "networks",
      "visualization",
      "ggplot2",
      "graph-layouts",
      "publication-ready"
    ],
    "best_for": "Publication-quality network visualization using ggplot2 grammar",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-ggplot2",
      "network-data-structures",
      "graph-theory-basics"
    ],
    "topic_tags": [
      "network-visualization",
      "ggplot2-extension",
      "graph-layouts",
      "social-networks",
      "R-package"
    ],
    "summary": "ggraph extends ggplot2's grammar of graphics to network data, enabling sophisticated network visualizations with consistent syntax. It provides specialized layouts, node and edge geometries, and faceting capabilities for creating publication-ready network plots. The package is particularly valuable for researchers and analysts who need to visualize complex relationships in social, biological, or technical networks.",
    "use_cases": [
      "Visualizing social media influence networks to identify key opinion leaders and information flow patterns",
      "Creating publication-ready plots of protein interaction networks showing different types of molecular relationships"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to visualize network data in R with ggplot2",
      "ggraph network visualization layouts",
      "Publication quality network plots R",
      "ggplot2 extension for graph data visualization"
    ]
  },
  {
    "name": "igraph",
    "description": "Comprehensive network analysis library with efficient algorithms for network creation, manipulation, and analysis. Provides centrality measures, community detection, graph visualization, and network statistics.",
    "category": "Network Analysis",
    "docs_url": "https://igraph.org/r/",
    "github_url": "https://github.com/igraph/rigraph",
    "url": "https://cran.r-project.org/package=igraph",
    "install": "install.packages(\"igraph\")",
    "tags": [
      "networks",
      "graph-algorithms",
      "centrality",
      "community-detection",
      "network-statistics"
    ],
    "best_for": "Comprehensive network analysis with efficient algorithms for centrality, community detection, and visualization",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "graph-theory-fundamentals",
      "numpy-arrays"
    ],
    "topic_tags": [
      "network-analysis",
      "graph-algorithms",
      "social-networks",
      "community-detection",
      "centrality-measures"
    ],
    "summary": "igraph is a powerful network analysis library that provides comprehensive tools for creating, analyzing, and visualizing complex networks. It's widely used by data scientists and researchers for studying social networks, biological systems, and infrastructure networks. The library offers efficient implementations of graph algorithms, centrality measures, and community detection methods.",
    "use_cases": [
      "Analyzing social media networks to identify influential users and community structures",
      "Studying protein interaction networks in computational biology to understand cellular processes"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python library for network analysis and graph algorithms",
      "how to detect communities in social networks",
      "centrality measures for network analysis in python",
      "igraph vs networkx for graph analysis"
    ]
  },
  {
    "name": "sna",
    "description": "Social network analysis tools including network visualization, centrality measures, and statistical models for network data. Part of the statnet suite for network regression and exponential random graph models.",
    "category": "Network Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/sna/sna.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sna",
    "install": "install.packages(\"sna\")",
    "tags": [
      "social-networks",
      "network-regression",
      "statnet",
      "ERGM",
      "centrality"
    ],
    "best_for": "Social network analysis and network regression as part of the statnet suite",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "graph-theory",
      "statistical-modeling"
    ],
    "topic_tags": [
      "social-networks",
      "network-analysis",
      "ERGM",
      "centrality-measures",
      "R-package"
    ],
    "summary": "The sna package provides comprehensive tools for analyzing social networks in R, including network visualization, centrality calculations, and statistical modeling. It's part of the statnet suite and enables researchers to fit exponential random graph models (ERGMs) to understand network formation processes. The package is widely used in sociology, economics, and organizational research for studying relationships and influence patterns.",
    "use_cases": [
      "Analyzing employee collaboration networks to identify key influencers and communication bottlenecks in organizations",
      "Modeling friendship networks in schools to understand how social ties form and influence academic outcomes"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for social network analysis",
      "how to calculate centrality measures in networks",
      "ERGM exponential random graph models R",
      "statnet suite network regression tools"
    ]
  },
  {
    "name": "tidygraph",
    "description": "Tidy data interface for network/graph data. Extends dplyr verbs to work with nodes and edges, enabling pipe-friendly network manipulation that integrates seamlessly with ggraph for visualization.",
    "category": "Network Analysis",
    "docs_url": "https://tidygraph.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/tidygraph",
    "url": "https://cran.r-project.org/package=tidygraph",
    "install": "install.packages(\"tidygraph\")",
    "tags": [
      "networks",
      "tidyverse",
      "graph-manipulation",
      "dplyr",
      "pipes"
    ],
    "best_for": "Tidy manipulation of network data with dplyr-style verbs for nodes and edges",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "r-dplyr",
      "ggplot2",
      "basic-graph-theory"
    ],
    "topic_tags": [
      "network-analysis",
      "tidyverse",
      "graph-manipulation",
      "data-wrangling",
      "r-package"
    ],
    "summary": "tidygraph brings the familiar dplyr grammar to network analysis, making it easy to manipulate nodes and edges using pipes and standard tidyverse verbs. It's designed for R users who want to analyze networks without learning specialized graph packages from scratch. The package integrates seamlessly with ggraph for creating publication-ready network visualizations.",
    "use_cases": [
      "Analyzing social network data to identify influential users and community structures",
      "Studying organizational hierarchies and communication patterns in corporate datasets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "tidyverse approach to network analysis in R",
      "dplyr verbs for graph data manipulation",
      "pipe-friendly network analysis R package",
      "integrate network analysis with ggplot workflow"
    ]
  },
  {
    "name": "Faer",
    "description": "State-of-the-art linear algebra for Rust with Cholesky, QR, SVD decompositions and multithreaded solvers for large systems.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/faer",
    "github_url": "https://github.com/sarah-quinones/faer-rs",
    "url": "https://crates.io/crates/faer",
    "install": "cargo add faer",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "performance"
    ],
    "best_for": "High-performance matrix decompositions for custom estimators",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "linear-algebra-fundamentals",
      "matrix-decomposition"
    ],
    "topic_tags": [
      "rust-linear-algebra",
      "matrix-decomposition",
      "high-performance-computing",
      "numerical-methods",
      "optimization-solvers"
    ],
    "summary": "Faer is a high-performance linear algebra library for Rust that provides state-of-the-art implementations of matrix decompositions like Cholesky, QR, and SVD. It features multithreaded solvers optimized for large-scale systems, making it ideal for computationally intensive applications requiring maximum performance. The library is designed for Rust developers who need fast, memory-safe linear algebra operations without sacrificing speed.",
    "use_cases": [
      "Building high-frequency trading systems that require ultra-fast matrix operations for portfolio optimization",
      "Developing machine learning inference engines in Rust that need efficient linear algebra for real-time predictions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust linear algebra library high performance",
      "fast matrix decomposition rust cholesky QR SVD",
      "multithreaded linear algebra solver rust",
      "rust alternative to numpy scipy linear algebra"
    ]
  },
  {
    "name": "JAX",
    "description": "High-performance numerical computing with autograd and XLA compilation on CPU/GPU/TPU.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://jax.readthedocs.io/",
    "github_url": "https://github.com/google/jax",
    "url": "https://github.com/google/jax",
    "install": "pip install jax",
    "tags": [
      "optimization",
      "computation"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "automatic-differentiation",
      "linear-algebra"
    ],
    "topic_tags": [
      "automatic-differentiation",
      "neural-networks",
      "gpu-computing",
      "scientific-computing",
      "jit-compilation"
    ],
    "summary": "JAX is a NumPy-compatible library for high-performance machine learning research that combines automatic differentiation with XLA compilation for CPUs, GPUs, and TPUs. It enables functional programming patterns for neural networks and scientific computing with just-in-time compilation for speed. Popular in research settings for implementing custom models and optimization algorithms that need to scale across accelerators.",
    "use_cases": [
      "Training custom neural network architectures with automatic gradient computation",
      "Implementing scientific computing simulations that need GPU acceleration"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast NumPy alternative for GPU computing",
      "automatic differentiation library for machine learning",
      "JAX vs PyTorch for research",
      "JIT compilation for scientific computing Python"
    ]
  },
  {
    "name": "Nalgebra",
    "description": "General-purpose linear algebra library for Rust with dense and sparse matrices, widely used in graphics and physics.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://www.nalgebra.org/",
    "github_url": "https://github.com/dimforge/nalgebra",
    "url": "https://crates.io/crates/nalgebra",
    "install": "cargo add nalgebra",
    "tags": [
      "rust",
      "linear algebra",
      "matrix",
      "sparse"
    ],
    "best_for": "General-purpose linear algebra in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "matrix-operations",
      "linear-algebra-fundamentals"
    ],
    "topic_tags": [
      "linear-algebra",
      "rust-libraries",
      "matrix-computation",
      "numerical-methods",
      "sparse-matrices"
    ],
    "summary": "Nalgebra is a comprehensive linear algebra library for Rust that provides efficient implementations of dense and sparse matrix operations. It's particularly popular in graphics programming, physics simulations, and scientific computing applications where performance and memory safety are critical. The library offers a clean API for vector spaces, transformations, and decompositions commonly needed in computational work.",
    "use_cases": [
      "Building 3D graphics engines requiring fast matrix transformations and geometric operations",
      "Implementing physics simulations with large sparse systems for finite element analysis"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust linear algebra library for matrix operations",
      "nalgebra vs numpy performance comparison",
      "sparse matrix computation in rust",
      "best rust library for scientific computing linear algebra"
    ]
  },
  {
    "name": "Ndarray",
    "description": "N-dimensional array library for Rust\u2014the NumPy equivalent with slicing, broadcasting, and BLAS/LAPACK integration.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://docs.rs/ndarray",
    "github_url": "https://github.com/rust-ndarray/ndarray",
    "url": "https://crates.io/crates/ndarray",
    "install": "cargo add ndarray",
    "tags": [
      "rust",
      "arrays",
      "numpy",
      "scientific computing"
    ],
    "best_for": "NumPy-style N-dimensional arrays in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "numpy-arrays",
      "linear-algebra-basics"
    ],
    "topic_tags": [
      "rust",
      "numerical-computing",
      "arrays",
      "scientific-computing",
      "performance"
    ],
    "summary": "Ndarray is Rust's equivalent to NumPy, providing efficient n-dimensional arrays with slicing, broadcasting, and mathematical operations. It enables high-performance scientific computing in Rust with BLAS/LAPACK integration for linear algebra operations. This library is essential for data scientists and researchers who need NumPy-like functionality but want Rust's memory safety and performance guarantees.",
    "use_cases": [
      "Building high-performance machine learning algorithms in Rust that require matrix operations and array manipulations",
      "Developing numerical simulation systems where memory safety and speed are critical requirements"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "numpy equivalent for rust",
      "rust scientific computing arrays",
      "high performance matrix operations rust",
      "ndarray vs numpy rust"
    ]
  },
  {
    "name": "PyTorch",
    "description": "Popular deep learning framework with flexible automatic differentiation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": "https://pytorch.org/",
    "github_url": "https://github.com/pytorch/pytorch",
    "url": "https://github.com/pytorch/pytorch",
    "install": "(See PyTorch website)",
    "tags": [
      "optimization",
      "computation",
      "machine learning"
    ],
    "best_for": "Solving optimization problems, numerical methods",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "numpy-arrays"
    ],
    "topic_tags": [
      "deep-learning",
      "neural-networks",
      "automatic-differentiation",
      "gpu-computing",
      "python-framework"
    ],
    "summary": "PyTorch is a flexible deep learning framework that provides automatic differentiation and dynamic computation graphs. It's widely used by researchers and practitioners for building and training neural networks, from simple models to complex architectures. The framework offers intuitive Python APIs and strong GPU acceleration for efficient model development.",
    "use_cases": [
      "Building and training recommendation systems for user behavior prediction",
      "Developing computer vision models for image classification or object detection"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "What deep learning framework should I use for my first neural network project",
      "PyTorch vs TensorFlow for recommendation systems",
      "How to implement gradient descent with automatic differentiation",
      "Best Python library for training deep learning models"
    ]
  },
  {
    "name": "jaxonometrics",
    "description": "JAX-ecosystem implementations of standard econometrics routines for GPU computation.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/py-econometrics/jaxonometrics",
    "url": "https://github.com/py-econometrics/jaxonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "JAX",
      "GPU"
    ],
    "best_for": "GPU-accelerated econometrics with JAX",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "econometrics-fundamentals",
      "python-numpy",
      "linear-regression"
    ],
    "topic_tags": [
      "econometrics",
      "gpu-computing",
      "jax",
      "numerical-methods",
      "computational-economics"
    ],
    "summary": "JAX-based econometrics package that accelerates standard econometric computations using GPU parallelization. Provides drop-in replacements for common econometric estimators with automatic differentiation and vectorization capabilities. Particularly useful for researchers running large-scale economic analyses or Monte Carlo simulations.",
    "use_cases": [
      "Running IV regression on millions of observations with GPU acceleration",
      "Performing bootstrap inference for difference-in-differences with thousands of replications"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GPU econometrics package Python",
      "JAX implementation econometric methods",
      "fast econometrics computation GPU",
      "accelerated regression analysis JAX"
    ]
  },
  {
    "name": "torchonometrics",
    "description": "Econometrics implementations in PyTorch. Leverages autodiff and GPU acceleration for econometric methods.",
    "category": "Numerical Optimization & Computational Tools",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/torchonometrics",
    "url": "https://github.com/apoorvalal/torchonometrics",
    "install": "GitHub Repository",
    "tags": [
      "optimization",
      "computation",
      "PyTorch"
    ],
    "best_for": "GPU-accelerated econometrics with PyTorch autodiff",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "pytorch-tensors",
      "linear-regression",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "econometrics",
      "pytorch",
      "gpu-acceleration",
      "autodiff",
      "causal-inference"
    ],
    "summary": "Torchonometrics brings classical econometric methods into the PyTorch ecosystem, enabling GPU acceleration and automatic differentiation for econometric estimation. It's particularly valuable for researchers and practitioners who need to scale econometric models to large datasets or integrate them with deep learning workflows. The package maintains econometric rigor while leveraging modern computational tools.",
    "use_cases": [
      "Estimating instrumental variables models on million-row datasets with GPU acceleration",
      "Building hybrid models that combine econometric identification strategies with neural network components"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "econometrics in pytorch",
      "gpu accelerated causal inference",
      "pytorch econometric models",
      "autodiff for econometrics"
    ]
  },
  {
    "name": "Argmin",
    "description": "Numerical optimization framework for Rust with Newton, BFGS, L-BFGS, trust region, and derivative-free methods for MLE/GMM.",
    "category": "Optimization",
    "docs_url": "https://docs.rs/argmin",
    "github_url": "https://github.com/argmin-rs/argmin",
    "url": "https://crates.io/crates/argmin",
    "install": "cargo add argmin",
    "tags": [
      "rust",
      "optimization",
      "BFGS",
      "MLE",
      "GMM"
    ],
    "best_for": "Maximum Likelihood and GMM estimation in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "numerical-optimization",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "numerical-optimization",
      "rust-library",
      "BFGS",
      "trust-region",
      "maximum-likelihood"
    ],
    "summary": "Argmin is a comprehensive numerical optimization framework for Rust that implements classical algorithms like BFGS, L-BFGS, Newton methods, and trust region approaches. It's designed for researchers and engineers who need efficient optimization routines for maximum likelihood estimation and generalized method of moments in Rust applications. The library provides both gradient-based and derivative-free optimization methods with a unified interface.",
    "use_cases": [
      "Estimating parameters in econometric models using maximum likelihood estimation in Rust applications",
      "Optimizing objective functions in high-performance computational economics research where Rust's speed is critical"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Rust optimization library for MLE",
      "BFGS implementation in Rust",
      "numerical optimization framework Rust",
      "Rust library for maximum likelihood estimation"
    ]
  },
  {
    "name": "cvxpy",
    "description": "Domain-specific language for convex optimization problems. Write math as code \u2014 the standard for convex problems.",
    "category": "Optimization",
    "docs_url": "https://www.cvxpy.org/",
    "github_url": "https://github.com/cvxpy/cvxpy",
    "url": "https://www.cvxpy.org/",
    "install": "pip install cvxpy",
    "tags": [
      "convex optimization",
      "linear programming",
      "quadratic programming"
    ],
    "best_for": "Convex optimization with intuitive syntax",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "mathematical-optimization"
    ],
    "topic_tags": [
      "convex-optimization",
      "linear-programming",
      "quadratic-programming",
      "mathematical-modeling",
      "python-package"
    ],
    "summary": "CVXPY is a Python library that lets you write convex optimization problems using intuitive mathematical syntax that closely mirrors how you'd write them on paper. It's the go-to tool for data scientists and researchers who need to solve portfolio optimization, resource allocation, or machine learning problems with constraints. The library automatically transforms your problem formulation into the appropriate solver format and handles the computational details.",
    "use_cases": [
      "Portfolio optimization with risk constraints and transaction costs",
      "Machine learning model training with regularization and fairness constraints"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python convex optimization library",
      "how to solve linear programming in python",
      "cvxpy vs scipy optimize",
      "portfolio optimization python package"
    ]
  },
  {
    "name": "gurobipy",
    "description": "Python interface for Gurobi, the best-in-class commercial solver. LP, QP, MIP, and MIQP.",
    "category": "Optimization",
    "docs_url": "https://www.gurobi.com/documentation/",
    "github_url": null,
    "url": "https://www.gurobi.com/",
    "install": "pip install gurobipy",
    "tags": [
      "optimization",
      "solver",
      "MIP",
      "commercial"
    ],
    "best_for": "Best-in-class solver \u2014 free for academics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-programming",
      "python-optimization",
      "mixed-integer-programming"
    ],
    "topic_tags": [
      "commercial-solver",
      "linear-programming",
      "mixed-integer-programming",
      "operations-research",
      "python-optimization"
    ],
    "summary": "Gurobipy is the Python interface for Gurobi Optimizer, a premium commercial solver for mathematical optimization problems. It's widely regarded as the fastest and most reliable solver for linear programming (LP), quadratic programming (QP), and mixed-integer programming (MIP) problems. Tech economists and data scientists use it for complex optimization tasks where performance and solution quality are critical.",
    "use_cases": [
      "Solving large-scale supply chain optimization problems with thousands of constraints and variables",
      "Implementing auction mechanisms and market design algorithms that require exact optimal solutions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "best commercial optimization solver for Python",
      "how to solve mixed integer programming problems in Python",
      "Gurobi vs open source solvers for large optimization problems",
      "Python package for linear programming with commercial license"
    ]
  },
  {
    "name": "ortools",
    "description": "Google's operations research toolkit. Constraint programming, routing, linear/integer programming, and scheduling.",
    "category": "Optimization",
    "docs_url": "https://developers.google.com/optimization",
    "github_url": "https://github.com/google/or-tools",
    "url": "https://developers.google.com/optimization",
    "install": "pip install ortools",
    "tags": [
      "OR",
      "routing",
      "scheduling",
      "constraint programming"
    ],
    "best_for": "Production-ready combinatorial optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "mathematical-optimization"
    ],
    "topic_tags": [
      "operations-research",
      "constraint-programming",
      "vehicle-routing",
      "linear-programming",
      "scheduling-optimization"
    ],
    "summary": "Google's OR-Tools is a comprehensive optimization library that provides solvers for constraint programming, routing problems, and linear/integer programming. It's widely used by data scientists and operations researchers to solve complex scheduling, routing, and resource allocation problems. The toolkit offers both high-level APIs and low-level solver access for various optimization challenges.",
    "use_cases": [
      "Optimizing delivery routes for logistics companies with constraints like vehicle capacity and time windows",
      "Scheduling employees or resources across shifts while satisfying coverage requirements and work regulations"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Google OR-Tools constraint programming tutorial",
      "vehicle routing problem solver Python",
      "employee scheduling optimization library",
      "linear programming solver comparison OR-Tools"
    ]
  },
  {
    "name": "scipy.optimize",
    "description": "Optimization algorithms built into SciPy. Minimization, root finding, curve fitting, and linear programming.",
    "category": "Optimization",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://docs.scipy.org/doc/scipy/reference/optimize.html",
    "install": "pip install scipy",
    "tags": [
      "optimization",
      "minimization",
      "root finding"
    ],
    "best_for": "General-purpose optimization \u2014 start here for basics",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-numpy",
      "calculus-derivatives",
      "linear-algebra"
    ],
    "topic_tags": [
      "optimization",
      "scipy",
      "numerical-methods",
      "curve-fitting",
      "linear-programming"
    ],
    "summary": "SciPy's optimization module provides a comprehensive suite of algorithms for finding optimal solutions to mathematical problems. It's widely used by data scientists and researchers for parameter estimation, model fitting, and solving constrained optimization problems. The package offers user-friendly interfaces to powerful numerical methods without requiring deep mathematical expertise.",
    "use_cases": [
      "Fitting machine learning model hyperparameters using grid search or gradient-based methods",
      "Estimating parameters for econometric models by minimizing loss functions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python optimization library for parameter fitting",
      "scipy minimize function tutorial",
      "how to do curve fitting in python",
      "linear programming solver python"
    ]
  },
  {
    "name": "FixedEffectModel",
    "description": "Panel data modeling with IV tests (weak IV, over-identification, endogeneity) and 2-step GMM estimation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": null,
    "github_url": "https://github.com/ksecology/FixedEffectModel",
    "url": "https://github.com/ksecology/FixedEffectModel",
    "install": "pip install FixedEffectModel",
    "tags": [
      "panel data",
      "fixed effects",
      "IV"
    ],
    "best_for": "Panel regression with comprehensive IV diagnostics",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "instrumental-variables"
    ],
    "topic_tags": [
      "panel-data",
      "fixed-effects",
      "instrumental-variables",
      "gmm-estimation",
      "causal-inference"
    ],
    "summary": "A Python package for panel data analysis with fixed effects modeling and comprehensive IV diagnostics. Provides econometric tools for handling endogeneity through instrumental variables and two-step GMM estimation. Essential for economists and data scientists working with longitudinal data where unobserved heterogeneity is a concern.",
    "use_cases": [
      "Analyzing the effect of policy changes on firm outcomes using company panel data with time-invariant unobserved characteristics",
      "Estimating causal effects of education on wages using individual longitudinal data with endogenous schooling decisions"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "panel data fixed effects python package",
      "IV tests weak instruments overidentification",
      "GMM estimation panel data econometrics",
      "fixed effects model endogeneity testing"
    ]
  },
  {
    "name": "FixedEffectModelPyHDFE",
    "description": "Solves linear models with high-dimensional fixed effects, supporting robust variance calculation and IV.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "github_url": null,
    "url": "https://pypi.org/project/FixedEffectModelPyHDFE/",
    "install": "pip install FixedEffectModelPyHDFE",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "econometric-theory"
    ],
    "topic_tags": [
      "fixed-effects",
      "panel-data",
      "high-dimensional",
      "instrumental-variables",
      "econometrics"
    ],
    "summary": "FixedEffectModelPyHDFE is a Python implementation for estimating linear models with high-dimensional fixed effects, commonly used in econometric analysis. It handles large categorical variables efficiently and supports robust standard errors and instrumental variable estimation. This package is essential for economists and data scientists working with panel data where controlling for unobserved heterogeneity is critical.",
    "use_cases": [
      "Estimating causal effects in worker-firm matched data with both worker and firm fixed effects",
      "Analyzing product pricing across markets while controlling for time-varying product and market characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for high dimensional fixed effects regression",
      "How to estimate two-way fixed effects model in Python",
      "FixedEffectModelPyHDFE vs Stata reghdfe equivalent",
      "Panel data regression with firm and time fixed effects Python"
    ]
  },
  {
    "name": "Linearmodels",
    "description": "Estimation of fixed, random, pooled OLS models for panel data. Also Fama-MacBeth and between/first-difference estimators.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://bashtage.github.io/linearmodels/",
    "github_url": "https://github.com/bashtage/linearmodels",
    "url": "https://github.com/bashtage/linearmodels",
    "install": "pip install linearmodels",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "econometrics-basics"
    ],
    "topic_tags": [
      "panel-data",
      "fixed-effects",
      "fama-macbeth",
      "econometrics",
      "causal-inference"
    ],
    "summary": "Linearmodels is a Python package for econometric analysis of panel data, offering comprehensive tools for fixed effects, random effects, and pooled OLS estimation. It includes specialized estimators like Fama-MacBeth for finance applications and between/first-difference methods for causal inference. The package is designed for economists and data scientists working with longitudinal data who need robust statistical methods beyond basic regression.",
    "use_cases": [
      "Analyzing employee productivity across companies over time while controlling for unobserved firm characteristics",
      "Estimating price elasticity effects in A/B tests across different user segments and time periods"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for panel data fixed effects",
      "how to run fama macbeth regression python",
      "panel data econometrics library python",
      "fixed effects estimation longitudinal data"
    ]
  },
  {
    "name": "PyFixest",
    "description": "Fast estimation of linear models with multiple high-dimensional fixed effects (like R's `fixest`). Supports OLS, IV, Poisson, robust/cluster SEs.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/pyfixest",
    "github_url": null,
    "url": "https://github.com/py-econometrics/pyfixest",
    "install": "pip install pyfixest",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "econometric-modeling"
    ],
    "topic_tags": [
      "fixed-effects",
      "panel-data",
      "econometrics",
      "causal-inference",
      "python-package"
    ],
    "summary": "PyFixest is a Python package for fast estimation of linear models with high-dimensional fixed effects, porting R's popular fixest functionality. It enables economists and data scientists to efficiently run OLS, IV, and Poisson regressions with multiple fixed effects and robust standard errors. The package is particularly valuable for causal inference work requiring control for unobserved heterogeneity across entities or time periods.",
    "use_cases": [
      "Estimating treatment effects in difference-in-differences studies with firm and time fixed effects",
      "Running wage regressions controlling for worker and employer fixed effects in matched employer-employee data"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for fixed effects regression",
      "PyFixest vs R fixest equivalent",
      "how to estimate panel data models in python",
      "high dimensional fixed effects python"
    ]
  },
  {
    "name": "alpaca",
    "description": "Fits generalized linear models (Poisson, negative binomial, logit, probit, Gamma) with high-dimensional k-way fixed effects. Partials out factors during log-likelihood optimization and provides robust/multi-way clustered standard errors, fixed effects recovery, and analytical bias corrections for binary choice models.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/alpaca/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/alpaca",
    "url": "https://cran.r-project.org/package=alpaca",
    "install": "install.packages(\"alpaca\")",
    "tags": [
      "glm",
      "fixed-effects",
      "poisson-regression",
      "negative-binomial",
      "gravity-models"
    ],
    "best_for": "Nonlinear panel models (Poisson, logit, probit, negative binomial) with multiple high-dimensional fixed effects, especially structural gravity models for international trade",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "generalized-linear-models",
      "panel-data-methods",
      "econometric-clustering"
    ],
    "topic_tags": [
      "fixed-effects",
      "high-dimensional-econometrics",
      "gravity-models",
      "bias-correction",
      "clustered-standard-errors"
    ],
    "summary": "Alpaca is a specialized package for fitting generalized linear models with high-dimensional fixed effects, particularly useful for gravity models and panel data with many categorical variables. It efficiently handles computational challenges by partialing out fixed effects during optimization and provides robust inference tools. The package is designed for econometric applications requiring both flexible modeling and reliable standard error estimation.",
    "use_cases": [
      "Estimating gravity models for international trade with country-pair and time fixed effects",
      "Analyzing firm-level productivity with multi-way fixed effects for industry, location, and year"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "high dimensional fixed effects GLM package",
      "gravity model estimation with fixed effects",
      "Poisson regression with multiple fixed effects",
      "bias correction for logit models with fixed effects"
    ]
  },
  {
    "name": "bife",
    "description": "Estimates fixed effects binary choice models (logit and probit) with potentially many individual fixed effects using a pseudo-demeaning algorithm. Addresses the incidental parameters problem through analytical bias correction based on Fern\u00e1ndez-Val (2009) and computes average partial effects.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/bife/vignettes/howto.html",
    "github_url": "https://github.com/amrei-stammann/bife",
    "url": "https://cran.r-project.org/package=bife",
    "install": "install.packages(\"bife\")",
    "tags": [
      "binary-choice",
      "fixed-effects",
      "logit-probit",
      "bias-correction",
      "panel-data"
    ],
    "best_for": "Fast estimation of fixed effects logit/probit models on large panel data with analytical bias correction for the incidental parameters problem",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "panel-data-methods",
      "logistic-regression",
      "R-programming"
    ],
    "topic_tags": [
      "binary-choice-models",
      "fixed-effects-estimation",
      "bias-correction",
      "panel-econometrics"
    ],
    "summary": "The bife package estimates binary choice models (logit/probit) with individual fixed effects using a pseudo-demeaning algorithm that handles many fixed effects efficiently. It addresses the incidental parameters problem through analytical bias correction and computes average partial effects. This is essential for researchers working with panel data where individual heterogeneity needs to be controlled for in binary outcome models.",
    "use_cases": [
      "Analyzing employee promotion decisions across firms while controlling for unobserved worker characteristics",
      "Studying customer purchase behavior over time while accounting for individual preferences and habits"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "fixed effects logit panel data R",
      "binary choice model with individual fixed effects",
      "how to handle incidental parameters problem logit",
      "bife package bias correction binary outcomes"
    ]
  },
  {
    "name": "duckreg",
    "description": "Out-of-core regression (OLS/IV) for very large datasets using DuckDB aggregation. Handles data that doesn't fit in memory.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/py-econometrics/duckreg",
    "github_url": null,
    "url": "https://github.com/py-econometrics/duckreg",
    "install": "pip install duckreg",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "SQL-queries",
      "linear-regression"
    ],
    "topic_tags": [
      "out-of-core-computing",
      "large-datasets",
      "regression-analysis",
      "duckdb",
      "memory-optimization"
    ],
    "summary": "Duckreg enables regression analysis on datasets too large to fit in memory by leveraging DuckDB's efficient aggregation capabilities. It supports both OLS and instrumental variables estimation through out-of-core computation, making big data econometrics accessible without requiring distributed computing infrastructure. The package is particularly valuable for researchers working with administrative datasets or large-scale observational data.",
    "use_cases": [
      "Analyzing multi-year administrative datasets with millions of observations for causal inference studies",
      "Running fixed effects regressions on large transaction or user behavior datasets that exceed RAM capacity"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "regression on datasets too big for memory",
      "out of core OLS estimation python",
      "how to run fixed effects on large datasets",
      "duckdb regression analysis large data"
    ]
  },
  {
    "name": "fixest",
    "description": "Fast and comprehensive package for estimating econometric models with multiple high-dimensional fixed effects, including OLS, GLM, Poisson, and negative binomial models. Features native support for clustered standard errors (up to four-way), instrumental variables, and modern difference-in-differences estimators including Sun-Abraham for staggered treatments.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://lrberge.github.io/fixest/",
    "github_url": "https://github.com/lrberge/fixest",
    "url": "https://cran.r-project.org/package=fixest",
    "install": "install.packages(\"fixest\")",
    "tags": [
      "fixed-effects",
      "panel-data",
      "clustered-standard-errors",
      "difference-in-differences",
      "instrumental-variables"
    ],
    "best_for": "Fast, production-ready estimation of linear/GLM models with multiple high-dimensional fixed effects and publication-quality regression tables via etable()",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "panel-data-concepts",
      "difference-in-differences"
    ],
    "topic_tags": [
      "fixed-effects",
      "panel-data",
      "clustered-standard-errors",
      "difference-in-differences",
      "R-package"
    ],
    "summary": "fixest is a high-performance R package for estimating econometric models with multiple high-dimensional fixed effects, supporting OLS, GLM, Poisson, and negative binomial models. It provides native support for clustered standard errors, instrumental variables, and modern difference-in-differences estimators including Sun-Abraham for staggered treatments. The package is optimized for speed and handles large datasets efficiently while offering comprehensive econometric functionality.",
    "use_cases": [
      "Analyzing the impact of policy changes across different states and time periods using difference-in-differences with staggered treatment timing",
      "Estimating demand elasticity from transaction data with customer and product fixed effects while clustering standard errors at the market level"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "fast R package for fixed effects regression",
      "how to handle staggered difference in differences",
      "R package for clustered standard errors",
      "fixest vs felm performance comparison"
    ]
  },
  {
    "name": "lfe",
    "description": "Efficiently estimates linear models with multiple high-dimensional fixed effects using the Method of Alternating Projections. Designed for datasets with factors having thousands of levels (hundreds of thousands of dummy variables), with full support for 2SLS instrumental variables and multi-way clustered standard errors.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/lfe/lfe.pdf",
    "github_url": "https://github.com/r-econometrics/lfe",
    "url": "https://cran.r-project.org/package=lfe",
    "install": "install.packages(\"lfe\")",
    "tags": [
      "high-dimensional-fe",
      "worker-firm",
      "memory-efficient",
      "instrumental-variables",
      "clustered-se"
    ],
    "best_for": "AKM-style wage decompositions and matched employer-employee data with hundreds of thousands of worker/firm fixed effects",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "panel-data-structure"
    ],
    "topic_tags": [
      "fixed-effects",
      "high-dimensional-data",
      "panel-econometrics",
      "R-package",
      "instrumental-variables"
    ],
    "summary": "The lfe package provides memory-efficient estimation of linear models with multiple high-dimensional fixed effects using alternating projections. It's designed for econometric analysis with datasets containing factors with thousands of levels, such as worker-firm matched data or multi-dimensional panel studies. The package supports instrumental variables and multi-way clustered standard errors for robust inference.",
    "use_cases": [
      "analyzing wage effects in worker-firm matched datasets with thousands of companies and workers",
      "estimating treatment effects in multi-dimensional panel data with firm, time, and industry fixed effects"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for high dimensional fixed effects",
      "how to estimate worker firm fixed effects efficiently",
      "lfe package tutorial panel data",
      "multi-way clustered standard errors R"
    ]
  },
  {
    "name": "panelhetero",
    "description": "Heterogeneity analysis across units in panel data. Detects and characterizes unit-level variation.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://github.com/tkhdyanagi/panelhetero",
    "github_url": "https://github.com/tkhdyanagi/panelhetero",
    "url": "https://github.com/tkhdyanagi/panelhetero",
    "install": "pip install panelhetero",
    "tags": [
      "panel data",
      "heterogeneity",
      "unit effects"
    ],
    "best_for": "Unit heterogeneity in panels",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "fixed-effects-regression",
      "panel-data-structure"
    ],
    "topic_tags": [
      "panel-data",
      "heterogeneity-analysis",
      "unit-effects",
      "econometrics",
      "python-package"
    ],
    "summary": "A Python package for analyzing heterogeneity across units in panel datasets, helping researchers detect and characterize unit-level variation beyond standard fixed effects. It provides tools to identify subgroups of units with similar behavior patterns and quantify the extent of heterogeneous treatment effects or parameter variation across panel units.",
    "use_cases": [
      "Analyzing whether a marketing campaign has different effects across geographic regions using sales panel data",
      "Detecting heterogeneous responses to policy changes across firms in longitudinal business datasets"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to detect heterogeneity in panel data",
      "python package for unit-level variation analysis",
      "analyzing different effects across panel units",
      "heterogeneous treatment effects in longitudinal data"
    ]
  },
  {
    "name": "panelr",
    "description": "Automates within-between (hybrid) model specification for panel/longitudinal data, combining fixed effects robustness to time-invariant confounding with random effects ability to estimate time-invariant coefficients. Uses lme4 for multilevel estimation with optional Bayesian (brms) and GEE (geepack) backends.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://panelr.jacob-long.com/",
    "github_url": "https://github.com/jacob-long/panelr",
    "url": "https://cran.r-project.org/package=panelr",
    "install": "install.packages(\"panelr\")",
    "tags": [
      "hybrid-models",
      "within-between",
      "panel-data",
      "longitudinal-analysis",
      "bell-jones"
    ],
    "best_for": "Researchers needing fixed effects-equivalent estimates while retaining time-invariant predictors and random slopes",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "panel-data-concepts",
      "lme4-package",
      "fixed-effects-models"
    ],
    "topic_tags": [
      "hybrid-models",
      "within-between",
      "panel-data",
      "longitudinal-analysis",
      "multilevel-modeling"
    ],
    "summary": "panelr automates the specification of within-between (hybrid) models for panel data analysis, combining the benefits of fixed effects and random effects approaches. It handles the complex data transformations needed to estimate both time-varying and time-invariant effects simultaneously. The package supports multiple estimation backends including lme4, brms, and geepack for different modeling needs.",
    "use_cases": [
      "Analyzing employee productivity over time while controlling for unobserved worker characteristics and estimating effects of education",
      "Studying firm performance across quarters while accounting for industry fixed effects and measuring impact of CEO characteristics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "within between model R package",
      "hybrid panel data models automation",
      "estimate time invariant effects with fixed effects",
      "panelr vs plm for panel data"
    ]
  },
  {
    "name": "plm",
    "description": "Comprehensive econometrics package for linear panel models providing fixed effects (within), random effects, between, first-difference, Hausman-Taylor, and nested random effects estimators. Includes GMM, FGLS, and extensive diagnostic tests for serial correlation, cross-sectional dependence, and panel unit roots.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://cran.r-project.org/web/packages/plm/vignettes/",
    "github_url": "https://github.com/ycroissant/plm",
    "url": "https://cran.r-project.org/package=plm",
    "install": "install.packages(\"plm\")",
    "tags": [
      "panel-data",
      "econometrics",
      "fixed-effects",
      "random-effects",
      "hausman-test"
    ],
    "best_for": "Comprehensive panel data analysis requiring within/between/random effects estimation, Hausman tests, and extensive diagnostic testing",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "panel-data",
      "fixed-effects",
      "econometrics",
      "causal-inference",
      "python-package"
    ],
    "summary": "The plm package is a comprehensive Python library for estimating linear panel data models with multiple econometric estimators. It provides tools for fixed effects, random effects, and advanced methods like Hausman-Taylor and GMM estimation. Essential for economists and data scientists working with longitudinal data who need to control for unobserved heterogeneity and perform rigorous causal inference.",
    "use_cases": [
      "Analyzing the effect of policy changes across states over time while controlling for state-specific unobserved factors",
      "Estimating the impact of company characteristics on firm performance using multi-year corporate panel data"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for panel data fixed effects",
      "how to run Hausman test for panel data",
      "plm vs statsmodels for longitudinal analysis",
      "panel data econometrics library python"
    ]
  },
  {
    "name": "pydynpd",
    "description": "Estimation of dynamic panel data models using Arellano-Bond (Difference GMM) and Blundell-Bond (System GMM). Includes Windmeijer correction & tests.",
    "category": "Panel Data & Fixed Effects",
    "docs_url": "https://doi.org/10.21105/joss.04416",
    "github_url": "https://github.com/dazhwu/pydynpd",
    "url": "https://github.com/dazhwu/pydynpd",
    "install": "pip install pydynpd",
    "tags": [
      "panel data",
      "fixed effects"
    ],
    "best_for": "Longitudinal analysis, controlling for unobserved heterogeneity",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-pandas",
      "panel-data-econometrics",
      "instrumental-variables"
    ],
    "topic_tags": [
      "dynamic-panel-data",
      "GMM-estimation",
      "arellano-bond",
      "econometric-methods",
      "python-package"
    ],
    "summary": "A Python package for estimating dynamic panel data models using advanced econometric methods including Arellano-Bond (Difference GMM) and Blundell-Bond (System GMM) estimators. It addresses endogeneity issues in panel data where lagged dependent variables are included as regressors. The package includes the Windmeijer finite-sample correction and diagnostic tests for model specification.",
    "use_cases": [
      "Estimating firm productivity dynamics where current performance depends on past performance and unobserved firm characteristics",
      "Analyzing how past government spending affects current economic growth across countries while controlling for country-specific factors"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package dynamic panel data GMM",
      "arellano bond estimation python",
      "how to estimate dynamic panel models with endogeneity",
      "blundell bond system GMM implementation"
    ]
  },
  {
    "name": "WebPower",
    "description": "Comprehensive collection of tools for basic and advanced statistical power analysis including correlation, t-test, ANOVA, regression, mediation analysis, structural equation modeling (SEM), and multilevel models. Features both R package and web interface.",
    "category": "Power Analysis",
    "docs_url": "https://webpower.psychstat.org/",
    "github_url": "https://github.com/johnnyzhz/WebPower",
    "url": "https://cran.r-project.org/package=WebPower",
    "install": "install.packages(\"WebPower\")",
    "tags": [
      "power-analysis",
      "SEM",
      "mediation",
      "multilevel-models",
      "cluster-randomized-trials"
    ],
    "best_for": "Advanced power analysis for SEM, mediation, and cluster randomized trials, implementing Zhang & Yuan (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "statistical-hypothesis-testing",
      "R-programming",
      "experimental-design"
    ],
    "topic_tags": [
      "power-analysis",
      "experimental-design",
      "SEM",
      "mediation-analysis",
      "statistical-software"
    ],
    "summary": "WebPower is a comprehensive R package and web application for statistical power analysis across various experimental designs and statistical models. It enables researchers to calculate sample sizes, detect effect sizes, and plan studies for everything from basic t-tests to complex structural equation models and multilevel designs. The tool is essential for proper experimental planning and ensuring studies have adequate statistical power to detect meaningful effects.",
    "use_cases": [
      "Planning sample size for a randomized controlled trial testing the effect of a new product feature on user engagement",
      "Determining statistical power for a mediation analysis examining how workplace training affects performance through employee motivation"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to calculate sample size for A/B test power analysis",
      "Statistical power analysis for mediation models",
      "Power analysis tools for multilevel experimental designs",
      "R package for SEM power calculations"
    ]
  },
  {
    "name": "pwr",
    "description": "Provides basic power calculations using effect sizes and notation from Cohen (1988). Supports t-tests, chi-squared tests, one-way ANOVA, correlation tests, proportion tests, and general linear models with analytical (closed-form) solutions.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/pwr/pwr.pdf",
    "github_url": "https://github.com/heliosdrm/pwr",
    "url": "https://cran.r-project.org/package=pwr",
    "install": "install.packages(\"pwr\")",
    "tags": [
      "power-analysis",
      "sample-size",
      "effect-size",
      "Cohen-d",
      "t-test"
    ],
    "best_for": "Basic power calculations for standard statistical tests following Cohen's conventions from Cohen (1988)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "hypothesis-testing",
      "t-tests",
      "basic-statistics"
    ],
    "topic_tags": [
      "power-analysis",
      "sample-size",
      "effect-size",
      "experimental-design",
      "Cohen-d"
    ],
    "summary": "The pwr package provides essential power calculations for common statistical tests using Cohen's standardized effect sizes. It helps researchers determine appropriate sample sizes before running experiments and assess the power of completed studies. The package covers fundamental tests like t-tests, chi-squared, ANOVA, and correlation analysis with easy-to-use analytical solutions.",
    "use_cases": [
      "Planning sample size for an A/B test to detect a meaningful conversion rate difference",
      "Determining if a completed experiment had sufficient power to detect small effect sizes"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to calculate sample size for t-test",
      "power analysis package R",
      "Cohen's d effect size calculator",
      "determine sample size for A/B test statistical power"
    ]
  },
  {
    "name": "simr",
    "description": "Calculates power for generalized linear mixed models (GLMMs) using Monte Carlo simulation. Designed to work with lme4 models; supports LMMs and GLMMs with crossed random effects, non-normal responses, and complex variance structures where analytical solutions are unavailable.",
    "category": "Power Analysis",
    "docs_url": "https://cran.r-project.org/web/packages/simr/vignettes/fromscratch.html",
    "github_url": "https://github.com/pitakakariki/simr",
    "url": "https://cran.r-project.org/package=simr",
    "install": "install.packages(\"simr\")",
    "tags": [
      "power-analysis",
      "mixed-models",
      "simulation",
      "lme4",
      "GLMM"
    ],
    "best_for": "Power analysis for hierarchical/multilevel models via simulation when analytical solutions don't exist, implementing Green & MacLeod (2016, MEE)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-mixed-models",
      "R-lme4",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "power-analysis",
      "mixed-models",
      "monte-carlo-simulation",
      "experimental-design",
      "R-package"
    ],
    "summary": "simr is an R package that calculates statistical power for generalized linear mixed models through Monte Carlo simulation. It's particularly valuable for researchers designing experiments with complex hierarchical data structures where traditional power calculation methods fall short. The package integrates seamlessly with lme4 models and handles non-normal responses and crossed random effects.",
    "use_cases": [
      "Planning sample sizes for A/B tests with user clustering and multiple treatment levels",
      "Determining power for educational interventions with students nested within schools and teachers"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "power analysis for mixed effects models",
      "sample size calculation for hierarchical data",
      "Monte Carlo power simulation R package",
      "lme4 power analysis tools"
    ]
  },
  {
    "name": "ADOpy",
    "description": "Bayesian Adaptive Design Optimization (ADO) for tuning experiments in real-time, with models for psychometric tasks.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adopy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/adopy/adopy",
    "url": "https://github.com/adopy/adopy",
    "install": "pip install adopy",
    "tags": [
      "power analysis",
      "experiments",
      "Bayesian"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "bayesian-inference",
      "python-scipy",
      "experimental-design"
    ],
    "topic_tags": [
      "adaptive-design",
      "bayesian-optimization",
      "psychometrics",
      "real-time-experiments"
    ],
    "summary": "ADOpy is a Python package that implements Bayesian Adaptive Design Optimization for running experiments that adapt in real-time based on incoming data. It's particularly useful for psychometric experiments where you want to optimize stimulus selection to maximize information gain. The package includes pre-built models for common psychological tasks and allows researchers to design more efficient experiments.",
    "use_cases": [
      "Optimizing A/B test allocation in real-time based on early results to minimize sample size needed",
      "Running psychophysical experiments that adaptively select stimuli to efficiently estimate perceptual thresholds"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "adaptive experiment design python",
      "bayesian optimization for A/B tests",
      "real-time experiment tuning",
      "psychometric adaptive testing tools"
    ]
  },
  {
    "name": "Adaptive",
    "description": "Parallel active learning library for adaptive function sampling/evaluation, with live plotting for monitoring.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://adaptive.readthedocs.io/en/latest/",
    "github_url": "https://github.com/python-adaptive/adaptive",
    "url": "https://github.com/python-adaptive/adaptive",
    "install": "pip install adaptive",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scipy",
      "bayesian-optimization",
      "parallel-computing"
    ],
    "topic_tags": [
      "adaptive-sampling",
      "active-learning",
      "function-evaluation",
      "parallel-optimization",
      "experimental-design"
    ],
    "summary": "Adaptive is a Python library for intelligent sampling and evaluation of functions using active learning techniques. It enables parallel computation to efficiently explore parameter spaces by adaptively choosing which points to evaluate next. The library includes live plotting capabilities to monitor the learning progress in real-time.",
    "use_cases": [
      "Hyperparameter tuning for machine learning models where function evaluations are expensive",
      "Optimizing simulation parameters in A/B testing frameworks with costly experimental runs"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "parallel active learning python library",
      "adaptive function sampling optimization",
      "smart parameter space exploration tools",
      "efficient hyperparameter optimization with live monitoring"
    ]
  },
  {
    "name": "Ambrosia",
    "description": "End-to-end A/B testing from MobileTeleSystems with PySpark support. Covers experiment design, multi-group splitting, matching, and inference.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/MobileTeleSystems/Ambrosia",
    "url": "https://github.com/MobileTeleSystems/Ambrosia",
    "install": "pip install ambrosia",
    "tags": [
      "A/B testing",
      "experimentation",
      "Spark"
    ],
    "best_for": "End-to-end A/B testing with PySpark",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "pyspark",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "a-b-testing",
      "experimental-design",
      "pyspark",
      "causal-inference",
      "python-package"
    ],
    "summary": "Ambrosia is a comprehensive A/B testing framework from MobileTeleSystems that handles the full experimental workflow from design to analysis using PySpark for scalability. It provides tools for multi-group randomization, matching methods, and statistical inference on large datasets. The package is particularly valuable for data scientists working with big data experimentation pipelines in production environments.",
    "use_cases": [
      "Running large-scale product feature tests across millions of mobile users with proper randomization and statistical analysis",
      "Designing and analyzing multi-arm experiments for marketing campaigns with matched treatment and control groups"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "PySpark A/B testing framework",
      "end-to-end experimentation pipeline Python",
      "multi-group randomization with matching",
      "scalable A/B testing package Spark"
    ]
  },
  {
    "name": "DoEgen",
    "description": "Automates generation and optimization of designs, especially for mixed factor-level experiments; computes efficiency metrics.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": null,
    "github_url": "https://github.com/sebhaan/DoEgen",
    "url": "https://github.com/sebhaan/DoEgen",
    "install": "pip install DoEgen",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "experimental-design-theory",
      "python-scipy",
      "statistical-power-analysis"
    ],
    "topic_tags": [
      "experimental-design",
      "design-optimization",
      "mixed-factors",
      "power-analysis",
      "automation"
    ],
    "summary": "DoEgen is a package that automatically generates and optimizes experimental designs, particularly for complex experiments with mixed factor levels (continuous and categorical variables). It computes key efficiency metrics like D-optimality and power to help researchers create statistically robust experimental setups. The tool is especially valuable for practitioners who need to balance multiple constraints while maximizing statistical power.",
    "use_cases": [
      "Optimizing A/B test designs with both continuous variables (price, timeout) and categorical factors (UI themes, algorithms)",
      "Planning manufacturing experiments with mixed quantitative process parameters and qualitative material choices"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "automated experimental design with mixed factors",
      "optimize experiment design efficiency metrics",
      "DoEgen mixed factor level experiments",
      "experimental design automation python package"
    ]
  },
  {
    "name": "pyDOE2",
    "description": "Implements classical Design of Experiments: factorial (full/fractional), response surface (Box-Behnken, CCD), Latin Hypercube.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://pythonhosted.org/pyDOE2/",
    "github_url": "https://github.com/clicumu/pyDOE2",
    "url": "https://github.com/clicumu/pyDOE2",
    "install": "pip install pyDOE2",
    "tags": [
      "power analysis",
      "experiments"
    ],
    "best_for": "Sample size calculation, experimental design, power analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "factorial-analysis"
    ],
    "topic_tags": [
      "design-of-experiments",
      "factorial-design",
      "latin-hypercube",
      "response-surface",
      "experimental-design"
    ],
    "summary": "pyDOE2 is a Python package that implements classical Design of Experiments methods for efficiently planning experiments and parameter studies. It provides tools for creating factorial designs, response surface methodologies, and space-filling designs like Latin Hypercube sampling. Data scientists and researchers use it to systematically explore parameter spaces with minimal experimental runs while maximizing information gain.",
    "use_cases": [
      "Planning A/B tests with multiple factors to understand interaction effects between UI changes, pricing, and user segments",
      "Optimizing machine learning hyperparameters using response surface methodology to find optimal configurations with fewer training runs"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python package for design of experiments",
      "how to create factorial designs in python",
      "latin hypercube sampling implementation",
      "response surface methodology python library"
    ]
  },
  {
    "name": "tea-tasting",
    "description": "Calculate A/B test statistics directly within data warehouses (BigQuery, ClickHouse, Snowflake, Spark) via Ibis interface. Supports CUPED/CUPAC.",
    "category": "Power Simulation & Design of Experiments",
    "docs_url": "https://tea-tasting.e10v.me/",
    "github_url": "https://github.com/e10v/tea-tasting",
    "url": "https://github.com/e10v/tea-tasting",
    "install": "pip install tea-tasting",
    "tags": [
      "A/B testing",
      "experimentation",
      "data warehouses"
    ],
    "best_for": "In-warehouse A/B test analysis with variance reduction",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "SQL-queries",
      "python-pandas",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "A-B-testing",
      "data-warehouses",
      "experimentation-platform",
      "CUPED",
      "statistical-inference"
    ],
    "summary": "Tea-tasting enables data scientists to run A/B test statistical analysis directly in cloud data warehouses without moving data locally. It provides a unified Python interface via Ibis to calculate test statistics across BigQuery, Snowflake, ClickHouse, and Spark, with built-in support for variance reduction techniques like CUPED and CUPAC.",
    "use_cases": [
      "Running experiment analysis on petabyte-scale user behavior data stored in BigQuery without data export",
      "Implementing CUPED variance reduction for subscription conversion tests using historical user metrics as covariates"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to run A/B tests in BigQuery without downloading data",
      "Python package for warehouse-native experimentation analysis",
      "CUPED implementation for cloud data warehouses",
      "Statistical testing directly in Snowflake or ClickHouse"
    ]
  },
  {
    "name": "CausalImpact",
    "description": "Python port of Google's R package for estimating causal effects of interventions on time series using Bayesian structural time-series models.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://google.github.io/CausalImpact/CausalImpact/CausalImpact.html",
    "github_url": "https://github.com/tcassou/causal_impact",
    "url": "https://github.com/tcassou/causal_impact",
    "install": "pip install causalimpact",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "Bayesian"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "bayesian-statistics",
      "time-series-analysis",
      "python-pandas"
    ],
    "topic_tags": [
      "causal-inference",
      "time-series",
      "bayesian-methods",
      "intervention-analysis",
      "python-package"
    ],
    "summary": "CausalImpact is a Python implementation of Google's Bayesian structural time-series package for measuring the causal effect of interventions on time series data. It's widely used by data scientists and researchers to evaluate the impact of marketing campaigns, policy changes, or product launches by comparing observed outcomes to counterfactual predictions. The package automatically handles seasonality, trends, and external covariates while providing uncertainty quantification.",
    "use_cases": [
      "Measuring the impact of a new feature launch on user engagement metrics",
      "Evaluating the effectiveness of a marketing campaign on sales revenue"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to measure causal impact of intervention on time series",
      "Python package for Bayesian causal inference time series",
      "Google CausalImpact Python implementation",
      "Evaluate marketing campaign effectiveness time series analysis"
    ]
  },
  {
    "name": "Differences",
    "description": "Implements modern difference-in-differences methods for staggered adoption designs (e.g., Callaway & Sant'Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://bernardodionisi.github.io/differences/",
    "github_url": "https://github.com/bernardodionisi/differences",
    "url": "https://github.com/bernardodionisi/differences",
    "install": "pip install differences",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-analysis",
      "python-statsmodels"
    ],
    "topic_tags": [
      "difference-in-differences",
      "staggered-adoption",
      "causal-inference",
      "program-evaluation",
      "python-package"
    ],
    "summary": "A Python package implementing state-of-the-art difference-in-differences estimators for staggered adoption designs, including Callaway & Sant'Anna's methods. Addresses common issues with two-way fixed effects models when treatment timing varies across units. Essential for practitioners working with modern DiD techniques in policy evaluation and A/B testing contexts.",
    "use_cases": [
      "Evaluating the impact of a policy rollout where different states adopt at different times",
      "Measuring the effect of a product feature launched across different markets on different dates"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Callaway Sant'Anna difference in differences python",
      "staggered adoption DiD implementation",
      "modern difference in differences package",
      "two way fixed effects bias solution"
    ]
  },
  {
    "name": "SyntheticControlMethods",
    "description": "Implementation of synthetic control methods for comparative case studies when panel data is available.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "url": "https://github.com/OscarEngelbrektson/SyntheticControlMethods",
    "install": "pip install SyntheticControlMethods",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "python-pandas",
      "linear-regression"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "panel-data",
      "comparative-case-studies"
    ],
    "summary": "SyntheticControlMethods implements synthetic control methodology for causal inference when you have one treated unit and multiple control units over time. It creates a synthetic version of the treated unit using weighted combinations of control units to estimate counterfactual outcomes. This is particularly useful for policy evaluation and comparative case studies where traditional randomized experiments aren't feasible.",
    "use_cases": [
      "Evaluating the impact of a new product launch in one market by comparing against a synthetic control made from similar untreated markets",
      "Measuring the effect of a policy change in one state/country by constructing a synthetic version from other unaffected regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic control methods python implementation",
      "how to evaluate policy impact with synthetic control",
      "difference between synthetic control and difference in differences",
      "synthetic control package for causal inference"
    ]
  },
  {
    "name": "TFP CausalImpact",
    "description": "TensorFlow Probability port of Google's CausalImpact. Bayesian structural time-series for intervention effects.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/google/tfp-causalimpact",
    "github_url": "https://github.com/google/tfp-causalimpact",
    "url": "https://github.com/google/tfp-causalimpact",
    "install": "pip install tfcausalimpact",
    "tags": [
      "causal impact",
      "time series",
      "Bayesian"
    ],
    "best_for": "TensorFlow-based causal impact analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-tensorflow",
      "time-series-analysis",
      "bayesian-inference"
    ],
    "topic_tags": [
      "causal-impact",
      "bayesian-time-series",
      "intervention-analysis",
      "structural-time-series",
      "tensorflow-probability"
    ],
    "summary": "TensorFlow Probability implementation of Google's CausalImpact package for measuring causal effects of interventions in time series data. Uses Bayesian structural time-series models to estimate what would have happened in the absence of an intervention. Particularly useful for A/B testing scenarios where randomization isn't feasible.",
    "use_cases": [
      "Measuring the impact of a marketing campaign on website traffic using control markets",
      "Evaluating the effect of a product feature launch on user engagement metrics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "tensorflow causal impact package",
      "bayesian time series intervention analysis",
      "causalimpact python tensorflow implementation",
      "measuring treatment effects time series data"
    ]
  },
  {
    "name": "csdid",
    "description": "Python adaptation of the R `did` package. Implements multi-period DiD with staggered treatment timing (Callaway & Sant\u2019Anna).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/d2cml-ai/csdid",
    "url": "https://github.com/d2cml-ai/csdid",
    "install": "pip install csdid",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "difference-in-differences",
      "causal-inference"
    ],
    "topic_tags": [
      "difference-in-differences",
      "staggered-treatment",
      "causal-inference",
      "python-package",
      "callaway-santanna"
    ],
    "summary": "Python implementation of the Callaway & Sant'Anna difference-in-differences estimator for settings with staggered treatment adoption across multiple time periods. This package handles the complexities of varying treatment timing and provides robust causal effect estimates. Essential tool for economists and data scientists conducting policy evaluation with panel data.",
    "use_cases": [
      "Evaluating the impact of minimum wage increases rolled out across different states at different times",
      "Measuring the effect of a product feature launched to different user cohorts in staggered phases"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python package for staggered difference in differences",
      "callaway sant'anna did implementation python",
      "multi-period difference in differences with varying treatment timing",
      "how to handle staggered treatment adoption in causal inference"
    ]
  },
  {
    "name": "didet",
    "description": "DiD with general treatment patterns. Handles effective treatment timing beyond simple staggered adoption.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didet",
    "github_url": "https://github.com/tkhdyanagi/didet",
    "url": "https://github.com/tkhdyanagi/didet",
    "install": "pip install didet",
    "tags": [
      "DiD",
      "treatment timing",
      "causal inference"
    ],
    "best_for": "DiD with general treatment patterns",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "difference-in-differences",
      "panel-data-analysis",
      "python-pandas"
    ],
    "topic_tags": [
      "difference-in-differences",
      "treatment-timing",
      "causal-inference",
      "python-package",
      "panel-data"
    ],
    "summary": "DiDet is a Python package for difference-in-differences analysis that handles complex treatment timing patterns beyond the standard staggered adoption setup. It allows researchers to analyze scenarios with varying treatment intensities, multiple treatment periods, and non-monotonic treatment patterns. The package is particularly useful for tech economists studying interventions with nuanced rollout strategies.",
    "use_cases": [
      "Analyzing the impact of a feature rollout that was turned on and off multiple times across different user segments",
      "Evaluating policy interventions where treatment intensity varied over time and across regions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "DiD with complex treatment timing python",
      "difference in differences non-staggered adoption",
      "how to handle varying treatment intensity DiD",
      "python package flexible treatment patterns causal inference"
    ]
  },
  {
    "name": "didhetero",
    "description": "Doubly robust estimation for group-time conditional average treatment effects. UCB for heterogeneous DiD.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://github.com/tkhdyanagi/didhetero",
    "github_url": "https://github.com/tkhdyanagi/didhetero",
    "url": "https://github.com/tkhdyanagi/didhetero",
    "install": "pip install didhetero",
    "tags": [
      "DiD",
      "heterogeneous effects",
      "doubly robust"
    ],
    "best_for": "Heterogeneous treatment effects in DiD",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "causal-inference",
      "doubly-robust-estimation"
    ],
    "topic_tags": [
      "heterogeneous-effects",
      "difference-in-differences",
      "causal-inference",
      "treatment-heterogeneity",
      "doubly-robust"
    ],
    "summary": "This package implements doubly robust estimation methods for identifying heterogeneous treatment effects in difference-in-differences designs. It allows researchers to estimate group-time conditional average treatment effects when treatment impacts vary across different subgroups or time periods. The methods provide robust inference by combining outcome regression and propensity score weighting approaches.",
    "use_cases": [
      "Evaluating how a policy rollout affects different demographic groups differently over time",
      "Measuring heterogeneous impacts of a product feature launch across user segments and time periods"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "heterogeneous difference in differences R package",
      "doubly robust DiD with varying treatment effects",
      "group-time average treatment effects estimation",
      "conditional ATET difference in differences"
    ]
  },
  {
    "name": "mlsynth",
    "description": "Implements advanced synthetic control methods: forward DiD, cluster SC, factor models, and proximal SC. Designed for single-treated-unit settings.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://mlsynth.readthedocs.io/en/latest/",
    "github_url": "https://github.com/jaredjgreathouse/mlsynth",
    "url": "https://github.com/jaredjgreathouse/mlsynth",
    "install": "pip install mlsynth",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "python-pandas",
      "causal-inference-methods"
    ],
    "topic_tags": [
      "synthetic-control",
      "difference-in-differences",
      "causal-inference",
      "policy-evaluation",
      "python-package"
    ],
    "summary": "Advanced Python package implementing cutting-edge synthetic control methods including forward DiD, cluster synthetic control, factor models, and proximal synthetic control. Designed specifically for rigorous causal inference in single-treated-unit scenarios where traditional methods may fail. Used by researchers and data scientists conducting policy evaluations and natural experiments.",
    "use_cases": [
      "Evaluating the impact of a new policy implemented in one state/city using other regions as synthetic controls",
      "Analyzing the causal effect of a product launch in a single market by constructing synthetic comparison units"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "synthetic control methods python package",
      "forward difference in differences implementation",
      "single treated unit causal inference tools",
      "advanced synthetic control cluster methods"
    ]
  },
  {
    "name": "pycinc",
    "description": "Changes\u2011in\u2011Changes (CiC) estimator for distributional treatment effects (Athey\u00a0&\u00a0Imbens\u202f2006).",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pycinc/",
    "github_url": null,
    "url": "https://pypi.org/project/pycinc/",
    "install": "pip install pycinc",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "quantile-regression",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "changes-in-changes",
      "distributional-effects",
      "causal-inference",
      "treatment-effects",
      "python-package"
    ],
    "summary": "PycinC implements the Changes-in-Changes estimator from Athey & Imbens (2006) for measuring treatment effects across the entire outcome distribution, not just the mean. This method extends difference-in-differences to account for distributional changes and is particularly useful when treatment effects vary across quantiles. Researchers use it to understand heterogeneous treatment impacts and policy effects on inequality.",
    "use_cases": [
      "Evaluating how minimum wage increases affect wage distributions at different percentiles",
      "Analyzing distributional impacts of education interventions on test score distributions across student ability levels"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "changes in changes estimator python implementation",
      "distributional treatment effects athey imbens",
      "quantile difference in differences python",
      "heterogeneous treatment effects distribution analysis"
    ]
  },
  {
    "name": "pyleebounds",
    "description": "Lee\u00a0(2009) sample\u2011selection bounds for treatment effects; trims treated distribution to match selection rates.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/pyleebounds/",
    "github_url": null,
    "url": "https://pypi.org/project/pyleebounds/",
    "install": "pip install pyleebounds",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD",
      "causal inference"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "difference-in-differences",
      "propensity-score-matching",
      "python-scipy"
    ],
    "topic_tags": [
      "sample-selection-bias",
      "treatment-effects",
      "causal-inference",
      "lee-bounds",
      "python-package"
    ],
    "summary": "PyLeeBounds implements Lee (2009) bounds for estimating treatment effects when treatment assignment affects sample selection. The method addresses selection bias by trimming the treated group distribution to match control group selection rates, providing bounds on the true treatment effect. It's essential for researchers dealing with endogenous sample selection in randomized or quasi-experimental settings.",
    "use_cases": [
      "Evaluating job training programs where treatment affects labor force participation rates",
      "Measuring educational intervention effects when treatment influences school dropout decisions"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Lee bounds sample selection bias treatment effects",
      "how to handle endogenous sample selection in RCT",
      "python package for Lee 2009 bounds estimation",
      "treatment effect bounds when selection is affected"
    ]
  },
  {
    "name": "pysyncon",
    "description": "Synthetic control method implementation compatible with R's Synth and augsynth packages.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/sdfordham/pysyncon",
    "url": "https://github.com/sdfordham/pysyncon",
    "install": "pip install pysyncon",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "R Synth-compatible synthetic control in Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "difference-in-differences",
      "panel-data-analysis"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "panel-data",
      "treatment-effects",
      "python-package"
    ],
    "summary": "Python implementation of the synthetic control method for causal inference with panel data, providing R Synth package compatibility. Used by researchers and data scientists to estimate treatment effects when randomized experiments aren't feasible. Constructs synthetic control units from donor pool to compare against treated units.",
    "use_cases": [
      "Evaluating impact of policy intervention on a specific region using other regions as controls",
      "Measuring effect of product launch in test market by creating synthetic control from similar markets"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "synthetic control method python implementation",
      "pysyncon vs R synth package comparison",
      "causal inference panel data python tools",
      "how to implement synthetic control analysis"
    ]
  },
  {
    "name": "rdd",
    "description": "Toolkit for sharp RDD analysis, including bandwidth calculation and estimation, integrating with pandas.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/evan-magnusson/rdd",
    "url": "https://github.com/evan-magnusson/rdd",
    "install": "pip install rdd",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "regression-discontinuity-design",
      "causal-inference"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "program-evaluation",
      "bandwidth-selection",
      "python-package"
    ],
    "summary": "A Python toolkit specifically designed for sharp regression discontinuity design (RDD) analysis with integrated pandas support. It provides automated bandwidth calculation and estimation methods for identifying causal effects at treatment thresholds. The package streamlines the technical implementation of RDD while maintaining statistical rigor.",
    "use_cases": [
      "Evaluating the impact of a scholarship program on student outcomes using GPA cutoffs",
      "Measuring the effect of credit score thresholds on loan approval and subsequent financial behavior"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for regression discontinuity design",
      "RDD bandwidth selection implementation",
      "sharp RDD analysis with pandas",
      "regression discontinuity toolkit python"
    ]
  },
  {
    "name": "rdrobust",
    "description": "Comprehensive tools for Regression Discontinuity Designs (RDD), including optimal bandwidth selection, estimation, inference.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": "https://pypi.org/project/rdrobust/",
    "github_url": "https://github.com/rdpackages/rdrobust",
    "url": "https://github.com/rdpackages/rdrobust",
    "install": "pip install rdrobust",
    "tags": [
      "DiD",
      "synthetic control",
      "RDD"
    ],
    "best_for": "Policy evaluation, natural experiments, quasi-experiments",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "regression-analysis",
      "causal-inference-fundamentals",
      "R-programming"
    ],
    "topic_tags": [
      "regression-discontinuity",
      "causal-inference",
      "program-evaluation",
      "R-package",
      "treatment-effects"
    ],
    "summary": "rdrobust is an R package that implements state-of-the-art methods for regression discontinuity designs, providing tools for bandwidth selection, robust estimation, and statistical inference. It's widely used by economists and data scientists to estimate causal effects when treatment assignment has a discontinuous cutoff rule. The package handles both sharp and fuzzy RDD designs with modern bias-correction and inference procedures.",
    "use_cases": [
      "Evaluating the impact of a scholarship program that awards aid to students scoring above a specific test threshold",
      "Measuring the effect of a policy change that applies to companies above a certain size cutoff"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for regression discontinuity analysis",
      "how to implement RDD with optimal bandwidth selection",
      "rdrobust vs manual regression discontinuity",
      "regression discontinuity tools for causal inference"
    ]
  },
  {
    "name": "synthlearners",
    "description": "Fast synthetic control estimators for panel data problems. Optimized ATT estimation with multiple SC algorithms.",
    "category": "Program Evaluation Methods (DiD, SC, RDD)",
    "docs_url": null,
    "github_url": "https://github.com/apoorvalal/synthlearners",
    "url": "https://github.com/apoorvalal/synthlearners",
    "install": "pip install synthlearners",
    "tags": [
      "synthetic control",
      "causal inference",
      "panel data"
    ],
    "best_for": "Optimized synthetic control with multiple algorithm options",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "difference-in-differences",
      "causal-inference-fundamentals"
    ],
    "topic_tags": [
      "synthetic-control",
      "causal-inference",
      "panel-data",
      "treatment-effects",
      "python-package"
    ],
    "summary": "synthlearners is a Python package providing optimized implementations of synthetic control methods for causal inference in panel data settings. It offers multiple algorithms for estimating average treatment effects (ATT) when units are treated at different times. The package is designed for speed and ease of use, making synthetic control methods accessible for practitioners working with observational data.",
    "use_cases": [
      "Evaluating the impact of a policy change on treated states/regions compared to untreated control units",
      "Measuring the causal effect of a product launch or marketing campaign across different markets using historical panel data"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "synthetic control python package for causal inference",
      "how to implement synthetic control method in python",
      "ATT estimation with synthetic control algorithms",
      "python library for panel data causal inference"
    ]
  },
  {
    "name": "pyqreg",
    "description": "Fast quantile regression solver using interior point methods, supporting robust and clustered standard errors.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/mozjay0619/pyqreg",
    "github_url": null,
    "url": "https://github.com/mozjay0619/pyqreg",
    "install": "pip install pyqreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "scipy-optimization"
    ],
    "topic_tags": [
      "quantile-regression",
      "robust-statistics",
      "interior-point",
      "clustered-errors",
      "distributional-analysis"
    ],
    "summary": "pyqreg is a Python package for fast quantile regression using interior point optimization methods. It provides robust alternatives to ordinary least squares by modeling conditional quantiles of the outcome distribution, with support for clustered and robust standard errors. The package is particularly useful for economists and data scientists analyzing heterogeneous treatment effects or non-normal error distributions.",
    "use_cases": [
      "Analyzing wage gaps across income distribution quantiles where effects vary by earnings level",
      "Estimating treatment effects on health outcomes when impacts differ across patient risk profiles"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "fast quantile regression python package",
      "how to estimate quantile regression with clustered standard errors",
      "pyqreg vs statsmodels quantile regression",
      "interior point methods for quantile regression implementation"
    ]
  },
  {
    "name": "pyrifreg",
    "description": "Recentered Influence\u2011Function (RIF) regression for unconditional quantile & distributional effects (Firpo\u202fet\u202fal.,\u202f2008).",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://github.com/vyasenov/pyrifreg",
    "github_url": null,
    "url": "https://github.com/vyasenov/pyrifreg",
    "install": "pip install pyrifreg",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "quantile-regression",
      "causal-inference",
      "python-scipy"
    ],
    "topic_tags": [
      "quantile-regression",
      "distributional-effects",
      "unconditional-quantile",
      "rif-regression",
      "causal-analysis"
    ],
    "summary": "PyRIFReg implements Recentered Influence Function (RIF) regression to estimate unconditional quantile effects and distributional impacts beyond the mean. This advanced econometric method allows researchers to understand how treatments or policies affect different parts of the outcome distribution, not just average effects. Particularly valuable for policy evaluation where distributional consequences matter as much as average impacts.",
    "use_cases": [
      "Analyzing how minimum wage increases affect different parts of the wage distribution rather than just average wages",
      "Evaluating educational interventions' impact on test score inequality by examining effects at various quantiles"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "RIF regression python implementation",
      "unconditional quantile effects estimation",
      "distributional impact analysis tools",
      "Firpo Fortin Lemieux method python"
    ]
  },
  {
    "name": "quantile-forest",
    "description": "Scikit-learn compatible implementation of Quantile Regression Forests for non-parametric estimation.",
    "category": "Quantile Regression & Distributional Methods",
    "docs_url": "https://zillow.github.io/quantile-forest/",
    "github_url": "https://github.com/zillow/quantile-forest",
    "url": "https://github.com/zillow/quantile-forest",
    "install": "pip install quantile-forest",
    "tags": [
      "quantile",
      "regression"
    ],
    "best_for": "Heterogeneous effects, distributional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "random-forests",
      "quantile-regression"
    ],
    "topic_tags": [
      "quantile-regression",
      "random-forests",
      "uncertainty-quantification",
      "scikit-learn",
      "python-package"
    ],
    "summary": "Quantile-forest is a scikit-learn compatible Python package that implements Quantile Regression Forests, extending random forests to estimate conditional quantiles instead of just point predictions. It enables non-parametric estimation of prediction intervals and full conditional distributions without distributional assumptions. The package is particularly useful for uncertainty quantification and risk assessment in machine learning applications.",
    "use_cases": [
      "Estimating confidence intervals for house price predictions to understand pricing uncertainty",
      "Risk management in financial forecasting by modeling tail quantiles of portfolio returns"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "quantile regression forest python implementation",
      "scikit-learn prediction intervals",
      "non-parametric uncertainty quantification random forests",
      "how to get confidence intervals from random forest predictions"
    ]
  },
  {
    "name": "broom",
    "description": "Converts messy output from 100+ statistical model types into consistent tidy tibbles using three verbs: tidy() for coefficient-level statistics, glance() for model-level summaries (R\u00b2, AIC), and augment() for fitted values and residuals.",
    "category": "Regression Output",
    "docs_url": "https://broom.tidymodels.org/",
    "github_url": "https://github.com/tidymodels/broom",
    "url": "https://cran.r-project.org/package=broom",
    "install": "install.packages(\"broom\")",
    "tags": [
      "tidy-data",
      "tidymodels",
      "statistical-models",
      "tidyverse",
      "modeling"
    ],
    "best_for": "Converting R statistical model output into consistent tidy data frames for analysis pipelines, based on Wickham (2014, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "tidyverse-dplyr"
    ],
    "topic_tags": [
      "model-output",
      "data-cleaning",
      "statistical-reporting",
      "tidyverse",
      "regression-analysis"
    ],
    "summary": "The broom package standardizes the messy, inconsistent output from R's statistical models into clean, analysis-ready data frames. It's essential for data scientists who need to extract coefficients, model statistics, or predictions from models in a consistent format. The package works with over 100 model types, making it a go-to tool for anyone doing statistical modeling in R.",
    "use_cases": [
      "Extracting regression coefficients from multiple models to create comparison tables",
      "Getting fitted values and residuals from various model types for diagnostic plotting"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to clean up R model output",
      "Extract coefficients from lm glm models R",
      "Tidy statistical model results R",
      "Get fitted values residuals from models R"
    ]
  },
  {
    "name": "gt",
    "description": "Build display tables from tabular data using a cohesive grammar of table parts (header, stub, body, footer). Enables progressive construction of publication-quality tables with extensive formatting, footnotes, and cell styling. Outputs to HTML, LaTeX, and RTF.",
    "category": "Regression Output",
    "docs_url": "https://gt.rstudio.com/",
    "github_url": "https://github.com/rstudio/gt",
    "url": "https://cran.r-project.org/package=gt",
    "install": "install.packages(\"gt\")",
    "tags": [
      "grammar-of-tables",
      "display-tables",
      "HTML-tables",
      "Posit",
      "formatting"
    ],
    "best_for": "Publication-ready display tables with precise formatting control and multiple output formats",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-basics",
      "data-frames"
    ],
    "topic_tags": [
      "table-formatting",
      "publication-ready",
      "HTML-output",
      "data-presentation",
      "R-visualization"
    ],
    "summary": "The gt package provides a grammar of tables approach for creating publication-quality display tables in R. It allows progressive table construction with extensive formatting options, footnotes, and styling capabilities. Outputs can be generated for HTML, LaTeX, and RTF formats, making it ideal for reports, presentations, and academic publications.",
    "use_cases": [
      "Creating formatted regression output tables for academic papers with proper styling and footnotes",
      "Building executive summary tables with custom formatting for business reports and dashboards"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for creating formatted tables",
      "how to make publication ready tables in R",
      "gt package tutorial for beginners",
      "best R package for HTML table formatting"
    ]
  },
  {
    "name": "gtsummary",
    "description": "Creates publication-ready analytical and summary tables (Table 1 demographics, regression results, survival analyses) with one line of code. Auto-detects variable types, calculates appropriate statistics, and formats regression models with reference rows and appropriate headers.",
    "category": "Regression Output",
    "docs_url": "https://www.danieldsjoberg.com/gtsummary/",
    "github_url": "https://github.com/ddsjoberg/gtsummary",
    "url": "https://cran.r-project.org/package=gtsummary",
    "install": "install.packages(\"gtsummary\")",
    "tags": [
      "summary-tables",
      "Table1",
      "clinical-tables",
      "regression-tables",
      "reproducible-research"
    ],
    "best_for": "Table 1 demographics and regression summary tables for medical/scientific publications, implementing Sjoberg et al. (2021, R Journal)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "regression-analysis",
      "data-manipulation"
    ],
    "topic_tags": [
      "summary-tables",
      "regression-output",
      "publication-ready",
      "clinical-research",
      "reproducible-research"
    ],
    "summary": "An R package that automatically generates publication-ready summary tables and regression output with minimal code. Popular in clinical research and academic settings for creating standardized Table 1 demographics and formatted regression results. Handles variable type detection and statistical formatting automatically.",
    "use_cases": [
      "Creating Table 1 demographics for a clinical trial publication",
      "Formatting logistic regression results for an A/B test report"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to make publication ready tables in R",
      "automatic Table 1 generator",
      "format regression output for papers",
      "R package for summary statistics tables"
    ]
  },
  {
    "name": "modelsummary",
    "description": "Creates publication-quality tables summarizing multiple statistical models side-by-side, plus coefficient plots, data summaries, and correlation matrices. Supports 100+ model types via broom/parameters with output to HTML, LaTeX, Word, PDF, PNG, and Excel.",
    "category": "Regression Output",
    "docs_url": "https://modelsummary.com/",
    "github_url": "https://github.com/vincentarelbundock/modelsummary",
    "url": "https://cran.r-project.org/package=modelsummary",
    "install": "install.packages(\"modelsummary\")",
    "tags": [
      "regression-tables",
      "model-summary",
      "coefficient-plots",
      "publication-tables",
      "tidyverse"
    ],
    "best_for": "Modern, flexible regression tables with extensive customization\u2014the successor to stargazer, implementing Arel-Bundock (2022, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "r-programming",
      "linear-regression",
      "ggplot2"
    ],
    "topic_tags": [
      "regression-tables",
      "model-comparison",
      "publication-ready",
      "statistical-output",
      "r-package"
    ],
    "summary": "An R package that creates professional-looking tables comparing multiple statistical models side-by-side, perfect for academic papers and reports. It automatically formats regression coefficients, standard errors, and model statistics across 100+ model types. The package also generates coefficient plots and exports to multiple formats including LaTeX, HTML, and Word.",
    "use_cases": [
      "PhD student comparing different regression specifications in their dissertation chapter",
      "Data scientist presenting A/B test results with multiple model variations to stakeholders"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to create publication quality regression tables in R",
      "compare multiple models side by side table",
      "R package for formatting regression output",
      "export regression results to LaTeX Word Excel"
    ]
  },
  {
    "name": "stargazer",
    "description": "Produces well-formatted LaTeX, HTML/CSS, and ASCII regression tables with multiple models side-by-side, plus summary statistics tables. Widely used in economics with journal-specific formatting styles (AER, QJE, ASR).",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=stargazer",
    "install": "install.packages(\"stargazer\")",
    "tags": [
      "LaTeX-tables",
      "regression-output",
      "academic-publishing",
      "economics",
      "HTML-tables"
    ],
    "best_for": "Quick, publication-ready LaTeX tables for economics journals with classic formatting",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "LaTeX-basics"
    ],
    "topic_tags": [
      "regression-tables",
      "academic-publishing",
      "LaTeX-output",
      "R-packages",
      "statistical-reporting"
    ],
    "summary": "stargazer is an R package that creates publication-ready regression tables and summary statistics in LaTeX, HTML, and ASCII formats. It's the go-to tool for economics researchers who need to present multiple regression models side-by-side with professional formatting. The package supports journal-specific styles and handles complex table layouts automatically.",
    "use_cases": [
      "Creating regression tables for economics papers with multiple model specifications to show robustness checks",
      "Generating formatted summary statistics tables for dataset descriptions in academic publications"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to make regression tables in R for papers",
      "stargazer package tutorial economics",
      "create LaTeX tables from R regression output",
      "format multiple regression models side by side"
    ]
  },
  {
    "name": "texreg",
    "description": "Converts coefficients, standard errors, significance stars, and fit statistics from statistical models into LaTeX, HTML, Word, or console output. Highly extensible with support for custom model types and confidence intervals.",
    "category": "Regression Output",
    "docs_url": "https://cran.r-project.org/web/packages/texreg/vignettes/texreg.pdf",
    "github_url": "https://github.com/leifeld/texreg",
    "url": "https://cran.r-project.org/package=texreg",
    "install": "install.packages(\"texreg\")",
    "tags": [
      "LaTeX-tables",
      "HTML-tables",
      "model-comparison",
      "Word-export",
      "extensible"
    ],
    "best_for": "Highly extensible regression tables with easy custom model type extensions, implementing Leifeld (2013, JSS)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-regression-models",
      "LaTeX-basics"
    ],
    "topic_tags": [
      "regression-tables",
      "LaTeX-output",
      "model-comparison",
      "academic-publishing",
      "R-package"
    ],
    "summary": "R package that automatically formats regression model outputs into publication-ready tables for LaTeX, HTML, Word, or console display. Essential tool for academics and data scientists who need to present multiple models side-by-side with proper formatting. Saves hours of manual table creation and ensures consistent, professional presentation of statistical results.",
    "use_cases": [
      "Creating comparison tables of multiple regression models for academic paper submission",
      "Generating HTML tables of A/B test results for stakeholder presentation"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "R package to export regression tables to LaTeX",
      "how to create publication ready regression tables",
      "compare multiple models in formatted table R",
      "texreg package tutorial regression output"
    ]
  },
  {
    "name": "here",
    "description": "Simple path construction from project root. Uses heuristics to find project root (RStudio, .git, .here) enabling portable paths that work across different machines and working directories.",
    "category": "Reproducibility",
    "docs_url": "https://here.r-lib.org/",
    "github_url": "https://github.com/r-lib/here",
    "url": "https://cran.r-project.org/package=here",
    "install": "install.packages(\"here\")",
    "tags": [
      "paths",
      "project-management",
      "reproducibility",
      "portability",
      "working-directory"
    ],
    "best_for": "Portable file paths from project root for reproducible scripts",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-basics",
      "file-systems"
    ],
    "topic_tags": [
      "file-paths",
      "project-structure",
      "R-package",
      "reproducible-research"
    ],
    "summary": "The 'here' package provides a simple solution for constructing file paths relative to your project root in R. It automatically detects project boundaries using common markers like .git folders or RStudio project files, eliminating hardcoded paths that break when code is shared or moved. Essential for creating reproducible analyses that work seamlessly across different machines and environments.",
    "use_cases": [
      "Loading data files in R scripts that need to work for collaborators with different folder structures",
      "Building reproducible research projects where analysis scripts reference data and output folders consistently"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for relative file paths",
      "how to make R scripts portable across machines",
      "here package R reproducible paths",
      "fix working directory issues R projects"
    ]
  },
  {
    "name": "renv",
    "description": "Project-local R dependency management. Creates reproducible environments by recording package versions in a lockfile, isolating project libraries, and enabling version restore.",
    "category": "Reproducibility",
    "docs_url": "https://rstudio.github.io/renv/",
    "github_url": "https://github.com/rstudio/renv",
    "url": "https://cran.r-project.org/package=renv",
    "install": "install.packages(\"renv\")",
    "tags": [
      "reproducibility",
      "package-management",
      "dependency-isolation",
      "lockfile",
      "environments"
    ],
    "best_for": "Project-local package management for reproducible R environments",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "package-installation",
      "version-control"
    ],
    "topic_tags": [
      "dependency-management",
      "reproducible-research",
      "R-environment",
      "project-isolation"
    ],
    "summary": "renv is an R package that creates isolated, reproducible project environments by managing package dependencies locally. It records exact package versions in a lockfile and allows teams to restore identical environments across different machines. Essential for ensuring research and analysis projects remain reproducible over time.",
    "use_cases": [
      "Sharing R analysis projects with collaborators while ensuring everyone uses identical package versions",
      "Maintaining multiple R projects with different package version requirements without conflicts"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to make R projects reproducible across different computers",
      "R package version management for data science projects",
      "renv vs packrat for R dependency management",
      "isolate R package versions by project"
    ]
  },
  {
    "name": "rmarkdown",
    "description": "Dynamic documents combining R code with Markdown text. Generates reproducible reports in HTML, PDF, Word, and slides. Foundation for literate programming and reproducible research in R.",
    "category": "Reproducibility",
    "docs_url": "https://rmarkdown.rstudio.com/",
    "github_url": "https://github.com/rstudio/rmarkdown",
    "url": "https://cran.r-project.org/package=rmarkdown",
    "install": "install.packages(\"rmarkdown\")",
    "tags": [
      "literate-programming",
      "reproducible-research",
      "dynamic-documents",
      "reporting",
      "Markdown"
    ],
    "best_for": "Literate programming and reproducible reports combining R code with Markdown",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-programming",
      "markdown-syntax"
    ],
    "topic_tags": [
      "reproducible-research",
      "report-generation",
      "literate-programming",
      "data-visualization",
      "scientific-writing"
    ],
    "summary": "R Markdown is a framework that combines R code with plain text to create dynamic, reproducible documents. It allows analysts to embed code, results, and visualizations directly into reports that can be exported to multiple formats. This makes it essential for creating transparent, updatable analysis reports in academic and industry settings.",
    "use_cases": [
      "Creating automated quarterly business reports that update with new data",
      "Writing reproducible research papers with embedded statistical analysis and plots"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "how to create reproducible reports in R",
      "R markdown tutorial for data analysis",
      "best tool for combining R code and documentation",
      "generate PDF reports from R analysis"
    ]
  },
  {
    "name": "targets",
    "description": "Make-like pipeline toolkit for R. Declares dependencies between pipeline steps, skips up-to-date targets, and supports parallel execution. Standard for reproducible research workflows.",
    "category": "Reproducibility",
    "docs_url": "https://docs.ropensci.org/targets/",
    "github_url": "https://github.com/ropensci/targets",
    "url": "https://cran.r-project.org/package=targets",
    "install": "install.packages(\"targets\")",
    "tags": [
      "pipelines",
      "reproducibility",
      "make",
      "dependency-tracking",
      "parallel"
    ],
    "best_for": "Make-like reproducible pipelines with automatic dependency tracking and parallel execution",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "command-line-basics",
      "data-analysis-workflows"
    ],
    "topic_tags": [
      "workflow-management",
      "reproducible-research",
      "pipeline-automation",
      "R-package",
      "dependency-tracking"
    ],
    "summary": "targets is R's premier pipeline management system that automatically tracks dependencies between analysis steps and only re-runs outdated components. It's become the gold standard for reproducible research workflows in R, replacing manual script orchestration with intelligent automation. Data scientists and researchers use it to build robust, scalable analysis pipelines that save time and prevent errors.",
    "use_cases": [
      "Managing a multi-step machine learning pipeline where data preprocessing, model training, and evaluation depend on each other",
      "Coordinating a research project with multiple data sources, statistical analyses, and report generation that need to update when underlying data changes"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R pipeline management tool",
      "how to automate R analysis workflows",
      "targets package tutorial",
      "reproducible research workflow R"
    ]
  },
  {
    "name": "clubSandwich",
    "description": "Provides cluster-robust variance estimators with small-sample corrections, including bias-reduced linearization (BRL/CR2). Includes functions for hypothesis testing with Satterthwaite degrees of freedom and Hotelling's T\u00b2 approximation\u2014essential when the number of clusters is small.",
    "category": "Robust Standard Errors",
    "docs_url": "https://jepusto.github.io/clubSandwich/",
    "github_url": "https://github.com/jepusto/clubSandwich",
    "url": "https://cran.r-project.org/package=clubSandwich",
    "install": "install.packages(\"clubSandwich\")",
    "tags": [
      "cluster-robust",
      "small-sample-corrections",
      "bias-reduced-linearization",
      "fixed-effects",
      "meta-analysis"
    ],
    "best_for": "Cluster-robust inference when the number of clusters is small, especially in panel data and meta-analysis, implementing Pustejovsky & Tipton (2018, JBES)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "fixed-effects-models",
      "clustered-data"
    ],
    "topic_tags": [
      "cluster-robust-standard-errors",
      "small-sample-corrections",
      "econometrics",
      "causal-inference",
      "r-package"
    ],
    "summary": "clubSandwich is an R package that provides cluster-robust variance estimators with small-sample corrections, particularly useful when you have few clusters. It implements advanced methods like bias-reduced linearization (CR2) and provides proper hypothesis testing procedures when standard asymptotic assumptions break down due to small cluster counts.",
    "use_cases": [
      "A/B testing analysis with only 8-12 treatment markets where standard cluster-robust SEs are unreliable",
      "Policy evaluation study with state-level treatment but only 20 states in the dataset"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "cluster robust standard errors with few clusters",
      "small sample correction for clustered data R",
      "bias reduced linearization clubSandwich",
      "CR2 variance estimator implementation"
    ]
  },
  {
    "name": "lmtest",
    "description": "Collection of tests for diagnostic checking in linear regression models. Provides the essential coeftest() function for testing coefficients with alternative variance-covariance matrices (pairs with sandwich), plus Breusch-Pagan, Durbin-Watson, and RESET tests.",
    "category": "Robust Standard Errors",
    "docs_url": "https://cran.r-project.org/web/packages/lmtest/vignettes/lmtest-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=lmtest",
    "install": "install.packages(\"lmtest\")",
    "tags": [
      "regression-diagnostics",
      "heteroskedasticity-test",
      "Breusch-Pagan",
      "Durbin-Watson",
      "serial-correlation"
    ],
    "best_for": "Testing coefficient significance with robust SEs and diagnostic tests for regression assumptions, implementing Zeileis & Hothorn (2002)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "sandwich-package"
    ],
    "topic_tags": [
      "regression-diagnostics",
      "robust-standard-errors",
      "heteroskedasticity",
      "econometrics",
      "R-package"
    ],
    "summary": "Essential R package for testing assumptions in linear regression models. Provides coeftest() for robust standard errors and diagnostic tests like Breusch-Pagan for heteroskedasticity and Durbin-Watson for serial correlation. Standard tool for econometric analysis and ensuring regression validity.",
    "use_cases": [
      "Testing whether your regression residuals have constant variance before trusting coefficient estimates",
      "Checking for serial correlation in time series regressions to validate model assumptions"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to test for heteroskedasticity in R",
      "coeftest function robust standard errors",
      "Breusch Pagan test R package",
      "regression diagnostic tests econometrics"
    ]
  },
  {
    "name": "sandwich",
    "description": "Object-oriented software for model-robust covariance matrix estimators including heteroscedasticity-consistent (HC0-HC5), heteroscedasticity- and autocorrelation-consistent (HAC/Newey-West), clustered, panel, and bootstrap covariances. Works with lm, glm, fixest, survival models, and many others.",
    "category": "Robust Standard Errors",
    "docs_url": "https://sandwich.R-Forge.R-project.org/",
    "github_url": null,
    "url": "https://cran.r-project.org/package=sandwich",
    "install": "install.packages(\"sandwich\")",
    "tags": [
      "robust-standard-errors",
      "heteroskedasticity-consistent",
      "HAC-covariance",
      "cluster-robust",
      "Newey-West"
    ],
    "best_for": "Computing robust standard errors for cross-sectional, time series, clustered, or panel data, implementing Zeileis (2004, 2006, 2020, JSS)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "R-programming",
      "statistical-inference"
    ],
    "topic_tags": [
      "robust-standard-errors",
      "heteroskedasticity",
      "clustered-standard-errors",
      "econometrics",
      "R-package"
    ],
    "summary": "The sandwich package provides robust covariance matrix estimators for regression models in R, addressing violations of standard assumptions like heteroskedasticity and autocorrelation. It's essential for econometric analysis where you need reliable standard errors and confidence intervals despite model misspecification. The package integrates seamlessly with popular modeling functions and offers various robust estimators including HC, HAC, and clustered variants.",
    "use_cases": [
      "Analyzing panel data with firm-level clustering to account for within-firm correlation in financial performance regressions",
      "Correcting for heteroskedasticity in cross-sectional wage equations where variance increases with education level"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to get robust standard errors in R regression",
      "sandwich package heteroskedasticity consistent standard errors",
      "clustered standard errors R implementation",
      "Newey West HAC covariance matrix R"
    ]
  },
  {
    "name": "(PySAL Core)",
    "description": "The broader PySAL ecosystem contains many tools for spatial data handling, weights, visualization, and analysis.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/",
    "github_url": "https://github.com/pysal/pysal",
    "url": "https://github.com/pysal/pysal",
    "install": "pip install pysal",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "geopandas",
      "spatial-autocorrelation"
    ],
    "topic_tags": [
      "spatial-analysis",
      "econometrics",
      "python-package",
      "geographic-data",
      "spatial-weights"
    ],
    "summary": "PySAL is Python's comprehensive spatial analysis library providing tools for exploratory spatial data analysis, spatial econometrics, and geographic modeling. It offers functionality for creating spatial weights matrices, testing for spatial autocorrelation, and running spatial regression models. The library is widely used by economists, geographers, and data scientists working with location-based data.",
    "use_cases": [
      "Analyzing housing price spillovers across neighborhoods using spatial lag models",
      "Testing for geographic clustering in economic outcomes like unemployment rates"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python library for spatial econometrics",
      "how to analyze geographic spillover effects",
      "PySAL spatial weights matrix tutorial",
      "spatial autocorrelation testing in python"
    ]
  },
  {
    "name": "Apache Sedona",
    "description": "Distributed spatial analytics engine (formerly GeoSpark) with spatial SQL, K-NN joins, and range queries for spatial econometrics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://sedona.apache.org/",
    "github_url": "https://github.com/apache/sedona",
    "url": "https://github.com/apache/sedona",
    "install": "pip install apache-sedona",
    "tags": [
      "spark",
      "spatial",
      "GIS",
      "distributed"
    ],
    "best_for": "Constructing spatial weight matrices and distance-based instruments at scale",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "apache-spark",
      "SQL-queries",
      "spatial-data-formats"
    ],
    "topic_tags": [
      "distributed-computing",
      "spatial-analysis",
      "geospatial-econometrics",
      "big-data",
      "spark-ecosystem"
    ],
    "summary": "Apache Sedona is a distributed spatial analytics engine built on Apache Spark that enables large-scale geospatial analysis through spatial SQL operations. It provides efficient spatial joins, range queries, and K-nearest neighbor operations for processing massive spatial datasets. Tech economists use it to analyze location-based economic phenomena like market concentration, urban development patterns, and regional economic spillovers.",
    "use_cases": [
      "Analyzing ride-sharing market competition by calculating spatial proximity between drivers and demand hotspots across metropolitan areas",
      "Measuring economic spillover effects by performing spatial joins between business locations and demographic data for regional policy research"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "distributed spatial analysis spark",
      "large scale GIS data processing",
      "spatial econometrics big data tools",
      "geospatial joins spark ecosystem"
    ]
  },
  {
    "name": "PySAL (spreg)",
    "description": "The spatial regression `spreg` module of PySAL. Implements spatial lag, error, IV models, and diagnostics.",
    "category": "Spatial Econometrics",
    "docs_url": "https://pysal.org/spreg/",
    "github_url": "https://github.com/pysal/spreg",
    "url": "https://github.com/pysal/spreg",
    "install": "pip install spreg",
    "tags": [
      "spatial",
      "geography"
    ],
    "best_for": "Geographic data, spatial autocorrelation, regional analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "geospatial-data"
    ],
    "topic_tags": [
      "spatial-regression",
      "econometrics",
      "geospatial-analysis",
      "python-package",
      "spatial-autocorrelation"
    ],
    "summary": "PySAL's spreg module provides Python implementations of spatial regression models that account for geographic relationships in data. It handles spatial lag models (where nearby observations influence each other), spatial error models (where errors are spatially correlated), and instrumental variable approaches. Essential for economists and data scientists analyzing location-based phenomena like housing prices, regional economic development, or disease spread.",
    "use_cases": [
      "Analyzing how housing prices in one neighborhood affect prices in adjacent areas",
      "Modeling regional unemployment rates while accounting for spillover effects between neighboring counties"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python package for spatial regression models",
      "how to handle spatial autocorrelation in econometrics",
      "PySAL spatial lag model implementation",
      "spatial error model python library"
    ]
  },
  {
    "name": "sf",
    "description": "The modern standard for spatial vector data in R, implementing Simple Features access (ISO 19125). Represents spatial data as data frames with geometry list-columns, enabling seamless tidyverse integration. Interfaces with GDAL (I/O), GEOS (geometry operations), PROJ (projections), and s2 (spherical geometry).",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/sf/",
    "github_url": "https://github.com/r-spatial/sf",
    "url": "https://cran.r-project.org/package=sf",
    "install": "install.packages(\"sf\")",
    "tags": [
      "simple-features",
      "spatial-data",
      "vector-data",
      "tidyverse",
      "GDAL-GEOS-PROJ"
    ],
    "best_for": "Reading, writing, manipulating, and visualizing spatial vector data; foundation for all spatial workflows, implementing Pebesma (2018)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-data-frames",
      "basic-GIS-concepts",
      "tidyverse-dplyr"
    ],
    "topic_tags": [
      "spatial-analysis",
      "GIS-data",
      "R-package",
      "vector-geometry",
      "geospatial"
    ],
    "summary": "The sf package is the modern R standard for working with spatial vector data like points, lines, and polygons. It integrates seamlessly with tidyverse workflows by storing spatial geometries as special columns in regular data frames. This makes spatial data manipulation as intuitive as working with regular tabular data while providing access to powerful geospatial operations.",
    "use_cases": [
      "Analyzing retail store locations and customer demographics within geographic boundaries",
      "Processing transportation networks and calculating route distances for logistics optimization"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for spatial data analysis",
      "how to work with shapefiles in R",
      "modern R spatial data tidyverse",
      "sf package tutorial geospatial analysis"
    ]
  },
  {
    "name": "spatialreg",
    "description": "Comprehensive package for spatial regression model estimation, split from spdep in 2019. Provides maximum likelihood, two-stage least squares, and GMM estimation for spatial lag (SAR), spatial error (SEM), and combined (SARAR/SAC) models, plus Spatial Durbin and SLX variants with impact calculations.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spatialreg/",
    "github_url": "https://github.com/r-spatial/spatialreg",
    "url": "https://cran.r-project.org/package=spatialreg",
    "install": "install.packages(\"spatialreg\")",
    "tags": [
      "spatial-regression",
      "maximum-likelihood",
      "spatial-lag",
      "spatial-error",
      "GMM"
    ],
    "best_for": "Estimating cross-sectional spatial regression models (SAR, SEM, SAC, SDM) with maximum likelihood or GMM, implementing Bivand & Piras (2015)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "spatial-autocorrelation",
      "linear-regression",
      "R-programming"
    ],
    "topic_tags": [
      "spatial-regression",
      "spatial-econometrics",
      "maximum-likelihood",
      "spatial-autocorrelation",
      "R-package"
    ],
    "summary": "The spatialreg package provides comprehensive tools for estimating spatial regression models in R, handling spatial dependencies in cross-sectional data. It offers multiple estimation methods including maximum likelihood and GMM for spatial lag, spatial error, and combined models. Essential for economists and data scientists working with geographically clustered data where standard regression assumptions fail.",
    "use_cases": [
      "analyzing house prices with neighborhood spillover effects",
      "studying regional economic growth with spatial interdependencies"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "spatial regression R package",
      "how to handle spatial autocorrelation in regression",
      "spatialreg vs spdep differences",
      "spatial lag model estimation R"
    ]
  },
  {
    "name": "spdep",
    "description": "The foundational R package for spatial weights matrix creation and spatial autocorrelation testing. Provides functions for creating spatial weights from polygon contiguities and point patterns, computing global statistics (Moran's I, Geary's C), local indicators (LISA), and Lagrange multiplier tests.",
    "category": "Spatial Econometrics",
    "docs_url": "https://r-spatial.github.io/spdep/",
    "github_url": "https://github.com/r-spatial/spdep",
    "url": "https://cran.r-project.org/package=spdep",
    "install": "install.packages(\"spdep\")",
    "tags": [
      "spatial-weights",
      "autocorrelation",
      "morans-i",
      "neighborhood-analysis",
      "spatial-statistics"
    ],
    "best_for": "Creating spatial weights matrices and testing for spatial autocorrelation in cross-sectional data, implementing Bivand & Wong (2018)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "spatial-data-structures",
      "regression-analysis"
    ],
    "topic_tags": [
      "spatial-weights",
      "spatial-autocorrelation",
      "morans-i",
      "neighborhood-analysis",
      "R-package"
    ],
    "summary": "The foundational R package for spatial econometrics that enables creation of spatial weights matrices and testing for spatial autocorrelation. Provides essential functions for defining neighborhood structures, computing Moran's I and Geary's C statistics, and running diagnostic tests for spatial dependence. Essential toolkit for any spatial analysis workflow in economics or regional science.",
    "use_cases": [
      "Testing whether house prices show spatial clustering patterns across neighborhoods",
      "Analyzing regional unemployment spillovers and economic contagion effects"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to create spatial weights matrix in R",
      "testing spatial autocorrelation Moran's I",
      "R package for neighborhood analysis",
      "spatial dependence diagnostic tests"
    ]
  },
  {
    "name": "splm",
    "description": "Maximum likelihood and GMM estimation for spatial panel data models. Implements fixed and random effects specifications with spatial lag and/or spatial error components, including the Kapoor-Kelejian-Prucha (2007) GM estimator. Provides diagnostic tests for spatial autocorrelation in panel settings.",
    "category": "Spatial Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/splm/splm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=splm",
    "install": "install.packages(\"splm\")",
    "tags": [
      "spatial-panel",
      "panel-data",
      "fixed-effects",
      "random-effects",
      "GMM"
    ],
    "best_for": "Estimating spatial econometric models with panel (longitudinal) data structures, implementing Millo & Piras (2012)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "panel-data-methods",
      "spatial-econometrics",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "spatial-panel",
      "econometric-methods",
      "maximum-likelihood",
      "GMM-estimation",
      "spatial-autocorrelation"
    ],
    "summary": "The splm package provides maximum likelihood and GMM estimation methods specifically designed for spatial panel data models. It handles complex econometric specifications including fixed and random effects with spatial lag and error components, implementing advanced estimators like Kapoor-Kelejian-Prucha. Essential for researchers analyzing panel data where observations exhibit spatial dependence across geographic units over time.",
    "use_cases": [
      "Analyzing regional economic growth patterns across states/countries over multiple years with spatial spillover effects",
      "Estimating housing price dynamics across metropolitan areas accounting for neighborhood spatial correlation in panel data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "spatial panel data estimation in R",
      "Kapoor Kelejian Prucha estimator implementation",
      "GMM estimation for spatial econometrics",
      "spatial autocorrelation tests panel data"
    ]
  },
  {
    "name": "Awesome Quant",
    "description": "Curated list of quantitative finance libraries and resources (many statistical/TS tools overlap with econometrics).",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://wilsonfreitas.github.io/awesome-quant/",
    "github_url": null,
    "url": "https://wilsonfreitas.github.io/awesome-quant/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "statistical-inference",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "quantitative-finance",
      "time-series",
      "statistical-libraries",
      "bootstrap",
      "econometrics"
    ],
    "summary": "A comprehensive collection of Python, R, and other libraries for quantitative finance, including tools for time series analysis, statistical modeling, and bootstrapping methods. Many of these libraries provide robust standard error calculations and resampling techniques that are directly applicable to econometric analysis. This curated list serves as a discovery tool for finding specialized packages for financial econometrics and statistical inference.",
    "use_cases": [
      "Finding Python libraries for calculating robust standard errors in financial time series models",
      "Discovering R packages for bootstrap methods when analyzing market microstructure data"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "python libraries for quantitative finance bootstrap methods",
      "curated list of econometrics packages for finance",
      "best tools for financial time series standard errors",
      "quantitative finance libraries with statistical inference"
    ]
  },
  {
    "name": "Beyond Jupyter (TransferLab)",
    "description": "Teaches software design principles for ML\u2014modularity, abstraction, and reproducibility\u2014going beyond ad hoc Jupyter workflows. Focus on maintainable, production-quality ML code.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://transferlab.ai/trainings/beyond-jupyter/",
    "github_url": null,
    "url": "https://transferlab.ai/trainings/beyond-jupyter/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "jupyter-notebooks",
      "python-scikit-learn",
      "git-version-control"
    ],
    "topic_tags": [
      "software-engineering",
      "ml-production",
      "code-organization",
      "reproducible-research",
      "python-packaging"
    ],
    "summary": "A comprehensive guide to transforming messy Jupyter notebook workflows into well-structured, maintainable ML codebases. Covers essential software engineering practices like modular design, abstraction layers, and automated testing for machine learning projects. Essential for data scientists transitioning from exploratory analysis to production-ready ML systems.",
    "use_cases": [
      "Refactoring a prototype ML model from notebooks into a deployable package",
      "Setting up reproducible experiment tracking and model versioning for a research team"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to organize ML code beyond Jupyter notebooks",
      "Best practices for production machine learning code",
      "Software engineering principles for data science",
      "How to make ML experiments reproducible and maintainable"
    ]
  },
  {
    "name": "Causal Inference for the Brave and True",
    "description": "Modern introduction to causal inference methods (DiD, IV, RDD, Synth, ML-based) with Python code examples.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://matheusfacure.github.io/python-causality-handbook/",
    "github_url": null,
    "url": "https://matheusfacure.github.io/python-causality-handbook/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "linear-regression",
      "hypothesis-testing"
    ],
    "topic_tags": [
      "causal-inference",
      "difference-in-differences",
      "instrumental-variables",
      "regression-discontinuity",
      "python-implementation"
    ],
    "summary": "A comprehensive guide to modern causal inference methods including Difference-in-Differences, Instrumental Variables, Regression Discontinuity Design, and Synthetic Controls with hands-on Python implementations. Designed for practitioners who want to move beyond correlational analysis to establish causal relationships in observational data. Combines theoretical foundations with practical coding examples for real-world applications.",
    "use_cases": [
      "Evaluating the causal impact of a product feature launch on user engagement using natural experiments",
      "Measuring the effectiveness of a marketing campaign while controlling for selection bias and confounding factors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "causal inference tutorial with Python examples",
      "how to implement difference in differences analysis",
      "instrumental variables regression discontinuity guide",
      "causal analysis methods for observational data"
    ]
  },
  {
    "name": "Coding for Economists",
    "description": "Practical guide by A. Turrell on using Python for modern econometric research, data analysis, and workflows.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://aeturrell.github.io/coding-for-economists/",
    "github_url": null,
    "url": "https://aeturrell.github.io/coding-for-economists/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "econometric-fundamentals"
    ],
    "topic_tags": [
      "python-econometrics",
      "bootstrap-methods",
      "standard-errors",
      "econometric-workflows",
      "data-analysis"
    ],
    "summary": "A comprehensive guide by Arthur Turrell that teaches economists how to use Python for econometric analysis, focusing on practical implementation of standard error calculations and bootstrap methods. The resource bridges the gap between theoretical econometrics and modern programming practices, making it ideal for researchers transitioning from Stata or R to Python workflows.",
    "use_cases": [
      "Implementing robust standard errors in Python for regression analysis when transitioning from Stata",
      "Setting up bootstrap confidence intervals for non-standard econometric estimators in research projects"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "How to calculate robust standard errors in Python for economics",
      "Bootstrap methods implementation guide for economists",
      "Python econometrics tutorial for beginners",
      "Coding workflows for econometric research in Python"
    ]
  },
  {
    "name": "Deep Learning Specialization (Coursera)",
    "description": "Intermediate 5-course series by Andrew Ng covering deep neural networks, CNNs, RNNs, transformers, and real-world DL applications using TensorFlow.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.coursera.org/specializations/deep-learning",
    "github_url": null,
    "url": "https://www.coursera.org/specializations/deep-learning",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors",
      "machine learning"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "linear-algebra",
      "calculus"
    ],
    "topic_tags": [
      "deep-learning",
      "neural-networks",
      "tensorflow",
      "computer-vision",
      "nlp"
    ],
    "summary": "Comprehensive 5-course specialization by Andrew Ng covering foundational and advanced deep learning concepts including CNNs, RNNs, and transformers. Provides hands-on experience with TensorFlow for building real-world deep learning applications. Ideal for practitioners transitioning from traditional ML to deep learning methods.",
    "use_cases": [
      "Building image classification models for product recommendation systems",
      "Implementing sequence-to-sequence models for natural language processing tasks"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Andrew Ng deep learning course TensorFlow",
      "best deep learning specialization for data scientists",
      "CNN RNN transformer course structured learning",
      "intermediate deep learning with practical applications"
    ]
  },
  {
    "name": "Machine Learning Specialization (Coursera)",
    "description": "Beginner-friendly 3-course series by Andrew Ng covering core ML methods (regression, classification, clustering, trees, NN) with hands-on projects.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.coursera.org/specializations/machine-learning-introduction/",
    "github_url": null,
    "url": "https://www.coursera.org/specializations/machine-learning-introduction/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "linear-algebra"
    ],
    "topic_tags": [
      "machine-learning",
      "supervised-learning",
      "neural-networks",
      "coursera",
      "andrew-ng"
    ],
    "summary": "Andrew Ng's comprehensive 3-course Machine Learning Specialization on Coursera provides a beginner-friendly introduction to core ML concepts including regression, classification, clustering, and neural networks. The program combines theoretical foundations with hands-on Python projects using popular libraries. Ideal for those starting their ML journey who want structured learning with practical implementation experience.",
    "use_cases": [
      "Building first predictive model for business metrics",
      "Transitioning from traditional analytics to machine learning methods"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "Andrew Ng machine learning course",
      "beginner ML specialization with hands-on projects",
      "coursera machine learning certification",
      "intro to neural networks and classification"
    ]
  },
  {
    "name": "Python for Econometrics",
    "description": "Comprehensive intro notes by Kevin Sheppard covering Python basics, core libraries, and econometrics applications.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2023.pdf",
    "github_url": null,
    "url": "https://www.kevinsheppard.com/files/teaching/python/notes/python_introduction_2023.pdf",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-python-syntax",
      "linear-regression"
    ],
    "topic_tags": [
      "python-econometrics",
      "bootstrap-methods",
      "standard-errors",
      "intro-tutorial",
      "kevin-sheppard"
    ],
    "summary": "Kevin Sheppard's comprehensive introduction to using Python for econometric analysis, covering essential libraries like NumPy, Pandas, and statsmodels. The notes bridge basic Python programming with econometric applications, focusing on practical implementation of standard error calculations and bootstrapping techniques. Ideal for economists transitioning from R/Stata to Python or data scientists learning econometric methods.",
    "use_cases": [
      "Learning to implement robust standard errors and clustered standard errors in Python instead of Stata",
      "Building bootstrap confidence intervals for econometric models using Python libraries"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python econometrics tutorial beginner",
      "how to calculate robust standard errors python",
      "bootstrap methods python economics",
      "kevin sheppard python econometrics notes"
    ]
  },
  {
    "name": "QuantEcon Lectures",
    "description": "High-quality lecture series on quantitative economic modeling, computational tools, and economics using Python/Julia.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://quantecon.org/lectures/",
    "github_url": null,
    "url": "https://quantecon.org/lectures/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "basic-statistics",
      "linear-regression"
    ],
    "topic_tags": [
      "bootstrap",
      "standard-errors",
      "quantitative-economics",
      "computational-methods",
      "python-lectures"
    ],
    "summary": "Comprehensive lecture series covering quantitative economic modeling with focus on computational methods for statistical inference. Teaches bootstrap methods and standard error calculation techniques using Python and Julia for economic analysis. Designed for economists and data scientists who need rigorous statistical foundations for empirical work.",
    "use_cases": [
      "Calculating confidence intervals for regression coefficients in economic models",
      "Implementing bootstrap procedures to assess uncertainty in policy impact estimates"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to calculate bootstrap standard errors in economics",
      "quantitative economics lectures python",
      "bootstrap methods for economic modeling",
      "standard error computation tutorial economics"
    ]
  },
  {
    "name": "SciPy Bootstrap",
    "description": "(`scipy.stats.bootstrap`) Computes bootstrap confidence intervals for various statistics using percentile, BCa methods.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://github.com/scipy/scipy",
    "install": "pip install scipy",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-scipy",
      "numpy-arrays",
      "confidence-intervals"
    ],
    "topic_tags": [
      "bootstrap",
      "confidence-intervals",
      "statistical-inference",
      "scipy",
      "uncertainty-quantification"
    ],
    "summary": "SciPy's bootstrap function provides a simple interface for computing bootstrap confidence intervals using resampling methods. It supports multiple confidence interval types including percentile and bias-corrected accelerated (BCa) methods. This is essential for quantifying uncertainty when analytical standard errors are difficult to derive.",
    "use_cases": [
      "Computing confidence intervals for complex statistics like median or correlation coefficients where analytical formulas are unavailable",
      "Estimating uncertainty in A/B test metrics when sample distributions are non-normal or when dealing with ratio metrics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to compute bootstrap confidence intervals in python",
      "scipy bootstrap function tutorial",
      "bootstrap standard errors for median",
      "BCa bootstrap confidence intervals scipy"
    ]
  },
  {
    "name": "Stargazer",
    "description": "Python port of R's stargazer for creating publication-quality regression tables (HTML, LaTeX) from `statsmodels` & `linearmodels` results.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": null,
    "github_url": "https://github.com/StatsReporting/stargazer",
    "url": "https://github.com/StatsReporting/stargazer",
    "install": "pip install stargazer",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-statsmodels",
      "regression-analysis",
      "latex-basics"
    ],
    "topic_tags": [
      "regression-tables",
      "publication-formatting",
      "statsmodels",
      "latex-output",
      "research-reporting"
    ],
    "summary": "Stargazer is a Python package that creates professional, publication-ready regression tables from statsmodels and linearmodels results. It formats output for HTML and LaTeX, making it easy to include statistical results in academic papers and reports. The package is particularly valuable for researchers who need clean, standardized table formatting without manual work.",
    "use_cases": [
      "Creating formatted regression tables for academic paper submission with multiple model comparisons",
      "Generating HTML tables of econometric results for internal research reports and presentations"
    ],
    "audience": [
      "Early-PhD",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "python package for regression tables like R stargazer",
      "how to format statsmodels output for latex",
      "publication quality regression tables python",
      "convert statsmodels results to formatted table"
    ]
  },
  {
    "name": "The Missing Semester of Your CS Education (MIT)",
    "description": "Teaches essential developer tools often skipped in formal education\u2014command line, Git, Vim, scripting, debugging, etc.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://missing.csail.mit.edu/",
    "github_url": null,
    "url": "https://missing.csail.mit.edu/",
    "install": "",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-programming",
      "command-line-basics"
    ],
    "topic_tags": [
      "developer-tools",
      "command-line",
      "git",
      "vim",
      "debugging"
    ],
    "summary": "MIT's comprehensive course covering essential developer tools and workflows that are typically missing from computer science curricula. Teaches practical skills like shell scripting, version control, text editors, and debugging that are crucial for day-to-day programming work. Perfect foundation for anyone entering tech roles who needs to master the development environment.",
    "use_cases": [
      "New data scientist needs to learn Git for collaboration and version control of analysis code",
      "PhD student wants to improve productivity with command-line tools and automated workflows for research"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "essential developer tools for data scientists",
      "learn command line and git basics",
      "missing programming skills not taught in school",
      "developer workflow tools tutorial"
    ]
  },
  {
    "name": "wildboottest",
    "description": "Fast implementation of various wild cluster bootstrap algorithms (WCR, WCU) for robust inference, especially with few clusters.",
    "category": "Standard Errors, Bootstrapping & Reporting",
    "docs_url": "https://py-econometrics.github.io/wildboottest/",
    "github_url": "https://github.com/py-econometrics/wildboottest",
    "url": "https://github.com/py-econometrics/wildboottest",
    "install": "pip install wildboottest",
    "tags": [
      "bootstrap",
      "standard errors"
    ],
    "best_for": "Robust inference, clustered SEs, result presentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "clustered-standard-errors",
      "bootstrap-methods",
      "python-regression"
    ],
    "topic_tags": [
      "wild-bootstrap",
      "clustered-inference",
      "robust-standard-errors",
      "few-clusters",
      "python-package"
    ],
    "summary": "A Python package implementing fast wild cluster bootstrap algorithms for robust statistical inference when you have few clusters. It's particularly useful for economists and data scientists who need reliable confidence intervals and p-values in clustered data settings where traditional asymptotic methods fail. The package offers both restricted (WCR) and unrestricted (WCU) wild bootstrap variants with computational optimizations.",
    "use_cases": [
      "A/B testing with few treatment groups where you need robust confidence intervals for treatment effects",
      "Evaluating policy interventions across states/regions when you have limited geographic clusters"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "wild bootstrap for few clusters python",
      "robust standard errors clustered data bootstrap",
      "wildboottest package implementation guide",
      "bootstrap inference few clusters econometrics"
    ]
  },
  {
    "name": "FilterPy",
    "description": "Focuses on Kalman filters (standard, EKF, UKF) and smoothers with a clear, pedagogical implementation style.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://filterpy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/rlabbe/filterpy",
    "url": "https://github.com/rlabbe/filterpy",
    "install": "pip install filterpy",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "bayesian-inference"
    ],
    "topic_tags": [
      "kalman-filter",
      "time-series",
      "state-estimation",
      "python-package",
      "signal-processing"
    ],
    "summary": "FilterPy is a Python library that provides clear, educational implementations of Kalman filters and related state estimation algorithms. It's designed with a pedagogical approach, making complex filtering concepts accessible through well-documented code and examples. The library covers standard Kalman filters, Extended Kalman Filters (EKF), Unscented Kalman Filters (UKF), and various smoothing algorithms.",
    "use_cases": [
      "tracking user engagement states over time in product analytics",
      "estimating hidden volatility regimes in financial time series data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python kalman filter implementation tutorial",
      "how to estimate hidden states in time series",
      "filterpy vs other state space modeling libraries",
      "kalman filter for volatility modeling python"
    ]
  },
  {
    "name": "Metran",
    "description": "Specialized package for estimating Dynamic Factor Models (DFM) using state-space methods and Kalman filtering.",
    "category": "State Space & Volatility Models",
    "docs_url": null,
    "github_url": "https://github.com/pastas/metran",
    "url": "https://github.com/pastas/metran",
    "install": "pip install metran",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "kalman-filtering",
      "time-series-analysis",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "dynamic-factor-models",
      "kalman-filtering",
      "state-space-models",
      "time-series",
      "econometrics"
    ],
    "summary": "Metran is a specialized package for estimating Dynamic Factor Models (DFM) that capture common trends across multiple time series using state-space representation and Kalman filtering. It's primarily used by econometricians and quantitative researchers working with high-dimensional time series data. The package enables decomposition of observed variables into common factors and idiosyncratic components for forecasting and dimensionality reduction.",
    "use_cases": [
      "Nowcasting GDP using mixed-frequency economic indicators",
      "Risk factor modeling in finance with multiple asset return series"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic factor model python package",
      "kalman filter state space econometrics",
      "DFM estimation mixed frequency data",
      "nowcasting with dynamic factors"
    ]
  },
  {
    "name": "PyKalman",
    "description": "Implements Kalman filter, smoother, and EM algorithm for parameter estimation, including support for missing values and UKF.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://pypi.org/project/pykalman/",
    "github_url": "https://github.com/pykalman/pykalman",
    "url": "https://github.com/pykalman/pykalman",
    "install": "pip install pykalman",
    "tags": [
      "volatility",
      "state space"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "time-series-analysis"
    ],
    "topic_tags": [
      "kalman-filter",
      "state-space-models",
      "time-series",
      "parameter-estimation",
      "python-package"
    ],
    "summary": "PyKalman is a Python implementation of Kalman filtering algorithms for state space modeling and time series analysis. It provides tools for filtering, smoothing, and parameter estimation using the EM algorithm, with robust handling of missing data. The package is particularly useful for economists and data scientists working with dynamic models and noisy time series data.",
    "use_cases": [
      "Tracking dynamic pricing models with noisy market data and missing observations",
      "Estimating hidden economic indicators like consumer sentiment from observable market variables"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python kalman filter implementation missing data",
      "state space model parameter estimation EM algorithm",
      "kalman smoother python package economics",
      "unscented kalman filter python library"
    ]
  },
  {
    "name": "PyMC Statespace",
    "description": "(See Bayesian) Bayesian state-space modeling using PyMC, integrating Kalman filtering within MCMC for parameter estimation.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://pymc-statespace.readthedocs.io/en/latest/",
    "github_url": "https://github.com/pymc-devs/pymc-statespace",
    "url": "https://github.com/pymc-devs/pymc-statespace",
    "install": "pip install pymc-statespace",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "PyMC-basics",
      "Kalman-filtering",
      "MCMC-methods"
    ],
    "topic_tags": [
      "state-space-models",
      "Bayesian-inference",
      "time-series",
      "volatility-modeling",
      "MCMC"
    ],
    "summary": "PyMC Statespace combines Bayesian inference with state-space modeling by integrating Kalman filtering within MCMC parameter estimation. It enables researchers to model latent states and time-varying parameters with full uncertainty quantification. This package is particularly valuable for economists and data scientists working with dynamic systems where traditional frequentist approaches fall short.",
    "use_cases": [
      "Modeling time-varying volatility in financial returns with parameter uncertainty",
      "Estimating latent economic states like business cycle phases with Bayesian credible intervals"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Bayesian state space modeling in Python",
      "PyMC Kalman filter MCMC",
      "time varying parameter estimation Bayesian",
      "volatility modeling with uncertainty quantification"
    ]
  },
  {
    "name": "stochvol",
    "description": "Efficient Bayesian estimation of stochastic volatility (SV) models using MCMC.",
    "category": "State Space & Volatility Models",
    "docs_url": "https://stochvol.readthedocs.io/en/latest/",
    "github_url": "https://github.com/rektory/stochvol",
    "url": "https://github.com/rektory/stochvol",
    "install": "pip install stochvol",
    "tags": [
      "volatility",
      "state space",
      "Bayesian"
    ],
    "best_for": "GARCH, stochastic volatility, Kalman filtering",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "MCMC-methods",
      "bayesian-inference",
      "time-series-analysis"
    ],
    "topic_tags": [
      "stochastic-volatility",
      "bayesian-estimation",
      "financial-econometrics",
      "time-series",
      "MCMC"
    ],
    "summary": "The stochvol package provides efficient Bayesian estimation of stochastic volatility models using Markov Chain Monte Carlo methods. It's designed for researchers and practitioners working with financial time series who need to model time-varying volatility patterns. The package offers fast implementation of various SV model specifications with comprehensive diagnostic tools.",
    "use_cases": [
      "Modeling volatility clustering in stock returns for risk management applications",
      "Estimating time-varying volatility parameters for option pricing and derivative valuation"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "bayesian stochastic volatility estimation R",
      "MCMC volatility modeling package",
      "stochastic volatility models implementation",
      "time varying volatility bayesian analysis"
    ]
  },
  {
    "name": "HypoRS",
    "description": "Hypothesis testing library for Rust with T-tests, Z-tests, ANOVA, Chi-square, designed to work seamlessly with Polars DataFrames.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lib.rs/crates/hypors",
    "github_url": "https://github.com/astronights/hypors",
    "url": "https://crates.io/crates/hypors",
    "install": "cargo add hypors",
    "tags": [
      "rust",
      "hypothesis testing",
      "t-test",
      "ANOVA",
      "polars"
    ],
    "best_for": "Statistical hypothesis testing with Polars integration",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "statistical-hypothesis-testing",
      "polars-dataframes"
    ],
    "topic_tags": [
      "hypothesis-testing",
      "rust-statistics",
      "statistical-inference",
      "polars-integration",
      "t-tests"
    ],
    "summary": "HypoRS is a comprehensive hypothesis testing library for Rust that implements classical statistical tests including T-tests, Z-tests, ANOVA, and Chi-square tests. It's designed with seamless Polars DataFrame integration, making it ideal for data scientists working in the Rust ecosystem. The library provides a modern, performant alternative to Python's scipy.stats for statistical inference tasks.",
    "use_cases": [
      "A/B testing analysis on user engagement metrics stored in Polars DataFrames",
      "Quality control testing in manufacturing data pipelines built with Rust"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust library for statistical hypothesis testing",
      "how to do t-tests in rust with polars",
      "statistical inference library rust dataframes",
      "rust alternative to scipy stats hypothesis testing"
    ]
  },
  {
    "name": "Pingouin",
    "description": "User-friendly interface for common statistical tests (ANOVA, ANCOVA, t-tests, correlations, chi\u00b2, reliability) built on pandas & scipy.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pingouin-stats.org/",
    "github_url": "https://github.com/raphaelvallat/pingouin",
    "url": "https://github.com/raphaelvallat/pingouin",
    "install": "pip install pingouin",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scipy-stats",
      "statistical-hypothesis-testing"
    ],
    "topic_tags": [
      "statistical-testing",
      "python-package",
      "anova",
      "correlation-analysis",
      "t-tests"
    ],
    "summary": "Pingouin is a beginner-friendly Python package that simplifies running common statistical tests like ANOVA, t-tests, and correlations with pandas-style syntax. It provides clean, interpretable outputs for hypothesis testing without requiring deep statistical programming knowledge. Perfect for data scientists who need reliable statistical inference tools with minimal setup complexity.",
    "use_cases": [
      "A/B testing conversion rates between different website variants using t-tests and effect size calculations",
      "Analyzing survey data to test correlations between user satisfaction scores and product features"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "easy to use python package for statistical tests",
      "beginner friendly ANOVA in python",
      "simple correlation analysis python library",
      "pandas compatible statistical testing package"
    ]
  },
  {
    "name": "PyWhy-Stats",
    "description": "Part of the PyWhy ecosystem providing statistical methods specifically for causal applications, including various independence tests and power-divergence methods.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://pywhy-stats.readthedocs.io/",
    "github_url": "https://github.com/py-why/pywhy-stats",
    "url": "https://github.com/py-why/pywhy-stats",
    "install": "pip install pywhy-stats",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scipy-stats",
      "causal-inference-basics"
    ],
    "topic_tags": [
      "independence-tests",
      "causal-inference",
      "statistical-testing",
      "python-package",
      "hypothesis-testing"
    ],
    "summary": "PyWhy-Stats is a specialized statistical package within the PyWhy ecosystem that provides independence tests and statistical methods tailored for causal inference applications. It offers implementations of various hypothesis tests and power-divergence methods specifically designed to support causal analysis workflows. Data scientists and researchers use it to test assumptions and validate statistical relationships in causal modeling pipelines.",
    "use_cases": [
      "Testing conditional independence assumptions before running a causal inference model",
      "Validating instrumental variable assumptions by testing independence between instruments and confounders"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "independence tests for causal inference python",
      "PyWhy statistical testing package",
      "how to test causal assumptions statistically",
      "python library for causal inference hypothesis testing"
    ]
  },
  {
    "name": "Scipy.stats",
    "description": "Foundational module within SciPy for a wide range of statistical functions, distributions, and hypothesis tests (t-tests, ANOVA, chi\u00b2, KS, etc.).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://docs.scipy.org/doc/scipy/reference/stats.html",
    "github_url": "https://github.com/scipy/scipy",
    "url": "https://github.com/scipy/scipy",
    "install": "pip install scipy",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-basics",
      "numpy-arrays",
      "descriptive-statistics"
    ],
    "topic_tags": [
      "statistical-testing",
      "probability-distributions",
      "python-statistics",
      "hypothesis-testing",
      "significance-testing"
    ],
    "summary": "Scipy.stats is the go-to Python module for statistical analysis, providing implementations of dozens of probability distributions and essential hypothesis tests. It's the foundational toolkit for data scientists and researchers who need to perform statistical inference, from basic t-tests to complex goodness-of-fit tests. The module handles both the computational heavy-lifting and provides intuitive interfaces for common statistical workflows.",
    "use_cases": [
      "A/B testing analyst comparing conversion rates between two app versions using t-tests or chi-square tests",
      "Research scientist validating that their experimental data follows a normal distribution before applying parametric methods"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python statistical tests package",
      "how to do t-test in python",
      "scipy stats tutorial for beginners",
      "python hypothesis testing library"
    ]
  },
  {
    "name": "Statrs",
    "description": "Comprehensive statistical distributions for Rust (Normal, T, Gamma, etc.) with PDF, CDF, quantile functions\u2014the scipy.stats equivalent.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://docs.rs/statrs",
    "github_url": "https://github.com/statrs-dev/statrs",
    "url": "https://crates.io/crates/statrs",
    "install": "cargo add statrs",
    "tags": [
      "rust",
      "statistics",
      "distributions",
      "probability"
    ],
    "best_for": "Probability distributions and basic statistics in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "probability-distributions",
      "scipy-stats"
    ],
    "topic_tags": [
      "statistical-distributions",
      "rust-statistics",
      "probability-functions",
      "quantile-methods"
    ],
    "summary": "Statrs is a Rust library providing comprehensive statistical distributions with PDF, CDF, and quantile functions, serving as the Rust equivalent to Python's scipy.stats. It's designed for data scientists and researchers who need high-performance statistical computations in Rust applications. The library supports major distributions like Normal, T, Gamma, and others with reliable numerical implementations.",
    "use_cases": [
      "Building high-performance statistical applications in Rust that require probability calculations",
      "Implementing Monte Carlo simulations or statistical models where Python's performance is insufficient"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust equivalent of scipy stats distributions",
      "statistical distributions library for rust",
      "how to compute pdf cdf in rust",
      "rust probability distributions package"
    ]
  },
  {
    "name": "expectation",
    "description": "E-values and game-theoretic probability for sequential testing. Enables early signal detection with proper error control.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/jakorostami/expectation",
    "url": "https://pypi.org/project/expectation/",
    "install": "pip install expectation",
    "tags": [
      "sequential testing",
      "e-values",
      "hypothesis testing"
    ],
    "best_for": "E-value based sequential hypothesis testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scipy",
      "hypothesis-testing",
      "sequential-analysis"
    ],
    "topic_tags": [
      "e-values",
      "sequential-testing",
      "game-theoretic-probability",
      "early-stopping",
      "statistical-inference"
    ],
    "summary": "E-values provide a game-theoretic approach to hypothesis testing that enables continuous monitoring and early stopping with guaranteed error control. This package implements sequential testing methods that can detect signals as soon as they emerge, without the multiple testing penalties of traditional p-values. It's particularly valuable for A/B testing and clinical trials where early decision-making is critical.",
    "use_cases": [
      "A/B testing with early stopping when treatment effect is detected",
      "Clinical trial monitoring for safety signals with continuous data collection"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "sequential testing with early stopping control",
      "e-values vs p-values for A/B testing",
      "game theoretic probability python package",
      "continuous monitoring hypothesis testing methods"
    ]
  },
  {
    "name": "gcimpute",
    "description": "Gaussian copula imputation for mixed variable types with streaming capability (Journal of Statistical Software 2024).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/udellgroup/gcimpute",
    "url": "https://github.com/udellgroup/gcimpute",
    "install": "pip install gcimpute",
    "tags": [
      "missing data",
      "imputation"
    ],
    "best_for": "Mixed-type missing data imputation with copulas",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "missing-data-theory",
      "copula-models"
    ],
    "topic_tags": [
      "gaussian-copula",
      "missing-data",
      "imputation",
      "mixed-data-types",
      "streaming-data"
    ],
    "summary": "A Python package implementing Gaussian copula-based imputation for datasets with mixed variable types (continuous, categorical, ordinal). It handles missing data by modeling the dependence structure between variables using copulas, with streaming capability for large datasets that don't fit in memory.",
    "use_cases": [
      "Imputing missing values in customer datasets with mixed demographics and behavioral features",
      "Preprocessing large streaming datasets with missing entries for real-time ML pipelines"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "gaussian copula imputation mixed data types",
      "streaming missing data imputation python",
      "copula based imputation package",
      "mixed variable imputation large datasets"
    ]
  },
  {
    "name": "hypothetical",
    "description": "Library focused on hypothesis testing: ANOVA/MANOVA, t-tests, chi-square, Fisher's exact, nonparametric tests (Mann-Whitney, Kruskal-Wallis, etc.).",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/aschleg/hypothetical",
    "url": "https://github.com/aschleg/hypothetical",
    "install": "pip install hypothetical",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "descriptive-statistics",
      "p-values"
    ],
    "topic_tags": [
      "hypothesis-testing",
      "statistical-tests",
      "anova",
      "t-tests",
      "nonparametric"
    ],
    "summary": "A Python library providing comprehensive hypothesis testing tools including parametric tests (t-tests, ANOVA) and nonparametric alternatives (Mann-Whitney, Kruskal-Wallis). Essential for data scientists and researchers who need to validate statistical claims and compare groups in their analyses. Offers both classical and robust testing methods with clear result interpretation.",
    "use_cases": [
      "Testing whether a new feature significantly improves user engagement metrics across different user segments",
      "Comparing treatment effectiveness across multiple groups in a clinical trial or A/B test experiment"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "python library for t-tests and ANOVA",
      "how to do hypothesis testing in python",
      "statistical significance testing tools",
      "nonparametric tests python package"
    ]
  },
  {
    "name": "lifelines",
    "description": "Comprehensive library for survival analysis: Kaplan-Meier, Nelson-Aalen, Cox regression, AFT models, handling censored data.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://lifelines.readthedocs.io/en/latest/",
    "github_url": "https://github.com/CamDavidsonPilon/lifelines",
    "url": "https://github.com/CamDavidsonPilon/lifelines",
    "install": "pip install lifelines",
    "tags": [
      "inference",
      "hypothesis testing"
    ],
    "best_for": "Hypothesis tests, confidence intervals, multiple testing",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scipy-stats",
      "regression-analysis"
    ],
    "topic_tags": [
      "survival-analysis",
      "kaplan-meier",
      "cox-regression",
      "censored-data",
      "python-library"
    ],
    "summary": "Lifelines is a Python library for survival analysis that handles time-to-event data with censoring. It provides implementations of key survival models including Kaplan-Meier curves, Cox proportional hazards, and accelerated failure time models. Data scientists and researchers use it to analyze customer churn, equipment failure, clinical trials, and other duration-based phenomena.",
    "use_cases": [
      "Analyzing customer subscription lifetimes and churn patterns for retention modeling",
      "Studying user engagement duration and feature adoption survival curves in product analytics"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python library for survival analysis",
      "how to do kaplan meier analysis",
      "cox regression python implementation",
      "analyzing time to event data with censoring"
    ]
  },
  {
    "name": "miceforest",
    "description": "LightGBM-accelerated multiple imputation by chained equations. Fast MICE for large datasets.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": "https://miceforest.readthedocs.io/",
    "github_url": "https://github.com/AnotherSamWilson/miceforest",
    "url": "https://github.com/AnotherSamWilson/miceforest",
    "install": "pip install miceforest",
    "tags": [
      "missing data",
      "imputation",
      "machine learning"
    ],
    "best_for": "Fast MICE imputation with LightGBM",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "missing-data-mechanisms",
      "gradient-boosting"
    ],
    "topic_tags": [
      "multiple-imputation",
      "missing-data",
      "lightgbm",
      "mice",
      "data-preprocessing"
    ],
    "summary": "miceforest is a Python package that implements Multiple Imputation by Chained Equations (MICE) using LightGBM as the underlying model. It provides a fast, scalable solution for handling missing data in large datasets by leveraging gradient boosting instead of traditional linear models. The package is particularly useful for data scientists working with complex datasets where missing data patterns are non-linear.",
    "use_cases": [
      "Imputing missing values in customer datasets with mixed data types before running machine learning models",
      "Handling missing survey responses in social science research with complex interaction patterns"
    ],
    "audience": [
      "Mid-DS",
      "Junior-DS"
    ],
    "synthetic_questions": [
      "fast multiple imputation python large datasets",
      "lightgbm mice implementation missing data",
      "how to handle missing values with gradient boosting",
      "miceforest vs sklearn iterative imputer performance"
    ]
  },
  {
    "name": "savvi",
    "description": "Safe Anytime Valid Inference using e-processes and confidence sequences (Ramdas et al. 2023). Valid inference at any stopping time.",
    "category": "Statistical Inference & Hypothesis Testing",
    "docs_url": null,
    "github_url": "https://github.com/assuncaolfi/savvi",
    "url": "https://pypi.org/project/savvi/",
    "install": "pip install savvi",
    "tags": [
      "sequential testing",
      "A/B testing",
      "anytime valid"
    ],
    "best_for": "Always-valid sequential inference for experiments",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "hypothesis-testing",
      "sequential-analysis",
      "martingale-theory"
    ],
    "topic_tags": [
      "sequential-testing",
      "anytime-valid-inference",
      "e-processes",
      "confidence-sequences",
      "optional-stopping"
    ],
    "summary": "SAVVI implements e-processes and confidence sequences for statistically valid inference at any stopping time, solving the optional stopping problem in sequential testing. It enables researchers to peek at results and stop experiments early while maintaining statistical validity. Particularly valuable for A/B testing and adaptive trial designs where traditional p-values break down under sequential monitoring.",
    "use_cases": [
      "Running A/B tests where you need to monitor results continuously and stop early if significant effects are detected",
      "Clinical trials or expensive experiments where you want valid statistical inference even when stopping rules depend on observed data"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "anytime valid inference python package",
      "sequential A/B testing without multiple testing corrections",
      "e-processes confidence sequences implementation",
      "optional stopping problem solution for experiments"
    ]
  },
  {
    "name": "Dolo",
    "description": "Framework for describing and solving economic models (DSGE, OLG, etc.) using a declarative YAML-based format.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://dolo.readthedocs.io/en/latest/",
    "github_url": "https://github.com/EconForge/dolo",
    "url": "https://github.com/EconForge/dolo",
    "install": "pip install dolo",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-programming",
      "dynamic-programming",
      "macroeconomic-theory"
    ],
    "topic_tags": [
      "dsge-models",
      "economic-modeling",
      "yaml-configuration",
      "dynamic-programming",
      "macroeconomics"
    ],
    "summary": "Dolo is a Python framework that allows economists to specify and solve complex dynamic economic models using human-readable YAML configuration files. It handles DSGE models, overlapping generations models, and other structural economic frameworks with built-in solution algorithms. The declarative approach separates model specification from solution methods, making economic models more reproducible and easier to modify.",
    "use_cases": [
      "Building and solving DSGE models for monetary policy analysis without writing low-level numerical code",
      "Comparing solution methods across different overlapping generations models using consistent YAML specifications"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "How to solve DSGE models in Python",
      "YAML-based economic modeling framework",
      "Tools for structural macroeconomic models",
      "Dolo vs Dynare for economic modeling"
    ]
  },
  {
    "name": "Greeners",
    "description": "Comprehensive Rust econometrics library with OLS, IV, panel data estimators, fixed effects, DiD, and heteroskedasticity-robust standard errors (HC0-HC3).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://docs.rs/greeners",
    "github_url": "https://github.com/greeners-rs/greeners",
    "url": "https://crates.io/crates/greeners",
    "install": "cargo add greeners",
    "tags": [
      "rust",
      "econometrics",
      "IV",
      "panel data",
      "robust SE"
    ],
    "best_for": "Academic econometrics in Rust: IV, DiD, robust SEs",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "linear-regression",
      "instrumental-variables"
    ],
    "topic_tags": [
      "rust-econometrics",
      "panel-data",
      "difference-in-differences",
      "robust-standard-errors",
      "instrumental-variables"
    ],
    "summary": "Greeners is a comprehensive Rust econometrics library offering core estimation methods including OLS, instrumental variables, and panel data models. It provides robust standard error corrections and implements difference-in-differences estimators with fixed effects capabilities. The library is designed for economists and data scientists who need high-performance econometric analysis in Rust environments.",
    "use_cases": [
      "Estimating causal effects of policy interventions using difference-in-differences with panel data",
      "Running instrumental variables regressions with heteroskedasticity-robust standard errors for program evaluation"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust econometrics library with IV estimation",
      "difference in differences implementation in rust",
      "panel data fixed effects rust package",
      "robust standard errors econometrics rust"
    ]
  },
  {
    "name": "HARK",
    "description": "Toolkit for solving, simulating, and estimating models with heterogeneous agents (e.g., consumption-saving).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://hark.readthedocs.io/en/latest/",
    "github_url": "https://github.com/econ-ark/HARK",
    "url": "https://github.com/econ-ark/HARK",
    "install": "pip install econ-ark",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "dynamic-programming",
      "maximum-likelihood-estimation",
      "numerical-optimization"
    ],
    "topic_tags": [
      "heterogeneous-agents",
      "consumption-saving",
      "structural-models",
      "lifecycle-models",
      "behavioral-economics"
    ],
    "summary": "HARK is a Python toolkit for building and estimating structural economic models with heterogeneous agents, particularly focused on consumption-saving problems. It provides pre-built model classes and numerical methods for solving dynamic optimization problems commonly found in macroeconomics and household finance research. Researchers use it to estimate parameters, run policy simulations, and test theoretical predictions against empirical data.",
    "use_cases": [
      "Estimating how different types of households respond to changes in interest rates or income uncertainty",
      "Simulating the distributional effects of tax policy changes on consumption and savings behavior"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "heterogeneous agent models python",
      "structural estimation consumption saving",
      "HARK toolkit economic modeling",
      "dynamic programming household behavior"
    ]
  },
  {
    "name": "QuantEcon.py",
    "description": "Core library for quantitative economics: dynamic programming, Markov chains, game theory, numerical methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://quantecon.org/python-lectures/",
    "github_url": "https://github.com/QuantEcon/QuantEcon.py",
    "url": "https://github.com/QuantEcon/QuantEcon.py",
    "install": "pip install quantecon",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-numpy",
      "linear-algebra",
      "dynamic-programming"
    ],
    "topic_tags": [
      "dynamic-programming",
      "markov-chains",
      "game-theory",
      "numerical-methods",
      "python-library"
    ],
    "summary": "QuantEcon.py is a comprehensive Python library providing computational tools for quantitative economics research and analysis. It implements core methods including dynamic programming solvers, Markov chain analysis, game theory algorithms, and numerical optimization routines. The library is widely used by economists and data scientists for structural modeling, policy analysis, and theoretical research.",
    "use_cases": [
      "Solving dynamic economic models like optimal consumption-savings problems with value function iteration",
      "Analyzing market competition using Nash equilibrium computation and evolutionary game dynamics"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "Python library for dynamic programming in economics",
      "How to solve Markov decision processes for economic models",
      "QuantEcon package for game theory and numerical methods",
      "Tools for structural econometric modeling in Python"
    ]
  },
  {
    "name": "dcegm",
    "description": "JAX-compatible DC-EGM algorithm for discrete-continuous dynamic programming (Iskhakov et al. 2017).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/dcegm",
    "url": "https://github.com/OpenSourceEconomics/dcegm",
    "install": "pip install dcegm",
    "tags": [
      "structural",
      "dynamic programming",
      "JAX"
    ],
    "best_for": "Discrete-continuous choice models with EGM",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "dynamic-programming",
      "JAX",
      "structural-models"
    ],
    "topic_tags": [
      "discrete-continuous-choice",
      "endogenous-grid-method",
      "computational-economics",
      "JAX-implementation",
      "dynamic-programming"
    ],
    "summary": "JAX implementation of the DC-EGM (Discrete-Continuous Endogenous Grid Method) algorithm for solving dynamic programming problems with both discrete and continuous choices. This method efficiently handles complex structural models where agents make simultaneous decisions across different choice dimensions. Essential for researchers working on lifecycle models, consumption-saving problems, or any structural model combining discrete and continuous optimization.",
    "use_cases": [
      "Estimating lifecycle consumption-saving models with discrete labor supply choices",
      "Solving dynamic investment problems with both portfolio allocation and participation decisions"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "JAX implementation discrete continuous dynamic programming",
      "DC-EGM algorithm Python package",
      "endogenous grid method discrete continuous choice",
      "structural model solver JAX compatible"
    ]
  },
  {
    "name": "econpizza",
    "description": "Solve nonlinear heterogeneous agent models (HANK) with perfect foresight. Efficient perturbation and projection methods.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://econpizza.readthedocs.io/",
    "github_url": "https://github.com/gboehl/econpizza",
    "url": "https://github.com/gboehl/econpizza",
    "install": "pip install econpizza",
    "tags": [
      "structural",
      "DSGE",
      "HANK"
    ],
    "best_for": "Nonlinear HANK models with aggregate shocks",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-programming",
      "dynamic-programming",
      "DSGE-modeling"
    ],
    "topic_tags": [
      "heterogeneous-agents",
      "DSGE",
      "macroeconomics",
      "nonlinear-models",
      "python-package"
    ],
    "summary": "Econpizza is a Python package for solving complex heterogeneous agent New Keynesian (HANK) models with nonlinear dynamics and perfect foresight. It implements efficient perturbation and projection methods for macroeconomic models where agents have different characteristics. Primarily used by macroeconomists and central bank researchers working on monetary policy analysis.",
    "use_cases": [
      "Analyzing monetary policy transmission through heterogeneous household responses to interest rate changes",
      "Studying fiscal policy effects when agents have different income levels and borrowing constraints"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for heterogeneous agent models",
      "How to solve HANK models with perfect foresight",
      "Nonlinear DSGE model solver Python",
      "Perturbation methods for heterogeneous agent models"
    ]
  },
  {
    "name": "gEconpy",
    "description": "DSGE modeling tools inspired by R's gEcon. Automatic first-order condition derivation with Dynare export.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/jessegrabowski/gEconpy",
    "url": "https://github.com/jessegrabowski/gEconpy",
    "install": "pip install gEconpy",
    "tags": [
      "structural",
      "DSGE",
      "estimation"
    ],
    "best_for": "Symbolic DSGE derivation with Dynare compatibility",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-sympy",
      "DSGE-modeling",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "DSGE",
      "structural-modeling",
      "macroeconomics",
      "dynare",
      "python-package"
    ],
    "summary": "gEconpy is a Python package for building and estimating Dynamic Stochastic General Equilibrium (DSGE) models, inspired by R's gEcon package. It automatically derives first-order conditions from economic models and exports them to Dynare for numerical solution. The tool is primarily used by macroeconomists and researchers working on structural economic models.",
    "use_cases": [
      "Building a New Keynesian DSGE model to analyze monetary policy transmission mechanisms",
      "Estimating parameters of a real business cycle model using Bayesian methods via Dynare integration"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python package for DSGE modeling",
      "automatic first order conditions derivation DSGE",
      "gEcon equivalent for Python",
      "Dynare integration Python DSGE tools"
    ]
  },
  {
    "name": "gegravity",
    "description": "General equilibrium structural gravity modeling for trade policy analysis. Only Python package for Anderson-van Wincoop GE gravity.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/peter-herman/gegravity",
    "url": "https://pypi.org/project/gegravity/",
    "install": "pip install gegravity",
    "tags": [
      "trade",
      "gravity models",
      "structural"
    ],
    "best_for": "GE structural gravity for trade policy",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "trade-economics",
      "structural-econometrics",
      "python-pandas"
    ],
    "topic_tags": [
      "gravity-models",
      "trade-policy",
      "general-equilibrium",
      "structural-econometrics",
      "python-package"
    ],
    "summary": "gegravity is a specialized Python package for structural gravity modeling in international trade, implementing the Anderson-van Wincoop general equilibrium framework. It's designed for trade economists and policy analysts who need to estimate how trade costs, tariffs, and other policies affect bilateral trade flows. The package is unique as the only Python implementation of GE gravity models, making advanced trade policy analysis more accessible.",
    "use_cases": [
      "Estimating the trade effects of Brexit or other trade policy changes using structural gravity models",
      "Analyzing how transportation infrastructure investments affect bilateral trade patterns through reduced trade costs"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Anderson van Wincoop gravity model Python",
      "structural gravity modeling trade policy",
      "general equilibrium trade analysis package",
      "Python gravity model international trade"
    ]
  },
  {
    "name": "pydsge",
    "description": "DSGE model simulation, filtering, and Bayesian estimation. Handles occasionally binding constraints.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pydsge",
    "url": "https://github.com/gboehl/pydsge",
    "install": "pip install pydsge",
    "tags": [
      "structural",
      "DSGE",
      "Bayesian"
    ],
    "best_for": "DSGE estimation with occasionally binding constraints",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "python-scipy",
      "bayesian-mcmc",
      "macroeconomic-theory"
    ],
    "topic_tags": [
      "DSGE-models",
      "bayesian-estimation",
      "macroeconomic-modeling",
      "structural-econometrics",
      "kalman-filtering"
    ],
    "summary": "Python package for Dynamic Stochastic General Equilibrium (DSGE) model simulation, filtering, and Bayesian estimation with support for occasionally binding constraints like zero lower bound. Primarily used by macroeconomists and central bank researchers for structural macroeconomic modeling and policy analysis. Enables full Bayesian estimation workflow from model specification to posterior inference.",
    "use_cases": [
      "Central bank economist estimating a New Keynesian DSGE model with zero lower bound constraints for monetary policy analysis",
      "Academic researcher conducting Bayesian estimation of medium-scale DSGE model to study business cycle dynamics"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Python package for DSGE model estimation",
      "Bayesian estimation of macroeconomic models",
      "DSGE modeling with zero lower bound",
      "structural macroeconomic model simulation Python"
    ]
  },
  {
    "name": "pynare",
    "description": "Python wrapper/interface to Dynare for DSGE model solving. Bridge between Python workflows and Dynare.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/gboehl/pynare",
    "url": "https://github.com/gboehl/pynare",
    "install": "pip install pynare",
    "tags": [
      "structural",
      "DSGE",
      "Dynare"
    ],
    "best_for": "Running Dynare models from Python",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-basics",
      "DSGE-modeling",
      "Dynare-syntax"
    ],
    "topic_tags": [
      "DSGE",
      "macroeconomic-modeling",
      "python-wrapper",
      "structural-estimation",
      "dynare"
    ],
    "summary": "Pynare is a Python wrapper that provides an interface to Dynare, allowing economists to solve Dynamic Stochastic General Equilibrium (DSGE) models within Python workflows. It bridges the gap between Python's data science ecosystem and Dynare's specialized DSGE modeling capabilities. This tool is particularly useful for researchers who want to integrate macroeconomic modeling with Python-based analysis pipelines.",
    "use_cases": [
      "Running DSGE model simulations and parameter estimation within Jupyter notebooks alongside other economic analysis",
      "Building automated pipelines that combine DSGE model results with Python data processing and visualization tools"
    ],
    "audience": [
      "Early-PhD",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "python wrapper for Dynare DSGE models",
      "how to run Dynare from Python",
      "DSGE modeling in Python",
      "integrate Dynare with Python workflow"
    ]
  },
  {
    "name": "respy",
    "description": "Simulation and estimation of finite-horizon dynamic discrete choice (DDC) models (e.g., labor/education choice).",
    "category": "Structural Econometrics & Estimation",
    "docs_url": "https://respy.readthedocs.io/en/latest/",
    "github_url": "https://github.com/OpenSourceEconomics/respy",
    "url": "https://github.com/OpenSourceEconomics/respy",
    "install": "pip install respy",
    "tags": [
      "structural",
      "estimation"
    ],
    "best_for": "Structural models, GMM estimation, BLP-style demand",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "maximum-likelihood-estimation",
      "dynamic-programming",
      "python-numpy"
    ],
    "topic_tags": [
      "dynamic-discrete-choice",
      "structural-models",
      "labor-economics",
      "simulation",
      "estimation"
    ],
    "summary": "respy is a Python package for simulating and estimating finite-horizon dynamic discrete choice models, commonly used in labor and education economics. It allows researchers to model sequential decision-making processes where agents make discrete choices (like work vs. education) over time. The package handles the computational complexity of solving and estimating these structural models using methods like maximum likelihood.",
    "use_cases": [
      "Modeling career decisions where individuals choose between working, attending school, or staying home over their lifetime",
      "Estimating the returns to education by modeling how people decide when to enter/exit schooling based on expected future earnings"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "dynamic discrete choice model python",
      "structural labor economics estimation package",
      "simulate education career decisions model",
      "respy dynamic programming estimation"
    ]
  },
  {
    "name": "upper-envelope",
    "description": "Fast upper envelope scan for discrete-continuous dynamic programming. JAX and numba implementations.",
    "category": "Structural Econometrics & Estimation",
    "docs_url": null,
    "github_url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "url": "https://github.com/OpenSourceEconomics/upper-envelope",
    "install": "pip install upper-envelope",
    "tags": [
      "structural",
      "dynamic programming",
      "optimization"
    ],
    "best_for": "Fast upper envelope computation for DC-EGM",
    "language": "Python",
    "difficulty": "advanced",
    "prerequisites": [
      "dynamic-programming",
      "JAX-optimization",
      "structural-models"
    ],
    "topic_tags": [
      "upper-envelope",
      "dynamic-programming",
      "structural-estimation",
      "JAX",
      "numba"
    ],
    "summary": "Fast upper envelope scan implementation for discrete-continuous dynamic programming problems in structural econometrics. Provides optimized JAX and numba backends for computing upper envelopes efficiently in complex optimization routines. Essential for researchers solving high-dimensional structural models where computational speed is critical.",
    "use_cases": [
      "Estimating dynamic labor supply models with discrete job choices and continuous hours decisions",
      "Solving firm investment problems with discrete capacity expansion and continuous investment levels"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "fast upper envelope algorithm dynamic programming",
      "JAX implementation structural model optimization",
      "discrete continuous dynamic programming solver",
      "upper envelope scan numba JAX performance"
    ]
  },
  {
    "name": "OpenMx",
    "description": "Extended SEM software with programmatic model specification via paths (RAM) or matrix algebra, supporting mixture distributions, item factor analysis, state space models, and behavior genetics twin studies.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://openmx.ssri.psu.edu/",
    "github_url": "https://github.com/OpenMx/OpenMx",
    "url": "https://cran.r-project.org/package=OpenMx",
    "install": "install.packages(\"OpenMx\")",
    "tags": [
      "SEM",
      "matrix-algebra",
      "twin-studies",
      "behavior-genetics",
      "IFA"
    ],
    "best_for": "Complex/advanced SEM, behavior genetics, and researchers needing maximum specification flexibility, implementing Neale et al. (2016)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "structural-equation-modeling",
      "matrix-algebra",
      "R-programming"
    ],
    "topic_tags": [
      "SEM",
      "latent-variables",
      "behavior-genetics",
      "twin-studies",
      "matrix-specification"
    ],
    "summary": "OpenMx is an advanced R package for structural equation modeling that allows programmatic model specification through path diagrams or matrix algebra. It extends beyond basic SEM to support complex models including mixture distributions, item factor analysis, state space models, and specialized behavior genetics applications like twin studies.",
    "use_cases": [
      "Modeling genetic and environmental influences on traits using twin study data",
      "Building custom latent variable models with non-standard distributions or constraints"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for twin study analysis behavior genetics",
      "structural equation modeling with matrix specification",
      "OpenMx vs lavaan for complex SEM models",
      "how to model genetic influences twin data R"
    ]
  },
  {
    "name": "blavaan",
    "description": "Bayesian latent variable analysis extending lavaan with MCMC estimation via Stan or JAGS, supporting Bayesian CFA, SEM, growth models, and model comparison with WAIC, LOO, and Bayes factors.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://ecmerkle.github.io/blavaan/",
    "github_url": "https://github.com/ecmerkle/blavaan",
    "url": "https://cran.r-project.org/package=blavaan",
    "install": "install.packages(\"blavaan\")",
    "tags": [
      "Bayesian-SEM",
      "Stan",
      "JAGS",
      "MCMC",
      "latent-variables"
    ],
    "best_for": "Bayesian inference for SEM models using familiar lavaan syntax, implementing Merkle & Rosseel (2018)",
    "language": "R",
    "difficulty": "advanced",
    "prerequisites": [
      "lavaan-SEM",
      "Stan-MCMC",
      "Bayesian-statistics"
    ],
    "topic_tags": [
      "Bayesian-SEM",
      "latent-variable-modeling",
      "MCMC-estimation",
      "structural-equation-modeling",
      "Stan"
    ],
    "summary": "blavaan extends the popular lavaan package to perform Bayesian structural equation modeling using MCMC estimation via Stan or JAGS. It enables researchers to incorporate prior knowledge, quantify uncertainty through posterior distributions, and perform robust model comparison using Bayesian information criteria. This package is essential for psychometricians and social scientists who need principled uncertainty quantification in their latent variable models.",
    "use_cases": [
      "Testing measurement invariance across groups while incorporating prior knowledge about factor loadings",
      "Building confirmatory factor analysis models with informative priors when sample sizes are small"
    ],
    "audience": [
      "Senior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "Bayesian structural equation modeling in R",
      "how to do Bayesian CFA with Stan",
      "lavaan Bayesian extension MCMC",
      "Bayesian latent variable analysis package"
    ]
  },
  {
    "name": "lavaan",
    "description": "Free, open-source latent variable analysis providing commercial-quality functionality for path analysis, confirmatory factor analysis, structural equation modeling, and growth curve models with intuitive model syntax.",
    "category": "Structural Equation Modeling",
    "docs_url": "https://lavaan.ugent.be/",
    "github_url": "https://github.com/yrosseel/lavaan",
    "url": "https://cran.r-project.org/package=lavaan",
    "install": "install.packages(\"lavaan\")",
    "tags": [
      "SEM",
      "CFA",
      "path-analysis",
      "latent-variables",
      "psychometrics"
    ],
    "best_for": "General-purpose structural equation modeling with accessible syntax for researchers, implementing Rosseel (2012)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "linear-regression",
      "factor-analysis"
    ],
    "topic_tags": [
      "structural-equation-modeling",
      "confirmatory-factor-analysis",
      "latent-variables",
      "psychometrics",
      "R-package"
    ],
    "summary": "lavaan is an R package for structural equation modeling that provides an intuitive syntax for specifying complex latent variable models. It's widely used by researchers in psychology, marketing, and social sciences for testing theoretical frameworks and measurement models. The package offers comprehensive functionality for path analysis, confirmatory factor analysis, and growth curve modeling with robust statistical output.",
    "use_cases": [
      "Testing whether customer satisfaction mediates the relationship between product quality and purchase intention",
      "Validating a multi-factor survey instrument by confirming the underlying latent construct structure"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for structural equation modeling",
      "how to do confirmatory factor analysis in R",
      "lavaan SEM tutorial",
      "latent variable modeling R package"
    ]
  },
  {
    "name": "SDV (Synthetic Data Vault)",
    "description": "Comprehensive library for generating synthetic tabular, relational, and time series data using various models.",
    "category": "Synthetic Data Generation",
    "docs_url": "https://sdv.dev/",
    "github_url": "https://github.com/sdv-dev/SDV",
    "url": "https://github.com/sdv-dev/SDV",
    "install": "pip install sdv",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "statistical-distributions"
    ],
    "topic_tags": [
      "synthetic-data",
      "privacy-preserving",
      "tabular-data",
      "data-simulation",
      "generative-models"
    ],
    "summary": "SDV is a comprehensive Python library for generating synthetic data that maintains statistical properties of original datasets. It supports tabular, relational, and time series data using various generative models including GANs and statistical approaches. Data scientists and researchers use it for privacy-preserving analytics, testing pipelines, and data augmentation.",
    "use_cases": [
      "Creating synthetic customer data for model testing when production data contains PII",
      "Generating additional training samples for machine learning models with limited data"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to generate synthetic data that preserves privacy",
      "Python library for creating fake tabular data",
      "SDV synthetic data vault tutorial",
      "Generate realistic synthetic datasets for testing"
    ]
  },
  {
    "name": "Synthpop",
    "description": "Port of the R package for generating synthetic populations based on sample survey data.",
    "category": "Synthetic Data Generation",
    "docs_url": null,
    "github_url": "https://github.com/alan-turing-institute/synthpop",
    "url": "https://github.com/alan-turing-institute/synthpop",
    "install": "pip install synthpop",
    "tags": [
      "synthetic data",
      "simulation"
    ],
    "best_for": "Privacy-preserving data, simulation, augmentation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "survey-sampling",
      "data-privacy-techniques"
    ],
    "topic_tags": [
      "synthetic-data",
      "survey-simulation",
      "privacy-preserving",
      "population-modeling",
      "r-package-port"
    ],
    "summary": "Synthpop is a Python port of the popular R package that generates synthetic datasets mimicking the statistical properties of original survey data while preserving privacy. It's widely used by researchers and data scientists who need realistic fake data for testing, sharing, or publication. The package implements multiple synthesis methods to create populations that maintain the relationships and distributions found in sensitive source data.",
    "use_cases": [
      "Creating shareable datasets for academic research when original survey data contains PII",
      "Generating realistic test data for validating analysis pipelines before accessing production survey data"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "how to generate synthetic survey data in python",
      "synthpop package for creating fake population data",
      "privacy preserving synthetic data generation tools",
      "python alternative to R synthpop package"
    ]
  },
  {
    "name": "quanteda",
    "description": "Comprehensive framework for quantitative text analysis. Provides fast text preprocessing, document-feature matrices, dictionary analysis, and integration with topic models. Standard for political science text analysis.",
    "category": "Text Analysis",
    "docs_url": "https://quanteda.io/",
    "github_url": "https://github.com/quanteda/quanteda",
    "url": "https://cran.r-project.org/package=quanteda",
    "install": "install.packages(\"quanteda\")",
    "tags": [
      "text-analysis",
      "NLP",
      "document-term-matrix",
      "text-preprocessing",
      "political-science"
    ],
    "best_for": "Comprehensive quantitative text analysis with fast preprocessing and document-feature matrices",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-programming",
      "document-term-matrices",
      "text-tokenization"
    ],
    "topic_tags": [
      "text-analysis",
      "R-package",
      "document-feature-matrix",
      "political-text-analysis",
      "NLP-preprocessing"
    ],
    "summary": "Quanteda is R's leading package for quantitative text analysis, offering efficient text preprocessing, document-feature matrix creation, and dictionary-based analysis. Widely adopted in political science and social science research for analyzing large text corpora. Provides seamless integration with topic modeling and other advanced text mining techniques.",
    "use_cases": [
      "analyzing political speeches and manifestos to identify policy positions across parties",
      "processing social media data to measure public sentiment toward policy changes"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for text analysis political science",
      "how to create document term matrix in R",
      "quanteda vs tidytext for NLP preprocessing",
      "best R tools for analyzing political text data"
    ]
  },
  {
    "name": "stm",
    "description": "Structural Topic Models incorporating document-level metadata as covariates affecting topic prevalence and content. Enables studying how topics vary across groups or time with uncertainty quantification.",
    "category": "Text Analysis",
    "docs_url": "https://www.structuraltopicmodel.com/",
    "github_url": "https://github.com/bstewart/stm",
    "url": "https://cran.r-project.org/package=stm",
    "install": "install.packages(\"stm\")",
    "tags": [
      "topic-models",
      "text-analysis",
      "covariates",
      "LDA",
      "document-metadata"
    ],
    "best_for": "Structural topic models with document metadata affecting topic prevalence and content",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "topic-modeling-LDA",
      "R-programming",
      "bayesian-inference"
    ],
    "topic_tags": [
      "structural-topic-models",
      "document-covariates",
      "text-mining",
      "bayesian-nlp",
      "metadata-analysis"
    ],
    "summary": "STM extends traditional topic modeling by incorporating document-level metadata (like author, time, or group) as covariates that can influence both topic prevalence and content. Unlike standard LDA, it allows researchers to study how topics systematically vary across different conditions while providing proper uncertainty quantification. Popular among social scientists and applied researchers analyzing textual data with rich metadata.",
    "use_cases": [
      "Analyzing how political speech topics change over time periods or across party affiliations",
      "Studying how product review themes vary by customer demographics or product categories"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "topic modeling with covariates R package",
      "how to include metadata in topic models",
      "structural topic models vs LDA comparison",
      "analyzing text data with document-level variables"
    ]
  },
  {
    "name": "text2vec",
    "description": "Efficient text vectorization with word embeddings (GloVe), topic models (LDA), and document similarity. Memory-efficient streaming API for large corpora with C++ backend.",
    "category": "Text Analysis",
    "docs_url": "https://text2vec.org/",
    "github_url": "https://github.com/dselivanov/text2vec",
    "url": "https://cran.r-project.org/package=text2vec",
    "install": "install.packages(\"text2vec\")",
    "tags": [
      "word-embeddings",
      "GloVe",
      "text-vectorization",
      "LDA",
      "document-similarity"
    ],
    "best_for": "Efficient word embeddings (GloVe) and text vectorization for large corpora",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-programming",
      "natural-language-processing",
      "matrix-operations"
    ],
    "topic_tags": [
      "text-vectorization",
      "word-embeddings",
      "topic-modeling",
      "document-similarity",
      "R-package"
    ],
    "summary": "text2vec is an R package for efficient text analysis providing word embeddings (GloVe), topic modeling (LDA), and document similarity calculations. It features a memory-efficient streaming API with C++ backend for processing large text corpora. Popular among data scientists for feature engineering and text preprocessing in machine learning pipelines.",
    "use_cases": [
      "Building recommendation systems by computing document similarity between user profiles and product descriptions",
      "Creating word embeddings for sentiment analysis or text classification models in production"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for word embeddings and text vectorization",
      "how to compute document similarity with GloVe embeddings",
      "memory efficient text processing for large corpora",
      "LDA topic modeling implementation in R"
    ]
  },
  {
    "name": "tidytext",
    "description": "Tidy data principles for text mining. Converts text to tidy format (one-token-per-row), enabling analysis with dplyr, ggplot2, and other tidyverse tools. Accompanies the book 'Text Mining with R'.",
    "category": "Text Analysis",
    "docs_url": "https://juliasilge.github.io/tidytext/",
    "github_url": "https://github.com/juliasilge/tidytext",
    "url": "https://cran.r-project.org/package=tidytext",
    "install": "install.packages(\"tidytext\")",
    "tags": [
      "text-mining",
      "tidyverse",
      "tokenization",
      "sentiment-analysis",
      "NLP"
    ],
    "best_for": "Tidy text mining with dplyr and ggplot2 integration\u2014accompanies 'Text Mining with R'",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-dplyr",
      "basic-text-preprocessing",
      "ggplot2"
    ],
    "topic_tags": [
      "text-mining",
      "tidyverse",
      "tokenization",
      "sentiment-analysis",
      "R-package"
    ],
    "summary": "tidytext is an R package that applies tidy data principles to text analysis by converting text into one-token-per-row format. It integrates seamlessly with tidyverse tools like dplyr and ggplot2, making text mining accessible to users familiar with tidy data workflows. The package is designed around the companion book 'Text Mining with R' and provides intuitive functions for tokenization, sentiment analysis, and text visualization.",
    "use_cases": [
      "Analyzing customer reviews or survey responses to extract sentiment and key themes",
      "Processing social media posts or news articles to identify trending topics and perform word frequency analysis"
    ],
    "audience": [
      "Junior-DS",
      "Curious-browser"
    ],
    "synthetic_questions": [
      "How to do text mining in R with tidyverse",
      "R package for sentiment analysis with dplyr",
      "Convert text to tidy format for analysis",
      "Text mining with R book package"
    ]
  },
  {
    "name": "ARCH",
    "description": "Specialized library for modeling and forecasting conditional volatility using ARCH, GARCH, EGARCH, and related models.",
    "category": "Time Series Econometrics",
    "docs_url": "https://arch.readthedocs.io/",
    "github_url": "https://github.com/bashtage/arch",
    "url": "https://github.com/bashtage/arch",
    "install": "pip install arch",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "maximum-likelihood-estimation"
    ],
    "topic_tags": [
      "volatility-modeling",
      "GARCH",
      "financial-econometrics",
      "heteroskedasticity",
      "python-package"
    ],
    "summary": "ARCH is a Python library for modeling time-varying volatility in financial and economic time series using ARCH, GARCH, and related models. It's widely used by quantitative analysts and researchers to forecast conditional variance and analyze volatility clustering. The package provides easy-to-use implementations of various volatility models with diagnostic tools and forecasting capabilities.",
    "use_cases": [
      "Modeling and forecasting stock return volatility for risk management",
      "Analyzing volatility clustering in cryptocurrency prices for trading strategies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GARCH model implementation python",
      "volatility forecasting time series",
      "conditional heteroskedasticity modeling",
      "ARCH GARCH python package"
    ]
  },
  {
    "name": "KFAS",
    "description": "State space modeling framework for exponential family time series with computationally efficient Kalman filtering, smoothing, forecasting, and simulation. Supports observations from Gaussian, Poisson, binomial, negative binomial, and gamma distributions.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/KFAS/KFAS.pdf",
    "github_url": "https://github.com/helske/KFAS",
    "url": "https://cran.r-project.org/package=KFAS",
    "install": "install.packages(\"KFAS\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "time-series",
      "forecasting",
      "exponential-family"
    ],
    "best_for": "Multivariate time series modeling with non-Gaussian observations (e.g., count data with Poisson), implementing Helske (2017)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "kalman-filters",
      "maximum-likelihood-estimation",
      "R-programming"
    ],
    "topic_tags": [
      "state-space-models",
      "kalman-filtering",
      "time-series-forecasting",
      "exponential-family",
      "R-package"
    ],
    "summary": "KFAS is an R package for state space modeling with exponential family distributions, providing efficient Kalman filter implementations for non-Gaussian time series. It enables researchers and practitioners to handle complex time series with count data, binary outcomes, or other non-normal distributions while maintaining computational efficiency. The package is particularly valuable for econometric modeling where traditional Gaussian assumptions are violated.",
    "use_cases": [
      "modeling-weekly-retail-sales-with-poisson-distributed-counts",
      "forecasting-binary-market-events-with-time-varying-probabilities"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "state space models for non-gaussian time series R",
      "kalman filter package poisson binomial data",
      "KFAS R package exponential family forecasting",
      "non-gaussian state space modeling econometrics"
    ]
  },
  {
    "name": "Kats",
    "description": "Broad toolkit for time series analysis, including multivariate analysis, detection (outliers, change points, trends), feature extraction.",
    "category": "Time Series Econometrics",
    "docs_url": "https://facebookresearch.github.io/Kats/",
    "github_url": "https://github.com/facebookresearch/Kats",
    "url": "https://github.com/facebookresearch/Kats",
    "install": "pip install kats",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "statsmodels",
      "time-series-basics"
    ],
    "topic_tags": [
      "time-series-analysis",
      "anomaly-detection",
      "changepoint-detection",
      "forecasting",
      "multivariate-analysis"
    ],
    "summary": "Kats is a comprehensive Python toolkit developed by Facebook for time series analysis and forecasting. It provides unified APIs for detection tasks like outlier identification, changepoint analysis, and trend detection, along with feature extraction capabilities. The package is designed for practitioners who need robust time series tools beyond basic forecasting.",
    "use_cases": [
      "Detecting anomalies in user engagement metrics for A/B test validity",
      "Identifying structural breaks in revenue time series during product launches"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "Python package for time series anomaly detection",
      "How to detect changepoints in multivariate time series",
      "Kats vs Prophet for time series analysis",
      "Time series feature extraction tools Python"
    ]
  },
  {
    "name": "LocalProjections",
    "description": "Community implementations of Jord\u00e0 (2005) Local Projections for estimating impulse responses without VAR assumptions.",
    "category": "Time Series Econometrics",
    "docs_url": null,
    "github_url": "https://github.com/elenev/localprojections",
    "url": "https://github.com/elenev/localprojections",
    "install": "Install from source",
    "tags": [
      "time series",
      "econometrics"
    ],
    "best_for": "ARIMA, cointegration, VAR models",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "linear-regression",
      "impulse-response-functions"
    ],
    "topic_tags": [
      "local-projections",
      "impulse-response",
      "time-series",
      "causal-inference",
      "econometrics"
    ],
    "summary": "Local Projections is a method for estimating impulse response functions that avoids the restrictive assumptions of Vector Autoregressions (VARs). It directly estimates the response of variables to shocks at different horizons using separate regressions, making it more robust to model misspecification. Popular in macroeconomics and finance for analyzing policy effects and market dynamics.",
    "use_cases": [
      "Estimating how GDP responds to monetary policy shocks over multiple quarters",
      "Analyzing stock market reactions to earnings announcements across different time horizons"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "local projections vs VAR comparison",
      "how to estimate impulse responses without VAR assumptions",
      "Jord\u00e0 local projections implementation",
      "time series causal inference methods"
    ]
  },
  {
    "name": "TS-Flint",
    "description": "Two Sigma's time-series library for Spark with optimized temporal joins, as-of joins, and distributed OLS for high-frequency data.",
    "category": "Time Series Econometrics",
    "docs_url": "https://ts-flint.readthedocs.io/",
    "github_url": "https://github.com/twosigma/flint",
    "url": "https://github.com/twosigma/flint",
    "install": "pip install ts-flint",
    "tags": [
      "spark",
      "time series",
      "temporal joins",
      "fintech"
    ],
    "best_for": "High-frequency financial data with inexact timestamp matching",
    "language": "Spark",
    "difficulty": "intermediate",
    "prerequisites": [
      "apache-spark",
      "python-pandas",
      "SQL-joins"
    ],
    "topic_tags": [
      "time-series",
      "spark-distributed",
      "temporal-joins",
      "financial-data",
      "econometrics"
    ],
    "summary": "TS-Flint is Two Sigma's open-source time series library built on Apache Spark for handling large-scale temporal data. It provides optimized temporal joins, as-of joins, and distributed statistical methods like OLS regression specifically designed for high-frequency financial and economic data. The library enables efficient analysis of time series data that's too large for single-machine processing.",
    "use_cases": [
      "Analyzing high-frequency trading data by joining price feeds with news events using as-of joins to avoid look-ahead bias",
      "Running distributed OLS regressions on large panels of stock returns with market factors across multiple time periods"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "spark library for time series analysis",
      "temporal joins spark high frequency data",
      "distributed OLS regression time series",
      "two sigma flint financial data processing"
    ]
  },
  {
    "name": "dlm",
    "description": "Maximum likelihood and Bayesian analysis of Normal linear state space models (Dynamic Linear Models). Features numerically stable SVD-based algorithms for Kalman filtering and smoothing, plus tools for MCMC-based Bayesian inference including forward filtering backward sampling (FFBS).",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dlm/vignettes/dlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dlm",
    "install": "install.packages(\"dlm\")",
    "tags": [
      "state-space",
      "kalman-filter",
      "Bayesian",
      "time-series",
      "dynamic-linear-models"
    ],
    "best_for": "Bayesian analysis of linear Gaussian state space models with MCMC methods (Gibbs sampling), implementing Petris (2010)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-algebra",
      "maximum-likelihood-estimation",
      "time-series-basics"
    ],
    "topic_tags": [
      "state-space-models",
      "kalman-filtering",
      "bayesian-inference",
      "time-series-econometrics",
      "r-package"
    ],
    "summary": "The dlm package provides robust implementations of Dynamic Linear Models for time series analysis with unobserved state variables. It offers both classical maximum likelihood estimation via numerically stable Kalman filtering and Bayesian MCMC methods including forward filtering backward sampling. Economists and data scientists use it for modeling time-varying parameters, forecasting with uncertainty quantification, and analyzing structural breaks in economic data.",
    "use_cases": [
      "modeling-time-varying-coefficients-in-regression",
      "forecasting-macroeconomic-variables-with-uncertainty"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for Kalman filtering and state space models",
      "how to implement dynamic linear models in R",
      "Bayesian time series analysis with state space models",
      "forward filtering backward sampling R implementation"
    ]
  },
  {
    "name": "dynlm",
    "description": "Provides an interface for fitting dynamic linear regression models with extended formula syntax. Supports convenient lag operators L(), differencing d(), trend(), season(), and harmonic components while preserving time series attributes.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/dynlm/dynlm.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=dynlm",
    "install": "install.packages(\"dynlm\")",
    "tags": [
      "dynamic-regression",
      "lag-operator",
      "time-series-regression",
      "distributed-lags",
      "formula-syntax"
    ],
    "best_for": "Time series regression with easy specification of lags, differences, and seasonal patterns using formula syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "time-series-analysis",
      "R-programming"
    ],
    "topic_tags": [
      "dynamic-regression",
      "lag-operators",
      "time-series-modeling",
      "econometric-packages",
      "R-package"
    ],
    "summary": "dynlm is an R package that simplifies fitting dynamic linear regression models by extending standard formula syntax with lag operators, differencing, and seasonal components. It's particularly useful for econometricians and data scientists working with time series data who need to model relationships with lagged variables. The package maintains time series properties while providing intuitive syntax for complex temporal modeling.",
    "use_cases": [
      "modeling-sales-with-lagged-advertising-effects",
      "estimating-distributed-lag-models-for-policy-impact-analysis"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "R package for dynamic regression with lags",
      "how to model lagged variables in time series regression",
      "distributed lag models in R",
      "econometric regression with lag operators"
    ]
  },
  {
    "name": "mFilter",
    "description": "Implements time series filters for extracting trend and cyclical components. Includes Hodrick-Prescott, Baxter-King, Christiano-Fitzgerald, Butterworth, and trigonometric regression filters commonly used in macroeconomics and business cycle analysis.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/mFilter/mFilter.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=mFilter",
    "install": "install.packages(\"mFilter\")",
    "tags": [
      "HP-filter",
      "Baxter-King",
      "trend-extraction",
      "business-cycles",
      "detrending"
    ],
    "best_for": "Decomposing time series into trend and cyclical components for business cycle analysis",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "python-pandas",
      "macroeconomic-indicators"
    ],
    "topic_tags": [
      "time-series-filtering",
      "business-cycle-analysis",
      "trend-extraction",
      "macroeconometrics",
      "detrending-methods"
    ],
    "summary": "mFilter is a Python package that implements classic econometric filters for decomposing time series into trend and cyclical components. It provides ready-to-use implementations of Hodrick-Prescott, Baxter-King, and other filters commonly used in macroeconomic research and business cycle analysis. Essential for economists and data scientists working with economic time series data.",
    "use_cases": [
      "Extracting business cycle fluctuations from GDP data to study economic recessions and expansions",
      "Detrending financial time series to isolate short-term volatility from long-term growth patterns"
    ],
    "audience": [
      "Early-PhD",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to implement Hodrick-Prescott filter in Python",
      "Business cycle analysis tools for economic data",
      "Time series detrending methods for macroeconomics",
      "Extract trend and cyclical components from GDP data"
    ]
  },
  {
    "name": "strucchange",
    "description": "Testing, monitoring, and dating structural changes in linear regression models. Implements the generalized fluctuation test framework (CUSUM, MOSUM, recursive estimates) and F-test framework (Chow test, supF, aveF, expF) with breakpoint estimation and confidence intervals.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/strucchange/vignettes/strucchange-intro.pdf",
    "github_url": null,
    "url": "https://cran.r-project.org/package=strucchange",
    "install": "install.packages(\"strucchange\")",
    "tags": [
      "structural-break",
      "CUSUM",
      "Chow-test",
      "breakpoints",
      "parameter-stability"
    ],
    "best_for": "Detecting and dating parameter instability and structural breaks in regression relationships, implementing Zeileis et al. (2002)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "linear-regression",
      "time-series-analysis",
      "R-programming"
    ],
    "topic_tags": [
      "structural-breaks",
      "econometric-testing",
      "time-series",
      "regression-diagnostics",
      "R-package"
    ],
    "summary": "The strucchange package provides comprehensive tools for detecting when coefficients in linear regression models change over time. It implements multiple testing frameworks including CUSUM tests and Chow tests to identify structural breaks and estimate breakpoint dates with confidence intervals. Essential for economists and data scientists analyzing policy changes, market shifts, or other regime changes in time series data.",
    "use_cases": [
      "Testing whether a marketing campaign caused a structural break in customer conversion rates",
      "Identifying when economic policy changes affected the relationship between unemployment and inflation"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to test for structural breaks in R",
      "CUSUM test for parameter stability",
      "Chow test implementation R package",
      "Detecting breakpoints in time series regression"
    ]
  },
  {
    "name": "tsDyn",
    "description": "Implements nonlinear autoregressive time series models including threshold AR (TAR/SETAR), smooth transition AR (STAR, LSTAR), and multivariate extensions (TVAR, TVECM). Enables regime-switching dynamics analysis with parametric and non-parametric approaches.",
    "category": "Time Series Econometrics",
    "docs_url": "https://github.com/MatthieuStigler/tsDyn/wiki",
    "github_url": "https://github.com/MatthieuStigler/tsDyn",
    "url": "https://cran.r-project.org/package=tsDyn",
    "install": "install.packages(\"tsDyn\")",
    "tags": [
      "nonlinear",
      "SETAR",
      "LSTAR",
      "threshold-VAR",
      "regime-switching"
    ],
    "best_for": "Modeling regime-switching dynamics and threshold cointegration in univariate and multivariate series",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "autoregressive-models",
      "R-programming"
    ],
    "topic_tags": [
      "nonlinear-time-series",
      "regime-switching",
      "threshold-models",
      "econometrics",
      "R-package"
    ],
    "summary": "tsDyn is an R package for fitting nonlinear autoregressive time series models that can switch between different regimes based on threshold values or smooth transitions. It's particularly valuable for economists and data scientists analyzing financial markets, macroeconomic indicators, or any time series where relationships change over time. The package supports both univariate models (TAR, SETAR, STAR) and multivariate extensions (TVAR, TVECM) for complex economic systems.",
    "use_cases": [
      "Modeling stock market volatility that behaves differently during bull vs bear markets",
      "Analyzing central bank policy transmission effects that vary across economic cycles"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "nonlinear time series models R package",
      "threshold autoregressive models implementation",
      "regime switching time series analysis",
      "SETAR LSTAR models R"
    ]
  },
  {
    "name": "urca",
    "description": "Implements unit root and cointegration tests commonly used in applied econometric analysis. Includes Augmented Dickey-Fuller, Phillips-Perron, KPSS, Elliott-Rothenberg-Stock, and Zivot-Andrews tests, plus Johansen's cointegration procedure for multivariate series.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/urca/urca.pdf",
    "github_url": "https://github.com/bpfaff/urca",
    "url": "https://cran.r-project.org/package=urca",
    "install": "install.packages(\"urca\")",
    "tags": [
      "unit-root",
      "cointegration",
      "ADF-test",
      "KPSS",
      "Johansen"
    ],
    "best_for": "Testing stationarity and finding cointegrating relationships in non-stationary time series, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "time-series-analysis",
      "linear-regression",
      "R-programming"
    ],
    "topic_tags": [
      "unit-root-testing",
      "cointegration",
      "time-series",
      "stationarity",
      "econometrics"
    ],
    "summary": "The urca package provides essential statistical tests for analyzing time series stationarity and long-run relationships between variables. It implements standard econometric tests like ADF and KPSS for unit roots, plus Johansen's method for testing cointegration between multiple series. These tools are fundamental for proper time series modeling and avoiding spurious regression results.",
    "use_cases": [
      "Testing whether stock prices or economic indicators are stationary before building forecasting models",
      "Analyzing long-run equilibrium relationships between related financial variables like interest rates and exchange rates"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "How to test for unit roots in time series data",
      "Cointegration testing in R",
      "ADF test vs KPSS test for stationarity",
      "Johansen cointegration test implementation"
    ]
  },
  {
    "name": "vars",
    "description": "Comprehensive package for Vector Autoregression (VAR), Structural VAR (SVAR), and Structural Vector Error Correction (SVEC) models. Provides estimation, lag selection, diagnostic testing, forecasting, Granger causality analysis, impulse response functions, and forecast error variance decomposition.",
    "category": "Time Series Econometrics",
    "docs_url": "https://cran.r-project.org/web/packages/vars/vars.pdf",
    "github_url": "https://github.com/bpfaff/vars",
    "url": "https://cran.r-project.org/package=vars",
    "install": "install.packages(\"vars\")",
    "tags": [
      "VAR",
      "SVAR",
      "impulse-response",
      "Granger-causality",
      "FEVD"
    ],
    "best_for": "Multivariate time series analysis with impulse response functions and variance decomposition, implementing Pfaff (2008)",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "multivariate-statistics",
      "R-programming",
      "time-series-analysis"
    ],
    "topic_tags": [
      "VAR-models",
      "impulse-response",
      "Granger-causality",
      "structural-econometrics",
      "R-package"
    ],
    "summary": "The vars package is R's comprehensive toolkit for Vector Autoregression analysis, enabling economists to model relationships between multiple time series variables simultaneously. It provides end-to-end functionality from model estimation and diagnostic testing to advanced techniques like impulse response functions and forecast error variance decomposition. Essential for researchers studying macroeconomic dynamics, policy transmission mechanisms, and causal relationships in multivariate time series data.",
    "use_cases": [
      "Analyzing how monetary policy shocks propagate through macroeconomic variables like GDP, inflation, and unemployment",
      "Studying dynamic relationships between tech company metrics like user growth, revenue, and engagement over time"
    ],
    "audience": [
      "Mid-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "VAR model R package",
      "impulse response functions econometrics",
      "Granger causality testing multivariate time series",
      "structural VAR analysis tools"
    ]
  },
  {
    "name": "Augurs",
    "description": "Time series forecasting and analysis for Rust with ETS, MSTL decomposition, seasonality detection, outlier detection, and Prophet-style models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://docs.augu.rs/",
    "github_url": "https://github.com/grafana/augurs",
    "url": "https://crates.io/crates/augurs",
    "install": "cargo add augurs",
    "tags": [
      "rust",
      "time series",
      "forecasting",
      "ETS",
      "MSTL"
    ],
    "best_for": "Time series forecasting and structural analysis in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "time-series-basics",
      "statistical-forecasting"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "rust-programming",
      "seasonality-detection",
      "outlier-detection",
      "exponential-smoothing"
    ],
    "summary": "Augurs is a Rust library for time series forecasting that implements classical methods like ETS (Exponential Smoothing) and MSTL decomposition alongside modern approaches like Prophet-style models. It provides a high-performance toolkit for seasonality detection, outlier identification, and production-ready forecasting workflows. The library is designed for developers who need fast, reliable time series analysis in Rust environments.",
    "use_cases": [
      "Building high-performance demand forecasting systems for e-commerce inventory management",
      "Implementing real-time anomaly detection in IoT sensor data streams"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust time series forecasting library",
      "ETS exponential smoothing rust implementation",
      "seasonality detection rust package",
      "time series outlier detection rust"
    ]
  },
  {
    "name": "MLForecast",
    "description": "Scalable time series forecasting using machine learning models (e.g., LightGBM, XGBoost) as regressors.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/mlforecast/",
    "github_url": "https://github.com/Nixtla/mlforecast",
    "url": "https://github.com/Nixtla/mlforecast",
    "install": "pip install mlforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "lightgbm-xgboost"
    ],
    "topic_tags": [
      "time-series",
      "machine-learning",
      "forecasting",
      "scalable-ml",
      "python-package"
    ],
    "summary": "MLForecast is a Python package that applies machine learning models like LightGBM and XGBoost to time series forecasting problems. It provides a scalable framework for converting time series data into supervised learning problems with engineered features. The package is designed for practitioners who want to leverage gradient boosting methods for forecasting at scale.",
    "use_cases": [
      "Forecasting daily sales across thousands of retail SKUs using historical patterns and external features",
      "Predicting server resource usage across multiple data centers using machine learning regressors"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "time series forecasting with lightgbm",
      "scalable forecasting python package",
      "machine learning for time series prediction",
      "MLForecast vs traditional forecasting methods"
    ]
  },
  {
    "name": "NeuralForecast",
    "description": "Deep learning models (N-BEATS, N-HiTS, Transformers, RNNs) for time series forecasting, built on PyTorch Lightning.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/neuralforecast/",
    "github_url": "https://github.com/Nixtla/neuralforecast",
    "url": "https://github.com/Nixtla/neuralforecast",
    "install": "pip install neuralforecast",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pytorch",
      "time-series-analysis",
      "neural-networks"
    ],
    "topic_tags": [
      "neural-forecasting",
      "deep-learning",
      "time-series",
      "pytorch-lightning"
    ],
    "summary": "NeuralForecast is a PyTorch Lightning-based library that implements state-of-the-art deep learning models for time series forecasting, including N-BEATS, N-HiTS, and Transformer architectures. It's designed for data scientists and researchers who need to apply advanced neural network methods to forecasting problems. The package provides production-ready implementations with built-in training workflows and model evaluation capabilities.",
    "use_cases": [
      "Forecasting daily sales for retail chains with complex seasonal patterns",
      "Predicting server resource usage for capacity planning in cloud infrastructure"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "deep learning time series forecasting python",
      "N-BEATS implementation pytorch",
      "neural network models for forecasting",
      "transformer time series prediction library"
    ]
  },
  {
    "name": "Prophet",
    "description": "Forecasting procedure for time series with strong seasonality and trend components, developed by Facebook.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://github.com/facebook/prophet",
    "install": "pip install prophet",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "time-series-concepts",
      "basic-regression"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "seasonality-modeling",
      "trend-analysis",
      "facebook-prophet",
      "business-forecasting"
    ],
    "summary": "Prophet is an automated forecasting tool designed for business time series with daily observations that display patterns on different time scales. It handles seasonality, holidays, and trend changes automatically with minimal parameter tuning. The package is particularly effective for forecasting problems where you have historical data with strong seasonal effects.",
    "use_cases": [
      "Forecasting daily active users or revenue for a tech product with weekly and yearly seasonality",
      "Predicting demand for e-commerce items accounting for holidays and promotional events"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "How to forecast time series with seasonality in Python",
      "Facebook Prophet tutorial for business forecasting",
      "Best tool for automated time series forecasting",
      "Prophet vs ARIMA for seasonal data"
    ]
  },
  {
    "name": "StatsForecast",
    "description": "Fast, scalable implementations of popular statistical forecasting models (ETS, ARIMA, Theta, etc.) optimized for performance.",
    "category": "Time Series Forecasting",
    "docs_url": "https://nixtla.github.io/statsforecast/",
    "github_url": "https://github.com/Nixtla/statsforecast",
    "url": "https://github.com/Nixtla/statsforecast",
    "install": "pip install statsforecast",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "time-series-analysis",
      "statistical-models"
    ],
    "topic_tags": [
      "statistical-forecasting",
      "time-series",
      "arima",
      "exponential-smoothing",
      "python-package"
    ],
    "summary": "StatsForecast is a high-performance Python package that implements classical statistical forecasting methods like ARIMA, ETS, and Theta models with optimized speed and scalability. It's designed for practitioners who need reliable, fast implementations of established forecasting techniques rather than cutting-edge ML approaches. The package is particularly useful for production forecasting workflows where interpretability and computational efficiency matter.",
    "use_cases": [
      "Forecasting daily sales across thousands of SKUs in retail",
      "Predicting server resource usage for capacity planning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fast ARIMA implementation python",
      "statistical forecasting package performance",
      "ETS exponential smoothing python library",
      "classical time series forecasting tools"
    ]
  },
  {
    "name": "fable",
    "description": "A tidyverse-native forecasting framework providing ETS, ARIMA, and other models for tidy time series (tsibble objects). Enables fitting multiple models across many time series simultaneously with a consistent formula-based interface.",
    "category": "Time Series Forecasting",
    "docs_url": "https://fable.tidyverts.org/",
    "github_url": "https://github.com/tidyverts/fable",
    "url": "https://cran.r-project.org/package=fable",
    "install": "install.packages(\"fable\")",
    "tags": [
      "time-series",
      "tidyverse",
      "ARIMA",
      "ETS",
      "tsibble"
    ],
    "best_for": "Tidy forecasting workflows handling many related time series with tidyverse-consistent syntax",
    "language": "R",
    "difficulty": "intermediate",
    "prerequisites": [
      "R-dplyr",
      "time-series-decomposition",
      "tsibble-objects"
    ],
    "topic_tags": [
      "forecasting",
      "tidyverse",
      "time-series",
      "R-package",
      "ARIMA"
    ],
    "summary": "fable is an R package that brings forecasting models like ARIMA and ETS into the tidyverse ecosystem, working seamlessly with tsibble time series objects. It allows analysts to fit multiple forecasting models across hundreds of time series simultaneously using familiar dplyr-style syntax. The package is particularly valuable for scaling forecasting workflows while maintaining code readability and reproducibility.",
    "use_cases": [
      "Forecasting demand across thousands of product SKUs in retail analytics",
      "Generating revenue predictions for multiple business units or geographic regions"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "tidyverse forecasting package R",
      "how to forecast multiple time series with dplyr",
      "fable vs forecast package comparison",
      "ARIMA modeling with tsibble objects"
    ]
  },
  {
    "name": "forecast",
    "description": "The foundational R package for univariate time series forecasting. Provides methods for exponential smoothing via state space models (ETS), automatic ARIMA modeling with auto.arima(), TBATS for complex seasonality, and comprehensive model evaluation tools.",
    "category": "Time Series Forecasting",
    "docs_url": "https://pkg.robjhyndman.com/forecast/",
    "github_url": "https://github.com/robjhyndman/forecast",
    "url": "https://cran.r-project.org/package=forecast",
    "install": "install.packages(\"forecast\")",
    "tags": [
      "time-series",
      "ARIMA",
      "exponential-smoothing",
      "ETS",
      "auto.arima"
    ],
    "best_for": "Classical statistical forecasting for univariate time series with automatic model selection, implementing Hyndman & Khandakar (2008)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "basic-R",
      "univariate-time-series",
      "descriptive-statistics"
    ],
    "topic_tags": [
      "time-series",
      "ARIMA",
      "exponential-smoothing",
      "forecasting",
      "R-package"
    ],
    "summary": "The forecast package is R's go-to tool for univariate time series forecasting, offering automated model selection and forecasting methods. It's widely used by analysts and researchers for its user-friendly interface to complex forecasting techniques like ARIMA and exponential smoothing. The package excels at handling seasonal patterns and provides comprehensive diagnostic tools for model evaluation.",
    "use_cases": [
      "forecasting monthly sales revenue for business planning",
      "predicting daily website traffic for capacity planning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "R package for automatic ARIMA modeling",
      "how to forecast time series with seasonality in R",
      "best R tools for exponential smoothing",
      "auto.arima function tutorial"
    ]
  },
  {
    "name": "pmdarima",
    "description": "ARIMA modeling with automatic parameter selection (auto-ARIMA), similar to R's `forecast::auto.arima`.",
    "category": "Time Series Forecasting",
    "docs_url": "https://alkaline-ml.com/pmdarima/",
    "github_url": "https://github.com/alkaline-ml/pmdarima",
    "url": "https://github.com/alkaline-ml/pmdarima",
    "install": "pip install pmdarima",
    "tags": [
      "forecasting",
      "time series"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "time-series-basics",
      "statsmodels"
    ],
    "topic_tags": [
      "auto-arima",
      "time-series-forecasting",
      "parameter-tuning",
      "python-package"
    ],
    "summary": "pmdarima is a Python package that automatically selects optimal ARIMA model parameters for time series forecasting, eliminating manual hyperparameter tuning. It provides a user-friendly interface similar to R's auto.arima function, making ARIMA modeling accessible to practitioners without deep statistical expertise. The package handles data preprocessing, model selection, and forecast generation in a streamlined workflow.",
    "use_cases": [
      "Forecasting monthly sales revenue with automatic model selection",
      "Predicting daily website traffic without manual ARIMA parameter tuning"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "automatic ARIMA parameter selection python",
      "pmdarima vs statsmodels ARIMA",
      "auto arima forecasting package python",
      "best python library for ARIMA time series"
    ]
  },
  {
    "name": "prophet",
    "description": "Automatic forecasting procedure based on an additive decomposable model with non-linear trends, yearly/weekly/daily seasonality, and holiday effects. Robust to missing data, trend shifts, and outliers; designed for business time series with strong seasonal patterns.",
    "category": "Time Series Forecasting",
    "docs_url": "https://facebook.github.io/prophet/",
    "github_url": "https://github.com/facebook/prophet",
    "url": "https://cran.r-project.org/package=prophet",
    "install": "install.packages(\"prophet\")",
    "tags": [
      "time-series",
      "Facebook",
      "decomposable-model",
      "seasonality",
      "holidays"
    ],
    "best_for": "Business time series forecasting with multiple seasonalities, holiday effects, and automated tunable forecasts, implementing Taylor & Letham (2018)",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "time-series-basics",
      "matplotlib"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "business-analytics",
      "seasonality",
      "trend-analysis",
      "automated-forecasting"
    ],
    "summary": "Prophet is Facebook's open-source forecasting tool that automatically generates predictions for business time series data with minimal configuration. It handles seasonal patterns, holidays, and missing data out-of-the-box, making it accessible to analysts without deep forecasting expertise. The tool excels at business metrics like daily active users, revenue, or inventory demand that show strong seasonal patterns.",
    "use_cases": [
      "Forecasting daily website traffic for capacity planning",
      "Predicting monthly sales revenue accounting for seasonal trends and holidays"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "easy time series forecasting tool for business data",
      "how to forecast sales with seasonality and holidays",
      "Facebook prophet vs traditional forecasting methods",
      "automated forecasting for daily active users"
    ]
  },
  {
    "name": "sktime",
    "description": "Unified framework for various time series tasks, including forecasting with classical, ML, and deep learning models.",
    "category": "Time Series Forecasting",
    "docs_url": "https://www.sktime.net/en/latest/",
    "github_url": "https://github.com/sktime/sktime",
    "url": "https://github.com/sktime/sktime",
    "install": "pip install sktime",
    "tags": [
      "forecasting",
      "time series",
      "machine learning"
    ],
    "best_for": "Prediction, demand forecasting, trend analysis",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-pandas",
      "scikit-learn",
      "time-series-decomposition"
    ],
    "topic_tags": [
      "time-series-forecasting",
      "scikit-learn-compatible",
      "python-package",
      "machine-learning",
      "classical-forecasting"
    ],
    "summary": "sktime is a scikit-learn compatible Python package that provides a unified interface for time series forecasting, classification, and regression. It combines classical statistical methods (ARIMA, exponential smoothing) with modern machine learning and deep learning approaches in a single framework. The package is designed to make time series analysis more accessible while maintaining the flexibility needed for advanced modeling.",
    "use_cases": [
      "Forecasting product demand across multiple SKUs with different seasonal patterns",
      "Predicting server resource usage to optimize cloud infrastructure scaling"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "python package for time series forecasting",
      "scikit-learn compatible time series library",
      "unified framework for classical and ML forecasting",
      "sktime vs prophet vs statsmodels for forecasting"
    ]
  },
  {
    "name": "CatBoost",
    "description": "Gradient boosting library excelling with categorical features (minimal preprocessing needed). Robust against overfitting.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://catboost.ai/docs/",
    "github_url": "https://github.com/catboost/catboost",
    "url": "https://github.com/catboost/catboost",
    "install": "pip install catboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "gradient-boosting",
      "cross-validation"
    ],
    "topic_tags": [
      "gradient-boosting",
      "categorical-features",
      "ensemble-methods",
      "overfitting-prevention"
    ],
    "summary": "CatBoost is a gradient boosting library that handles categorical features natively without requiring extensive preprocessing like one-hot encoding. It's particularly popular among data scientists for its robustness against overfitting and competitive performance in machine learning competitions and production systems.",
    "use_cases": [
      "E-commerce recommendation systems with mixed categorical and numerical user/product features",
      "Customer churn prediction using demographic categories and behavioral metrics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "gradient boosting library for categorical data",
      "CatBoost vs XGBoost for mixed data types",
      "best ensemble method for categorical features",
      "overfitting resistant boosting algorithm"
    ]
  },
  {
    "name": "LightGBM",
    "description": "Fast, distributed gradient boosting (also supports RF). Known for speed, low memory usage, and handling large datasets.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://lightgbm.readthedocs.io/",
    "github_url": "https://github.com/microsoft/LightGBM",
    "url": "https://github.com/microsoft/LightGBM",
    "install": "pip install lightgbm",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-scikit-learn",
      "gradient-boosting",
      "pandas-dataframes"
    ],
    "topic_tags": [
      "gradient-boosting",
      "ensemble-methods",
      "high-performance-ml",
      "large-datasets",
      "python-package"
    ],
    "summary": "LightGBM is a high-performance gradient boosting framework that excels at handling large datasets with minimal memory usage. It's widely adopted by data scientists and ML engineers for its speed and efficiency compared to other boosting methods like XGBoost. The package supports both gradient boosting and random forests, making it versatile for various prediction tasks.",
    "use_cases": [
      "Predicting user conversion rates on large e-commerce datasets with millions of records",
      "Building real-time recommendation systems that need fast inference on high-dimensional feature sets"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "fast gradient boosting for large datasets",
      "LightGBM vs XGBoost performance comparison",
      "memory efficient machine learning python",
      "best gradient boosting library for production"
    ]
  },
  {
    "name": "Linfa",
    "description": "Rust ML toolkit inspired by scikit-learn with GLMs, clustering (K-Means), PCA, SVM, and regularization (Lasso/Ridge).",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://rust-ml.github.io/linfa/",
    "github_url": "https://github.com/rust-ml/linfa",
    "url": "https://crates.io/crates/linfa",
    "install": "cargo add linfa",
    "tags": [
      "rust",
      "machine learning",
      "clustering",
      "PCA",
      "SVM"
    ],
    "best_for": "scikit-learn style ML in Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "scikit-learn",
      "linear-regression"
    ],
    "topic_tags": [
      "rust-ml",
      "scikit-learn-alternative",
      "systems-programming",
      "performance-optimization",
      "ml-toolkit"
    ],
    "summary": "Linfa is a Rust-based machine learning toolkit that provides scikit-learn-inspired APIs for common ML algorithms including GLMs, clustering, and dimensionality reduction. It offers memory-safe, high-performance implementations suitable for production systems where speed and reliability are critical. The library is ideal for developers building ML pipelines in Rust or those seeking alternatives to Python-based ML stacks.",
    "use_cases": [
      "Building high-performance ML microservices in Rust where Python's GIL creates bottlenecks",
      "Implementing ML algorithms in embedded systems or edge computing environments requiring memory safety"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust machine learning library like scikit-learn",
      "high performance ML toolkit for systems programming",
      "linfa rust clustering PCA implementation",
      "scikit-learn alternative for production systems"
    ]
  },
  {
    "name": "NGBoost",
    "description": "Extends gradient boosting to probabilistic prediction, providing uncertainty estimates alongside point predictions. Built on scikit-learn.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://stanfordmlgroup.github.io/ngboost/",
    "github_url": "https://github.com/stanfordmlgroup/ngboost",
    "url": "https://github.com/stanfordmlgroup/ngboost",
    "install": "pip install ngboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "gradient-boosting",
      "probability-distributions"
    ],
    "topic_tags": [
      "uncertainty-quantification",
      "probabilistic-prediction",
      "gradient-boosting",
      "ensemble-methods"
    ],
    "summary": "NGBoost extends traditional gradient boosting to output full probability distributions instead of just point predictions, enabling uncertainty quantification in machine learning models. It's particularly valuable for data scientists who need to understand prediction confidence and risk assessment. Built on scikit-learn, it maintains familiar APIs while adding probabilistic capabilities.",
    "use_cases": [
      "Predicting customer lifetime value with confidence intervals for business planning",
      "Medical diagnosis predictions where uncertainty bounds are critical for clinical decisions"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "gradient boosting with uncertainty estimates",
      "probabilistic prediction python package",
      "how to get confidence intervals from tree models",
      "NGBoost vs regular gradient boosting uncertainty"
    ]
  },
  {
    "name": "Scikit-learn Ens.",
    "description": "(`RandomForestClassifier`/`Regressor`) Widely-used, versatile implementation of Random Forests. Easy API and parallel processing support.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://scikit-learn.org/stable/modules/ensemble.html#random-forests",
    "github_url": "https://github.com/scikit-learn/scikit-learn",
    "url": "https://github.com/scikit-learn/scikit-learn",
    "install": "pip install scikit-learn",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "beginner",
    "prerequisites": [
      "python-pandas",
      "scikit-learn-basics",
      "train-test-split"
    ],
    "topic_tags": [
      "random-forest",
      "ensemble-methods",
      "classification",
      "regression",
      "feature-importance"
    ],
    "summary": "Scikit-learn's Random Forest implementation provides an easy-to-use interface for building ensemble models that combine multiple decision trees. It's one of the most popular machine learning algorithms for both classification and regression tasks, offering built-in feature importance scores and robust performance across diverse datasets. The implementation includes automatic parallelization and handles mixed data types well.",
    "use_cases": [
      "Predicting customer churn using mixed categorical and numerical features",
      "Estimating house prices with feature importance ranking for real estate analysis"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "random forest python implementation",
      "scikit learn ensemble methods",
      "how to use RandomForestClassifier",
      "best random forest package for beginners"
    ]
  },
  {
    "name": "SmartCore",
    "description": "Rust ML library with regression, classification, clustering, matrix decomposition (SVD, PCA), and model selection tools.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rs/smartcore",
    "github_url": "https://github.com/smartcorelib/smartcore",
    "url": "https://crates.io/crates/smartcore",
    "install": "cargo add smartcore",
    "tags": [
      "rust",
      "machine learning",
      "regression",
      "classification"
    ],
    "best_for": "Comprehensive ML algorithms in pure Rust",
    "language": "Rust",
    "difficulty": "intermediate",
    "prerequisites": [
      "rust-programming",
      "linear-algebra",
      "supervised-learning"
    ],
    "topic_tags": [
      "rust-ml",
      "systems-programming",
      "performance-optimization",
      "cross-platform-ml",
      "native-compilation"
    ],
    "summary": "SmartCore is a comprehensive machine learning library built in Rust, offering high-performance implementations of regression, classification, clustering, and dimensionality reduction algorithms. It's designed for developers who need fast, memory-safe ML computations with native performance. The library provides a full suite of model selection and evaluation tools alongside core algorithms.",
    "use_cases": [
      "Building high-performance ML pipelines in production systems where speed and memory safety are critical",
      "Developing cross-platform ML applications that need to compile to native binaries without runtime dependencies"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "rust machine learning library with PCA and clustering",
      "fast ML library alternative to scikit-learn in rust",
      "high performance regression and classification rust",
      "memory safe ML algorithms for production systems"
    ]
  },
  {
    "name": "XGBoost",
    "description": "High-performance, optimized gradient boosting library (also supports RF). Known for speed, efficiency, and winning competitions.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://xgboost.readthedocs.io/",
    "github_url": "https://github.com/dmlc/xgboost",
    "url": "https://github.com/dmlc/xgboost",
    "install": "pip install xgboost",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-sklearn",
      "decision-trees",
      "cross-validation"
    ],
    "topic_tags": [
      "gradient-boosting",
      "ensemble-methods",
      "competition-ml",
      "tree-models",
      "feature-importance"
    ],
    "summary": "XGBoost is a highly optimized gradient boosting framework that combines multiple weak learners (typically decision trees) to create powerful predictive models. It's widely adopted in industry and competitive machine learning for its exceptional performance, built-in regularization, and efficient handling of missing values. The library provides both regression and classification capabilities with extensive hyperparameter tuning options.",
    "use_cases": [
      "Predicting customer churn with mixed categorical and numerical features",
      "Building recommendation system ranking models for e-commerce platforms"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "best gradient boosting library for tabular data",
      "XGBoost vs random forest performance comparison",
      "how to tune XGBoost hyperparameters",
      "ensemble methods for structured prediction problems"
    ]
  },
  {
    "name": "cuML (RAPIDS)",
    "description": "GPU-accelerated implementation of Random Forests for significant speedups on large datasets. Scikit-learn compatible API.",
    "category": "Tree & Ensemble Methods for Prediction",
    "docs_url": "https://docs.rapids.ai/api/cuml/stable/",
    "github_url": "https://github.com/rapidsai/cuml",
    "url": "https://github.com/rapidsai/cuml",
    "install": "conda install ... (See RAPIDS docs)",
    "tags": [
      "machine learning",
      "prediction"
    ],
    "best_for": "Random forests, gradient boosting, prediction tasks",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "scikit-learn",
      "python-pandas",
      "CUDA-programming"
    ],
    "topic_tags": [
      "random-forest",
      "GPU-acceleration",
      "scikit-learn-compatible",
      "large-scale-ml",
      "RAPIDS"
    ],
    "summary": "cuML is NVIDIA's GPU-accelerated machine learning library that provides scikit-learn compatible implementations of popular algorithms like Random Forests. It delivers significant performance speedups on large datasets by leveraging GPU parallelization while maintaining familiar APIs. Data scientists can drop it in as a replacement for scikit-learn models when working with datasets that benefit from GPU acceleration.",
    "use_cases": [
      "Training Random Forest models on millions of e-commerce transactions for fraud detection",
      "Running large-scale feature selection experiments on genomics datasets with thousands of features"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "GPU accelerated random forest python",
      "scikit-learn compatible GPU machine learning",
      "RAPIDS cuML vs scikit-learn performance",
      "how to speed up random forest training large datasets"
    ]
  },
  {
    "name": "CausalLift",
    "description": "Uplift modeling for observational (non-RCT) data using inverse probability weighting.",
    "category": "Uplift Modeling",
    "docs_url": "https://causallift.readthedocs.io/",
    "github_url": "https://github.com/Minyus/causallift",
    "url": "https://github.com/Minyus/causallift",
    "install": "pip install causallift",
    "tags": [
      "uplift modeling",
      "observational data",
      "IPW"
    ],
    "best_for": "Uplift from observational data with IPW",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "propensity-score-matching",
      "python-scikit-learn",
      "randomized-controlled-trials"
    ],
    "topic_tags": [
      "uplift-modeling",
      "causal-inference",
      "treatment-effects",
      "observational-data",
      "python-package"
    ],
    "summary": "CausalLift is a Python package for uplift modeling that estimates treatment effects from observational data using inverse probability weighting. It helps data scientists identify which individuals are most likely to respond positively to treatments when randomized experiments aren't feasible. The package is particularly useful for marketing campaigns and policy interventions where you need to estimate causal effects from non-experimental data.",
    "use_cases": [
      "Estimating which customers would increase purchases if targeted with a marketing campaign using historical transaction data",
      "Evaluating the effectiveness of a new feature rollout on user engagement when A/B testing wasn't initially implemented"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling python package observational data",
      "how to estimate treatment effects without randomized experiment",
      "inverse probability weighting for causal inference",
      "CausalLift tutorial for marketing campaigns"
    ]
  },
  {
    "name": "UpliftML",
    "description": "Booking.com's enterprise uplift modeling via PySpark and H2O. Six meta-learners plus Uplift Random Forest with ROI-constrained optimization.",
    "category": "Uplift Modeling",
    "docs_url": null,
    "github_url": "https://github.com/bookingcom/upliftml",
    "url": "https://github.com/bookingcom/upliftml",
    "install": "pip install upliftml",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Enterprise-scale uplift with ROI optimization",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "pyspark-ml",
      "causal-inference",
      "scikit-learn"
    ],
    "topic_tags": [
      "uplift-modeling",
      "treatment-effects",
      "marketing-optimization",
      "causal-ml",
      "pyspark"
    ],
    "summary": "UpliftML is Booking.com's production-ready uplift modeling library that combines six meta-learners with Uplift Random Forest for measuring treatment effects. Built on PySpark and H2O for enterprise scale, it includes ROI-constrained optimization to maximize business impact. Data scientists use it to optimize marketing campaigns and personalization strategies by identifying which customers will respond positively to treatments.",
    "use_cases": [
      "Optimizing email marketing campaigns by identifying customers most likely to convert when contacted",
      "Personalizing discount offers by finding users who need incentives versus those who would purchase anyway"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling library for marketing campaigns",
      "pyspark causal inference treatment effects",
      "booking.com uplift ml implementation",
      "roi constrained uplift optimization tools"
    ]
  },
  {
    "name": "pylift",
    "description": "Wayfair's uplift modeling wrapping sklearn for speed with rigorous Qini curve evaluation.",
    "category": "Uplift Modeling",
    "docs_url": "https://pylift.readthedocs.io/",
    "github_url": "https://github.com/wayfair/pylift",
    "url": "https://github.com/wayfair/pylift",
    "install": "pip install pylift",
    "tags": [
      "uplift modeling",
      "treatment effects",
      "marketing"
    ],
    "best_for": "Fast uplift with Qini curve evaluation",
    "language": "Python",
    "difficulty": "intermediate",
    "prerequisites": [
      "python-sklearn",
      "causal-inference",
      "A-B-testing"
    ],
    "topic_tags": [
      "uplift-modeling",
      "treatment-effects",
      "marketing-attribution",
      "qini-curves",
      "causal-ml"
    ],
    "summary": "pylift is Wayfair's Python package that extends scikit-learn for uplift modeling, enabling practitioners to identify which customers will respond positively to treatments. It provides fast implementations with rigorous evaluation through Qini curves, making it practical for marketing and product experimentation at scale.",
    "use_cases": [
      "Identifying which customers to target with promotional campaigns based on predicted treatment response",
      "Optimizing product recommendation strategies by modeling individual-level causal effects"
    ],
    "audience": [
      "Mid-DS",
      "Senior-DS"
    ],
    "synthetic_questions": [
      "uplift modeling python package",
      "how to measure treatment effects in marketing campaigns",
      "qini curve evaluation for causal inference",
      "wayfair pylift sklearn treatment effects"
    ]
  },
  {
    "name": "cowplot",
    "description": "Publication-ready ggplot2 themes and plot arrangement utilities. Provides clean themes, plot annotations, and functions for combining plots with shared axes.",
    "category": "Visualization",
    "docs_url": "https://wilkelab.org/cowplot/",
    "github_url": "https://github.com/wilkelab/cowplot",
    "url": "https://cran.r-project.org/package=cowplot",
    "install": "install.packages(\"cowplot\")",
    "tags": [
      "ggplot2",
      "themes",
      "publication-ready",
      "plot-arrangement",
      "annotations"
    ],
    "best_for": "Publication-ready ggplot2 themes and multi-plot arrangements with annotations",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "R-ggplot2",
      "data-visualization",
      "R-programming"
    ],
    "topic_tags": [
      "data-visualization",
      "publication-graphics",
      "plot-themes",
      "R-package"
    ],
    "summary": "Cowplot is an R package that extends ggplot2 with publication-ready themes and utilities for combining multiple plots. It provides clean, minimalist themes and functions to arrange plots with shared legends and aligned axes. Popular among researchers and analysts who need to create professional figures for papers, reports, and presentations.",
    "use_cases": [
      "Creating multi-panel figures for academic papers with consistent formatting",
      "Building dashboard-style layouts combining different chart types with shared legends"
    ],
    "audience": [
      "Junior-DS",
      "Early-PhD"
    ],
    "synthetic_questions": [
      "how to make publication ready plots in R",
      "ggplot2 themes for academic papers",
      "combine multiple ggplot2 plots into one figure",
      "cowplot tutorial for clean data visualizations"
    ]
  },
  {
    "name": "patchwork",
    "description": "Compose multiple ggplot2 plots into publication-ready multi-panel figures. Uses intuitive operators (+, |, /) for arrangement with automatic alignment and shared legends.",
    "category": "Visualization",
    "docs_url": "https://patchwork.data-imaginist.com/",
    "github_url": "https://github.com/thomasp85/patchwork",
    "url": "https://cran.r-project.org/package=patchwork",
    "install": "install.packages(\"patchwork\")",
    "tags": [
      "ggplot2",
      "multi-panel",
      "figure-composition",
      "visualization",
      "publication-ready"
    ],
    "best_for": "Composing multi-panel ggplot2 figures with intuitive + and | operators",
    "language": "R",
    "difficulty": "beginner",
    "prerequisites": [
      "ggplot2-basics",
      "R-programming"
    ],
    "topic_tags": [
      "data-visualization",
      "publication-figures",
      "multi-panel-plots",
      "ggplot2-extension"
    ],
    "summary": "Patchwork is an R package that simplifies combining multiple ggplot2 plots into cohesive multi-panel figures using intuitive operators. It automatically handles plot alignment, shared legends, and spacing, making it easy to create publication-quality composite visualizations. The package is essential for researchers and analysts who need to present multiple related plots in a single figure.",
    "use_cases": [
      "Creating multi-panel figures for academic papers showing results across different conditions or datasets",
      "Building dashboards with multiple related visualizations that need consistent alignment and shared aesthetics"
    ],
    "audience": [
      "Junior-DS",
      "Mid-DS"
    ],
    "synthetic_questions": [
      "how to combine multiple ggplot2 plots into one figure",
      "R package for multi-panel publication figures",
      "patchwork ggplot2 plot composition",
      "create subplot layouts in R ggplot2"
    ]
  },
  {
    "name": "Ax (Meta Adaptive Experimentation)",
    "description": "Meta's open-source platform for adaptive experimentation. Bayesian optimization, multi-objective optimization, and automated experiment design. Built on BoTorch for AI-assisted experimentation.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://ax.dev/docs/why-ax",
    "github_url": "https://github.com/facebook/Ax",
    "url": "https://ax.dev",
    "install": "pip install ax-platform",
    "tags": [
      "adaptive experimentation",
      "Bayesian optimization",
      "multi-objective"
    ],
    "best_for": "Adaptive experimentation, Bayesian optimization, automated experiment design",
    "language": "Python"
  },
  {
    "name": "Confidence (Spotify)",
    "description": "Spotify's experimentation platform for feature flagging and A/B testing. SDK for controlled rollouts with built-in statistical analysis.",
    "category": "Adaptive Experimentation & Bandits",
    "docs_url": "https://confidence.spotify.com/docs",
    "github_url": null,
    "url": "https://confidence.spotify.com",
    "install": "pip install spotify-confidence",
    "tags": [
      "feature flags",
      "A/B testing",
      "experimentation platform"
    ],
    "best_for": "Feature flagging, controlled rollouts, A/B testing infrastructure",
    "language": "Python"
  }
]
